----- admin_guide/admin-toolkit.txt -----
:page-version: 6.1
:page-permalink: 9c6da7
:page-title: Run the admin toolkit
:page-order: 130

The admin toolkit is an always-on tool deployed alongside Domino in the Kubernetes cluster that allows Domino administrators to run a system scan on the Domino deployment to do the following:

* Check for unpatched known issues
* Validate the health of core services
* Get information to assist troubleshooting, run remediation actions, or provide context about issues affecting the deployment
* Provide the toolkit report to Domino Support as part of an incident

The toolkit has a front-end web interface provided at `\https://<your-domino-url>/toolkit/` where you can view scan reports and initiate new scans.
There is also a CLI tool to initiate scans and help manage the toolkit deployment.

You can run scans as many times as necessary because the checks performed by the toolkit are only reading information and not making any modifications.
You can run a scan during any of these stages:

* After a new Domino install
* At the start of an incident
* In the midst of an incident
* After incident resolution to verify health
* Before and after a Domino upgrade
* Before and after a Domino migration

You can find details about the specific tests that are included in the scan in the https://docs.toolkit.re.domino.tech/[Admin toolkit documentation^].

You can find check runbooks at our https://tickets.dominodatalab.com/hc/en-us/sections/13015804020244-Admin-Toolkit[Support site knowledge base^]  (requires sign-in).

== Access Admin Toolkit

Admin Toolkit is available at the URL `\https://<your-domino-domain>/toolkit/`
 or through the Domino Admin UI under *Advanced* > *Diagnostics*.

Out of the box, Admin Toolkit should be configured with SSO so that users will automatically be logged in to the Toolkit UI if they are logged in to Domino.
If they are not currently logged in to Domino or their session has expired, then they will see the Admin Toolkit login screen.

Clicking on `Continue with Domino SSO` will take the user to the normal Domino login screen where they can log in using their Domino credentials as normal. Once logged in, Domino will redirect back to the Toolkit UI.

All Domino SysAdmin users are automatically granted the Toolkit Admin role.

== Login when SSO is unavailable

When SSO is not working (for example, Keycloak is unavailable for some reason), then there is a backup method to access Toolkit with Admin privileges.

=== Get the Admin Toolkit password

When the toolkit is installed, the installer generates a random password that does not change for the life of the toolkit deployment.
If you installed the toolkit on the command line using the CLI, this password (along with the ingress URL) is displayed at the end of the installation process.

If you forget the password,
or if toolkit was installed by the Domino installer,
you can retrieve the password using this command:

[source,console]
----
./toolkit.sh get-password
----

Navigate to the Toolkit login page and do the following:

. Enter the login you retrieved above.
. Enter the password you retrieved above.
. Click *Login*.

=== Get the Admin Toolkit password without the toolkit.sh CLI

The Admin Toolkit password is stored in a secret called `domino-admin-toolkit-http` in the `domino-platform` namespace.
You can also retrieve the password with `kubectl`:

[source,shell]
----
kubectl get secret -n domino-platform domino-admin-toolkit-http -o jsonpath='{.data.webui-password}' | base64 -d; echo
----

The default username is `admin-toolkit`.

== Run a system scan

You can run a system scan using the Admin Toolkit web interface or CLI.

=== Run a scan using the web interface

. Navigate to *`\https://<your-domino-url>/toolkit/`* and log in.
. Click the *Run New Report* button at the bottom of the page:
+
image::/images/6.0/toolkit_ui.png[alt="Toolkit user interface", width=1200, role=noshadow]
+
When the scan has finished, the web interface displays a link to the report.

=== Run a scan using the CLI

From your bastion host or laptop where you have `toolkit.sh` saved, run this command:

[source,console]
----
./toolkit.sh test
----

This starts a scan, displaying the progress to the console. The completed scan is available as HTML in the web interface.

=== Run a scan on an air-gapped deployment

If your Domino deployment blocks AWS access, run the following command to output the report only in the console:

[source,console]
----
./toolkit.sh pytest
----

This runs the report and displays it to the console with no colorizing of the output, making it easier to redirect the output to a file or copy and paste from the console into a text file that can be sent to Domino.
Contact Domino support at support@dominodatalab.com to pre-arrange a secure way to send the text file to Domino.

=== Troubleshooting

Attempting to run `./toolkit.sh` results in the following error:
----
/usr/local/bin/python: Error while finding module specification for 'domino_admin_toolkit.test_runner' (ModuleNotFoundError: No module named 'domino_admin_toolkit')
error: Internal error occurred: error executing command in container: Internal error occurred: error executing command in container: command terminated with exit code 1
----

This indicates there is still a v1 toolkit installation present. Run `./toolkit.sh uninstall; ./toolkit.sh install` to remove it and install the latest version.

== View the reports

[[tr4]] Use the Admin Toolkit's web interface to review the reports in HTML format.
The web interface displays a list of the 10 most recent reports that have been generated by the toolkit.
Click the link of the report you wish to view.

image::/images/6.0/toolkit_web_report.png[alt="Toolkit web report", width=1200, role=noshadow]

NOTE: Reports are stored on the toolkit's pod storage and are not persisted across pod restarts. With kubectl access you can download all stored reports using: `kubectl cp -n domino-platform -c domino-admin-toolkit <pod_name>:/app/domino_admin_toolkit/ui/templates/reports/ ./reports/` replacing `<pod_name>` with the complete name of the toolkit pod.

== Send reports to Domino

Regardless of whether a scan runs from the web interface or CLI, the toolkit can automatically and securely upload the report to a secure centralized S3 bucket owned by Domino within Domino’s internal AWS account. Access to the reports is restricted to authorized Domino engineers through a secure single sign-on (SSO) system with granted permissions.

This behavior is on by default, but can be disabled by either selecting the option in the web interface to opt out of uploads, applying a `ConfigMap`, or when using the command line by running with the `--local-only` option:

[source,console]
----
./toolkit.sh test --local-only
----
NOTE: The `--local-only` option applies to that run only and does not change the default behavior.

If you have opted out of uploads, you can still upload a report to Domino using the CLI:

[source,console]
----
./toolkit.sh test --upload-report
----

There are many benefits to uploading reports to Domino:

* Quicker resolution of support tickets.
* Validation of deployment configuration.
* Receive more proactive, rather than reactive, support.

Domino can also use the data collected from multiple customers to better understand how customer-specific configuration and infrastructure affect the overall performance and stability of the Domino platform so that we can deliver a more robust and performant product with each release.

IMPORTANT: Admin Toolkit reports do contain potentially sensitive information such as IP addresses, hostnames, partial log files and usernames.
_Passwords are not included_.
If you have concerns about any of the information collected, contact us at support@dominodatalab.com or through your CSM or TAM to discuss your concerns.
Since the always-on Admin Toolkit can be updated independently of the Domino version of your deployment, addressing such concerns can be quick.

== Enable/disable reports to be sent to Domino

_By default, the functionality to have reports uploaded to Domino is enabled_.

Each newly generated toolkit report is securely sent to Domino Support and Engineering.
This allows Domino engineers to gain access to the reports faster during an outage and get historical context to previous outages.
You can opt out of this service (or opt in again) via the web interface or through an applied Kubernetes `ConfigMap`.

To opt out of sending reports to Domino via the web interface, navigate to the *Settings* tab and then click the *Opt Out of sending reports to Domino* button. To opt back in, click the *Opt Into sending reports to Domino* button.

To opt out of sending reports to Domino through an applied Kubernetes `ConfigMap`, follow the instructions below.

NOTE: These reports only collect deployment and service verification data and do not contain any sensitive data.

===  ConfigMap

A `ConfigMap` is generated by default. Here is an example of the default `ConfigMap`:

[source,console]
----
apiVersion: v1
name: domino-admin-toolkit-config
data:
  crontab_entry: '* * * * *'
  send_to_domino: "True"
kind: ConfigMap
----

To opt out of sending reports to Domino, change the `send_to_domino` value to `"False"`.

== Scheduled reports

Scheduled reporting lets you generate Domino health reports at regular intervals. Having a "healthy state" report handy provides a comparative baseline during system disruptions. This can be invaluable in swiftly identifying system anomalies, thus accelerating the issue resolution process.

=== Set up scheduled reporting

Follow the steps below to set up scheduled reporting:

. Open the toolkit web interface and navigate to the *Settings* tab.
. Under the *Schedule Admin Toolkit* section, specify the frequency and timing for the reports as per your requirement.
This is accepted in cron formatting only.
. Click *Submit*.

image::/images/6.0/toolkit_scheduled_reports.png[alt="Toolkit scheduled reports", width=1200, role=noshadow]

Now the toolkit generates and stores system health reports according to the schedule you've set. These reports can be accessed and reviewed anytime, providing you with a consistent and regular overview of your system's health.

[IMPORTANT]
====
By default:

* The schedule is set to once a day.
* A maximum number of 10 reports are saved at any one time.
====

=== Utilize scheduled reports during system disruptions

In case of a system disruption or outage:

. Access the most recent "healthy state" report from the stored scheduled reports.
. Generate an immediate system health report (if possible).
. Compare the two reports to identify any deviations or unusual activities.

Sharing this information with the Domino support team can expedite the troubleshooting process and guide us toward the root cause of the problem more efficiently.

By leveraging scheduled reporting, you can manage system health more effectively, contributing to smoother, more reliable operations in your Domino environment.

== Understand and resolve failures

The Admin Toolkit documentation provides comprehensive information about the checks, their meanings, and any details on resolving issues. Use the toolkit and its documentation to quickly identify and resolve issues, and ensure optimal performance of the Domino platform. You can access the documentation in two ways:

* Publicly-accessible online documentation:
** https://docs.toolkit.re.domino.tech/ (always updated to the latest version of the toolkit).
** https://tickets.dominodatalab.com/hc/en-us/sections/13015804020244-Admin-Toolkit[Support site knowledge base^] (requires sign-in).

* Documentation included within the Domino deployment: `\https://<your-domino-url>/toolkit/docs/` (specific to the deployed version of the toolkit).

----- admin_guide/architecture/domino-on-aks.txt -----
:page-version: 6.1
:page-title: Set up Domino on AKS
:page-permalink: 7d0b3e
:page-order: 20
// /! NOTE: Please do not unduly edit -- these pages are needed for cloud certification.

[[tr1]]
Domino can run on a Kubernetes cluster provided by the https://azure.microsoft.com/en-us/services/kubernetes-service/[Azure Kubernetes Service (AKS)^]:

image::/images/diagrams/azure-architecture.png[alt="A map of the Azure architecture to set up a Domino deployment", width=1000, role=noshadow]
// Original diagram in Lucid: https://lucid.app/lucidchart/a578cdea-1d3b-4319-becb-fd5e40a7c99d/edit?invitationId=inv_4d333cfd-bf6a-418b-b38f-d01a255ecff4

*Runtime platform (https://azure.microsoft.com/en-us/products/kubernetes-service[Azure Kubernetes Service^]):* +
*A -* AKS cluster deployed in three https://learn.microsoft.com/en-us/azure/reliability/availability-zones-overview[Availability Zones^].
The cluster must match Domino's compatible Kubernetes versions.

*Nodes/instances (https://learn.microsoft.com/en-us/azure/aks/create-node-pools[AKS node pools^]):* +
*B -* System Pool: Scales 1-3 per zone, Standard_DS4_v2. +
*C -* Platform Pool: Scales 1-3 per zone, Standard_D8S_v4. +
*D -* Compute Pool: Scales 0-10 per zone, Standard_D8S_v4. +
*E -* GPU Pool: Scales 0-3 per zone, Standard_NC6s_v3.

*Networking:* +
*F -* https://learn.microsoft.com/en-us/azure/load-balancer/load-balancer-overview[Ingress load balancer^].

*Storage:* +
*G -* Environment and model images; https://azure.microsoft.com/en-us/products/container-registry[Azure Container Registry^]. +
*H -* https://learn.microsoft.com/en-us/azure/storage/common/storage-account-overview[Storage Account^]; Azure Files access for shared file system; https://azure.microsoft.com/en-us/products/storage/files[Datasets^]; https://azure.microsoft.com/en-us/products/storage/blobs[Azure Blob^] API access for Project files, Logs, and Backups.

When running on AKS, the Domino architecture uses Azure resources to fulfill the link:25b6dc[Domino cluster requirements] as follows:

* For a complete Terraform module for Domino-compatible AKS provisioning, see https://github.com/dominodatalab/terraform-azure-aks[terraform-azure-aks on GitHub^].
* The AKS control plane, with managed Kubernetes masters, handles Kubernetes control.
* The AKS cluster's default https://docs.microsoft.com/en-us/cli/azure/ext/aks-preview/aks/nodepool?view=azure-cli-latest[node pool^] is configured to host the Domino platform.
* Additional AKS node pools provide compute nodes for user workloads.
* When Domino is deployed in AKS, it is compatible with the `containerd` runtime, which is the AKS default runtime for Kubernetes 1.19 and above.
* When using the `containerd` runtime, the Azure Container Registry stores Domino images.
* An https://docs.microsoft.com/en-us/azure/storage/common/storage-account-overview[Azure storage account^] stores Domino blob data and datasets
* The `kubernetes.io/azure-disk` provisioner creates persistent volumes for Domino executions.
* The Advanced Azure CNI is used for cluster networking, and Calico enforces network policy.
* An SSL-terminating https://docs.microsoft.com/en-us/azure/application-gateway/overview[Application Gateway^] that points to a Kubernetes load balancer handles Ingress to the Domino application.
* Domino recommends provisioning with https://www.terraform.io/[Terraform^] for extended control and customizability of all resources.
When you set up your Azure Terraform provider, add a `partner_id` with a value of `31912fbf-f6dd-5176-bffb-0a01e8ac71f2` to enable https://docs.microsoft.com/en-us/azure/marketplace/azure-partner-customer-usage-attribution#use-terraform[usage attribution^].


== Set up an AKS cluster for Domino

This section describes how to configure an AKS cluster for use with Domino.

=== Resource groups

[[tr2]]
You can provision the cluster, storage, and application gateway in an existing
https://docs.microsoft.com/en-us/azure/azure-resource-manager/management/manage-resource-groups-portal#what-is-a-resource-group[resource group^].
When Azure creates the cluster, it will create a separate resource group that will contain the cluster components themselves.

=== Namespaces

[[tr3]]
You do not have to configure namespaces prior to install.
Domino will create three namespaces in the cluster during installation, according to the following specifications:

[cols="1a,2a",options="header"]
|===
|Namespace |Contains
|`platform` |Durable Domino application, metadata, platform services required for platform operation.

|`compute` |Ephemeral Domino execution pods launched by user actions in the application.

|`domino-system` |Domino installation metadata and secrets.
|===

=== Node pools

[[tr4]]
The AKS cluster must have at least two node pools: `platform` (for platform nodes) and `default` (for compute nodes).
The cluster can also include a `default-GPU` node pool.
All node pools must contain worker nodes with the following specifications and distinct node labels:

[cols="2a,^1a,^2a,^1a,4a",options="header"]
|===
|Pool |Min-Max |VM |Disk |Labels
|`platform` |4-6 |Standard_DS5_v2 |128G
|`dominodatalab.com/node-pool: platform`

|`default` |1-20 |Standard_DS4_v2 |128G
|`dominodatalab.com/node-pool: default` `domino/build-node: true`

|Optional: `default-gpu` |0-5 |Standard_NC6_v3 |128G
|`dominodatalab.com/node-pool: default-gpu` `nvidia.com/gpu: true`
|===

The recommended architecture creates the `platform` node pool by configuring the cluster's initial default node pool.
See the following cluster Terraform resource for a complete example.

[source,shell]
----
resource "azurerm_kubernetes_cluster" "aks" {

  name                       = example_cluster
  enable_pod_security_policy = false
  location                   = "East US"
  resource_group_name        = "example_resource_group"
  dns_prefix                 = "example_cluster"
  private_cluster_enabled    = false

  default_node_pool {
    enable_node_public_ip = false
    name                  = "platform"
    node_count            = 4
    node_labels           = { "dominodatalab.com/node-pool" : "platform" }
    vm_size               = "Standard_DS5_v2"
    availability_zones    = ["1", "2", "3"]
    max_pods              = 250
    os_disk_size_gb       = 128
    node_taints           = []
    enable_auto_scaling   = true
    min_count             = 1
    max_count             = 4
  }

  network_profile {
    load_balancer_sku  = "Standard"
    network_plugin     = "azure"
    network_policy     = "calico"
    dns_service_ip     = "100.97.0.10"
    docker_bridge_cidr = "172.17.0.1/16"
    service_cidr       = "100.97.0.0/16"
  }

}
----

You must add the `default` compute node pool after the cluster is created.
This is _not_ the initial cluster default node pool, but a separate node pool named `default`.
It contains the default Domino compute nodes.
See the following node pool Terraform resource for a complete example.

[source,shell]
----
resource "azurerm_kubernetes_cluster_node_pool" "aks" {

  enable_node_public_ip = false
  kubernetes_cluster_id = "example_cluster_id"
  name                  = "default"
  node_count            = 1
  vm_size               = "Standard_DS4_v2"
  availability_zones    = ["1", "2", "3"]
  max_pods              = 250
  os_disk_size_gb       = 128
  os_type               = "Linux"
  node_labels = {
    "domino/build-node"            = "true"
    "dominodatalab.com/build-node" = "true"
    "dominodatalab.com/node-pool"  = "default"
  }
  node_taints           = []
  enable_auto_scaling   = true
  min_count             = 1
  max_count             = 20

}
----

[[tr5]]
You can add node pools with distinct `dominodatalab.com/node-pool` labels to make other instance types available for Domino executions.
See link:eca4b2[Manage Compute Resources] to learn how these different node types are referenced by label from the Domino application.
When you add GPU node pools, consider the Azure best practices on https://docs.microsoft.com/en-us/azure/aks/gpu-cluster[using GPU nodes in AKS^].

=== Network plugin

[[tr6]]
The Domino-hosting cluster must use the Advanced Azure CNI with network policy enforcement by Calico.
See the following `network_profile` configuration example.

[source,shell]
----
network_profile {
  load_balancer_sku  = "Standard"
  network_plugin     = "azure"
  network_policy     = "calico"
  dns_service_ip     = "100.97.0.10"
  docker_bridge_cidr = "172.17.0.1/16"
  service_cidr       = "100.97.0.0/16"
}
----

=== Dynamic block storage

AKS clusters come equipped with several `kubernetes.io/azure-disk` backed storage classes by default.
Domino requires use of premium disks for adequate input and output performance.
You can use the `managed-premium` class that is created by default.
Consult the following storage class specification as an example.

[source,yaml]
----
allowVolumeExpansion: true
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  labels:
    kubernetes.io/cluster-service: "true"
  name: managed-premium
  selfLink: /apis/storage.k8s.io/v1/storageclasses/managed-premium
parameters:
  cachingmode: ReadOnly
  kind: Managed
  storageaccounttype: Premium_LRS
reclaimPolicy: Delete
volumeBindingMode: Immediate
----

=== Persistent blob and data storage

Domino uses one Azure storage account for both blob data and files.
See the following configuration for the two resources required, the storage account itself and a blob container inside the account.

[source,shell]
----
resource "azurerm_storage_account" "domino" {
  name                     = "example_storage_account"
  resource_group_name      = "example_resource_group"
  location                 = "East US"
  account_kind             = "StorageV2"
  account_tier             = "Standard"
  account_replication_type = "LRS"
  access_tier              = "Hot"
}

resource "azurerm_storage_container" "domino_registry" {
  name                  = "docker"
  storage_account_name  = "example_storage_account"
  container_access_type = "private"
}
----

Record the names of these resources for use when you install Domino.

=== Domain

[[tr7]]
Domino must be configured to serve from a specific FQDN.
To serve Domino securely over HTTPS, you need an SSL certificate that covers the chosen name.
Record the FQDN for use when installing Domino.

IMPORTANT: A Domino install can't be hosted on a subdomain of another Domino install.
For example, if you have Domino deployed at `data-science.example.com`, you can't deploy another instance of Domino at `acme.data-science.example.com`.

== Example installer configuration

See the following example configuration file for the Installation Process based on the previous provisioning examples.

[source,yaml,subs="attributes"]
----
schema: '1.0'
name: domino-deployment
version: 4.1.9
hostname: domino.example.org
pod_cidr: '100.97.0.0/16'
ssl_enabled: true
ssl_redirect: true
request_resources: true
enable_network_policies: true
enable_pod_security_policies: true
create_restricted_pod_security_policy: true
namespaces:
  platform:
    name: domino-platform
    annotations: {}
    labels:
      domino-platform: 'true'
  compute:
    name: domino-compute
    annotations: {}
    labels: {}
  system:
    name: domino-system
    annotations: {}
    labels: {}
ingress_controller:
  create: true
  gke_cluster_uuid: ''
storage_classes:
  block:
    create: false
    name: managed-premium
    type: azure-disk
    access_modes:
    - ReadWriteOnce
    base_path: ''
    default: false
  shared:
    create: true
    name: dominoshared
    type: azure-file
    access_modes:
    - ReadWriteMany
    efs:
      region: ''
      filesystem_id: ''
    nfs:
      server: ''
      mount_path: ''
      mount_options: []
    azure_file:
      storage_account: ''
blob_storage:
  projects:
    type: shared
    s3:
      region: ''
      bucket: ''
      sse_kms_key_id: ''
    azure:
      account_name: ''
      account_key: ''
      container: ''
    gcs:
      bucket: ''
      service_account_name: ''
      project_name: ''
  logs:
    type: shared
    s3:
      region: ''
      bucket: ''
      sse_kms_key_id: ''
    azure:
      account_name: ''
      account_key: ''
      container: ''
    gcs:
      bucket: ''
      service_account_name: ''
      project_name: ''
  backups:
    type: shared
    s3:
      region: ''
      bucket: ''
      sse_kms_key_id: ''
    azure:
      account_name: ''
      account_key: ''
      container: ''
    gcs:
      bucket: ''
      service_account_name: ''
      project_name: ''
  default:
    type: shared
    s3:
      region: ''
      bucket: ''
      sse_kms_key_id: ''
    azure:
      account_name: ''
      account_key: ''
      container: ''
    gcs:
      bucket: ''
      service_account_name: ''
      project_name: ''
    enabled: true
autoscaler:
  enabled: false
  cloud_provider: azure
  groups:
  - name: ''
    min_size: 0
    max_size: 0
  aws:
    region: ''
  azure:
    resource_group: ''
    subscription_id: ''
spotinst_controller:
  enabled: false
  token: ''
  account: ''
external_dns:
  enabled: false
  provider: aws
  domain_filters: []
  zone_id_filters: []
git:
  storage_class: managed-premium
email_notifications:
  enabled: false
  server: smtp.customer.org
  port: 465
  encryption: ssl
  from_address: domino@customer.org
  authentication:
    username: ''
    password: ''
monitoring:
  prometheus_metrics: true
  newrelic:
    apm: false
    infrastructure: false
    license_key: ''
helm:
  tiller_image: gcr.io/kubernetes-helm/tiller
  appr_registry: quay.io
  appr_insecure: false
  appr_username: '$QUAY_USERNAME'
  appr_password: '$QUAY_PASSWORD'
private_docker_registry:
  server: quay.io
  username: '$QUAY_USERNAME'
  password: '$QUAY_PASSWORD'
internal_docker_registry:
  s3_override:
    region: ''
    bucket: ''
    sse_kms_key_id: ''
  gcs_override:
    bucket: ''
    service_account_name: ''
    project_name: ''
  azure_blobs_override:
    account_name: 'example_storage_account'
    account_key: 'example_storage_account_key'
    container: 'docker'
telemetry:
  intercom:
    enabled: false
  mixpanel:
    enabled: false
gpu:
  enabled: false
fleetcommand:
  enabled: false
  api_token: ''
teleport_kube_agent:
  enabled: false
  proxyAddr: teleport-domino.example.org:443
  authToken: TOKEN
----

== Next steps

* Deploy workloads across multiple Kubernetes clusters with Domino's link:c65074[Nexus Hybrid Architecture].
* Find out more about Domino's link:0e34ae[on-premises deployment service architecture].
* Learn about how Domino uses Keycloak to manage link:4d7b3b[user accounts].
----- admin_guide/architecture/domino-on-eks.txt -----
:page-version: 6.1
:page-title: Set up Domino on EKS
:page-permalink: b5da89
:page-order: 10
// /! NOTE: Please do not unduly edit -- these pages are needed for cloud certification.

[[tr1]]
Domino can run on a Kubernetes cluster provided by https://aws.amazon.com/eks/[AWS Elastic Kubernetes Service^].
When running on EKS, the Domino architecture uses AWS resources to fulfill the link:25b6dc[Domino cluster requirements] as follows:

image::/images/5.1/aws-arch-new.png[alt="A map of the Amazon Web Services architecture you'll need to set up a Domino deployment", width=1200, role=noshadow]

* EKS features a fully managed Kubernetes control plane.
* Domino uses a dedicated Auto Scaling Group (ASG) of EKS workers to host the Domino platform.
* ASGs of EKS workers host elastic compute for Domino executions.
* AWS S3 stores user data,
internal Docker registry,
backups, and logs.
* Elastic Container Registry can be configured as an external Docker registry.
* AWS EFS stores Domino Datasets.
* The `ebs.csi.aws.com` provisioner creates persistent volumes for Domino executions.
* https://docs.aws.amazon.com/eks/latest/userguide/calico.html[Calico^] is a network plugin that supports https://kubernetes.io/docs/concepts/services-networking/network-policies/[Kubernetes network policies^].
* Domino cannot be installed on EKS Fargate, since Fargate does not support stateful workloads with persistent volumes.
* Domino recommends provisioning via our https://github.com/dominodatalab/terraform-aws-eks[Terraform modules^].
* Domino recommends nodes have private IPs fronted by a load balancer with proper security controls. Nodes in the cluster can egress to the Internet through a NAT gateway.

Note that the use of GPU compute instances is optional.

Your annual Domino license fee will not include any charges incurred from using AWS services.
You can find detailed pricing information for the Amazon services listed above at https://aws.amazon.com/pricing[https://aws.amazon.com/pricing^].

== Set up an EKS cluster for Domino
IMPORTANT: Domino maintains https://github.com/dominodatalab/terraform-aws-eks[Terraform modules^] that provision AWS infrastructure in your VPC in a manner consistent with how we validate Domino internally. To promote standardization, stability, and easier troubleshooting, Domino highly recommends that you deploy our Terraform, working with a Domino Solutions Architect to customize as needed.

This section describes how to configure an Amazon EKS cluster for use with Domino.
You should be familiar with the following AWS services:

* Elastic Kubernetes Service (EKS)
* Identity and Access Management (IAM)
* Virtual Private Cloud (VPC) Networking
* Elastic Block Store (EBS)
* Elastic File System (EFS)
* S3 Object Storage

Additionally, a basic understanding of Kubernetes concepts like node pools, network CNI, storage classes, autoscaling, and Docker are useful when deploying the cluster.

=== Security considerations

You must create IAM policies in the AWS console to provision an EKS cluster.
Domino recommends that you grant the least privilege when you create IAM policies.
Grant elevated privileges when necessary.
See information about the https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#grant-least-privilege[grant least privilege concept^].

=== Service quotas

Amazon maintains default service quotas for each of the services listed previously.
Log in to the https://console.aws.amazon.com/servicequotas/home[AWS Service Quotas console^] to check the https://docs.aws.amazon.com/general/latest/gr/aws-service-information.html[default service quotas^] and manage your quotas.


=== VPC networking

If you plan to do https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html[VPC peering^] or set up a https://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html[site-to-site VPN connection^] to connect your cluster to other resources like data sources or authentication services, https://docs.aws.amazon.com/eks/latest/userguide/network_reqs.html[configure your cluster VPC accordingly^] to avoid address space collisions.

=== Namespaces

You do not have to configure namespaces prior to installation.
Domino will create the following namespaces in the cluster during installation, according to the following specifications:

[cols="1a,2a",options="header"]
|===
|Namespace |Contains
|`domino-platform` |Persistent Domino platform services and metadata required for platform operation (control plane).

|`domino-compute` |Ephemeral Domino execution pods launched by user actions in the application (workspaces, Domino endpoints, apps, etc.).

|`domino-system` |Domino installation metadata and secrets.
|===

=== Node pools

The EKS cluster must have at least two ASGs that produce worker nodes with the following specifications and distinct node labels, and it might include an optional GPU pool:

[cols="2a,^1a,^2a,^1a,4a",options="header"]
|===
|Pool |Min-Max |Instance |Disk |Labels
|`platform` | 4-6 |m7i-flex.2xlarge |128G
|`dominodatalab.com/node-pool: platform`

|`compute` |1-20 |m6i.2xlarge |400G
|`dominodatalab.com/node-pool: default` `domino/build-node: true`

|Optional: `gpu`  |0-5 |g5.2xlarge |400G
|`dominodatalab.com/node-pool: default-gpu` `nvidia.com/gpu: true`
|===

The `platform` ASG can run in one availability zone or across three availability zones.
If you want Domino to run with some components deployed as highly available https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/[ReplicaSets^] you must use three availability zones.
Using two zones is not supported, as it results in an even number of nodes in a single failure domain.
All compute node pools you use must have corresponding ASGs in any AZ used by other node pools.
If you set up an isolated node pool in one zone, you might encounter volume affinity issues.

To run the `default` and `default-gpu` pools across multiple availability zones, you must duplicate ASGs in each zone with the same configuration, including the same labels, to ensure pods are delivered to the zone where the required ephemeral volumes are available.

To get suitable drivers onto GPU nodes, use the https://docs.aws.amazon.com/eks/latest/userguide/gpu-ami.html[EKS-optimized accelerated AMI distributed by Amazon] as the machine image for the GPU node pool.

You can add ASGs with distinct `dominodatalab.com/node-pool` labels to make other instance types available for Domino executions.
See link:eca4b2[Manage Compute Resources] to learn how these different node types are referenced by labels from the Domino application.

=== Network plugin

Domino relies on https://kubernetes.io/docs/concepts/services-networking/network-policies/[Kubernetes network policies^] to manage secure communication between pods in the cluster.
The network plugin implements network policies, so your cluster must use a networking solution that supports `NetworkPolicy`, such as https://www.tigera.io/project-calico/[Calico^].

See the https://docs.aws.amazon.com/eks/latest/userguide/calico.html[AWS documentation] about installing Calico for your EKS cluster.

If you use the https://github.com/aws/amazon-vpc-cni-k8s[Amazon VPC CNI^] for networking, with only NetworkPolicy enforcement components of Calico, ensure the subnets you use for your cluster have CIDR ranges of sufficient size, as every deployed pod in the cluster will be assigned an elastic network interface and consume a subnet address.
Domino recommends at least a /23 CIDR for the cluster.


[[dynamic-block-storage]]
=== Dynamic block storage

The EKS cluster must be equipped with an EBS-backed storage class that Domino will use to provision ephemeral volumes for user execution.
GP2 and GP3 volume types are supported.
See the following for an example storage class specification:

[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: dominodisk-gp3
parameters:
  type: gp3
provisioner: ebs.csi.aws.com
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
----

When using GP3, your IAM policy must allow additional permissions to operate on these ephemeral volumes.
Use this link:https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/helm-chart-aws-ebs-csi-driver-2.1.1/docs/example-iam-policy.json#L85/[example IAM policy] as a reference.

[[datasets-storage]]
=== Datasets storage

To store Datasets in Domino, you must configure an EFS (Elastic File System).
You must provision the EFS file system and configure an access point to allow access from the EKS cluster.

Configure the access point with the following key parameters, also shown in the following image.

* Root directory path: `/domino`
* User ID: `0`
* Group ID: `0`
* Owner user ID: `0`
* Owner group ID: `0`
* Root permissions: `777`

image::/images/4.x/admin_guide/efs_access_point.png[alt="EFS access point", width=1000, role=noshadow]

Record the file system and access point IDs for use when you install Domino.


[[blob-storage]]
=== Blob storage

When running in EKS, Domino can use Amazon S3 for durable object storage.

Create the following S3 buckets:

* One bucket for user data
* One bucket for the internal Docker registry
* One bucket for logs
* One bucket for backups

Configure each bucket to permit read and write access from the EKS cluster.
This means that you must apply an IAM policy to the nodes in the cluster like the following:

[source,json,subs="attributes"]
----
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:ListBucket",
        "s3:GetBucketLocation",
        "s3:ListBucketMultipartUploads"
     ],
      "Resource": [
        "arn:aws:s3:::$your-logs-bucket-name",
        "arn:aws:s3:::$your-backups-bucket-name",
        "arn:aws:s3:::$your-user-data-bucket-name",
        "arn:aws:s3:::$your-registry-bucket-name"
     ]
    },
    {
      "Effect": "Allow",
      "Action": [
        "s3:PutObject",
        "s3:GetObject",
        "s3:DeleteObject",
        "s3:ListMultipartUploadParts",
        "s3:AbortMultipartUpload"
     ],
      "Resource": [
        "arn:aws:s3:::$your-logs-bucket-name/*",
        "arn:aws:s3:::$your-backups-bucket-name/*",
        "arn:aws:s3:::$your-user-data-bucket-name/*",
        "arn:aws:s3:::$your-registry-bucket-name/*"
     ]
    }
 ]
}
----

Record the names of these buckets for use when you install Domino.

=== Autoscaler access

If you intend to deploy the link:7f4331#autoscaler[Kubernetes Cluster Autoscaler^] in your cluster, the instance profile used by your platform nodes must have the necessary https://docs.aws.amazon.com/eks/latest/userguide/cluster-autoscaler.html[AWS Auto Scaling permissions^].

See the following example policy:

[source,json,subs="attributes"]
----
{
 "Version": "2012-10-17",
 "Statement": [
     {
         "Action": [
             "autoscaling:DescribeAutoScalingGroups",
             "autoscaling:DescribeAutoScalingInstances",
             "autoscaling:DescribeLaunchConfigurations",
             "autoscaling:DescribeTags",
             "autoscaling:SetDesiredCapacity",
             "autoscaling:TerminateInstanceInAutoScalingGroup",
             "ec2:DescribeLaunchTemplateVersions",
             "ec2:DescribeInstanceTypes"
        ],
         "Resource": "*",
         "Effect": "Allow"
     }
]
}
----

=== Domain

You must configure Domino to serve from a specific FQDN.
To serve Domino securely over HTTPS, you also need an SSL certificate that covers the chosen name.
Record the FQDN for use when you install Domino.

IMPORTANT: A Domino install can't be hosted on a subdomain of another Domino install.
For example, if you have Domino deployed at `data-science.example.com`, you can't deploy another instance of Domino at `acme.data-science.example.com`.

== Next steps

* Deploy workloads across multiple Kubernetes clusters with Domino's link:c65074[Nexus Hybrid Architecture].
* Find out more about Domino's link:0e34ae[on-premises deployment service architecture].
* Learn about how Domino uses Keycloak to manage link:4d7b3b[user accounts].
----- admin_guide/architecture/domino-on-gke.txt -----
:page-version: 6.1
:page-title: Set up Domino on GKE
:page-permalink: 6e290a
:page-order: 30
// /! NOTE: Please do not unduly edit -- these pages are needed for cloud certification.

[[tr1]]
Domino can run on a Kubernetes cluster provided by the https://cloud.google.com/kubernetes-engine/[Google Kubernetes Engine (GKE)^].

image::/images/5.1/gcp-arch.png[A map of the Google Cloud architecture you'll need to set up a Domino deployment,width=1200,role=noshadow]

When running on GKE, the Domino architecture uses Google Cloud Provider (GCP) resources to fulfill the link:25b6dc[Domino cluster requirements] as follows:

* The https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-architecture[GKE cluster^] manages Kubernetes control.
* Domino uses one https://cloud.google.com/kubernetes-engine/docs/concepts/node-pools[node pool^] of three https://cloud.google.com/compute/docs/machine-types[n1-standard-8^] worker nodes to host the Domino platform.
* Additional node pools host elastic compute for Domino executions with optional GPU accelerators.
* https://cloud.google.com/filestore/docs/[Cloud Filestore^] stores user data, backups, logs, and Domino Datasets.
* A https://cloud.google.com/storage/docs/creating-buckets[Cloud Storage Bucket^] stores the Domino Docker Registry.
* The `kubernetes.io/gce-pd` provisioner creates persistent volumes for Domino executions.


== Set up a GKE cluster for Domino

This section describes how to configure an GKE cluster for use with Domino.

=== Namespaces

You don't have to configure namespaces prior to install.
Domino creates the following namespaces in the cluster during installation, according to the following specifications:

[cols="1a,2a",options="header"]
|===
|Namespace |Contains
|`platform` |Durable Domino application, metadata, platform services required for platform operation.

|`compute` |Ephemeral Domino execution pods launched by user actions in the application.

|`domino-system` |Domino installation metadata and secrets.
|===

=== Node pools

[[tr2]]
The GKE cluster must have at least two node pools that produce worker nodes with the following specifications and distinct node labels, and it might include an optional GPU pool:

[cols="2a,^1a,^2a,^1a,4a",options="header"]
|===
|Pool |Min-Max |Instance |Disk |Labels
|`platform` |4-6 |n1-standard-8 |128G
|`dominodatalab.com/node-pool: platform`

|`default` |1-20 |n1-standard-8 |400G
|`dominodatalab.com/node-pool: default` `domino/build-node: true`

|Optional: `default-gpu` |0-5 |n1-standard-8 |400G
|`dominodatalab.com/node-pool: default-gpu`
|===

[[tr3]]
If you want to configure the `default-gpu` pool, you must add a GPU accelerator the node pool.
See the GKE documentation about https://cloud.google.com/kubernetes-engine/docs/how-to/gpus[available accelerators^] and deploying a DaemonSet that automatically installs the https://cloud.google.com/kubernetes-engine/docs/how-to/gpus#installing_drivers[necessary drivers^].

[[tr4]]
You can add node pools with distinct `dominodatalab.com/node-pool` labels to make other instance types available for Domino executions.
See link:eca4b2[Manage Compute Resources] to learn how these different node types are referenced by labels from the Domino application.

Consult the following https://www.terraform.io/[Terraform^] snippets for code representations of the required node pools.

*Platform pool*

[source,shell]
----
resource "google_container_node_pool" "platform" {
  name     = "platform"
  location = $YOUR_CLUSTER_ZONE_OR_REGION
  cluster  = $YOUR_CLUSTER_NAME

  initial_node_count = 3
  autoscaling {
    max_node_count = 3
    min_node_count = 3
  }

  node_config {
    preemptible  = false
    machine_type = "n1-standard-8"

    labels = {
      "dominodatalab.com/node-pool" = "platform"
    }

    disk_size_gb    = 128
    local_ssd_count = 1
  }

  management {
    auto_repair  = true
    auto_upgrade = true
  }

  timeouts {
    delete = "20m"
  }
}
----

*Default compute pool*

[source,shell]
----
resource "google_container_node_pool" "compute" {
  name     = "compute"
  location = $YOUR_CLUSTER_ZONE_OR_REGION
  cluster  = $YOUR_CLUSTER_NAME

  initial_node_count = 1
  autoscaling {
    max_node_count = 20
    min_node_count = 1
  }

  node_config {
    preemptible  = false
    machine_type = "n1-standard-8"

    labels = {
      "domino/build-node"            = "true"
      "dominodatalab.com/build-node" = "true"
      "dominodatalab.com/node-pool"  = "default"
    }

    disk_size_gb    = 400
    local_ssd_count = 1
  }

  management {
    auto_repair  = true
    auto_upgrade = true
  }

  timeouts {
    delete = "20m"
  }
}
----

*Optional GPU pool*

[source,shell]
----
resource "google_container_node_pool" "gpu" {
  provider = google-beta
  name     = "gpu"
  location = $YOUR_CLUSTER_ZONE_OR_REGION
  cluster  = $YOUR_CLUSTER_NAME

  initial_node_count = 0

  autoscaling {
    max_node_count = 5
    min_node_count = 0
  }

  node_config {
    preemptible  = false
    machine_type = "n1-standard-8"

    guest_accelerator {
      type  = "nvidia-tesla-p100"
      count = 1
    }

    labels = {
      "dominodatalab.com/node-pool" = "default-gpu"
    }

    disk_size_gb    = 400
    local_ssd_count = 1

    workload_metadata_config {
      node_metadata = "GKE_METADATA_SERVER"
    }
  }

  management {
    auto_repair  = true
    auto_upgrade = true
  }

  timeouts {
    delete = "20m"
  }
}
----

=== Network policy enforcement

[[tr5]]
Domino relies on https://kubernetes.io/docs/concepts/services-networking/network-policies/[Kubernetes network policies] to manage secure communication between pods in the cluster.
By default, the network plugin in GKE will not enforce these policies.
To run Domino securely on GKE, you must enable enforcement of network policies.

See the https://cloud.google.com/kubernetes-engine/docs/how-to/network-policy[GKE documentation^] for instructions about how to enable network policy enforcement for your cluster.

=== Dynamic block storage

[[tr6]]
The Domino installer will automatically create a storage class like the following example for use provisioning GCE persistent disks as link:cd38c2[execution volumes].
No manual setup is necessary for this storage class.

[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: dominodisk
parameters:
  replication-type: none
  type: pd-standard
provisioner: kubernetes.io/gce-pd
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
----

=== Shared storage

[[tr7]]
A Cloud File store instance must be provisioned with at least 10T of capacity and it must be configured to allow access from the cluster.
You must provide the IP address and mount path of this instance to the Domino installer, and it will create an NFS storage class like the following.

[source,yaml]
----
allowVolumeExpansion: true
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  labels:
    app.kubernetes.io/instance: nfs-client-provisioner
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/name: nfs-client-provisioner
    helm.sh/chart: nfs-client-provisioner-1.2.6-0.1.4
  name: domino-shared
parameters:
  archiveOnDelete: "false"
provisioner: cluster.local/nfs-client-provisioner
reclaimPolicy: Delete
volumeBindingMode: Immediate
----

=== Docker registry storage

[[tr8]]
You need one https://cloud.google.com/storage/docs/creating-buckets[Cloud Storage Bucket^] accessible from your cluster to be used to store the internal Domino Docker Registry.

=== Domain

[[tr9]]
You must configure Domino to serve from a specific FQDN.
To serve Domino securely over HTTPS, you need an SSL certificate that covers the chosen name.
Record the FQDN for use when you install Domino.

IMPORTANT: A Domino install can't be hosted on a subdomain of another Domino install.
For example, if you have Domino deployed at `data-science.example.com`, you can't deploy another instance of Domino at `acme.data-science.example.com`.

After Domino is deployed into your cluster, you must set up DNS for this name to point to an
https://cloud.google.com/load-balancing/docs/choosing-load-balancer[HTTPS Cloud Load Balancer^] that has an SSL certificate for the chosen name, and forwards traffic to port 80 on your platform nodes.

== Next steps

* Deploy workloads across multiple Kubernetes clusters with Domino's link:c65074[Nexus Hybrid Architecture].
* Find out more about Domino's link:0e34ae[on-premises deployment service architecture].
* Learn about how Domino uses Keycloak to manage link:4d7b3b[user accounts].
----- admin_guide/architecture/index.txt -----
:page-version: 6.1
:page-title: Domino architecture
:page-permalink: a0b173
:page-order: 10
:page-section: Infrastructure management

Domino is distributed as a set of containerized, https://kubernetes.io[Kubernetes^] native applications. A set of accompanying Helm charts and an installer is used to manage Domino installation and upgrades. Domino is validated on major Kubernetes services in the public cloud (EKS, AKS, and GKE). It is also possible to run Domino in self-managed Kubernetes environments in your private data center.

The following diagram shows Domino's conceptual architecture.

image::/images/5.10/domino-architecture-diagram-conceptual.jpg[alt="High-level feature architecture", width=1200]

The Domino application is separated into three primary areas of concern:

Clients::
External clients interact with Domino via HTTPS. Clients can take the form of a command-line interface, web browsers, and programmatic API clients (including link:8dbc91[Domino endpoint] clients).

Control Plane (Platform)::
The Domino control plane houses platform components including the Domino UI server, Domino API server, container orchestration, storage of metadata, and various supporting services.

Data Plane (Compute)::
The Domino data plane is where user workloads are run including machine learning and AI Jobs, models, Apps, and data science Workspaces.

The remainder of this page describes various services and the role they play.

== Control Plane services

The following services run in the Control Plane:

Domino UI::
A Node.js web server that serves a single-page React application to clients using web browsers. Primary users include data science practitioners and Domino admins.

Secrets Vault::
Storage for sensitive data including credentials, tokens, certificates, and API keys.

Domino API::
Exposes API endpoints that handle REST API requests from various clients (CLI, web browser, programmatic clients).

Message Broker::
A message broker (RabbitMQ) facilitating asynchronous communication between Domino services.

Identity::
Domino’s authentication service used for storing user identities and properties. Identity federation to SSO systems and third party identity providers is possible.

Application Data Stores::
Various application databases (e.g. MongoDB, PostgreSQL) store Domino metadata including Projects, users, and organizations. Application data stores can store references to blob storage stored in object storage systems (e.g. Amazon S3).

Experiment Manager::
Track and monitor data science experiments with logging, view and compare across experiments, and collaborate securely.

Environment Image Builder::
The Image Builder is capable of building Domino Environments, based on Docker containers. These Environments can include user-specified libraries needed to support data science work. It also builds Domino endpoint images.

Model Registry::
Discover and track models, record model metadata for auditability and reproducibility, and handle model versioning.

== Data Plane services

The following services run in the Data Plane cluster(s):

Data plane agent::
Applies resource changes to remote data planes.  Responsible for running workloads on remote clusters.

Cluster Autoscaler::
Adds additional compute capacity to the cluster dynamically based on resource requirements (e.g. CPU, memory, GPU).

Domino Workspaces::
Used for housing interactive data science Environments (e.g. Jupyter Notebooks). Implemented as ephemeral pods that must be synced to long-term storage.

Domino Apps::
User-defined web applications able to serve user requests, hosted within Domino.

Domino endpoints::
Model inference endpoints hosted within Domino. Implemented as ephemeral pods.

Domino Jobs::
Short-lived batch Jobs run by Domino. Implemented as ephemeral pods.

Distributed Compute Clusters::
Use clustered compute environments to scale out compute-intensive workloads in Domino.

image::/images/5.10/domino-workloads-logos.jpg[alt="Tools and frameworks compatible with Domino workloads", width=1000]

== External & additional services

The following services execute outside of Domino clusters:

Object Storage::
Object (blob) storage, used for durably storing logs, container images, Domino backups, and various user data.

File shares::
File-based storage, used to store Domino entities like Datasets.

Remote data sources::
Pull data residing in remote data sources into data science environments.

Git providers::
Domino uses Git to revision projects and files. Git clients residing in the data plane can also be used to interact with external repositories to access code or data.

Environment registry::
Select from a catalog of predefined and user-defined Environments containing libraries needed to support data science.

== Next steps

* Set up Domino on a Kubernetes cluster provided by:
** link:b5da89[AWS Elastic Kubernetes Service (EKS)].
** link:7d0b3e[Azure Kubernetes Service (AKS)].
** link:6e290a[Google Kubernetes Engine (GKE)].
* Deploy workloads across multiple Kubernetes clusters with Domino's link:c65074[Nexus Hybrid Architecture].
* Find out more about Domino's link:0e34ae[on-premises deployment service architecture].
* Learn about how Domino uses Keycloak to manage link:4d7b3b[user accounts].

----- admin_guide/architecture/nexus-hybrid-architecture.txt -----
:page-version: 6.1
:page-permalink: c65074
:page-title: Nexus Hybrid Architecture
:page-order: 40

Domino Nexus provides a hybrid architecture that enables you to deploy workloads across multiple Kubernetes clusters, including in multiple cloud regions, cloud providers, or on-premises.

image::/images/5.5/hybrid/hybrid-architecture.png[alt="The Nexus hybrid architecture", role=noshadow, width=90%]

A Domino Nexus deployment consists of a “control plane”, which is a Kubernetes cluster hosting Domino platform services (above, light blue), and many “data planes” (above, dark blue) which are distinct Kubernetes clusters that run a small set of Domino services and are used for executing user workloads.

The Domino control plane is also capable of executing user workloads in what is called the “local” data plane. This allows for the continued use of Domino features which are not yet supported for remote data planes.

[NOTE]
====
This Domino feature is not available in _remote_ data planes: Starburst-powered data sources

The `local` data plane (hosted in the control plane) supports the full range of Domino features.
====

Your Domino field representative can help you enable Domino Nexus in your deployment.  Once it is enabled, you can link:5781ea[manage your data planes] and your Domino users can link:95520d[use data planes].

== Connectivity

image::/images/5.5/hybrid/hybrid-connectivity.png[alt="Hybrid connectivity", role=noshadow]

User connectivity to Domino Nexus consists primarily of a browser connection to the Domino control plane.
For Domino Workspaces, the user connects directly to the data plane where the workspace is running.
This avoids the possibility of proxying sensitive or region-locked data through the control plane.

image::/images/5.5/hybrid/connectivity2.png[alt="Hybrid connectivity in more detail", role=noshadow]

== Supported Kubernetes versions

See link:7b2cbe[Kubernetes Compatibility] for more information.

* Nexus *control planes* have the same Kubernetes cluster requirements as any other Domino deployment.
* Nexus *data planes* have the same requirements, except:
** There is no requirement for shared storage (RWX storage class).
** Only one (compute) namespace is required.
** Ingress configuration is different (see link:491fe8[Enable a Data Plane for Workspaces]).

== Control plane load balancers

Domino control planes expose these services to data planes:

* RabbitMQ
* Vault
* Docker Registry (unless external registry is used)
* Domino API

With the exception of the Domino API, a load balancer must be configured to allow ingress to these services from data planes.

There are a number of important security considerations; see link:98ad4f[Control plane security guidance].

----- admin_guide/architecture/on-premises-services.txt -----
:page-version: 6.1
:page-permalink: 0e34ae
:page-title: On-premises service architecture
:page-order: 50

On-premises deployment architecture depends on the deployment service.

See examples of on-premises deployment services.

link:3fb861[Domino on OpenShift]::
Domino runs on the https://www.openshift.com/products/container-platform[OpenShift Container Platform^] (OCP) and https://www.openshift.com/products/kubernetes-engine[OpenShift Kubernetes Engine^] (OKE).

----- admin_guide/architecture/user-accounts.txt -----
:page-version: 6.1
:page-title: User accounts
:page-permalink: 4d7b3b
:page-order: 60

Domino uses Keycloak to manage user accounts.
Keycloak supports the following modes of authentication to Domino.

== Local accounts
[[tr1]]
// As a user with network access to a deployment I can create a domino account
[[tr2]]
// As an admin of the Domino deployment, I can manage accounts through the application
[[tr3]]
// As a user of a Domino deployment, I can use multi-factor authentication to secure my account
When using local accounts, anyone with network access to the Domino application can create a Domino account.
Users supply a username, password, and email address on the signup page to create a Domino-managed account.
You can track, manage, and deactivate these accounts through the application.
You can configure Domino with multi-factor authentication and password requirements through Keycloak.

== Identity federation
[[tr4]]
// As an admin, I can configure Domino/Keycloak to authenticate users using Active Directory
[[tr5]]
// As an admin, I can configure Domino/Keycloak to authenticate users using LDAP(S)
[[tr6]]
// When identity federation is enabled, local account authentication is disabled
[[tr7]]
// When identity federation is enabled, Domino/Keycloak acquires usernames and email addresses from the IdP Keycloak can be configured to integrate with an Active Directory (AD) or LDAP(S) identity provider (IdP).
When identity federation is enabled, local account creation is disabled and Keycloak authenticates users
against identities in the external identity provider (IdP) and retrieves configurable properties about those users, such as Domino usernames and email addresses.

See https://www.keycloak.org/docs/latest/server_admin/index.html#_user-storage-federation[Keycloak identity federation^] for more information.

== Identity brokering
[[tr8]]
// As an admin, I can configure Domino/Keycloak to authenticate users using external authentication or SSO system(s)
[[tr9]]
// As a user of Domino, after external or SSO login I am redirected back to Domino
You can configure Keycloak to broker authentication between Domino and an external authentication or SSO system.
When identity brokering is enabled, Domino redirects users in the authentication flow to a SAML, OAuth, or OIDC service for authentication.
Following authentication in the external service, the user is routed back to Domino with a token containing user properties.

See https://www.keycloak.org/docs/latest/server_admin/index.html#_identity_broker[Keycloak identity brokering^] for more information.

----- admin_guide/authentication/index.txt -----
:page-version: 6.1
:page-permalink: 3539d1
:page-title: User authentication
:page-sidebar: Authentication
:page-order: 80

Domino uses https://www.keycloak.org[Keycloak^] authentication service that runs in a pod on your Domino cluster to view, create, and manage groups of users.

== Enterprise authentication and identity

Domino seamlessly integrates with Enterprise Identity Management systems and access control policies.

Learn how to link:1a7dfa[Set up the Keycloak authentication services] and get started with the following to manage identities:

* link:0e334d[Single sign-on federation (SSO)] with SAML and OIDC.
* Standardized link:9b43ee[LDAP federation].
* link:bc00bd#role-synchronization[Group and role synchronization].
* link:eb6a88#aws-cred-prop[Credential propagation].

== Setup authentication services

To set up authentication services you must first set up link:1a7dfa[locally hosted authentication on the Domino Cluster].

Once link:1a7dfa[Keycloak is set up] for local authentication, you can then use the following guides:

* link:9b43ee[Federated Authentication through an LDAP Active Directory Provider].
* link:373f69[Set up SSO (Single-Sign On)].
* link:021788[Limit concurrent user sessions].

----- admin_guide/authentication/keycloak-set-up.txt -----
:page-version: 6.1
:page-permalink: 1a7dfa
:page-title: Set up Keycloak
:page-order: 10

https://www.keycloak.org[Keycloak^] is a user authentication service that runs in a pod on your Domino cluster.

Discover how to set up Keycloak, manage user groups, and configure local usernames and passwords.

link:#enable_disable_registration[Enable user registration] via the Keycloak console so users can sign up to join your new Domino installation.

Keycloak offers various authentication methods:

* link:9b43ee[Federated authentication through an LDAP Active Directory Provider]
* link:0e334d[Brokered authentication through single sign-on (SSO)]

[[tr1]]

== Access Keycloak

To access your Keycloak instance, visit `https://<your-domino-domain>/auth/`.

You must sign in to the Keycloak admin console at least once to initialize the authentication service.

=== Retrieve the admin password

Before your first sign-in, you must retrieve the admin password.

If you haven't already, install link:https://kubernetes.io/docs/tasks/tools/[kubectl^] and sign in to Kubernetes.

. Retrieve the administrator password from the `keycloak-http` https://kubernetes.io/docs/concepts/configuration/secret/[Kubernetes secret^]:
+
[source, shell]
----
kubectl -n <domino-platform-namespace> get secret keycloak-http -o yaml
----
+
The following is a sample response:
+
[source,yaml]
----
apiVersion: v1
data:
  password: <encrypted-password>
kind: Secret
metadata:
  creationTimestamp: 2019-09-09T21:23:15Z
  labels:
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/name: keycloak
    helm.sh/chart: keycloak-4.14.1-0.10.2
  name: keycloak-http
  namespace: domino
  resourceVersion: "6746"
  selfLink: /api/v1/namespaces/domino/secrets/keycloak-http
  uid: 09009f96-d348-11e9-9ea1-0aa417381fd6
type: Opaque
----

. Decrypt the password from the response:
+
[source,shell]
----
echo '<encrypted-password>' | base64 --decode
----

. Use this password to sign in to `https://<your-domino-domain>/auth/` with the username `keycloak`.

Domino automatically configures Keycloak with a realm named `DominoRealm`, to create and manage new users.
When you review or change settings for Domino authentication, ensure that you have `DominoRealm` selected.
image::/images/5.9/admin_guide/authentication-authorization/keycloak-realm.png[Keycloak realm configuration]

[[enable_disable_registration]]
== Enable or disable user registration

You can enable or disable Domino user registration from the Keycloak admin console.
Make sure to enable user registration, so users can join your fresh Domino install.

. In the Keycloak sidebar menu, click *Realm Settings*.
. Click the *Login* tab, and toggle *User registration* to *On* or *Off*.

== Local username and password configuration

The simplest option for authentication to Domino is to use local usernames and passwords.
When you choose local authentication, Keycloak stores all user data in Postgres.

Set login and email configurations in Keycloak for easy authentication to Domino.

=== Login configuration

When using local usernames and passwords, you find important login settings in the *Login* tab of the `DominoRealm` settings page.

image::/images/5.9/admin_guide/authentication-authorization/keycloak-local-login-config.png[Keycloak login]

NOTE: *Email as username* is not supported.
If you want to use the `Verify Email` option, an SMTP connection must be configured in the *Email* tab.

=== Email configuration

You can configure Keycloak to be able to send emails, such as for password resets. In the DominoRealm, navigate to *Realm Settings* > *Email*.
The form contains fields that map to values provided by an SMTP email service provider or SMTP server.

NOTE: You can use the email configuration admin page to set SMTP settings for the deployment and Keycloak simultaneously instead of using the Keycloak panel. See link:aa19d4[Email configuration] for more info.

[[user-management]]
=== User management

Your link:#tr1[first sign-in as administrator] automatically configures Keycloak with realms.
A realm is the Keycloak equivalent of a tenant.
It's used to manage groups of users.
The default Keycloak admin account is in the Master realm.
Master realm accounts can view, create, or update accounts in other realms.
Domino also automatically configures a realm called _DominoRealm_.
DominoRealm accounts are non-admins and have ordinary user privileges.

In the Keycloak sidebar, from the menu, click *DominoRealm*.

image::/images/5.9/admin_guide/authentication-authorization/keycloak-add-realm.png[Keycloak realm admin]


[[tr3]]

== Next steps

* link:9b43ee[Federated authentication through an LDAP Active Directory Provider]
* link:373f69[Brokered authentication through single sign-on (SSO)]
* link:021788[Limit concurrent user sessions]

----- admin_guide/authentication/ldap-ad-federation.txt -----
:page-version: 6.1
:page-permalink: 9b43ee
:page-title: Manage LDAP / AD federation
:page-order: 20

When you choose federated authentication, Keycloak connects to the provider and caches user information.

== Add a provider

. . In the Keycloak console, go to *User Federation* > *Add LDAP providers*.
+
See https://www.keycloak.org/docs/latest/server_admin/#_user-storage-federation[the official Keycloak documentation^] for full details about user storage federation.
+
If you migrate from an older Domino version, use your `ldap.conf` from the Domino front end to see what inputs to use for the provider settings.
+
// Why don't we include a list of all of them?
Some of these inputs include:
+
[cols="^2a,^2a",options="header"]
|===
^|ldap.conf name ^|Keycloak user federation setting name

|Search principal |Bind DN
|Search base |Users DN
|Search filter |Additional Filtering
|===
+
You can synchronize Domino administrative user roles and organization membership with attributes in your SAML identity provider.
Use this to externalize management of these roles and memberships to the identity provider.
+
. Use an link:https://www.keycloak.org/docs/latest/server_admin/#_ldap_mappers[LDAP mapper^] to import user attributes to Keycloak.
+
. Follow the steps in link:0e334d#role-synchronization[Synchronize SSO Group and Role] related to Client Mappers to map from Keycloak to Domino.

NOTE: Updates to a user's group or role will not fully synchronize to Domino until the user signs in.

== Configure mappers

Review the LDAP mapper associated with your provider.
You must make sure that there are LDAP mappers for the following attributes:

* `username`
* `firstName`
* `lastName`
* `email`

For more details, read the official Keycloak documentation on link:https://www.keycloak.org/docs/latest/server_admin/#_ldap_mappers[LDAP mappers^].


=== Group and Role Synchronization

You can synchronize Domino administrative user roles and organization membership with attributes in your SAML identity provider.
Use this to externalize management of these roles and memberships to the identity provider. Please note that the Keycloak user attributes cannot be set directly from LDAP group memberships, there have to be corresponding attributes in LDAP.

. Use an link:https://www.keycloak.org/docs/latest/server_admin/#_ldap_mappers[LDAP mapper^] to import user attributes to Keycloak.

. Follow the steps in link:0e334d#role-synchronization[Synchronize SSO Group and Role] related to Client Mappers to map from Keycloak to Domino.

NOTE: Updates to a user's group or role will not fully synchronize to Domino until the user signs in.



== Next steps

* link:1a7dfa[Set up Keycloak]
* link:373f69[Brokered authentication through single sign-on (SSO)]
* link:021788[Limit concurrent user sessions]

----- admin_guide/authentication/limit-sessions.txt -----
:page-version: 6.1
:page-permalink: 021788
:page-title: Limit concurrent user sessions
:page-order: 40

To limit the number of sessions a user can run at once, configure a Keycloak link:https://wjw465150.gitbooks.io/keycloak-documentation/content/server_admin/topics/authentication/flows.html[authentication flow^].
The flow that you must configure depends on how you authenticate users:

* Configure a browser flow if you use local or LDAP/AD authentication.
* Configure a post-sign in client flow if you use SSO.

NOTE: The browser flow is internal to Keycloak and can't be modified, so you must make a copy of it first.

In addition, to limit concurrent sessions for API access, you must set up a link:#first-broker-flow[Domino First Broker Login flow].

You can link:021788[limit] the number of active user sessions that a user can have open at one time.
When a user reaches the user session limit, they must end their current user sessions before they begin a new session.
You can stop user sessions from the Keycloak admin console, or users can sign out on their own.

[[user-mgmnt]]
== Use Keycloak's user management

Concurrent user session limits must be applied individually to each flow.
If you have multiple clients or flows, you must add a concurrent user session limit for each one.

=== Browser flow for Local and LDAP/AD authentication

. Copy the default browser flow (this default flow can't be customized directly).

. Click the copy of the default flow.

. Click the *<Flow Name> Forms* row from XYZ.

. From *Actions*, click *Add Execution*.

. Click *User Session Count Limiter* if the limit should be applied to a single user.
Click *Realm Session Count Limiter* if the limit should be applied to all the users in a realm.
A realm is the Keycloak version of a tenant.

. Return to the flow page.

. Set the execution you created to *Required*.

. From the *Actions* menu, click *Config*.


. Create a name for the execution and configure the authenticator.
For *User Session Count Limiter*, you can select *Deny new session* or *Terminate oldest session* as the desired behavior.

. Click *Clients* > *domino-play*.

. In *Authentication Flow Overrides*, from *Browser Flow*, click your copied browser flow, and click *Save*.

[[first-broker-flow]]
=== First broker flow for API access

. Copy the direct grant flow (this default flow can't be customized directly).

. Click the copy of the default flow.

. Click *Add Execution* on the *Actions* menu.

. Click *User Session Count Limiter* if the limit should be applied to a single user.
Click  *Realm Session Count Limiter*, if the limit should be applied to a group of users.

. Return to the flow page.

. Set the execution you just created to *Required* and click *Config* from the *Actions* menu in the same line.


. Create a name for the execution and configure the authenticator.
For *User Session Count Limiter*, you can select *Deny new session* or *Terminate oldest session* as the desired behavior.

. Click *Clients* > *domino-play*.

. In *Authentication Flow Overrides*, from *Direct Grant Flow*, click your copied direct grant flow, and click *Save*.

[[tr11]]

== SSO version

Unlike other kinds of flows, you can't directly add a user session limit to a client authentication flow.
However, you can still limit the number of user sessions if you add a flow that executes after the user signs in:

. From the Keycloak sidebar, click *Authentication*.

. Click *New* on the *Flow Definition* page.

. Name the new flow, set the flow type to *generic* and click *Save*.

. Click *Add Execution* on the *Actions* menu.

. Click *User Session Count Limiter* if the limit should be applied to a single user. Click *Realm Session Count Limiter* if the limit should be applied to a group of users.

. Return to the flow page.

. Set the execution you just created to *Required* and click *Config* from the *Actions* menu in the same line.


. Create a name for the execution and configure the authenticator.
For *User Session Count Limiter*, you can select *Deny new session* or *Terminate oldest session* as the desired behavior.

. From the sidebar, click *Identity Providers* and click your SSO provider.

. Open the *Post Login Flow* menu and click the flow you just created.

. Click *Save*.

== Next steps

* link:1a7dfa[Set up Keycloak]
* link:9b43ee[Federated Authentication through an LDAP Active Directory Provider]

----- admin_guide/authentication/sso/aws-credential-propagation.txt -----
:page-version: 6.1
:page-permalink: eb6a88
:page-title: Configure AWS credential propagation
:page-order: 40


If you enable SSO for Domino, you can configure Amazon Web Service (AWS) credential propagation.
Use this to let Domino automatically assume temporary credentials for AWS roles that are based on roles assigned to users in the upstream identity provider.

To learn more about how to use a credential file with the AWS SDK, see the blog on https://aws.amazon.com/blogs/security/a-new-and-standardized-way-to-manage-credentials-in-the-aws-sdks/[A new and standardized way to manage credentials in the AWS SDKs^].

The following describes the overall workflow from user login to credential usage:

image::/images/4.x/admin_guide/keycloak-upstream-idp-trust.png[width=1000, alt="Keycloak Upstream IDP Trust", role=noshadow]

== Validations in the AssumeRoleWithSAML workflow

The Identity Provider Relying Party/Application validates the Issuer element in the AuthnRequest (SAML request) sent by Domino.
. Domino validates the *Audience (Entity ID of the SP)* in the *SAML Response* sent by the *Identity Provider Relying Party/Application*.
. *AWS AssumeRole* validates that the `Issuer` of the *SAML Response* passed on from Domino matches the `Issuer` of the *Identity Provider Relying Party/Application*.
You can set up validations for other fields.

The following diagram describes the credential workflow when a user launches a Workspace:

image::/images/4.x/admin_guide/keycloak-launching-workspace-run.png[width=800, alt="Keycloack launching Workspace", role=noshadow]



== SAML provider configuration prerequisites

You must configure federation between your AWS account and your identity provider independently of Domino.
See https://aws.amazon.com/blogs/security/aws-federated-authentication-with-active-directory-federation-services-ad-fs/[AWS Federated Authentication with Active Directory Federation Services (AD FS)^] for an example.

The SAML provider application connected to Domino must include the appropriate AWS federation attributes based on the roles that each user will be allowed to assume.

== Create AWS IAM resources 

* *Add an Identity Provider:*
+
Select `SAML` as the provider type and add a preferred name for it. Then click *Choose file* under the Metadata document heading to upload the Federation Metadata file from your ADFS.

image::/images/4.x/admin_guide/keycloak/add_identity_provider.png[width=750, alt="Identity provider configuration", role=noshadow] +

* *Create a role:*
+
Since Domino will refresh the user's credentials during an active session, you must ensure that any IAM role that you propagate to a user has `assume-self` policy. 
+
. First create a role, selecting `SAML 2.0 federation` as the trusted entity type. In the SAML Federation config, select the IdP that was created in the previous step and select programmatic and console access. This will be the initial configuration that will need to be updated to reference itself.
+
image::/images/4.x/admin_guide/keycloak/create_role.png[width=1000, alt="Role's initial configuration", role=noshadow]
+
In the last step, provide a name for the role and finish its creation.
+
image::/images/4.x/admin_guide/keycloak/role_name.png[width=600, alt="Role's name", role=noshadow]
+
. Once the role is created, go into its definition, select *Trust relationships*, and edit its trust policy.
+
image::/images/4.x/admin_guide/keycloak/edit_trust_policy_of_a_role.png[width=850, alt="Trust policy configuration", role=noshadow]
+
. Override the current value with the following value:
+
[source, json]
----
{
  "Version": "2012-10-17",
  "Statement": [
      {
          "Effect": "Allow",
          "Principal": {
              "Federated": "<ARN for the Identity Provider>"
          },
          "Action": "sts:AssumeRoleWithSAML"
      },
      {
          "Effect": "Allow",
          "Principal": {
              "AWS": "<ARN for the role>"
          },
          "Action": "sts:AssumeRole"
      }
  ]
}
----

* *Create a policy:*
. To specify the permissions for the policy, in the first step of the policy creation, select the JSON editor, and add this value:
+
[source, json]
----
{
  "Version": "2012-10-17",
  "Statement": [
      {
          "Effect": "Allow",
          "Action": [
              "sts:AssumeRole",
              "sts:AssumeRoleWithSAML"
          ],
          "Resource": "<ARN for the role>"
      }
  ]
}
----
+
. Next, provide a name for the policy and validate that the role is added to the permissions defined for the policy.

image::/images/4.x/admin_guide/keycloak/create_policy.png[width=850, alt="Provider policy creation", role=noshadow]


* *Attach the policy to the role:* +
. After the policy is created, go back to the role definition and select *Attach policies*.
+
image::/images/4.x/admin_guide/keycloak/attach_policy_to_a_role.png[width=850, alt="Attach policy to role", role=noshadow]
+
. Look for the policy that was created in the previous step and add it to the role.
+
image::/images/4.x/admin_guide/keycloak/permissions_policies_of_a_role.png[width=850, alt="Provider policy expected configuration for role", role=noshadow]

== Expected SAML attributes

* Attribute with Name `https://aws.amazon.com/SAML/Attributes/Role`
** *Multi-valued:* `Yes`
** *Value format:*
*** Comma-separated key-value pair of provider and role.
**** `<provider arn>,<role arn>`
**** `arn:aws:iam::<**acct #**>:saml-provider/<**provider name**>,arn:aws:iam::<**acct #**>:role/<**role name**>`
* Attribute with Name `https://aws.amazon.com/SAML/Attributes/RoleSessionName`
** *Multi-valued:* `No`
** *Value:*
*** String to be used as an identifier for the temporary credentials.
*** Usually set to the email of the user.
* Attribute with Name `https://aws.amazon.com/SAML/Attributes/SessionDuration`
** *Multi-valued:* `No`
** *Value:*
*** Duration in seconds of how long a user can stay logged in before the initial set of credentials for each of the roles is invalidated.
*** The duration must be smaller than the maximum allowable duration for each of the roles made available for a given user.

To validate that you've established trust between AWS and the identity provider, sign in to Domino and check that the SAML response contains the expected SAML attributes.
You can use the *SAML-tracer* extension available for https://chrome.google.com/webstore/detail/saml-tracer/mpdajninpobndbfcldcmbpnnbhibjmch?hl=en[Chrome^] and https://addons.mozilla.org/en-US/firefox/addon/saml-tracer/[Firefox^] to examine SAML requests and responses to see that the appropriate attributes appear.

Example:

[source, xml]
----
<saml2:AttributeStatement xmlns:saml2="urn:oasis:names:tc:SAML:2.0:assertion">
  <saml2:Attribute Name="https://aws.amazon.com/SAML/Attributes/Role">
      <saml2:AttributeValue xsi:type="xs:string">
          arn:aws:iam::123456789012:saml-provider/acme-saml,arn:aws:iam::123456789012:role/role1
      </saml2:AttributeValue>
      <saml2:AttributeValue xsi:type="xs:string">
          arn:aws:iam::123456789012:saml-provider/acme-saml,arn:aws:iam::123456789012:role/role2
      </saml2:AttributeValue>
  </saml2:Attribute>
  <saml2:Attribute Name="https://aws.amazon.com/SAML/Attributes/RoleSessionName">
      <saml2:AttributeValue xsi:type="xs:string">
          john.smith@acme.org
      </saml2:AttributeValue>
  </saml2:Attribute>
  <saml2:Attribute Name="https://aws.amazon.com/SAML/Attributes/SessionDuration">
      <saml2:AttributeValue xsi:type="xs:string">
          900
      </saml2:AttributeValue>
  </saml2:Attribute>
</saml2:AttributeStatement>
----

== Map AWS federation attributes

. Go to the *Mappers* tab and configure an *Attribute Importer* mapper.
+
. Go to *Identity providers* > Select the identity provider > *Mappers* > *Add mapper*.

* *AWS Roles*
** *Name:* AWS Roles
** *Mapper Type:* `Attribute Importer`
** *Attribute Name:* `https://aws.amazon.com/SAML/Attributes/Role`
** *Friendly Name:* <blank>
** *User Attribute Name:* `aws-roles`
* *AWS Role Session Name*
+
** *Name:* AWS Role Session Name
** *Mapper Type:* `Attribute Importer`
** *Attribute Name:* `https://aws.amazon.com/SAML/Attributes/RoleSessionName
** *Friendly Name:* <blank>
** *User Attribute Name:* `aws-role-session-name`

* *AWS Session Duration*
+
** *Name:* AWS Session Duration
** *Mapper Type:* `Attribute Importer`
** *Attribute Name:* `https://aws.amazon.com/SAML/Attributes/SessionDuration`
** *Friendly Name:* <blank>
** *User Attribute Name:* `aws-session-duration`

== Enable credential propagation in Domino

. After these prerequisites are configured, enable credential propagation in the Admin application by clicking on *Platform Settings* > *Configuration Records* and set the following configuration values.
+
* *Key:* `com.cerebro.domino.auth.aws.sts.enabled`
+
*Value:* `true`
* *Key:* `com.cerebro.domino.auth.aws.sts.region`
+
*Value:* Short AWS region name where your Domino is deployed, such as `us-west-2`
* *Key:* `com.cerebro.domino.auth.aws.sts.defaultSessionDuration`
+
*Value:* Default session duration, such as `1h` for 1 hour
+
Example of a valid configuration:
+
image::/images/4.x/admin_guide/keycloak-2_1-enable-cred-prop.png[width=900, alt="Valid credential propagation", role=noshadow]

. To make the changes take effect, restart the services.


=== Validate credential propagation

. Once configured properly the first time, you must log out and log back into Domino.

. To confirm that credentials are propagating correctly to users, start a Workspace and check the Environment variable
`AWS_SHARED_CREDENTIALS_FILE` and that your credential file appears at `/var/lib/domino/home/.aws/credentials`.

. To test your configuration outside Domino, perform an `AssumeRoleWithSAML` call with the SAML token provided to Domino by your identity provider.

Example:

[source, bash]
----
aws sts assume-role-with-saml 
--role-arn arn:aws:iam::521624712688:role/DataScientist-dev 
--principal-arn arn:aws:iam::521624712688:saml-provider/ADFS-DOMINO 
--saml-assertion "PHNhb.......VzcG9uc2U+"
----

This should be sufficient for a user to connect to AWS resources without further configuration.
See this example of how to link:dd93d4[connect to S3^].

Learn more about https://aws.amazon.com/blogs/security/a-new-and-standardized-way-to-manage-credentials-in-the-aws-sdks/[using a credential file with AWS SDK^].


== Additional provider configuration

To give Domino access to users' SAML assertions, enable the following settings for the identity provider:

* *Store Tokens:* On
* *Store Tokens Readable:* On

image::/images/5.9/admin_guide/authentication-authorization/keycloak-tokens.png[width=700, alt="Keycloak Tokens", role=noshadow]

== Domino-client configuration

The *domino-play* OpenID Connect (OIDC) client is pre-populated with client mappers, so that identity provider-mapped SAML information will flow into Domino.


To see the *Mappers*:

. Go to *Clients* > *domino-play*.
+
image::/images/5.9/admin_guide/authentication-authorization/domino-play.png[width=600, alt="Keycloak Client", role=noshadow]
+
. Select the *Client Scopes* tab and click *domino-play-dedicated*.
+
image::/images/5.9/admin_guide/authentication-authorization/keycloak-client-scopes.png[width=800, alt="Keycloak Client Scopes", role=noshadow]
+
The following are the default *domino-play client mappers*:
+
image::/images/5.9/admin_guide/authentication-authorization/keycloak-client-mapper.png[width=600, alt="Keycloak Client Mappers", role=noshadow]
+
. Create a new mapper with type *User Session Note* and the following settings:
* *Name:* `identity-provider-mapper`
* *Mapper Type:* `User Session Note`
* *User Session Note:* `identity_provider`
* *Token Claim Name:* `idpbroker`
* *Claim JSON Type:* `string`
* *Add to ID token:* `On`
* *Add to access token:* `On`
+
image::/images/5.9/admin_guide/authentication-authorization/keycloak-2_3_1-mappers2.png[width=750, alt="Create a new Keycloak Client Mapper"]
+
. To make the changes take effect, sign out and sign back into Domino.

== Next steps

Learn how to link:021788[limit concurrent user sessions].

----- admin_guide/authentication/sso/group-role-synchronization.txt -----
:page-version: 6.1
:page-permalink: bc00bd
:page-title: Synchronize SSO groups and roles
:page-order: 30

Domino can synchronize Domino administrative user roles and organization membership with attributes in your SAML identity provider.
Use this to externalize management of these roles and memberships to the identity provider.

[[tr8]]

== SAML Group to Organization synchronization

The SAML provider application connected to Domino must include group membership as a multi-valued attribute.

[[central-configuration]]
To enable this feature, you must update the Configuration key setting `authentication.oidc.externalOrgsEnabled` to `true`, then restart Domino services.

=== Group synchronization attributes

You must include the following attributes if you synchronize groups in Domino.

* Domino Organizations
** *Name:* Can be any name since Domino can map attributes.
** *Multi-valued:* Yes
** *Values:* One or more of the groups the user belongs to in your centralized identity provider.
For any groups specified here, the user will be automatically enrolled in a Domino organization with the same name.

Example:

[source,xml]
----
<saml2:AttributeStatement xmlns:saml2="urn:oasis:names:tc:SAML:2.0:assertion">
  <saml2:Attribute Name="DominoOrganizations">
    <saml2:AttributeValue>nyc-data-scientists</saml2:AttributeValue>
    <saml2:AttributeValue>all-data-scientists</saml2:AttributeValue>
    <saml2:AttributeValue>sensitive-claims-users</saml2:AttributeValue>
  </saml2:Attribute>
</saml2:AttributeStatement>
----

=== Attribute mapper

Add an *Attribute Importer* mapper to the provider configuration in Keycloak.

* *Name:* Domino Groups
* *Mapper Type:* `Attribute Importer`
* *Attribute Name:* Name attribute for the element that contains the groups for the user.
* *Friendly Name:* `FriendlyName` attribute for the element that contains the groups for the user.
* *User Attribute Name:* `domino-groups`.

image::/images/5.9/admin_guide/authentication-authorization/attribute-importer.png[width=800, alt="Keycloak mapper", role=noshadow]

=== Domino client mapper

When Domino is installed, it automatically creates the `domino-group-mapper` client mapper.

. To review the client mapper, go to *Clients* > *domino-play*.
+
image::/images/5.9/admin_guide/authentication-authorization/domino-play.png[width=600, alt="Keycloak Client"]
+
. To see the mappers: Go to *Clients* > *domino-play* > *Client Scopes* > *domino-play-dedicated*.
+
image::/images/5.9/admin_guide/authentication-authorization/keycloak-client-scopes.png[width=800, alt="Keycloak Client", role=noshadow]
+
. You can see the `domino-group-mapper`.
+
image::/images/5.9/admin_guide/authentication-authorization/keycloak-client-mapper.png[width=600, alt="Keycloak Client"]

[[tr9]]

== Role synchronization

In addition to group membership, you can also automatically assign Domino link:2611b7[administrative and/or user roles] to users based on attributes from your SAML identity provider.

=== Prerequisite

The SAML identity provider application connected to Domino must include attributes that can be mapped to specific Domino roles.

=== Attribute mapper

You must add an *Attribute Importer* mapper to the provider configuration in Keycloak:

* *Name:* Domino System Roles.
* *Mapper Type:* `Attribute Importer`.
* *Attribute Name:* Name attribute for the element that contains the Domino system roles for the user.
* *Friendly Name:* `FriendlyName` attribute for the element that contains the groups for the user.
* *User Attribute Name:* `domino-system-roles`.

To see the *Mappers*, go to *Clients* > *domino-play* > *Client Scopes* > *domino-play-dedicated*.

Domino system roles are controlled by the user's membership in a set of special user groups in Keycloak.
When the user is added or removed from such a group, the role change is propagated to Domino either on the next login or, if the user is currently logged in, within a few minutes (5 minutes by default).

When the user's SAML token carries the role information as described above, the group membership will be updated automatically.
The administrator can change the user roles from the link:2611b7[Admin User management page], but it will be overwritten on the next SSO login if the `Domino System Roles` mapper is enabled.

=== Enable Flows

For customers who already have SSO and External Role Sync enabled, an authenticator has been added to Keycloak when upgrading to Domino 5.7.0.
For customers with SSO enabled, but External Role Sync disabled, the authenticator is _not_ being added to the Authentication Flows when upgraded to 5.7.0.

For existing customers who don’t have the External Role Sync but now want to enable it, the following manual actions must be taken in the Keycloak UI:

. Add a new execution to the *Domino First Broker Login* flow:

.. Navigate to *Authentication* > *Flow details* > *Domino First Broker Login*, then click the *Add step* button.
.. Search for `Role Group Assigner` then select the *Domino Role Group assigner* step. Click *Add*.
.. Mark the new *Domino Role Group assigner* execution as `Required`.

. If the `Domino Post Login` authentication flow doesn’t exist, create it:

.. Navigate to *Authentication* > *Flow details* > *Domino Post Login*, then click the *Add step* button.
.. Search for `Role Group Assigner` then select the *Domino Role Group assigner* step. Click *Add*.
.. Mark the new *Domino Role Group assigner* execution as `Required`.

. Add the new flows to the Identity providers configuration:

.. Navigate to *Identity providers*.
.. Next to *First login flow override*, select `Domino First Broker Login` from the list.
.. Next to *Post login flow*, select `Domino Post Login` from the list.


== Next steps

Learn how to link:eb6a88[propagate AWS credentials].

----- admin_guide/authentication/sso/index.txt -----
:page-version: 6.1
:page-permalink: 373f69
:page-title: Manage single sign-on (SSO)
:page-order: 30

You must first complete the link:1a7dfa[initial set up for Keycloak authentication service] before configuring SSO.

Once Keycloak is set up for local authentication, you can use the following guides to set up SSO:

* link:0e334d[SSO configuration]
* link:bc00bd[Group and role synchronization]
* link:eb6a88[AWS credential propagation]

----- admin_guide/authentication/sso/sso-configuration.txt -----
:page-version: 6.1
:page-permalink: 0e334d
:page-title: Configure single sign-on (SSO)
:page-sidebar: Configure SSO
:page-order: 10

When you choose SSO authentication, you can integrate Domino with a SAML 2.0 or OpenID Connect (OIDC) identity provider.
SSO delegates authentication to the identity provider, so users don't have to create a username and password specific to Domino.


Domino can integrate with a SAML 2.0 or OIDC identity provider for single sign-on (SSO) with the steps outlined below.


NOTE: For the SSO configuration of a Domino Cloud instance, see the simplified document at link:76e38c[Domino Cloud SSO configuration]


== Create a Domino SAML service provider (SP)
. From the Keycloak sidebar menu, click *Identity providers* >  *Add provider* > *SAML v2.0*.
+
image::/images/5.9/admin_guide/authentication-authorization/keycloak-identity-providers.png[alt="keycloak identity providers", role=noshadow, width=1200]

== Configure alias and redirect URI

. Provide an *Alias* for the newly created provider.
The *alias* field is a unique name for the provider in Keycloak.
It's part of the *Redirect URI* used by the provider service to route SAML responses and redirect users after their account is authenticated.
+
The redirect URI is case-sensitive.
It takes the following form:
+
[source,shell]
----
`https://<deployment_domain>/auth/realms/DominoRealm/broker/<alias>/endpoint`
----
+
For example, if the deployment's domain is *domino.acme.org* and the provider's alias is *domino-credentials*, the redirect URL will be:
+
[source,shell]
----
https://domino.acme.org/auth/realms/DominoRealm/broker/domino-credentials/endpoint
----
+
image::/images/5.9/admin_guide/authentication-authorization/keycloak-add-identity-provider.png[width=900, alt="Keycloak Mapper", role=noshadow]
+
IMPORTANT: Don't save the identity provider entry yet, as you can't import your provider settings after it is saved.

== Create a SAML endpoint in your upstream identity provider

. Create a SAML application in the identity provider that will be integrated with Domino.
To create the application you need the *Redirect URI* from the previous step.
+
The specific procedure to create the SAML endpoint depends on your identity provider.
Domino can integrate through SAML with Okta, Azure AD, Ping, and any other provider that implements SAML v2.0.
+
. After you create and configure the SAML endpoint, you must export an XML metadata file to complete the configuration of the provider in Keycloak.
The following are important properties of the SAML endpoint you will create in the provider.

=== NameID policy format

`NameID` controls the format of the `<saml2:NameID>` element in the SAML response.
This is used to derive the SSO username in Domino.

Option 1:: `urn:oasis:names:tc:SAML:1.1:nameid-format:emailAddress`

Users are uniquely identified by their email, and their username will be automatically derived from it.

Example:

[source,xml]
----
<saml2:NameID Format="urn:oasis:names:tc:SAML:1.1:nameid-format:emailAddress">
john.smith@acme.org
</saml2:NameID>
----

Option 2:: `urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified`

The SAML endpoint must respond with a string that can be used as a username without any modification.

Example:

[source,xml]
----
<saml2:NameID Format="urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified">
jsmith
</saml2:NameID>
----

Option 3:: `urn:oasis:names:tc:SAML:1.1:nameid-format:persistent`

Typically, the SAML endpoint returns a NameID which is a GUID that is not suitable for a username.
If the endpoint must use this format, then an additional attribute that contains the username must be returned.

=== Assertion attributes

To automatically populate the user's Domino profile, the SAML response must contain the following attributes.
Without these attributes, the user will be prompted to complete the required elements of their user profile.

The required attributes are:

* `firstName`
* `lastName`
* `email`
* `username` (if `nameId` is not an email or does not represent the username).
* `destination`

No specific attribute names are expected as these can be mapped in Keycloak.

IMPORTANT: The SAML response must contain signed assertions.

== Import metadata

Use the metadata file from the previous step to complete the provider configuration in Keycloak.

=== Import IdP metadata


. While creating the SAML provider, turn off *Use entity descriptor* to reveal the options to import config files.


. Choose the file to import.
+
Most of the provider settings are configured automatically.

. Save the configuration.

=== Additional settings

After you've completed the initial configuration, confirm the following settings are configured correctly.

.Email settings
*Trust Email* - `Yes`

Ensures that emails supplied by IdP are trusted even if Email Verification is enabled for DominoRealm.


.Sync mode settings
*Sync Mode* - `force`

Ensures that user data is updated during every login with the identity provider, allowing Keycloak to
pick up changes from the upstream identity provider.

.Username settings
*NameID Policy Format*

This was configured on import, but verify that it matches the option configured on the external endpoint.

.Authentication settings
* *Want Assertions Signed* - `Yes`
* *Validate Signature* - `Yes`
* *Want AuthnRequests signed* - `Yes`
* *Want Assertions signed* - `Yes`
* *Want Assertions encrypted* - `Yes`


The respective signature field must already be populated based on the metadata you imported when you set up SSO authentication.


NOTE: See https://www.keycloak.org/docs/latest/server_admin/#_identity_broker[Keycloak SAML v2 Identity Providers^] for more documentation about all supported SAML settings.

.Export metadata from Keycloak


After you save the provider in Keycloak, you get the XML metadata for the provider to automatically configure an external endpoint:

. In Keycloak go to *Identify providers* > select your provider > *Settings*.
. Click on *SAML 2.0 Service Provider Metadata*.

The metadata will also be available at:

[source,console]
----
https://<deployment domain>/auth/realms/DominoRealm/broker/<alias>/endpoint/descriptor
----

[[configure-attribute-mappers]]
== Configure attribute mappers

To make the experience of new users signing in for the first time seamless, and not require them to complete their profile on initial login, you must pass several SAML attributes in the SAML response, and ensure these are correctly mapped to Domino user attributes.

If the attributes are not properly mapped, upon first login users will be prompted to complete the empty fields in their profile.

=== Map first name, last name, and email

To map these values from the SAML assertion attributes to the user profile model, you must configure an *Attribute Importer* mapper from
the *Mappers* tab.



image::/images/5.9/admin_guide/authentication-authorization/identify-provider-mapper.png[width=900, alt="Keycloak Mapper", role=noshadow]

First Name mapper::

* *Name:* `First Name`
* *Sync Mode Override:* `inherit`
* *Mapper Type:* `Attribute Importer`
* *Attribute Name:* Name attribute for the `<saml2:Attribute>` element that contains the value to be mapped to `First Name`.
* *Friendly Name:* `FriendlyName` attribute (optionally available) for the `<saml2:Attribute>` element that contains the value to be mapped to `First Name`.
* *User Attribute Name:* `firstName`

Last Name mapper::

* *Name:* `Last Name`
* *Sync Mode Override:* `inherit`
* *Mapper Type:* `Attribute Importer`
* *Attribute Name:* Name attribute for the element that contains the value for `Last Name`.
* *Friendly Name:* `FriendlyName` attribute for the `<saml2:Attribute>` element that contains the value for `Last Name`.
* *User Attribute Name:* `lastName`

Email mapper::

* *Name:* `Email`
* *Sync Mode Override:* `inherit`
* *Mapper Type:* `Attribute Importer`
* *Attribute Name:* Name attribute for the element that contains the value for `Email`
* *Friendly Name:* `FriendlyName` attribute for the `<saml2:Attribute>` element that contains the value for `Email`.
* *User Attribute Name:* `email`

NOTE: Setting the *Sync Mode Override* property to `inherit` has the same effect as setting it to `force`
if the identity provider is configured as recommended above.

The following example illustrates how to map `First Name` from an assertion with the following payload:

[source,xml]
----
<saml2:Attribute Name="customSAMLFirstName" FriendlyName="FriendlyFirstName">
  <saml2:AttributeValue>John</saml2:AttributeValue>
</saml2:Attribute>
----

You can map this with:


* *Name*: `First name`
* *Sync mode override*: `inherit`
* *Mapper type*: `Attribute importer`
* *Attribute Name*: `CustomSAMLFirstName`
* *Friendly Name*: 
* *User Attribute Name*: `firstName`

Alternatively, you can map this with:


* *Name*: `First name`
* *Sync mode override*: `inherit`
* *Mapper type*: `Attribute importer`
* *Attribute Name*:
* *Friendly Name*: `friendlyFirstName`
* *User Attribute Name*: `firstName`

=== Map username

The mapper configuration for the username depends on how the external endpoint is configured with respect to NameID Policy options.

[[option-1]]
* *Option 1:* `urn:oasis:names:tc:SAML:1.1:nameid-format:emailAddress`
** Use `Email Prefix as UserName Importer`
** Example:
+
[source,xml]
----
<saml2:NameID Format="urn:oasis:names:tc:SAML:1.1:nameid-format:emailAddress">
  john.smith@acme.org
</saml2:NameID>
----
+
Map as shown:

+
image::/images/5.9/admin_guide/authentication-authorization/email-prefix-importer.png[width=900, alt="Keycloak Email Prefix", role=noshadow]
+
* *Name*: `User Name`
* *Sync mode override*: `Inherit`
* *Mapper type* `Email Prefix as Username Importer`

* *Option 2:* `urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified`
** *No need to do an importer.* The username will be mapped automatically to the `NameID` value
** Example:
+
[source,xml]
----
<saml2:NameID Format="urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified">
  jsmith
</saml2:NameID>
----
* *Option 3:* `urn:oasis:names:tc:SAML:1.1:nameid-format:persistent`
** Use `Username Template Importer` with Template of `${ATTRIBUTE.<attribute Name>}` or `${ATTRIBUTE.<attribute FriendlyName>}`
** Example:
+
[source,xml]
----
<saml2:NameID Format="urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified">
  jsmith
</saml2:NameID>
<saml2:Attribute Name="customUserName">
  <saml2:AttributeValue>jsmith</saml2:AttributeValue>
</saml2:Attribute>
----
+
Map as follows:
+
image::/images/5.9/admin_guide/authentication-authorization/nameid-persistent.png[width=900, alt="Keycloak Image", role=noshadow]
+
* *Name*: `User Name`
* *Sync mode override*: `Inherit`
* *Mapper type*: `Username Template Importer`
* *Template*: `${Attribute.customUserName}`
* *Target*: `LOCAL`

=== Attribute mapping documentation

See https://www.keycloak.org/docs/latest/server_admin/#_protocol-mappers[Mapping claims and assertions^] for additional information on attribute mapping.

=== Email Prefix as Username Importer

When you use the *Email Prefix as Username Importer* mapper as illustrated previously (link:#option-1[Option 1]), the generated username will be the email prefix, stripped of all characters except alphanumeric and underscores.
Special characters will be converted to underscores. Characters with diacritics will be converted to their ascii base.

If a username from an external system is already present, the same transformation rules will be applied to it.

For example, a username `$jöhn.smîth#home$` or an email address `$jöhn.smîth#home$@somehost.com` would both be mapped to the username `john_smith_home`.


=== Troubleshoot attribute mapping

When you troubleshoot SAML attribute mapping, refer to a specification for the SAML response that your identity provider endpoint sends back to Keycloak after authentication.
A thorough specification will detail the `NameID` policy format and attributes sent in the response.

If such a specification is not available, or the attribute mapping does not function as expected, you might have to examine a SAML response that is returned from a login attempt.
One way to do this is to use the *SAML-tracer* extension available for https://chrome.google.com/webstore/detail/saml-tracer/mpdajninpobndbfcldcmbpnnbhibjmch?hl=en[Chrome^] and https://addons.mozilla.org/en-US/firefox/addon/saml-tracer/[Firefox^].
This allows you to examine decoded SAML requests and responses to:

* See the returned attributes (and whether any are missing).
* Verify that the names or formats are as expected.

== Domino First Broker Login authentication flow

To configure the recommended login authentication flow, click the `Domino First Broker Login` flow:

image::/images/5.9/admin_guide/authentication-authorization/first-broker-login.png[width=900, alt="Keycloak first login"]

See https://wjw465150.gitbooks.io/keycloak-documentation/content/server_admin/topics/authentication/flows.html[authentication flows^] for more information.


[[tr5]]

== Restrict access for SSO users to Domino

Typically, when you configure the SAML endpoint that will provide SSO authentication for Domino, the provider administrator restricts the endpoint to a subset of users who are allowed to authenticate through it.

NOTE: This is the preferred method to restrict access to a subset of users with valid enterprise credentials.

In rare cases, where limitations in the provider software don't allow you to constrain the set of users who can authenticate against the endpoint, the provider must pass an additional SAML attribute.
This attribute specifies if a user is allowed to access Domino.
The value of that attribute depends on a specific rule for each user.
Usually, it is based on membership in a group in your identity provider.

Use the following as a last resort, if all identity provider restriction options are exhausted.

=== Prerequisites

The Domino Keycloak instance must have `keycloak.profile.feature.scripts` enabled.

=== Expected SAML attributes

An attribute must indicate whether a properly authenticated user can log in to Domino.

AttributeName::
** Suggested: *rolesForDomino*
** Can be anything as this is mapped.
Multi-valued: `Yes`::
Value::
** Contains one or more values that can be used to gate access.
Typically, these values are roles or groups.

=== Attribute mapper

You must add an additional mapper to your provider configuration in Keycloak.

Use an *Attribute Importer* mapper type.

* *Sync mode override* `inherit`
* *Name:* `Allow in Domino`
* *Mapper Type:* `Attribute Importer`
* *Attribute Name:* `Name` attribute for the element that contains the flag.
* *Friendly Name:* `FriendlyName` attribute for the element that contains the groups for the user.
* *User Attribute Name:* `accessforDomino`

Example:

[source,xml]
----
<saml2:Attribute Name="rolesForDomino">
  <saml2:AttributeValue>dave-users</saml2:AttributeValue>
  <saml2:AttributeValue>it-users</saml2:AttributeValue>
</saml2:Attribute>
----

==== Create the post-login authentication flow

By default, Domino doesn't provide a post login flow so you must add one.

==== Add a post-login flow

. Create a Javascript file (such as `authenticator.js`) using the following script as an example.
+
[source,javascript]
----
/*
* Template for JavaScript-based authenticators.
* See org.keycloak.authentication.authenticators.browser.ScriptBasedAuthenticatorFactory
*/

// import enum for error lookup
AuthenticationFlowError = Java.type("org.keycloak.authentication.AuthenticationFlowError");


/**
* An example authenticate function.
*
* The following variables are available for convenience:
* user - current user {@see org.keycloak.models.UserModel}
* realm - current realm {@see org.keycloak.models.RealmModel}
* session - current KeycloakSession {@see org.keycloak.models.KeycloakSession}
* httpRequest - current HttpRequest {@see org.jboss.resteasy.spi.HttpRequest}
* script - current script {@see org.keycloak.models.ScriptModel}
* authenticationSession - current authentication session {@see org.keycloak.sessions.AuthenticationSessionModel}
* LOG - current logger {@see org.jboss.logging.Logger}
*
* You one can extract current HTTP request headers through:
* httpRequest.getHttpHeaders().getHeaderString("Forwarded")
*
* @param context {@see org.keycloak.authentication.AuthenticationFlowContext}
*/
function authenticate(context) {
    //name of the attribute that needs to be 'true' to allow a user to authenticate in Domino
    var requiredAttrMustContain = "dave-users";
    var errorMessageId = "userNotAssignedToDominoInIdp";
    var errorPageTemplate = "error.ftl";

    if (user === null) {
        context.success();
        return;
    }

    LOG.info(script.name +
            " trace script auth for: " +
            user.username);


    var requiredAttrValues = user.getAttributes().accessforDomino;

    LOG.info("User gated on attribute: " + requiredAttrName);
    LOG.info("Attribute values from SSO: " + requiredAttrValues)
    LOG.info("Attribute must contain: " + requiredAttrMustContain);

    if (requiredAttrValues === null ||
        requiredAttrValues.size() === 0 ||
        requiredAttrValues.contains(requiredAttrMustContain)) {
            //user is explicitly allowed in Domino
            LOG.info("User is allowed in Domino.");
            context.success();
            return;
    }

    //user is not authorized to access Domino
    LOG.info("User is not allowed in Domino.");
    context.failure(AuthenticationFlowError.IDENTITY_PROVIDER_DISABLED, context.form().setError(errorMessageId, null).createForm(errorPageTemplate));
}
----

. Create a JAR file with the following structure:
+
[source,shell]
----
META-INF/keycloak-scripts.json

authenticator.js
----
+
The `META-INF/keycloak-scripts.json` is a file descriptor that provides metadata information about the scripts
you want to deploy. It is a JSON file with the following structure:
+
[source,shell]
----
{
    "authenticators": [
        {
            "name": "Your Custom Authenticator",
            "fileName": "authenticator.js",
            "description": "Authenticator from a JS file"
        }
    ]
}
----

. Deploy the JAR file to the Keycloak server. Make sure to not accidentally overwrite existing files.
Assuming the JAR file is called `script.jar`, run the following command:
+
[source,shell]
----
kubectl cp ./script.jar <domino-platform-namespace>/keycloakv22-0:/domino/shared/custom-resources/providers
----
After running this command, restart each Keycloak replica pod.


. Go to *Authentication* > *Flows* > *Create Flow*.
. Create a new flow with the following information:
+
* *Name*: `Post Login Flow`
* *Flow type*: `Basic flow`
+
image::/images/5.9/admin_guide/authentication-authorization/create-post-login-flow.png[width=900, alt="Create a post login flow"]
+
. Click *Add execution* > Select your Authenticator, in this example, *My Custom Authenticator*.


See https://www.keycloak.org/docs/latest/server_development/#_script_providers[JavaScript providers^] for more information.

=== Customize the SSO button

. In Keycloak, go to the default `domino-theme`.
. Update the *Display text*.
+
If *Display text* is blank or corresponds to the identity provider's *Alias*, the button shows the default text, *Continue with Single Sign On*.
+
If any text other than the value of the *Alias* field is used, that value is used as the text on the button.

== Test and troubleshoot

If you encounter errors from the Keycloak service when you attempt an SSO login, you can view the Keycloak request logs through `kubectl`.
Run the following:

[source,shell]
----
kubectl -n <domino-platform-namespace> logs keycloak-0
----

[[tr6]]

== Session and token timeouts

Domino interacts with Keycloak through the https://openid.net/connect/[OpenID Connect protocol^], which uses two JWT tokens:

* Access token gives its bearer access to the secured resource (can be used to authenticate requests).
* Refresh token is not recognized by the secured resources, but it is used to request a fresh valid access token from the identity provider (Keycloak).

When the user authenticates with Keycloak and gets redirected to Domino, both tokens are saved in the Domino server-side session.
Every time a request from the user's browser is made to Domino, the server-side session is checked.
If the access token is about to expire or has expired, Domino attempts a background call to Keycloak to request another access token.
If this call is successful, the user's request is served, otherwise the user is logged out.
If the refresh token is expired, the user is logged out.

Keycloak has several timeouts that affect this behavior:

Access Token Lifespan:: This defines how long an access token is valid.
Domino will not re-check the validity of the user's authentication with Keycloak until this token expires.
The default value is 5 minutes.
SSO Session Max:: This setting has no effect on the lifespan of any tokens, but it limits the maximum length of the user's session with Keycloak.
Even if the user refreshes their tokens and actively uses Domino, they will be logged out at the end of this period.
The default value is 60 days.
SSO Session Idle:: This defines the maximum period of time in which the user's session on Keycloak must be refreshed, otherwise it will be terminated.
It defines the lifespan of the refresh token.
This period must be longer than Access Token Lifespan, and not longer than SSO Session Max.
+
IMPORTANT: If the user leaves their browser window open with Domino, the background requests keep the session alive.
The idle countdown only starts if the user closes the window or navigates away from the Domino application.

TIP: If the *Offline Session Idle* value is too low, your users might encounter intermittent failures such as scheduled Jobs failing and Workspaces not starting.
Domino recommends that you don't change the default that we set.

See the https://wjw465150.gitbooks.io/keycloak-documentation/content/server_admin/topics/sessions/timeouts.html[Keycloak documentation^] for more information about timeouts.

[[aws-cred-prop]]

[[tr7]]


== SSO attributes

You must have the following attributes to establish single sign-on between Domino and your identity provider:

* Username
** `NameID` (In Subject element)
** Preferred format: `urn:oasis:names:tc:SAML:1.1:nameid-format:email`
* First Name
** Attribute name: Can be any name since Domino allows attribute mapping.
* Last Name
** Attribute name: Can be any name since Domino allows attribute mapping.
* Email
** Attribute name: Can be any name since Domino allows attribute mapping.
* Destination
// We need an SME to confirm that this can be any name -- or tell us what it should be

Example:

[source,xml]
----
<saml2:Subject xmlns:saml2="urn:oasis:names:tc:SAML:2.0:assertion">
  <saml2:NameID Format="urn:oasis:names:tc:SAML:1.1:nameid-format:email">
    john.smith@acme.org
  </saml2:NameID>
...
</saml2:Subject>
  <saml2:AttributeStatement xmlns:saml2="urn:oasis:names:tc:SAML:2.0:assertion">
    <saml2:Attribute Name="DominoEmail">
      <saml2:AttributeValue xsi:type="xs:string">
        john.smith@acme.org
      </saml2:AttributeValue>
    </saml2:Attribute>
    <saml2:Attribute Name="DominoFirstName">
      <saml2:AttributeValue xsi:type="xs:string">
        John
      </saml2:AttributeValue>
    </saml2:Attribute>
    <saml2:Attribute Name="DominoLastName">
      <saml2:AttributeValue xsi:type="xs:string">
        Smith
      </saml2:AttributeValue>
    </saml2:Attribute>
  </saml2:AttributeStatement>
----

== Administrative role synchronization attributes

You must include the following attributes if you sync administrative roles in Domino:

* Domino System Roles
** *Name:* Can be any name since Domino can map attributes.
** *Multi-valued:* Yes
** *Values:*
*** One or more values that are an exact, case-sensitive match to one of the Domino administrative roles.
**** Practitioner
**** SysAdmin
**** Librarian
**** ReadOnlySupportStaff
**** SupportStaff
**** ProjectManager

Example:

[source,xml]
----
<saml2:AttributeStatement xmlns:saml2="urn:oasis:names:tc:SAML:2.0:assertion">
  <saml2:Attribute Name="DominoSystemRoles">
    <saml2:AttributeValue xsi:type="xs:string">
      SysAdmin
    </saml2:AttributeValue>
    <saml2:AttributeValue xsi:type="xs:string">
      Librarian
    </saml2:AttributeValue>
  </saml2:Attribute>
</saml2:AttributeStatement>
----

== Next steps

Learn how to link:bc00bd[synchronize groups and roles].

----- admin_guide/configuration/central-configuration.txt -----
:page-version: 6.1
:page-title: Configuration records
:page-permalink: 71d6ad
:page-order: 50


The Configuration record is where all global settings for a Domino installation are listed.

. Go to the _Admin_ portal.
. Click *Platform settings > Configuration records*.
. On the _Configuration records_ page, you can:

** Click an existing record to edit its attributes.
** Click *Add Record* to create a new setting.
If no record is created in the application, the system uses the default value.
+
You must link:#restart-services[restart the Domino services] for changes to take effect.

+
IMPORTANT: Any configuration that is designated as a secret will have its corresponding value obfuscated. A placeholder text `&#42;&#42;&#42;&#42;&#42;&#42;&#42;&#42;` will be displayed instead of the actual value to enhance security and prevent inadvertent exposure.

[[restart-services]]
== Restart services

Configuration record is read by Domino services at process startup time. So, for configuration record changes to take affect,
you must restart the Domino services. One way to do this is by clicking the *restart services* link in the Configuration records page.

You can also use a CLI tool like `kubectl` instead of the Domino UI to restart services. When doing so, ensure that you restart:

. Every `nucleus-*` deployment.
. Each `data-plane-agent` in the local and (if applicable) remote data planes.

[[tr10]]

[[authentication]]
== Authentication

[[idle-timeout]]
=== Idle Timeout configuration

These options relate to automatic session management through the Idle Timeout functionality. When users remain inactive across all Domino tabs for a specified period, the system will automatically log them out of their entire session for enhanced security. This intelligent timeout mechanism monitors actual user interaction rather than just session duration, providing better security without disrupting active work.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description

3+|`com.cerebro.domino.frontend.idleTimeoutValueInMinutes`
| |`0` |This is the total inactivity time in minutes after which the user will be automatically logged out. The default value of `0` means that this functionality is turned off.

3+|`com.cerebro.domino.frontend.idleTimeoutPromptValueInMinutes`
| |`0` |This is the amount of time before the idle timeout when a user will receive an inactivity notice. With the default value of `0`, users will be notified 3 minutes before being logged out unless this value is changed. This setting must be less than the value of `com.cerebro.domino.frontend.idleTimeoutValueInMinutes`.
|===


=== Organization membership

This option relates to the link:1a7dfa[Keycloak authentication service].
They are available in namespace `common` and must be recorded with no `name`.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description
3+|`authentication.oidc.externalOrgsEnabled`
| |`false` |Enables Domino organization membership to synchronize with SAML identity provider attributes so that membership can be managed by the identity provider.
|===

[[tr20]]

[[authorization]]
== Authorization

These options relate to authorization and user roles.


[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description

3+|`com.cerebro.domino.auth.refreshTokenInRun.proxyPort`
| |`8899` | The port on which the API proxy operates. Do not change this value.

CAUTION: Contact a Customer Success Manager if you want to change this value.

3+|`com.cerebro.domino.restrictPublishing`
| |`false` |If `true`, only SupportStaff and SysAdmins can create launchers, schedule runs, or publish apps and Domino endpoints.
3+|`com.cerebro.domino.authorization.restrictManageCollaborators`
| |`false` |If `true`, Project Owners can manage project collaborators. However, the *Invite* button in the _Collaborators and permissions_ section on the *Access & Sharing* tab in *Project Settings* is not available for other users, even if they are collaborators. See link:4b6411[Invite collaborators].
3+|`com.cerebro.domino.frontend.restrictCollaboratorsToOrganizations`
| |`false` |Collaborators on a project are able to invite more collaborators to a project. If this flag is set to `true`, the only eligible additions will be those from the inviting user’s organizations. This only affects invitations to projects, therefore it prevents unauthorized viewing of any data scoped to the project like its datasets, the Domino File System (DFS) storage, and project environment variables. It does not affect any data external to a project like an External Data Volume (EDV), data source, or model endpoint.
3+|`com.cerebro.domino.authorization.limitProjectSharing`
| |`false` |If `true`, then only users with the Project Manager Domino user role or administrators can manage project permissions. See link:7876f1[Settings permissions].
|===


== Blob storage

Domino can store long-term, unstructured data in blob storage buckets.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description

3+|`com.cerebro.domino.blobStorageMedium`
| |""
|Determines the DFS storage host for the deployment.
For example if set to `S3` or `AzureBlob`, the deployment uses the corresponding object store.
|===

=== S3 storage options
These options relate to Domino File System support for AWS S3 storage.
This is available for link:e3bf0a[AWS deployments] only.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description

3+|`com.cerebro.domino.blobs.s3.connectionManagerTimeoutDuration`
| |`60s`
|The timeout duration for a connection from the connection manager.

3+|`com.cerebro.domino.blobs.s3.connectionTimeoutDuration`
| |`60s`
|The timeout duration for the connection to S3 storage.

3+|`com.cerebro.domino.blobs.s3.bucketInPath`
| |`false`
|Configures the S3 client to use path-style access for all requests.

3+|`com.cerebro.domino.blobs.s3.bucket`
| |""
|Required: Name of the S3 bucket in which you want to store blobs.

3+|`com.cerebro.domino.blobs.s3.defaultS3BucketPrefix`
| |""
|Prefix that is added to the container name.
The user can set this, but this prefix must also be on the container in S3.

3+|`com.cerebro.domino.blobs.s3.defaultS3BucketSuffix`
| |""
|Suffix that is added to the container name.
The user can set this, but this suffix must also be on the container in S3.

3+|`com.cerebro.domino.blobs.s3.endpointUrl`
| |""
|Overrides the S3 client endpoint.

3+|`com.cerebro.domino.blobs.s3.maxConnections`
| |`100`
|Determines the pool size of max blobs to transfer concurrently.

3+|`com.cerebro.domino.blobs.s3.path`
| |""
|Carried over from the S3 settings.

3+|`com.cerebro.domino.blobs.s3.region`
| |""
|The region of the S3 account.

3+|`com.cerebro.domino.blobs.s3.signedUrlTimeoutDuration`
| |`5 minutes`
|The timeout duration to access the S3 blob store through a signed URL.
This pertains to the CLI only.

3+|`com.cerebro.domino.blobs.s3.socketTimeoutDuration`
| |`60s`
|The timeout duration for packets to reach the server.

3+|`com.cerebro.domino.blobs.s3.sseKmsKeyId`
| |""
|The KMS key ID for use with server-side encryption.
|===


== Azure blob storage

Domino can store long-term, unstructured data in blob storage buckets.

These options relate Domino File System (DFS) support for Azure blob storage.
This is available for new link:bf888c[Azure deployments] only.

NOTE: If you have DFS project files stored in Azure File Storage, you can contact Domino's Customer Success team for assistance migrating that data to an Azure Blob Storage deployment.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description

3+|`com.cerebro.domino.azureblob.accountKey`
| |""
|Required: The account key of the Azure blob storage account for the user.
Without this, projects won't load.
You can find this information on the Azure Portal Storage Account page under Access Keys.

3+|`com.cerebro.domino.azureblob.accountName`
| |""
|Required: The account name of the Azure blob storage account for the user.
Without setting this, files from projects won't load.
You can find this information on the Azure Portal Storage Account page under Access Keys.

3+|`com.cerebro.domino.azureblob.blobStorageMedium`
| |""
|Determines the DFS storage host for the deployment.
For example if set to `Azure`, the deployment uses the Azure blob store for DFS.

3+|`com.cerebro.domino.azureblob.connectionManagerTimeoutDuration`
| |`60s`
|The timeout duration for a connection from the connection manager.

3+|`com.cerebro.domino.azureblob.connectionTimeoutDuration`
| |`60s`
|The timeout duration for the connection to Azure blob storage.

// 3+|`com.cerebro.domino.azureblob.containerInPath`
// | |`false`
// |Carried over from the S3 settings.
// Not used in the Azure blob settings.

3+|`com.cerebro.domino.azureblob.containerName`
| |""
|Required: Container name of the blob container in which you want to store blobs.

3+|`com.cerebro.domino.azureblob.defaultAzureBlobContainerPrefix`
| |""
|Prefix that is added to the container name.
The user can set this, but this prefix must also be on the container in Azure.

3+|`com.cerebro.domino.azureblob.defaultAzureBlobContainerSuffix`
| |""
|Suffix that is added to the container name.
The user can set this, but this suffix must also be on the container in Azure.

3+|`com.cerebro.domino.azureblob.defaultEndpointsProtocol`
| |`https`
|The protocol to use to hit Azure endpoints.
Do not change this value.

3+|`com.cerebro.domino.azureblob.endpointSuffix`
| | `core.windows.net`
|The common endpoint suffix for all Azure endpoints.
All Azure endpoints end with `core.windows.net`.

// 3+|`com.cerebro.domino.azureblob.endpointUrl`
// | |""
// |Carried over from the S3 settings.
// Not used in the Azure blob settings.

3+|`com.cerebro.domino.azureblob.maxConnections`
| |`100`
|Determines the pool size of max blobs to transfer concurrently.
Not required for Azure blob storage.

// 3+|`com.cerebro.domino.azureblob.path`
// | |""
// |Carried over from the S3 settings.
// Not used in the Azure blob settings.

// 3+|`com.cerebro.domino.azureblob.region`
// | |`OnPremise`
// |Carried over from the S3 settings.
// Not used in the Azure blob settings.

//not yet implemented so let's not document yet
//3+|`com.cerebro.domino.azureblob.signedUrlTimeoutDuration`
//| |`5 minutes`
//|The timeout duration to access the Azure blob store through a signed URL.
//This pertains to the CLI only.


3+|`com.cerebro.domino.azureblob.socketTimeoutDuration`
| |`60s`
|The timeout duration for packets to reach the server.

// 3+|`com.cerebro.domino.azureblob.sseKmsKeyId`
// | |""
// ||Carried over from the S3 settings.
// Not used in the Azure blob settings.
// By default, Azure blob store data is encrypted by the storage medium.
|===



[[tr40]]

[[caching]]
== Caching

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description
3+|`com.cerebro.domino.caching.featureFlagOverridesCacheTimeMs`
| |`10000` |The length of time from which the feature flag information is requested to the next time they will be retrieved from the server.
|===



[[tr50]]

[[compute-cluster-auto-scaling]]
== Compute Cluster auto-scaling

These settings are related to the ability to enable auto-scaling of Spark, Ray, and Dask on-demand clusters.
They are available in namespace `common` and must be recorded with no `name`.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description
3+|`com.cerebro.domino.computegrid.computeCluster.autoscaling.targetCpuUtilizationPercent`
| |None |Target CPU utilization percentage when scale up of clusters should trigger. If not set, the Kubernetes default of 80% is used.
3+|`com.cerebro.domino.computegrid.computeCluster.autoscaling.targetMemoryUtilizationPercent`
| |None |Target memory utilization percentage when scale up of clusters should trigger.
//DOCS-1092
3+|`com.cerebro.domino.computegrid.computeCluster.autoscaling.scaleDownStabilizationWindowSeconds`
| |None |Scale down stabilization window.
//DOCS-1092
On lower versions, 300 seconds will apply.
|===

The following table describes the interaction of the auto-scaling settings.

[cols="^2a,^2a,4a",options="header"]
|===
|targetCpuUtilizationPercent |targetMemoryUtilizationPercent |behavior
|Not set |Not set |The default Kubernetes setting of 80% CPU utilization applies.
|`X` |Not set |`X%` CPU utilization threshold applies. Memory utilization is not considered.
|Not set |`Y` |CPU utilization is not considered. `Y%` Memory utilization threshold applies.
|`X` |`Y` |Scaling will trigger based on reaching either `X%` CPU OR `Y%` memory utilization.
|===

For more information on compute cluster auto-scaling, you can see the https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/[Kubernetes HPA documentation^].


[[tr60]]

[[compute-grid]]
== Compute grid

These options relate to the link:eca4b2[compute grid].
They are available in namespace `common` and must be recorded with no `name`.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description

3+|`com.cerebro.domino.computegrid.eventHistory.executions.allowVpn`
| |`false`
|Must be true to support runs using environments configured to use a VPN. Adds the `NET_RAW` capability to the run container when using a VPN, which has security drawbacks.

3+|`com.cerebro.domino.computegrid.eventHistory.garbageCollection.isEnabled`
| |`true`
|If `true`, the garbage collector will run periodically to manage the size of the memory of event history in MongoDB.


3+|`com.cerebro.domino.computegrid.eventHistory.garbageCollection.periodicity`
| |`1 hour`
|How often the garbage collector runs to manage the size of the memory for the event history in MongoDB.


3+|`com.cerebro.domino.computegrid.kubernetes.apps.nginx.clientBodyMaxSizeMiB`
| |`25MiB`
|Sets the http://nginx.org/en/docs/http/ngx_http_core_module.html#client_max_body_size[client_body_max_size^] property for the nginx reverse proxy in workspace pods.

NOTE: If a file upload to a workspace (such as Jupyter or RStudio) fails with a 413 HTTP code, it might be because the file is larger than this limit. If necessary, increase this limit.


3+|`com.cerebro.domino.computegrid.kubernetes.apps.nginx.connectTimeout`
| |`300s` if the key is not configured upon deployment.
|The timeout for waiting to connect to the Nginx proxy server running in the run pod. You must use finite duration syntax to configure the time. For example, 30s for 30 seconds, 5m for 5 minutes, and 1h for 1 hour. See link:a0b173[the Architecture documentation] for application services.
3+|`com.cerebro.domino.computegrid.kubernetes.apps.nginx.readTimeout`
| |`300s` if the key is not configured upon deployment.
|The timeout for waiting to read data from the Nginx proxy server running in the run pod. You must use finite duration syntax to configure the time. For example, 30s for 30 seconds, 5m for 5 minutes, and 1h for 1 hour. See link:a0b173[the Architecture documentation] for application services.

3+|`com.cerebro.domino.computegrid.kubernetes.volume.gcFrequency`
| |`10min` |Controls how often the link:c29bdf[garbage collector] runs to delete old or excess link:cd38c2[persistent volumes].
3+|`com.cerebro.domino.computegrid.kubernetes.volume.maxAge`
| |`7 days` |Setting a value here will cause link:cd38c2[persistent volumes] older than that to be automatically deleted by the link:c29bdf[garbage collector].
3+|`com.cerebro.domino.computegrid.kubernetes.volume.maxIdle`
| |`32` |Maximum number of idle link:cd38c2[persistent volumes] to keep. Idle volumes in excess of this number will be deleted by the link:c29bdf[garbage collector].


3+|`com.cerebro.domino.computegrid.kubernetes.volume.storageClass`
| |`dominodisk` |https://kubernetes.io/docs/concepts/storage/storage-classes/[Kubernetes storage class^] that will be used to dynamically provision link:cd38c2[persistent volumes]. This is set initially to the value of `storage_classes.block.name` in the link:7f4331#storage-classes[installer storage classes configuration].
3+|`com.cerebro.domino.computegrid.kubernetes.volume.volumesSizeInGB`
| |`15` |Size in GB of compute grid link:cd38c2[persistent volumes]. This is the total amount of disk space available to users in runs and workspaces.

3+|`com.cerebro.domino.computegrid.kubernetes.nonRootExecutions.enabled`
| |`false` |If `true`, user code within executions (the "run" container) never runs as root. If `false`, the user code bootstrap process runs as root, then switches to non-root when the 
bootstrap process is complete. Note that non-user execution processes (for example, the "executor" container) always run as non-root regardless of whether this setting is `true` or `false`.

[[tr33]]
// As an admin, I can configure the Deploying state timeout so that an execution in the deploying state will timeout and fail after the specified threshold.

[[tr34]]
// As an admin, I can configure the Preparing state timeout so that an execution in the preparing state will timeout and fail after the specified threshold.

3+|`com.cerebro.domino.computegrid.timeouts.sagaStateTimeouts.deployingStateTimeoutSeconds`
| |`60 * 60` (1 hour) | The number of seconds an execution pod in a deploying state will wait before timing out.

3+|com.cerebro.domino.computegrid.timeouts.sagaStateTimeouts.executionsOverQuotaStateTimeoutSeconds`
| |`24 * 60 * 60` (24 hours) |

The number of seconds an execution pod that cannot be assigned due to execution quota limitations will wait for resources to become available before timing out.

3+|`com.cerebro.domino.computegrid.timeouts.sagaStateTimeouts.preparingStateTimeoutSeconds`
| |`60 * 60` (1 hour) |
The number of seconds an execution pod in a preparing state will wait before timing out.

3+|`com.cerebro.domino.computegrid.userExecutionsQuota.maximumExecutionsPerUser`
| |`25` |This is the maximum number of executions each user will be allowed to run concurrently. If a user attempts to start additional executions in excess of this those executions will be queued until some of the user's other executions finish.

3+|`com.cerebro.domino.computegrid.userExecutionsQuota.userExecutionQueueLimit`
| |`100` | The maximum number of executions that can be queued per user.
If a user tries to queue more than this, the excess executions will fail.


3+|`com.cerebro.domino.computegrid.userExecutionsQuota.globalExecutionQueueLimit`
| |`1000` |The maximum total number of executions that can be queued across all users.
If users try to queue more than this, the excess executions will fail.
|===

[[tr80]]

== Database

These options customize MongoDB connections.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description


3+|`com.cerebro.domino.caching.userPersisterCacheTimeMs`
| |`10000` |Domino recommends consulting your Domino representative before changing this key. Sets the time (in milliseconds) after which the user object is retrieved from the MongoDB rather than from the cache.
3+|`com.cerebro.domino.centralConfig.isEnabled`
| |`true` |Deprecated. Set to `false` to use an external MongoDB specified by the URI in `centralConfig.mongoURI`.
3+|`com.cerebro.domino.centralConfig.mongoCollection`
| |`config` |_Do not change the value of this key_. The name of the MongoDB collection that stores configuration record data set at initial deployment.

3+|`com.cerebro.domino.centralConfig.mongoURI`
| |Empty |Deprecated. The URI for an external MongoDB used to store Domino metadata.

3+|`com.cerebro.domino.database.retry.initialBackoffDuration`
| |`1 second` |Sets the initial backoff duration for any database operation retries that use an exponential backoff algorithm with the MongoDB.

3+|`com.cerebro.domino.database.retry.maxAttempts`
| |`8` |Sets the maximum attempts for MongoDB operation retries with exponential backoff.

3+|`com.cerebro.domino.database.retry.policy`
| |`Exponential` |Indicates whether MongoDB operations will be retried with exponential backoff or not. Values are `Exponential` or `None`.

3+|`com.cerebro.domino.organizations.cache.enable`
| |`true` |Specifies whether the enter organization's Mongo collection is cached in memory to improve performance in the Domino application.

3+|`com.cerebro.domino.organizations.cache.ttlMs`
| |`500` |Specifies the cache lifetime (in milliseconds) for `organizations.cache.enable`.


3+|`mongodb.default.settings.connectionPool.maxWaitQueueSize`
| |`500` |The maximum number of threads allowed to wait for a MongoDB connection.
|===


[[tr85]]

[[data-planes]]
== Data planes

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description

3+|`com.cerebro.domino.hybrid.internalRegistryExternal.host`
| |`123456789012.dkr.ecr.us-west-2.amazonaws.com` |Docker registry host name. Auto populated for hybrid-enabled control planes deployed with version 5.4 and later.

3+|`com.cerebro.domino.computegrid.kubernetes.executor.rabbitMqExternal.host`
| |`acme-rabbitmq.sandbox.domino.tech` |RabbitMQ host name. Used in the data plane install command. Auto populated for hybrid-enabled control planes deployed with version 5.4 and later.

3+|`com.cerebro.domino.computegrid.kubernetes.executor.rabbitMqExternal.port`
| |`5672` |Optional: RabbitMQ port.

3+|`com.cerebro.domino.hybrid.vaultExternal.host`
| |`acme-vault.sandbox.domino.tech` |Vault host name. Used in the data plane install command. Auto populated for hybrid-enabled control planes deployed with version 5.4 and later.
|===



[[tr90]]

[[ds-auth]]
== Data Source authentication

See link:6a8639[Configure Data Source Authentication] for details about how to use these options.
They are available in namespace `common` and must be recorded with no `name`.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description

// from https://dominodatalab.atlassian.net/wiki/spaces/~802751909/pages/2182479954/Architecture+OAuth+for+DataSources#Central-Config-Settings
//Compare with the list at https://github.com/cerebrotech/admin-docs/pull/424/files#diff-991f4099de3e90d765a67e4c220713b18fb1d9ed6e6c5daab54b1e714b7c68ec
3+|`com.cerebro.domino.datasource.ADLSConfig.enabledAuthTypes`
| |`Basic` |A comma-separated string specifying the enabled authentication types for link:06b9bd[ADLS data sources]; only `Basic` is supported.

3+|`com.cerebro.domino.datasource.GCSConfig.enabledAuthTypes`
| |`Basic` |A comma-separated string specifying the enabled authentication types for link:95d78a[GCS data sources]; only `Basic` is supported.

3+|`com.cerebro.domino.datasource.GenericS3Config.enabledAuthTypes`
| |`Basic` |A comma-separated string specifying the enabled authentication types for _generic_ S3 data sources; only `Basic` is supported.  For Amazon S3 data source authentication, see `com.cerebro.domino.datasource.S3Config.enabledAuthTypes`.

3+|`com.cerebro.domino.datasource.MySQLConfig.enabledAuthTypes`
| |`Basic` |A comma-separated string specifying the enabled authentication types for link:69b09a[MySQL data sources]; `Basic` and `AWSIAMRole` are supported.

3+|`com.cerebro.domino.datasource.OracleConfig.enabledAuthTypes`
| |`Basic` |A comma-separated string specifying the enabled authentication types for link:77bf79[Oracle data sources]; only `Basic` is supported.

3+|`com.cerebro.domino.datasource.PostgreSQLConfig.enabledAuthTypes`
| |`Basic` |A comma-separated string specifying the enabled authentication types for link:f095e7[PostgreSQL data sources]; `Basic` and `AWSIAMRole` are supported.

3+|`com.cerebro.domino.datasource.RedshiftConfig.enabledAuthTypes`
| |`Basic` |A comma-separated string specifying the enabled authentication types for link:444abc[Redshift data sources]; `Basic` and `AWSIAMRole` are supported.

3+|`com.cerebro.domino.datasource.S3Config.enabledAuthTypes`
| |`Basic` |A comma-separated string specifying the enabled authentication types for link:947ddd[Amazon S3 data sources]; `Basic` and `AWSIAMRole` are supported.

3+|`com.cerebro.domino.datasource.SnowflakeConfig.enabledAuthTypes`
| |`Basic` |A comma-separated string specifying the enabled authentication types for link:d4ef2b[Snowflake data sources]; `Basic` and `OAuth` are supported.  See link:e3a1a9[Snowflake OAuth] for instructions about setting up Keycloak integration between Domino and Snowflake.

3+|`com.cerebro.domino.datasource.SQLServerConfig.enabledAuthTypes`
| |`Basic` |A comma-separated string that identifies the enabled authentication types for link:634c13[SQLServer data sources]; only `Basic` is supported.
|===






[[tr110]]

== Domino API options

These options relate to link:f35c19[Domino API].

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description

3+|`com.cerebro.domino.api.isEnabled`
| |`true` |When `false`, users cannot generate API keys and existing API keys cannot be used as an authentication method for the Domino API.

3+|`com.cerebro.domino.superuser.apiKey`
| |N/A|Typically set at deployment, the Superuser's API key is used for interactions between Domino components. Contact your Domino representative if you need assistance.

3+|`com.cerebro.domino.superuser.username`
| |N/A|Typically set at deployment, the Superuser's username is used for interactions between Domino components. Contact your Domino representative if you need assistance.

3+|`com.cerebro.domino.public.api.enabled`
| |`true` |When `false`, the Domino Public API is disabled.

3+|`com.cerebro.domino.publicAPI.maxFetchLimit`
| |`1000` |Sets the upper bound for number of objects accessible at once.

|===



[[tr120]]

== Domino Command-Line Interface (CLI)

These options relate to link:30b067[Domino CLI].

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description

3+|`com.cerebro.domino.frontend.clientBlobModeOverride`
| |N/A |Identifies what will handle requests to S3. If set to S3, then the Domino CLI will interact directly with S3. If set to API, then the CLI will interact with the Domino instance, and Domino will then interact with S3.

3+|`com.cerebro.domino.frontend.cliInstallerLocation`
| |`$UserHost/assets/cli/default` |Used to separately host the Domino Command Line Interface (CLI). An example of when this might be used is when a critical fix is needed before the next Domino upgrade.
|===



== Domino Volumes for NetApp ONTAP

These options relate to link:a7d4f1[NetApp Volumes].

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description

3+|`com.cerebro.domino.netApp.canUserDelete`
| |`false` |Determines whether non-admin users can fully delete a NetApp Volume or only mark it for deletion.

3+|`com.cerebro.domino.netApp.maxVolumesPerUser`
| |`5` |The maximum number of NetApp Volumes a user can create.
|===



== Email notifications

[[tr130]]

These options relate to link:5b84c5[email notifications from Domino].
They are available in namespace `common` and must be recorded with no `name`.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description

3+|`com.cerebro.domino.email.notificationFromAddress`
| | N/A|Deprecated. Set this value in Domino's administrator application. To configure the email address from which to get notifications, go to *Admin > Platform settings > Email* and complete the *Notifications FROM Address* field.

3+|`com.cerebro.domino.email.shouldSendWelcomeEmail`
| |`true` |When `true`, new users receive a welcome email from Domino. Email settings must be configured in *Admin > Platform settings > Email*.

3+|`com.cerebro.domino.email.smtp.emptyPassword`
| | `false`|Deprecated. If you want to set SMTP to bypass password authentication, go to *Admin > Platform settings > Email* and select *SMTP*. Then, select the *No Password* check box.

3+|`com.cerebro.domino.email.smtp.emptyUser`
| | `false`|Deprecated. If you want to set SMTP to bypass user authentication, go to *Admin > Platform settings > Email* and select *SMTP*. Then, select the *No Username* check box.

3+|`com.cerebro.domino.email.smtp.transportType`
| |N/A |Deprecated. Go to *Admin > Platform settings > Email* and select the transport type as *SES*, *SMTP*, or *Logging*.


3+|`com.cerebro.domino.supportAlerter.enableEmailSupportAlert`
| |`false` |Enable email notifications for the runs which resulted in errors or warnings.

3+|`com.cerebro.domino.email.smtp.host`
| |None |Hostname of SMTP relay to use for sending emails from Domino.
3+|`com.cerebro.domino.email.smtp.user`
| |None |Username to use for authenticating to the SMTP host.
3+|`com.cerebro.domino.email.smtp.port`
| |`25` |Port to use for connecting to SMTP host.
3+|`com.cerebro.domino.email.smtp.ssl`
| |`false` |Whether the SMTP host uses SSL.

3+|`com.cerebro.domino.supportAlerter.enableEmailSupportAlert`
| |`false` |Enable email notifications for the runs which resulted in errors or warnings.



3+|`com.cerebro.domino.supportAlerter.errorRecipients`
| |None |Comma-separated list of email recipients who should get the error notifications. Needs to be explicitly set if `enableEmailSupportAlert` is set to `true`.
3+|`com.cerebro.domino.supportAlerter.warningRecipients`
| |None |Comma-separated list of email recipients who should get the warning notifications. Needs to be explicitly set if `enableEmailSupportAlert` is set to `true`.


//Outstanding - asked if these can be removed
//3+|`smtp.from`
//| |None |The 'from' address for email notifications sent by Domino.
//3+|`smtp.password`
//| |None |Password for the SMTP user.
|===

[[tr140]]

== Environments

[[tr2]]
These options relate to link:f51038[Domino Environments].
They are available in namespace `common` and must be recorded with no `name`.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description
3+|`com.cerebro.domino.environments.canNonSysAdminsCreateEnvironments`
| |`true` |If set to `false` only system administrators can edit environments.
3+|`com.cerebro.domino.executableTarget.enabled`
| | `false` | If set to `true`, allows users to create custom file handlers.
Custom file handlers override built-in file handlers for supported file types.
3+|`com.cerebro.domino.workbench.restrictedAssets.enabled`
| | `false` | If set to `true`, allows users to create restricted projects and admins to link:0093e8[classify restricted environments].
3+|`com.cerebro.domino.environments.buildPropagation.enabled`
| |`true` |If set to `true`, users will have the option to toggle whether environment revisions should be marked as active, and will have the option to automatically rebuild Environments whenever the active revision of a base environment changes. 

|===


== Feedback

[[tr145]]

These options relate to the Domino Feedback feature.
See link:cc8a91[Send Feedback].

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description
3+|`com.cerebro.domino.frontend.enableFeedbackModal`
| |`true` |If true, and the SMTP server is configured, enables the feedback button in the Domino UI.
3+|`com.cerebro.domino.feedback.settings.sender.name`
| |`Domino Feedback` |Name of the feedback email sender.
3+|`com.cerebro.domino.feedback.settings.sender.address`
| |`feedback@dominodatalab.com` |Email address of the feedback email sender.
3+|`com.cerebro.domino.feedback.settings.recipient.address`
| |`feedback@dominodatalab.com` |Email address of the feedback email recipient.
|===


[[tr150]]

== File download API

These options relate to the `file contents download` link:8c929e#_getProjectFileContents[API endpoint].
They are available in namespace `common` and must be recorded with no `name`.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description
3+|`com.cerebro.domino.restrictBlobApi`
| |`false` |Set to `true` to require an admin API key to download files through API. When `false`, any user with the blob ID for a file might download it through API.
3+|`com.cerebro.domino.frontend.clientBlobModeOverride`
| |None |Set to `API` to download blobs directly in the Domino API. Set to `S3` to download blobs through S3. You cannot set the blob mode override in `site_config.json`.
|===


[[flows]]

== Flows

NOTE: Domino Flows is only supported in Cloud, AWS, and Azure deployments.

[cols="2,2,4",options="header"]
|===
|Key |Default |Description
3+|`com.cerebro.domino.flows.enabled`
| |`True` |Enables link:78acf5[Domino Flows] capabilities.
|===

[[tr160]]


== ImageBuilder

[[tr1]]
These options relate to the Domino ImageBuilder V3.

Use the ImageBuilder to create new environment revision and Domino endpoint version Docker images.
To satisfy requirements around heightened security and support for non-Docker container runtimes (such as cri-o for OpenShift), the ImageBuilder uses an open-source image building engine named https://github.com/moby/buildkit[Buildkit^] and wraps in a suitable fashion for Domino's use.
The ImageBuilder acts as a controller, built around the https://kubernetes.io/docs/concepts/extend-kubernetes/operator/[Kubernetes operator^] pattern in which it acts on custom resources (`ContainerImageBuild`) using standard CRUD actions.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description
3+|`com.cerebro.domino.builder.remoteRegistryCredentials.server`
| |`quay.io` |The external Docker registry URI to pull Domino base images from.
3+|`com.cerebro.domino.builder.remoteRegistryCredentials.additionalServers`
| | None | If you use multiple external registries, a comma-separated list of Docker registry URIs from which to build the Domino base image.
3+|`com.cerebro.domino.builder.remoteRegistryCredentials.secretName`
| |`domino-quay-repos` |The K8s secret containing credentials for authentication to an external Docker registry.
3+|`com.cerebro.domino.builder.remoteRegistryCredentials.secretNamespace`
| |<Domino Compute Namespace> |The namespace where the external Docker registry secret is located.
// com.cerebro.domino.builder.job.environment.imageSizeLimit, com.cerebro.domino.builder.job.model.imageSizeLimit, com.cerebro.domino.builder.job.resource.limits.cpu, and com.cerebro.domino.builder.job.resource.limits.memory and no longer valid in 5.4.
|===


[[tr170]]

[[long-running]]
== Long-running Workspaces

These options relate to long-running link:e6e601[workspace sessions].
They are available in namespace `common` and must be recorded with no `name`.

NOTE: These long-running workspace options are designed to prevent users from starting a workspace and then forgetting to shut it down.
However, these options do not apply to long-running Jobs.
Jobs are intended to run to completion without interruption once started.
This ensures that production runs execute reliably and that Domino resources are used in the most cost-efficient manner.

*Workload notification options:*

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description
3+|`com.cerebro.domino.workloadNotifications.longRunningWorkloadDefinitionInSeconds`
| |
`86400`
|Defines how long a workspace must run in seconds before the workspace is classified as 'long-running' and begins to generate notifications or becomes subject to automatic shutdown.

3+|`com.cerebro.domino.workloadNotifications.isEnabled`
| |`false` |Set to `true` to enable the option for email notifications to users when their workspaces become long-running.
If `com.cerebro.domino.workloadNotifications.isEnabled` is `true` and `com.cerebro.domino.workloadNotifications.isRequired` is `false`, users can turn these notifications on or off in their account settings.
3+|`com.cerebro.domino.workloadNotifications.isRequired`
| |`false` |Set to `true` to turn on long-running workspace notifications for all users. While this is `true` users cannot turn off long-running workspace notifications.
3+|`com.cerebro.domino.workloadNotifications.maximumPeriodInSeconds`
| |`7200` |Maximum time (in seconds) that a user can set as the period between receiving long-running notification emails.

NOTE: This does not change how often users will receive notifications. They receive repeated notifications about long-running workspaces based on the frequency that they set in the application. If your users want to change the frequency of their notifications, provide them the following information: https://tickets.dominodatalab.com/hc/en-us/articles/4413702251284-Auto-Shut-Down-Long-Running-Workspace[Auto Shut Down: Long Running Workspace^].
|===

*Workspace auto shutdown options:*

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description
3+|`com.cerebro.domino.workspaceAutoShutdown.isEnabled`
| |`false` |Set to `true` to enable automatic shutdown of long-running workspaces. Users can turn automatic shutdown for their workspaces on or off from their account settings.
3+|`com.cerebro.domino.workspaceAutoShutdown.isRequired`
| |`false` |Set to `true` to turn on automatic shutdown of long-running workspaces for all users. While this is `true` users cannot turn off automatic shutdown of their long-running workspaces.
3+|`com.cerebro.domino.workspaceAutoShutdown.globalMaximumLifetimeInSeconds`
| |`259200` |Longest time in seconds a long-running workspace will be allowed to continue before automatic shutdown. Users cannot set their automatic shutdown timer to be longer than this.

|===

[[tr180]]

== Domino endpoints

These options relate to link:8dbc91[Domino endpoints].
They are available in namespace `common` and must be recorded with no `name`.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description
3+|`com.cerebro.domino.modelmanager.instances.defaultNumber`
| |`2` |Default number of instances per Domino endpoint used for link:9dece2[scaling].
3+|`com.cerebro.domino.modelmanager.instances.maximumNumber`
| |`32` |Maximum number of instances per Domino endpoint used for link:8dbc91#Scale-the-deployment[scaling].
3+|`com.cerebro.domino.modelManager.nodeSelectorLabelKey`
| |`dominodatalab.com/node-pool` |Key used in https://kubernetes.io/docs/concepts/overview/working-with-objects/labels[Kubernetes label^] node selector for Domino endpoint pods.
3+|`com.cerebro.domino.modelManager.nodeSelectorLabelValue`
| |`default` |Value used in https://kubernetes.io/docs/concepts/overview/working-with-objects/labels[Kubernetes label^] node selector for Domino endpoint pods.
3+|`com.cerebro.domino.modelmanager.uWsgi.workerCount`
| |`1` |The uWSGI worker count. This link:9dece2[scales all Python Domino endpoints] by setting the degree of parallelism.
3+|`com.cerebro.domino.modelmanager.harnessProxy.maxBodyLogLengthInBytes`
| |`1024` | The maximum size, after truncation, of the JSON representation of Domino endpoint requests and responses that are written to stdout.
3+|`com.cerebro.domino.modelmanager.harnessProxy.clientMaxBodySizeMiB`
| |`10` | Sets the http://nginx.org/en/docs/http/ngx_http_core_module.html#client_max_body_size[client_max_body_size^] property for the Nginx reverse proxy in Domino endpoint pods.
|===


[[tr190]]

[[model-monitoring]]
== Model Monitoring

These options customize how prediction data is captured for link:715969[monitoring]:

[[tr200]]

[[tr3]]
// Admin configures retention of parquet files

[[tr4]]
// Admin configures parquet conversion job settings

=== Data retention and deletion options

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description
3+|`com.cerebro.domino.modelManager.enableDominoModelMonitoringForModelAPIs`
| |`true` |When `false`, this disables Model Monitoring features in Domino and overrides the feature flag `enableModelMonitoringForModelAPIs`.
3+|`domino.parquet.cleanup_job.retention_days`
| |30 |Retention of the parquet files (in number of days) before they get deleted to free up space.
3+|`domino.parquet.conversion_job.autodelete_key`
| |autodelete |Key of the {key: value} pair used to select a file for auto-deletion
3+|`domino.parquet.conversion_job.autodelete_value`
| |TRUE |Value of the {key: value} pair used to select a file for auto-deletion
3+|`domino.parquet.conversion_job.raw_data_debug_grace_days`
| |1 |Grace period to keep the source raw log files post processing
|===

[[tr210]]


[[tr5]]
// Admin configures Domino endpoint-specific options for model monitoring

=== Domino endpoint-specific options

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description
3+|`com.cerebro.domino.modelmanager.pvc.name`
| |shared-$stage-compute (Same as domino filecache) |PVC name for storing prediction data.
3+|`com.cerebro.domino.modelmanager.pvc.mountPoint`
| |/domino/shared |PVC mount point for storing prediction data.
3+|`com.cerebro.domino.modelmanager.pvc.subdir`
| |scratch |PVC sub mount point.
3+|`com.cerebro.domino.modelmanager.fluentBit.image`
| |Supplied from Domino Charts |Fluent-bit image.
3+|`com.cerebro.domino.modelmanager.logrotate.image`
| |Supplied from Domino Charts |Logrotate image.
3+|`com.cerebro.domino.modelmanager.ingress.tls.secret`
| |Supplied from Domino Charts |The secret for the ingress route for Domino endpoint publishing.

TIP: The value is `<secretname>` and not `<namespace>/<secretname>`.

3+|`com.cerebro.domino.modelApis.async.maxMsgSizeInBytes`
| |10Kb |This property determines the maximum size of message in input and output queues. Therefore this property restricts the size of prediction input payload as well as inference output from model.
3+|`com.cerebro.domino.modelApis.async.mongoDb.minimumAvailableBytes`
| |500000000 |Minimum available MongoDB disk storage for asynchronous Domino endpoints.  See link:bfd8e5[Asynchronous Domino Endpoints Capacity Planning] for more information.
3+|`com.cerebro.domino.modelApis.async.rabbitmq.availableAsyncLimitBytes`
| |1000000000 |Minimum available RabbitMQ disk storage for asynchronous Domino endpoints.  See link:bfd8e5[Asynchronous Domino Endpoints Capacity Planning] for more information.

|===


[[tr220]]

[[tr6]]
// Admin configures Cohort Analysis options

[[cohort-analysis-options]]
=== Cohort Analysis options

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description
3+|`com.cerebro.domino.actionable.insights.project.name`
| |DominoActionableInsights |The name of the project for Actionable Insights.
3+|`com.cerebro.domino.actionable.insights.dataset.name`
| |DominoActionableInsightsDataset |The name of the dataset for Actionable Insights.
3+|`com.cerebro.domino.actionable.insights.environment.id`
| | |Environment ID for the Actionable Insights Job. If not defined
3+|`com.cerebro.domino.actionable.insights.compute.environment.id`
| | |Environment ID for the Actionable Insights Spark Cluster. If not defined
3+|`com.cerebro.domino.actionable.insights.hardware.tier.id`
| |small-k8s |Hardware Tier ID for the Actionable Insights Job.
3+|`com.cerebro.domino.actionable.insights.master.hardware.tier.id`
| |medium-k8s |Hardware Tier ID for the Actionable Insights Spark Master.
3+|`com.cerebro.domino.actionable.insights.worker.hardware.tier.id`
| |medium-k8s |Hardware Tier ID for the Actionable Insights Spark Workers.
3+|`com.cerebro.domino.actionable.insights.worker.count`
| |2 |Number of workers for the Spark cluster.
|===



[[tr230]]

== Multi-storage support for Datasets

These options relate to link:4f5b2a[Multi-Storage Support for Datasets].
They are available in namespace `common` and must be recorded with no `name`. _Do not change these configurations unless you have reached out to your field team and have been instructed to do so._

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description
3+|`com.cerebro.domino.datacache.pvc.names`
| |`domino-shared-store-domino-compute` |This is a comma-separated list of all compute-PVC names used for dataset/snapshot storage at Domino. Note that if this is set, it is crucial that the original PVC name is also included in the list.
3+|`com.cerebro.domino.datacache.pvc.primaryName`
| |The first value in the array above |The compute-PVC name of the volume in which to store every next dataset and snapshot.
3+|`com.cerebro.domino.datacache.pvc.originalName`
| |shared-$stage-compute (same as domino filecache) |The compute-PVC name corresponding to the original Domino storage.
3+|`com.cerebro.domino.datacache.pvc.<COMPUTE-PVC-NAME>.mountPoint`
| |`/domino/shared` |For each PVC name specified in the values of `com.cerebro.domino.datacache.pvc.names`, specify the compute-PVC name in the key, and mount point at which the PVC is mounted in the `nucleus-*` deployments in the value.
|===
IMPORTANT: All the PVC names specified in the settings above must correspond to the PVC in the compute namespace.



[[tr240]]

== Notifications

The `ShortLived.EnableUserNotifications` feature flag enables the Notifications feature.
This means that it shows the following:

* Notifications page for Administrators where they can create and manage notifications.
* Notifications icon and indicator to identify the criticality of the notifications in the navigation pane.
* Notifications page where users can view their notifications.

If this flag is turned off, all these items are hidden.

See link:b889e6[Event Notifications] in the _User Guide_ and link:ba9786[Notifications] in this _Admin Guide_.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description
3+|`com.cerebro.domino.userNotifications.dbCleanup.jobFrequency`
||`10 minutes` |Frequency with which notifications will be checked for automatic expiry (`dbCleanup.expirationEnabled`) or deletion (`dbCleanup.deletionEnabled`).
3+|`com.cerebro.domino.userNotifications.dbCleanup.expirationEnabled`
||`true`|Enables the job that expires notifications. Notifications without a set end time are expired based on the setting in `com.cerebro.domino.userNotifications.dbCleanup.expirationThreshold`.
3+|`com.cerebro.domino.userNotifications.dbCleanup.expirationThreshold`
||`30 days` |Sets an expiration time (in days) for notifications without an end date.
3+|`com.cerebro.domino.userNotifications.dbCleanup.deletionThreshold`
||`30 days` |Specifies the time (in days) after which expired notifications will be deleted.
3+|`com.cerebro.domino.userNotifications.limit`
||`5000`|Specifies the maximum number of notifications allowed in the system.
3+|`com.cerebro.domino.userNotifications.telemetry.enabled`
||`true`|Enables backend telemetry (statistics about the number and type of generated notifications) for notifications.
3+|`com.cerebro.domino.userNotifications.telemetry.initialDelay`
||`2 minutes`|The delay before Notifications telemetry is executed the first time. This delays the impact on database processing during initial system startup.
3+|`com.cerebro.domino.userNotifications.telemetry.interval`
||`2 hours`|The time between when the notification statistics are updated.
3+|`com.cerebro.domino.userNotifications.telemetry.perUserMetricsEnabled`
||`true`|If true, the system shows metrics for each user about the number and types of notifications generated. If false, the system shows metrics about all notifications.
3+|`com.cerebro.domino.notification.maxFilesToAttach`
||`10`|The maximum number of files that can be attached to a single notification.
3+|`com.cerebro.domino.notification.maxFileSizeToAttachInMB`
||`2`|The maximum size, in megabytes, of any single file attached to a notification.
|===



[[tr250]]

== Notifications for monitoring

The options relate to link:386451[Notification channels].

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description
3+|`com.cerebro.domino.email.smtp.from`
||N/A |The email address from which Domino sends email notifications.

3+|`com.cerebro.domino.email.smtp.host`
||N/A|The host address of the SMTP server from which Domino sends emails.

3+|`com.cerebro.domino.email.smtp.password`
||N/A|The password for the SMTP server, which is typically the same password for your web server, from which Domino sends emails.

3+|`com.cerebro.domino.email.smtp.port`
||25|The TCP port to use to communicate with your SMTP server.

3+|`com.cerebro.domino.email.smtp.ssl`
||`false` |Indicates whether the SMTP server uses Secure Sockets Layer (SSL) for secure communications.

3+|`com.cerebro.domino.email.smtp.user`
||N/A |The username used by the client to authenticate to the SMTP server to send email.

|===


[[tr260]]

== On-demand MPI

The options relate to the link:d60880[on-demand MPI clusters].
They are available in namespace `common` and must be recorded with no `name`.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description

3+|`com.cerebro.domino.computegrid.computeCluster.checkClusterStatusIntervalSeconds`
||`1.0` |Frequency in seconds to run status checks on on-demand MPI clusters.

3+|`com.cerebro.domino.computegrid.timeouts.executor.responseTimeout`
||`1.0` |How long the frontend waits for a response, in seconds, after a file sync request before sending an error.

3+|`com.cerebro.domino.computegrid.timeouts.computeCluster.fileSyncTimeout`
||`1 hour` |The maximum duration a sync runs before being considered to have timed out.

3+|`com.cerebro.domino.computegrid.computeCluster.checkFileSyncStatusIntervalSeconds`
||`15` |The interval, in seconds, the Job launcher script checks the compute cluster file sync status waiting for ready status.

3+|`com.cerebro.domino.computegrid.computeCluster.secretName`
||`domino-compute-cluster` |The name of the secret in the domino-compute namespace containing the SSH key material used when configuring SSH on MPI workers.

3+|`com.cerebro.domino.computegrid.computeCluster.storageMountPath`
||`/tmp` |Volume mount path location of additional storage for the compute cluster.

3+|`com.cerebro.domino.computegrid.computeCluster.mpi.disableIstio`
||`false` |Whether to inject the Istio Proxy sidecar into worker Pods.

3+|`com.cerebro.domino.computegrid.computeCluster.mpi.istioMutualTLSMode`
|||Configures the `istioMutualTLSMode` for MPI if Istio is enabled. Valid values are: `STRICT` and `PERMISSIVE`.

|===

[[tr270]]

== On-demand Spark

These options relate to the link:482ec5[on-demand Spark clusters].
They are available in namespace `common` and must be recorded with no `name`.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description
3+|`com.cerebro.domino.integrations.spark.checkClusterStatusIntervalSeconds`
| |`1` |Frequency in seconds to run status checks on on-demand Spark clusters.
3+|`com.cerebro.domino.integrations.spark.onDemand.workerStorageMountPath`
| |`/tmp` |File system path on which Spark worker storage is mounted.
3+|`com.cerebro.domino.integrations.spark.sparkConfDirDefault`
| |None |Option to supply alternative default configuration directory for on-demand Spark clusters.
3+|`com.cerebro.domino.workbench.onDemandSpark.worker.memoryOverheadMinMiB`
| |`384` |Minimum amount of memory in MiB to use for Spark worker overhead.
3+|`com.cerebro.domino.workbench.onDemandSpark.worker.memoryOverheadFactor`
| |`0.1` |Spark worker overhead scaling factor.
3+|`com.cerebro.domino.computegrid.computeCluster.spark.proxyCompatability`
| |None |Set to `legacy` when the Spark UI for on-demand Spark on Domino needs to be compatible with Spark versions prior to 3.1.1.
|===

[[tr280]]

[[performance]]
== Performance

The following configuration settings are used for caching.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description

3+|`com.cerebro.domino.controlCenter.cacheMaxLoadPeriodInMonths`
||`12`|Use this key to modify the period (in months) of historical data that the link:d5d2ac[Control Center] uses. You might have to change this value to less than `12` to prevent timeout issues when loading data into the application. Do not set `cacheMaxLoadPeriodInMonths` to greater than `12` months or the chart cannot be used. The report options remain the same: Current Month, Previous Month, Last 3 months, Last 6 months, and Custom. This might improve performance.

CAUTION: Contact a Customer Success Manager if you want to change this value.
3+|`com.cerebro.domino.controlCenter.cacheTimeToLiveInMinutes`
||`30`|Specifies how often the cache is refreshed in minutes. This cache is used in the Control Center and improves performance. However, if the cache is refreshed every 30 minutes some recent data will not be included in the reports.

|===

[[tr290]]

[[projects]]
== Projects

This option is available in namespace `common` and must be recorded with no `name`.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description
3+|`com.cerebro.domino.frontend.overrideDefaultProject`
| | |String of comma-separated project paths. For example,
`admin-user/getting-started-project, admin-user/sample-app-project`.
See link:bb077c[Change The Default Project For New Users].
3+|`com.cerebro.domino.frontend.defaultMaxFileSizeToRenderInBytes`
| | |String, indicating the biggest file that may be rendered in the filebrowser: 5&nbsp;MB, 10&nbsp;kB, 1&nbsp;GB, 7&nbsp;B
3+|`com.cerebro.domino.frontend.configuredMaxFileSizes`
| | |Specify the largest file size that can be rendered in the file browser for various file types. 

`com.cerebro.domino.frontend.configuredMaxFileSizes` overrides the default size limits set by `com.cerebro.domino.frontend.defaultMaxFileSizeToRenderInBytes`. 

The CC Flag expects a JSON string with the following format: `{"fileType1": fileSizeInBytes, "fileType2": fileSizeInBytes, "fileTypeN": fileSizeInBytes}`

For example, to set the max render size for PDFs to 1MB and CSVs to 5MB: 

`{"pdf": 1048576, "csv": 5242880}`

Make sure that your JSON is valid before inserting values.  
3+|`com.cerebro.domino.workbench.project.projectCopy.githubCopyEnabled`
| |`false` |`true` to enable copying GitHub-backed projects.
3+|`com.cerebro.domino.workbench.project.projectCopy.gitlabCopyEnabled`
| |`false` |`true` to enable copying GitLab-backed projects.
|===

=== Project templates

These options enable Git-based project copy for the specified git providers.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description

3+|`com.cerebro.domino.workbench.project.projectCopy.githubCopyEnabled`
| |`true`
|Enable copy project for a Git-based project created from a Github repository.

3+|`com.cerebro.domino.workbench.project.projectCopy.gitlabCopyEnabled`
| |`true`
|Enable copy project for a Git-based project created from a Gitlab repository.

|===

=== File uploads

These options can be used to configure file uploads to the projects.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description

3+|`com.cerebro.domino.upload.maxUploadFileSizeInMegabytes`
| |`550`
|Maximum size (MB) file that can be uploaded.

3+|`com.cerebro.domino.upload.maxUploadFilesCount`
| |`1000`
|Maximum number of files that can be uploaded.

3+|`com.cerebro.domino.upload.restrictedFileTypes`
| |`""` (empty string)
|String, comma separated list of file extensions that cannot be uploaded, e.g., `php`, `exe`, `asp`.

|===

[[ai-hub]]
=== AI Hub

These options configure different aspects about the AI Hub.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description

3+|`com.cerebro.domino.workbench.project.projectTemplateHubEnabled`
| |`true`
|Enables the AI Hub in the UI and activates the corresponding public API.

|===

=== Project visibility options

These options relate to link:71afc6[project visibility settings].
They are available in namespace `common` and must be recorded with no `name`.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description
3+|`com.cerebro.domino.publicProjects.enabled`
| |`true` |If set to `false`, Practitioners cannot set projects to link:71afc6#tr1[public visibility]. However, an Admin can still create public projects. A change in this setting does not affect existing projects.
3+|`com.cerebro.domino.defaultProjectVisibility`
| |`Public` |Controls the default link:71afc6#tr1[visibility setting] for new projects. Options are `Public` or `Private`.
|===

[[project-sizes]]
=== Project sizes

The Project size can be used to determine the persistent volume capacity used by Project files, either as an auditing mechanism or to estimate volume sizes for Jobs or Workspaces.

The following options control the background scheduled process that computes Project sizes.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description
3+|`com.cerebro.domino.projectSizeScheduledService.isEnabled`
| |`false` |Enable/disable the scheduled service that computes projects sizes.
3+|`com.cerebro.domino.projectSizeScheduledService.runDelayInMinutes`
| |`120` |Defines a fixed delay (in minutes) between each execution of the service. A longer delay prolongs the time it takes for changes in Project files to impact size estimates, while a shorter delay keeps the estimates more up-to-date but consumes additional resources.
3+|`com.cerebro.domino.projectSizeScheduledService.batchSize`
| |`20` |Defines the maximum number of Projects the sizing services loads into memory simultaneously. The sizing  service only loads Project metadata, not files. Increasing this value will demand more heap memory from the service to handle Project sizing.
3+|`com.cerebro.domino.projectSizeScheduledService.maxBatchesPerRun`
| |`Int.MaxValue (2,147,483,647)` |Defines the maximum number of Project batches to process in each scheduled execution of the sizing service. This can be used to shorten the execution time of the service, by processing less Projects on each run.
|===

The following option controls the visibility of the Project size information in the Admin portal.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description
3+|`com.cerebro.domino.frontend.admin.projects.sizeColumnEnabled`
| |`false` |Show/hide the Project size column in the *Admin > Projects* report.
|===

=== Git and Jira credentials

These options enable storing Git and Jira credentials in Vault.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description
3+|`com.cerebro.domino.jira.vault.enabled`
| |`true` |Consult your Domino representative before changing this key. If set to `true`, Jira credentials are stored in Vault. If `false`, they are stored in Mongo.
3+|`com.cerebro.domino.workbench.project.vaultGitCredentials`
| |`true` |Consult your Domino representative before changing this key. If set to `true`, user Git credentials are stored in Vault. If `false`, they are stored in S3's blob store.
|===


[[tr300]]

== Public applications

This option is related to
link:71635d[Grant Access to Domino Apps].
This is available in namespace `common` and must be recorded with no `name`.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description
3+|`com.cerebro.domino.launchpad.allowPublicModelProducts`
| |TRUE |Set to FALSE to disable the *Anyone, including anonymous users* and *Anyone with an account* access permissions. See link:71635d[Grant Access to Domino Apps] for more information about these permissions.
|===



== Restricted Environments and Projects

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description
3+|`com.cerebro.domino.workbench.restrictedAssets.enabled`
| |FALSE |Set to TRUE to enable marking environments and projects as restricted.
|===


[[tr310]]

== Read-write Datasets

These options relate to link:0a8d11[read-write datasets].
They are available in namespace `common` and must be recorded with no `name`.
Scratch spaces have been deprecated starting with Domino 4.5.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description
3+|`com.cerebro.domino.dataset.graceTimeForDeletion`
| |`15min` |The time before the system deletes a dataset that was marked for deletion. If you deleted a dataset, you have this time to retrieve the dataset. After this time expires, the dataset cannot be recovered. See link:ba5bad[Datasets and Snapshots].
3+|`com.cerebro.domino.dataset.maxFileListingLength`
| |`1000` |The maximum number of files shown in the dataset snapshot file viewer
3+|`com.cerebro.domino.dataset.fileCacheTimeout`
| |`2s` |The timeout for fetching files in the dataset snapshot file viewer
3+|`com.cerebro.domino.dataset.quota.enabled`
| |`true`|If `true`, the dataset (`com.cerebro.domino.dataset.quota.maxActiveSnapshotsPerDataset`) and snapshot (`com.cerebro.domino.dataset.quota.maxDatasetsPerProject`) limits will be enforced. If `false`, these settings are ignored.
3+|`com.cerebro.domino.dataset.quota.maxActiveSnapshotsPerDataset`
| |`20` |The maximum number of snapshots a user can create for a dataset. `com.cerebro.domino.dataset.quota.enabled` must be `true` for this to work.

If the user reaches the maximum number of snapshots, the next time they create a snapshot, Domino shows a warning that they have reached their snapshot limit and that if they proceed, their oldest snapshot will be marked for deletion.

See link:dbdbff[Create a Snapshot of a Dataset] and link:ba5bad[Datasets and Snapshots].

3+|`com.cerebro.domino.dataset.quota.maxDatasetsPerProject`
| |`50` |The maximum number of Datasets you can create in a Project. `com.cerebro.domino.dataset.quota.enabled` must be `true` for this to work.

If the user reaches the maximum number of datasets, Domino shows a message about the limit.

3+|`com.cerebro.domino.dataset.quota.maxFileSizeForPreview`
| |`5000000` |The maximum file size (in bytes) that the Data renderer will support to preview files. If a file is larger than this limit, the renderer will default to a message recommending file download.

See link:ba5bad[Datasets and Snapshots].
//Removed as per DOM-35306 per Ronit comments in spreadsheet
//3+|`com.cerebro.domino.dataset.datasetTelemetryReportingPeriod`
//| |`2 minutes` |Mandates interval at which telemetry reporter reports periodic metrics.
//Removed as per DOM-35306 per Ronit comments in spreadsheet
//.3+|`com.cerebro.domino.dataset.datasetTelemetryReportingInitialDelay`
//| |`1 day` |Mandates initial delay for telemetry reporter to start reporting periodic dataset metrics.
3+|`com.cerebro.domino.dataset.containerHome`
| |`/domino/datasets` |Set the path to mount datasets in Domino projects. Users see this path in the Path column on the Domino Datasets tab on the Data page.

NOTE: This is not for git-based projects.

TIP: Add '/' to the beginning of the path to make it clear that it is an absolute path.

See link:0a8d11[Domino Datasets].
3+|`com.cerebro.domino.dataset.batchDownloadArchiveFormat`
| | `zip` |The file format of the created archive file when downloading multiple files and/or folders, including downloading snapshots. `zip` and `tar` are supported formats.
3+|`com.cerebro.domino.dataset.gitBasedContainerHome`
| |`/mnt` |Path at which datasets reside in git-based projects.

TIP: Add '/' to the beginning of the path to make it clear that it is an absolute path.

3+|`com.cerebro.domino.dataset.executor.snapshotSizeTimeout`
| |1 minute |The time allotted to gather all file sizes to calculate the size of the snapshot. If the time expires and the size hasn't finished calculating, Domino shows the current calculation for the snapshot but doesn't notify the user that the calculation is incomplete.
3+|`com.cerebro.domino.dataset.storageUsageWarningThreshold`
| |`70` |The percentage of a user's dataset storage quota that, when reached, triggers warning notifications.
3+|`com.cerebro.domino.dataset.storageUsageCriticalThreshold`
| |`85` |The percentage of a user's dataset storage quota that, when reached, triggers email notifications.
3+|`com.cerebro.domino.dataset.snapshotSizingPeriod`
| |`7 days` |Interval during which the size of a snapshot are not recalculated.
3+|`com.cerebro.domino.dataset.thresholdActionPeriod`
| |`7 days` |Interval during which notifications to users about their storage usage are not repeated.
3+|`com.cerebro.domino.dataset.unitCostDollarsPerGbPerMonth`
| |`None` |Estimate unit cost of a dataset (in dollars/GB/month). This value is multiplied linearly by the size of a dataset to estimate its cost per month.

|===

== Remote Connection to Workspaces via SSH

These options relate to remote connection to user workspaces via SSH (provided that the feature is enabled in the cluster).

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description
3+|`com.cerebro.domino.workbench.workspace.ssh.enabled`
| |`false` |
Enables the UI that displays the checkbox to enable SSH in a Workspace and to display other SSH-related information to the user in the UI.
3+|`com.cerebro.domino.workbench.workspace.ssh.defaultValueOnFirstLaunch`
| |`false` |
Sets the default value for the checkbox in the Create Workspace modal in the UI.

|===

== Run Results

These options relate to the link:e1c37a[Execution Results].
They are used to limit the number of file comparisons and number of differences found.

[NOTE]
====
If too many files are compared, you might have to increase the request timeout.
See https://tickets.dominodatalab.com/hc/en-us/articles/10595248082580[Increase Request Timeout to Compare Jobs^].
====

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description

3+|`com.cerebro.domino.runResults.maximumNumberOfInputComparisons`
| |`1000` |
The maximum number of input files to compare.
3+|`com.cerebro.domino.runResults.maximumNumberOfInputDiffs`
| |`250` |
The maximum number of input file comparisons that will be found.
If this value is reached, the comparison will stop.
3+|`com.cerebro.domino.runResults.maximumNumberOfResultsComparisons`
| |`1000` |
The maximum number of result files to compare.
3+|`com.cerebro.domino.runResults.maximumNumberOfResultsDiffs`
| |`250` |
The maximum number of result file comparisons that will be found.
If this value is reached, the comparison will stop.

|===

== Starburst
These options relate to using link:ae2e6b#starburst[Starburst-powered] Data Sources.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description

3+|`com.cerebro.domino.datasource.starburstJdbc.enabledConnectors`
| |`db2,netezza,sap_hana` |
A comma-separated list of link:f32f69[Starburst JDBC] data entities available to the user.
The full list includes: `clickhouse,druid,db2,synapse,greenplum,ignite,mariadb,singlestore,vertica,generic_jdbc,sap_hana,netezza`.
|===

== System log
[[tr320]]
These options relate to the link:e34be1[system log].
They are available in namespace `common` and must be recorded with no `name`.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description
3+|`com.cerebro.domino.audit.enabled`
| |`true` |Enables Domino to store audit events and shows the *Audit Log* button. See link:e34be1[Download the audit log]. Domino recommends that you do not change this value unless you must disable auditing.
3+|`com.cerebro.domino.audit.pagination.page.size`
| |`100` |Page size for the number of records fetched from MongoDB.
3+|`com.cerebro.domino.audit.pdf.generation.memory.limit`
| |`25` |The maximum memory limit used by PDF generation. If the memory usage is greater than 25 MB, temporal files are used.
3+|`com.cerebro.domino.requireESignatureWorkflow`
| |`Disabled` |There are three valid values for this Configuration: `Global`, `PerProject`, and `Disabled`.

`Global` enables the e-signature workflow across the whole deployment. Critical actions, regardless of project or user, will require an e-signature to complete.

`PerProject` enables a new project setting called `Require E-Signature Workflow`. When checked, critical actions within the project will require the e-signature workflow.

`Disabled` means that no e-signature is required for any Domino actions.
3+|`com.cerebro.domino.eSignatureActionOptions`
| |See list in Description|Allows the admin to configure the values seen in the *Reason for change* dropdown during the e-signature workflow. By default it is set to `Data obsolescence`, `Compliance`, `Business decision`, `Technical reason`, and `Other`.

To enter custom values, use a comma and space to separate entries, such as `Data obsolescence, Compliance, Business decision`. The default values will be overwritten as soon as custom text is entered.
|===


== Usage reports

These options relate to the link:6e50e8[User Activity Reports].

// tag::usage-reports[]
[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description

3+|`com.cerebro.domino.email.usageReportRecipient`
| |`usage@dominodatalab.com` |Sets the default recipient for the user activity report. To access this report, go to *Admin* > *Advanced* > *User Activity Report*.

3+|`com.cerebro.domino.email.EmailToDomino`
| |`true` |When `true`, the system sends a scheduled user activity report to usage@dominodatalab.com to help improve Domino.

3+|`com.cerebro.domino.Usage.RecentUsageDays`
| |`30` |Specifies the number of days to report for recent activity in the user activity reports. For example, the default value includes activity within the past 30 days in the Recent Activity section.

IMPORTANT: The cron entry is set in UTC.

3+|`com.cerebro.domino.Usage.ReportFrequency`
| |`0 0 2 * * ?` |Defines the frequency for automatically scheduled user activity reports. The default cron string value is set to daily at 02:00.

NOTE: This value must be a QUARTZ cron string expression. See the http://www.quartz-scheduler.org/documentation/quartz-2.3.0/tutorials/crontrigger.html[Cron Quartz tutorial^].

3+|`com.cerebro.domino.Usage.ReportRecipients`
| |Empty |Identifies a comma-separated list of email addresses that receive automatic scheduled user activity reports. This is not shown in the Configuration record unless it is set explicitly. Example values are: `email1@domain.com`, `email2@domain.com`.

3+|`com.cerebro.domino.Usage.IncludeUsernameAndEmail`
| |`false` |When `true`, automatically generated and emailed user activity reports include username and email address columns.
|===
// end::usage-reports[]


[[tr330]]

== Vault

In Domino, secrets are stored in an instance of HashiCorp Vault.
By default, Vault does not require any configuration for specific secrets to be stored in encrypted form at rest.
Supported Secrets are:

* User environment variables
* User API keys
* Data source access secrets
* Project environment variables

The following configuration settings are used to connect to Vault.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description

3+|`com.cerebro.domino.vault.auth.token.tokenFile`
| |N/A |*Beta feature:* Contact your Domino representative for assistance. Used to configure Domino to work with your Vault installation outside the Domino cluster.

This is the path where the Vault token is present. If the .token config key is present, this is ignored.

3+|`com.cerebro.domino.vault.auth.token.token`
| |N/A |*Beta feature:* Contact your Domino representative for assistance. Used to configure Domino to work with your Vault installation outside the Domino cluster.

This is the literal value of the Vault token that overrides the .tokenFile config key.
3+|`com.cerebro.domino.vault.auth.token.refreshEvery`
| |N/A |*Beta feature:* Contact your Domino representative for assistance. Used to configure Domino to work with your Vault installation outside the Domino cluster.

Specifies how often to reread the token when configuring an external Vault integration. This setting is only useful when the token is configured with tokenFile. Example values are: 2s, 10m, 1h. See https://github.com/lightbend/config/blob/master/HOCON.md#duration-format[duration format^] for syntax information.

3+|`com.cerebro.domino.vault.secretstore.baseUrl`
| |N/A |*Beta feature:* Contact your Domino representative for assistance. Used to configure Domino to work with your Vault installation outside the Domino cluster.

The URL with port for the Vault's API endpoint which is used to configure the external Vault integration.

3+|`com.cerebro.domino.vault.secretsengine.kv2.basePath`
| |`domino/kv` |*Beta feature:* Contact your Domino representative for assistance. Used to configure Domino to work with your Vault installation outside the Domino cluster.

The path in the Vault to the key-value store that Domino uses.

3+|`com.cerebro.domino.vault.secretsengine.kv2.subPath`
| |`nucleus` |*Beta feature:* Contact your Domino representative for assistance. Used to configure Domino to work with your Vault installation outside the Domino cluster.

An optional path in the key-value store that serves as the root for all Domino-stored secrets.

|===


[[tr340]]

== Web Apps

*IFrame Security*

Web apps in Domino are served in https://developer.mozilla.org/en-US/docs/Web/HTML/Element/iframe[HTML inline frames^], also known as “iframes”.
To improve iframe security, a “sandbox” attribute can be set for iframe elements.
When this attribute is set, extra security restrictions are applied to the iframes serving web apps in Domino, like blocking cross-origin requests, form submissions, script executions, and much more.

In Domino, this “sandbox” attribute can be toggled with the `ShortLived.iFrameSecurityEnabled` feature flag.
Setting this flag to “TRUE” will apply the sandbox attribute to the iframe and apply the extra security restrictions.
If the flag is set to “FALSE”, no security restrictions will be applied to the iframe.
By default, in Domino 4.4.1 the `ShortLived.iFrameSecurityEnabled` flag is set to *FALSE*.

IMPORTANT: This feature flag will be deprecated in future versions of Domino.
Domino recommends implementing web app security using content security policies instead (described below).

*Content Security Policies*

A content security policy allows Domino web apps to access specific, whitelisted external resources.
Any request made to non-whitelisted external resources, however, will be blocked.

In Domino, you can toggle this feature with the `EnableContentSecurityPolicyforApps` feature flag.
Setting this flag to “TRUE” will block requests to all non-whitelisted resources and allow requests to whitelisted resources.
Setting this flag to “FALSE” will allow all requests to resources (that is, no blocking of any kind).
By default, in Domino 4.4.1 the `EnableContentSecurityPolicyforApps` is set to *FALSE*.

The keys and default values associated with this feature flag are listed in the table below.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description
3+|`com.cerebro.domino.apps.contentSecurityPolicy.whiteListAppHost`
| |`true` |Whitelist the app's own host URL for all resource types.
This can be toggled to `false` for an even more stringent security policy.
For example, to prevent internal attacks by disabling all JavaScript in web apps, you can set this flag to `false` and `com.cerebro.domino.apps.contentSecurityPolicy.whiteListedScriptSrcList` to `none` or the empty string.
3+|`com.cerebro.domino.apps.contentSecurityPolicy.whiteListedImageSrcList`
| |`data:` |Allows images to be inserted directly into a web app using a `data:` URL. This allows Domino Apps to include images in the app's HTML without loading the image from an outside resource. Learn more about `data:` URLs here: https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/Data_URIs[Mozilla - Data URLs^].
3+|`com.cerebro.domino.apps.contentSecurityPolicy.whiteListedScriptSrcList`
| |`cdnjs.cloudflare.com/ajax/libs/jquery/3.1.0/jquery.js` `cdn.plot.ly/plotly-latest.min.js` `'unsafe-eval' 'unsafe-inline'` |Whitelists the URLs of the scripts that the demo Apps in the `quick-start` project load to display their interactive charts. It also allows an app to define scripts in HTML using the `<script>` tag. Also allows JavaScript to create more JavaScript as the app runs using the built-in JavaScript function `eval`.
3+|`com.cerebro.domino.apps.contentSecurityPolicy.whiteListedStyleSrcList`
| |`'unsafe-inline'` |Allows apps to define their own styles with `<style>`, `javascript:` URLs, and inline `<script>` elements.
3+|`com.cerebro.domino.apps.contentSecurityPolicy.whiteListedConnectSrcList`
| |`ws:` |Allows the app to use WebSockets, which use URLs that begin with `ws:`, to communicate with other resources.
|===

To whitelist a resource:

. Go to *Configuration Management* (that is, *Configuration records*) in your Domino admin settings.
. Click *Add Record*.
. Set the key to `com.cerebro.domino.apps.contentSecurityPolicy.whiteListedConnectSrcList`.
. Set the value to `ws:` followed by the URL of the resource you'd like to whitelist (that is, `ws: https://foobar.buz.bax/`).
You must work with your team to figure out which URLs have to be whitelisted.
For more details, see: link:aec000[Identify Resources to Whitelist].
. Save the record and restart Domino services.

*IFrame Security in combination with Content Security Policies*

In Domino 4.4.1, the `ShortLived.iFrameSecurityEnabled` and `EnableContentSecurityPolicyforApps` feature flags coexist.
The matrix below describes the blocking behavior for requests based on both feature flags.

[[tr350]]

[IMPORTANT]
====
The IFrame feature flag will be deprecated in future versions of Domino.
Domino recommends implementing web app security using content security policies instead.
====

[cols="3a,3a,3a",options="header"]
|===
| |ShortLived.iFrameSecurityEnabled = FALSE |ShortLived.iFrameSecurityEnabled = TRUE
|*EnableContent* *SecurityPolicyForApps = FALSE* |No blocking occurs. All requests to external resources are allowed. |All requests from web apps to external resources are blocked.
|*EnableContent* *SecurityPolicyForApps = TRUE* |Only requests to whitelisted external resources are allowed. All other requests to external resources are blocked. |All requests from web apps to external resources are blocked.
|===

*Multiple apps per Project*

Domino 6.1.0 introduces support for multiple apps within a single Project. The following configuration options allow administrators to set limits on creating and running multiple apps per Project.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description
3+|`com.cerebro.domino.apps.maxAppsPerProject`
| |`10` |The maximum number of apps that can be created in a Project. Users cannot create a new app in the Project once this limit is reached.
3+|`com.cerebro.domino.apps.maxActiveAppRunsPerProject`
| |`4` |The maximum number of active app runs allowed in a Project. Users cannot start an app in the Project once this limit is reached.
|===

[[whitelabel]]
== White labeling

Use these options to customize the Domino application with your organization's brand.
See link:d05707[White Labeling].

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description

3+|`com.cerebro.domino.frontend.footerCustomization.customContactUsHref`
| |N/A
|Set a URL that directs your users to a web-based form or email address (mailto:support@domain.com).

3+|`com.cerebro.domino.frontend.footerCustomization.customFooterHtml`
| |N/A
|Set the custom HTML to show immediately above the page footer.

3+|`com.cerebro.domino.frontend.footerCustomization.customFooterImageUrl`
| |N/A
|Set the URL for the image that you want shown in the footer.
The image displays on the same line as the Domino logo.
If `customFooterHTML` is set, this displays below the custom HTML, inside the footer.

3+|`com.cerebro.domino.whitelabel.jsonConfig`
| |N/A
|A JSON-formatted list of white labeling configuration parameters, such as:

// Sample below is from https://docs.google.com/document/d/1acFrozMi9zKA_V4M6hjw8o7RivXqZOqeV_EMUxwnau4/edit
[source,json]
----
{
"appLogo": "https://s3-us-west-2.amazonaws.com/your-logo.png",
"appName": "Your Company Name",
"favicon": "https://www.your-website.com/favicon.ico",
"helpContentUrl": "https://your-support-website.com",
"showSupportButton": true,
"supportEmail": "support@your-email-domain.com",
"errorPageContactEmail": "support-error@your-email-domain.com",
"hidePopularProjects": true,
"hideSuggestedProjects": true,
"gitCredentialsDescription": "Authenticate to your Git Account by clicking Add a New Git Credential",
"hideDownloadDominoCli": true,
"pageFooter": "<p>Your Custom Footer</p>",
"hideSearchableProjects": true
}
----

|===

[[banner]]
== Custom HTML banner


Use these options to display a custom banner on every page in the Domino application.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description

3+|`com.cerebro.domino.frontend.globalBanner.content`
| |N/A
|Required: HTML markup specifying your banner's content and style. Example: `<div style="background-color: blue; font-size: 50px;">Global Banner</div>`

3+|`com.cerebro.domino.frontend.globalBanner.isClosable`
| |`false`
|If `true`, lets users close the banner.

3+|`com.cerebro.domino.frontend.globalBanner.reappearTimeAfterCloseInSec`
| |N/A
|Optional: Time, in seconds, until the banner reappears after closing the banner. The banner reappears when a page is refreshed or loaded. If not set, the closed banner will stay closed. Clearing hide-global-banner in the local storage will make the banner reappear.

|===


[[tr360]]

[[workspaces]]
== Workspaces

These options relate to Domino workspaces.

[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description
3+|`com.cerebro.domino.workbench.project.defaultVolumeSizeGiB`
| |
`10`
|Controls default allocated persistent volume size for a new workspace.


3+|`com.cerebro.domino.workbench.project.minVolumeSizeGiB`
| |`4` |Controls min allocated persistent volume size for a new workspace.
3+|`com.cerebro.domino.workbench.project.maxVolumeSizeGiB`
| |`200` |Controls max allocated persistent volume size for a new workspace.
3+|`com.cerebro.domino.workbench.workspace.maxWorkspacesPerUserPerProject`
| |`4`
|Sets a limit on the number of provisioned workspaces per user per project.
3+|`com.cerebro.domino.workbench.workspace.maxWorkspacesPerUser`
| |`16`
|Sets a limit on the number of provisioned workspaces per user across all projects.
3+|`com.cerebro.domino.workbench.workspace.maxWorkspaces`
| |`3000`
|Sets a limit on the number of provisioned workspaces across the whole Domino.
3+|`com.cerebro.domino.workbench.workspace.maxAllocatedVolumeSizeAcrossAllWorkspacesGiB`
| |`None` |Sets a limit on the total volume size of all provisioned workspaces across the whole Domino combined.
3+|`com.cerebro.domino.workbench.workspace.stopToDeleteDelayDuration`
| |`20.seconds` |The number of seconds the frontend waits after the workspace stops before making the delete request to the backend. This allows for enough time after workspace stop for the workspace's persistent volume to be released. If users frequently receive an error after trying a delete, then this value should be increased.
3+|`com.cerebro.domino.workbench.workspace.volume.enableSnapshots`
| |
`true`
|Whether to capture snapshots of workspace persistent volumes in AWS.
 Workspace volume snapshotting is disabled for remote data planes.
3+|`com.cerebro.domino.workbench.workspace.volume.snapshotCleanupFrequency`
| |`1.day` |How often to delete all but the X most recent snapshots
, including snapshots of deleted workspaces.
 Where X is a number defined by `workbench.workspace.volume.numSnapshotsToRetain`.
Only snapshots older than two hours are cleaned up.
Snapshots of deleted workspaces are cleaned up regardless of their age.
3+|`com.cerebro.domino.workbench.workspace.volume.numSnapshotsToRetain`
| |`5` |The number of snapshots to retain. All older snapshots beyond this limit will be deleted during a periodic cleanup.
Domino does not retain snapshots for deleted workspaces.
3+|`com.cerebro.domino.workbench.workspace.volume.recommendedSizeFactor`
| |`1.2` |The number by which Domino multiples the project size to calculate the recommended volume size.
3+|`com.cerebro.domino.workbench.workspace.volume.highDiskUsageThresholdPercent`
| |`90` |The threshold, as a percentage, at which Domino notifies users and recommends reducing the project size or adjusting the volume size to avoid performance issues.
3+|`com.cerebro.domino.workbench.workspace.enabledPVCleanup`
| |`false` |"True" to enable the unused workspace volume cleaner job, which deletes unused disk volumes to reduce cost.
3+|`com.cerebro.domino.workbench.workspace.cleanupFrequencyDays`
| |`1.day` |The frequency, in days, at which the workspace volume cleaner job runs.
3+|`com.cerebro.domino.workbench.workspace.idlePeriodThresholdDays`
| |`10` |The number of days a workspace can remain unused before it is marked for deletion and the grace period begins.
3+|`com.cerebro.domino.workbench.workspace.unusedGracePeriodDays`
| |`30` |The number of days between when a workspace is marked for deletion and when it is actually deleted, also called the grace period. During this period, the workspace owner is notified in Domino and through email.
3+|`com.cerebro.domino.workbench.workspace.persistence.packages.enabled`
| |`false` |If set to `true`, users have the option to persist packages in environments between Workspace sessions.
3+|`com.cerebro.domino.workbench.workspace.persistence.directories.enabled`
| |`false` |If set to `true`, adds an option on the Workspace Launch modal to persist the home directory contents between Workspace sessions.
|===


// oldest content below

----- admin_guide/configuration/core-catalog.txt -----
:page-version: 6.1
:page-permalink: d284e6
:page-title: Core and Extended Catalog Services
:page-order: 10

Starting with Domino 5.11, helm chart releases are split into two categories:

*Core Services*: Standard services that should be available for all Domino installs.

* Most services are in the core catalog.
* Domino Minimal is the default Domino Environment that includes core services.

*Extended Services*: These are optional services that are not required for standard Domino operations. Services are classified as extended for a variety of reasons:

* They are newer, experimental services.
* They are services that Domino can't directly control - the frequency of functional and security updates.
* Services for testing and internal use.

Notably, extended services can include:

* New Relic
* Starburst
* Falco
* csi-driver-smb
* dcgm-exporter

*Extended Domino Environments*: These default Domino environment images live in the extended catalog:

* Domino Standard Environment
* Domino Spark Environment

These environments are added to the default environments automatically when `extended_catalog` is enabled.

Other than these environments, `extended_catalog` functions only as a gate, but does not result in the installation of extended catalog services.

For example, enabling `extended_catalog` does not install New Relic, but you do have to enable `extended_catalog` in order to install New Relic.

*Fleetcommand-agent*: In the `fleetcommand-agent` installer, the extended catalog is gated by the `extended_catalog` configuration flag. By default, this flag is `off` and the installer cannot install services in the extended catalog. It does not enable all extended catalog services automatically, but rather just functions as a gate to prevent installing services that may have extra consideration in sensitive production environments without clear intent.

== When to use `extended_catalog`

* *You're upgrading from a 5.10 or earlier release*:
+
For 5.10 and earlier, `extended_catalog` will be set to `true` when migrating the config schema to 5.11. Existing installs may have features from the extended catalog enabled, so this provides maximum compatibility with prior versions.

* *You want to use a service in the extended catalog*:
+
You may want to use a service that's in the extended catalog, such as New Relic or Starburst. Set `extended_catalog` to enable this.
See the link:#configured-releases-command[configured-releases command] information below to identify the full list of extended catalog releases for a given Domino version.

== Check for Extended Catalog Services

[[configured-releases-command]]
The fleetcommand-agent's `configured-releases` command can be used to determine what core and extended catalog services will be installed for a given install.

The standard usage is as follows:

    usage: fleetcommand-agent configured-releases [-h] [-f FILE] [--helmfile-dir HELMFILE_DIR] [--kubeconfig KUBECONFIG]
                                                  [--kube-context KUBE_CONTEXT] [--installed-only] [--removed-only]
                                                  [--extended-only] [-l LOG_LEVEL] [-v] [-q]

    options:
      -f FILE, --file FILE  Name of the configuration file
      --installed-only      Only list installed items
      --removed-only        Only list removed items
      --extended-only       Only list extended catalog items

Below is an abbreviated example output in list format:

    # fleetcommand-agent configured-releases -f /tmp/domino.yml
    2024-05-13 21:40:33,957 - Caching catalog into working directory
    2024-05-13 21:40:33,958 - Using archived catalog as source, /resources/catalog/catalog.tar.xz -> /app/helmfile/catalog
    2024-05-13 21:40:34,599 - Caching any missing charts
    2024-05-13 21:40:34,620 - Running HELM_SECRETS_BACKEND="/usr/bin/helm-secrets-k8s.sh" /usr/local/bin/helmfile --file helmfile/catalog list --output json --log-level error
    2024-05-13 21:40:36,006 - SERVICE                                    | STATUS INTENT | CORE/EXT
    2024-05-13 21:40:36,006 - ------------------------------------------------------------------------
    2024-05-13 21:40:36,006 - backend                                    | uninstalled   | N/A
    2024-05-13 21:40:36,007 - nvidia-device-plugin                       | not installed | core
    2024-05-13 21:40:36,007 - aws-cloud-controller-manager               | not installed | core
    2024-05-13 21:40:36,008 - csi-driver-smb                             | not installed | extended
    2024-05-13 21:40:36,008 - custom-cert-manager                        | installed     | core
    2024-05-13 21:40:36,008 - nri-metadata-injection                     | installed     | extended
    2024-05-13 21:40:36,008 - ** NOT COMPATIBLE with current setting: extended_catalog - false! **

The columns are as follows:

* *SERVICE*: The name of the service.
* *STATUS INTENT*: What the `fleetcommand-agent` installer intends to do when running the install.
** `installed`: The installer intends to install this service.
** `not installed`: The installer does not intend to install this service.
** `uninstalled`: If the service is installed in the Kubernetes cluster, the installer will remove it. This is only used with deprecated services.
* *CORE/EXT*:
** `core`: This service is a core catalog release.
** `extended`: This service is an extended catalog release.
** `N/A`: For uninstalled services, which have no core/extended catalog distinction.

Note that if the provided configuration file has `extended_catalog` set to `false`, but also configures a release using the extended catalog, the following error will be displayed in the `configured-releases` list:

----
** NOT COMPATIBLE with current setting: extended_catalog - false! **
----

If `extended_catalog` is `false`, the  `--extended-only` and `--installed-only` flags can be combined to provide an easy way to see if you are attempting to install an extended catalog release:

    # head -10 domino.yml
    schema: '1.9'
    name: example
    version: 5.11.0
    hostname: domino.example.com
    extended_catalog: false
    monitoring:
      prometheus_metrics: true
      newrelic:
        apm: true
        infrastructure: true

The following example demonstrates how an extended catalog service, New Relic, is being installed (i.e., `monitoring.newrelic.infrastructure` is `true`), but `extended_catalog` is `false`. Below is the result of the `configured-releases` command:

    # fleetcommand-agent configured-releases -f domino.yml --extended-only --installed-only
    2024-05-13 21:48:29,446 - Caching catalog into working directory
    2024-05-13 21:48:29,446 - Using archived catalog as source, /resources/catalog/catalog.tar.xz -> /app/helmfile/catalog
    2024-05-13 21:48:30,153 - Caching any missing charts
    2024-05-13 21:48:30,174 - Running HELM_SECRETS_BACKEND="/usr/bin/helm-secrets-k8s.sh" /usr/local/bin/helmfile --file helmfile/catalog list --output json --log-level error
    2024-05-13 21:48:31,512 - SERVICE                                    | STATUS INTENT | CORE/EXT
    2024-05-13 21:48:31,512 - ------------------------------------------------------------------------
    2024-05-13 21:48:31,512 - nri-metadata-injection                     | installed     | extended
    2024-05-13 21:48:31,512 - ** NOT COMPATIBLE with current setting: extended_catalog - false! **
    2024-05-13 21:48:31,512 - newrelic-events                            | installed     | extended
    2024-05-13 21:48:31,512 - ** NOT COMPATIBLE with current setting: extended_catalog - false! **
    2024-05-13 21:48:31,512 - newrelic-infrastructure                    | installed     | extended
    2024-05-13 21:48:31,512 - ** NOT COMPATIBLE with current setting: extended_catalog - false! **
    2024-05-13 21:48:31,512 - newrelic-logging                           | installed     | extended
    2024-05-13 21:48:31,512 - ** NOT COMPATIBLE with current setting: extended_catalog - false! **
    2024-05-13 21:48:31,512 - newrelic-open-metrics                      | installed     | extended
    2024-05-13 21:48:31,512 - ** NOT COMPATIBLE with current setting: extended_catalog - false! **
    2024-05-13 21:48:31,513 - newrelic-prometheus-agent                  | installed     | extended
    2024-05-13 21:48:31,513 - ** NOT COMPATIBLE with current setting: extended_catalog - false! **

----- admin_guide/configuration/customize-ui.txt -----
:page-version: 6.1
:page-permalink: d05707
:page-title: Customize the Domino UI
:page-sidebar: Customize the UI
:page-order: 70

If you are using an on-premises deployment of Domino, you can use white labeling to customize the Domino interface with your organization's brand, external resource links, banner, footer, and more. You can also remove specific UI elements to hide functionality according to your organization's needs.

White labeling is controlled by certain Configuration keys.
See link:71d6ad#whitelabel[Configuration records] for definitions of these keys.


== Customize the banner

Create a custom HTML banner for every page on Domino to notify users of important announcements and information.

. In the admin portal, go to *Platform settings > Configuration records*.
. Set `com.cerebro.domino.frontend.globalBanner.content` to the HTML that you want shown in the banner.
.. Example: `<div style="background-color: blue; font-size: 50px;">Global Banner</div>`
. Set `com.cerebro.domino.frontend.globalBanner.isClosable` to let users close the banner.
. Set `com.cerebro.domino.frontend.globalBanner.reappearTimeAfterCloseInSec` to specify how many seconds pass before the banner reappears. If the key is not specified, the banner will stay closed.


== Customize the footer

. In the admin portal, go to *Platform settings > Configuration records*.
. Set `com.cerebro.domino.frontend.footerCustomization.customFooterHtml` to the custom HTML that you want shown just above the page footer.

== Customize the footer image

. In the admin portal, go to *Platform settings > Configuration records*.
. Set `com.cerebro.domino.frontend.footerCustomization.customFooterImageUrl` to a URL for the image that you want shown in the footer.
The image displays on the same line as the Domino logo.
If `customFooterHTML` is set, this displays below the custom HTML, inside the footer.

== Customize the Contact Us link

. In the admin portal, go to *Platform settings > Configuration records*.
. Set `com.cerebro.domino.frontend.footerCustomization.customContactUsHref` to a URL that directs your users to a web-based form or email address (`mailto:support@domain.com`).

== Advanced customization

The `com.cerebro.domino.whitelabel.jsonConfig` Configuration key supports additional white labeling parameters in JSON format:

[cols="2a,4a",options="header"]
|===
|Key |Description

2+| _Design elements_
| `appLogo`
| *Optional*: The absolute URL of the logo image, or `none` to display the Domino logo.

| `appLogoBgColor`
| *Optional*: The background color of the logo; the default is "transparent". Any CSS color value is valid.

| `appURL`
| *Optional*: The application URL; the default is "https://domino.ai/".

| `appName`
| The application name (such as Domino).

| `defaultProjectName`
| The default project name.

| `favicon`
| The absolute URL of the favicon image.

| `gitCredentialsDescription`
| A description to show in the Git credentials configuration window.

| `helpContentUrl`
| The URL of the help content.

| `hideAddProjectAction`
| `True` to hide the *Add Project* button.

| `hidePopularProjects`
| `True` to hide the list of popular Projects.

| `hideDownloadDominoCli`
| `True` to hide the Domino client download link.

| `hideLearnMoreOnFile`
| `True` to hide learn more on file.
// What's that?

| `hideMarketingDisclaimer`
| `True` to hide the marketing disclaimer.

| `pageFooter`
| The custom HTML that you want showing just above the page footer.

| `showSupportButton`
| `True` to show the *Support* button.

2+| _Git security_

| `gitCredentialsDescription`
| A description to show in the Git credentials configuration window.

| `hideGitSshKey`
| `True` to hide the SSH key in Git configurations.

2+| _Project visibility_

| `hidePopularProjects`
| `True` to hide the list of popular projects.

| `hidePublicProjects`
| `True` to hide the list of public Projects.

| `hideSearchableProjects`
| `True` to hide the list of searchable Projects.

| `hideSuggestedProjects`
| `True` to hide the list of suggested Projects.

| `hideLearnMoreOnFile`
| `True` to hide "Learn more" on file.
// What's that?

| `errorPageContactEmail`
| The contact email to be used in error pages.

| `appName`
| The application name (such as Domino).

| `pageFooter`
| The custom HTML shown just above the page footer.

| `showSupportButton`
| `True` to show the *Support* button.

| `supportEmail`
| The support email address.

| `supportUrl`
| The support page URL.

| `pdfTemplateUrl`
| The PDF template URL.


| `pdfAppName`
| The title for the PDF template.
|===

.Example
[source,json]
----
{
  "appLogo": "https://example.datascience.org/sku_setup/logo/for-light-backgrounds/example_logo_blue.png",
  "appLogoBgColor": "#2DEB5E",
  "appName": "My Data Science Platform",
  "appURL" : "https://examples.data.org/app/",
  "defaultProjectName": "my-datascience-project",
  "errorPageContactEmail": "admin@example.datascience.org",
  "favicon": "https://example.datascience.org/favicon.ico",
  "gitCredentialsDescription": "My customized Git credentials description.",
  "helpContentUrl": "https://help.example.datascience.org",
  "hideAddProjectAction": false,
  "hideDownloadDominoCli": false,
  "hideGitSshKey": false,
  "hideLearnMoreOnFile": false,
  "hideMarketingDisclaimer": false,
  "hidePopularProjects": false,
  "hidePublicProjects": false,
  "hideSearchableProjects": false,
  "hideSuggestedProjects": false,
  "pageFooter": "<div style='width:100%; height: 50px; background-color: black; color: white;text-align: center;'>Example Page footer</div>",
  "showSupportButton": true,
  "supportEmail": "support@example.datascience.org",
  "supportUrl": "https://example.datascience.org/support/",
  "pdfTemplateUrl": "https://example.com/?export=download_link", 
  "pdfAppName": "My Data Science Platform PDF title"
}
----

----- admin_guide/configuration/deployment-telemetry.txt -----
:page-version: 6.1
:page-permalink: 06c620
:page-title: Deployments telemetry
:page-order: 30

Domino provides a deployment management service named Fleetcommand that uses an agent to capture and securely transmit the telemetry and health status of deployments to the Domino-managed service. This telemetry is then used to provide proactive support whenever required.

== What data is being captured?

NOTE: No sensitive data is captured as part of the telemetry. Any potentially sensitive configuration is redacted _before_ being transmitted.

The following data is captured and transmitted to the Fleetcommand service:

* Installation configuration
* Agent version
* Health checks
* Logs
* Enabled feature flags
* Domino configuration record keys
* Nucleus and Frontend service versions
* OS system data
* Cloud provider data
* Kubernetes data

Installation Configuration::

YAML file used by the link:7f4331[Domino installation process]. Any potentially sensitive configuration information is redacted before being transmitted. All configuration keys containing the following words are automatically redacted:

* `password`
* `license_key`
* `api_token`
* `secret`
* `token`
* `authToken`
* `username`


Agent version::

The version of the agent that is running on the cluster.

Health checker::

Health ensures the health of the running Agent instance by pinging the central server's `/_status` endpoint every 5s. By default, one successful (code = `200`) response is required for a Health instance to report as healthy.

Logs::

The logs from the Agent Installer (inside Fleetcommand Agent) used for installing or upgrading domino. Any secrets or keys are redacted _before_ being transmitted.

Enabled Feature Flags::

The list of link:6469bf[feature flags] enabled on the deployment.

Domino configuration record keys and values::

Any potentially sensitive configuration information is redacted before being transmitted. All configuration keys containing the following words are automatically redacted:

* `password`
* `apiKey`
* `secret`
* `token`
* `jiraPrivateKey`
* `privateKey`
* `consumerKey`
* `secretAccessKey`
* `secretKey`
* `elasticSearchPassword`

Nucleus Frontend service versions::

Dispatcher and Frontend service versions.

Operating system data::

Basic OS data from the cluster, including the OS image, OS, architecture, container runtime version, and kubelet/kube proxy versions.

Cloud provider data::

Basic info about the cloud service provider, such as the accountId, name, and region from the cluster.

Kubernetes data::

Cluster details such as the autoscaler status.

== How is the data sent to Fleetcommand?

The data captured from the deployment is sent through the *Fleetcommand Reporter Service*.

.URLs / Destinations

* *Url:* `update.domino.tech`
* *Port:* 443
* *Protocol:* HTTPS
* *Authentication Method:* Using a custom HTTP Header `X-Api-Token` to know which is the deployment associated with the collected information.
* *Endpoints:*
** `/api/agent/log_records`
** `/_status`
** `/api/agent/release`
** `/api/agent/status`

.Does it require and ingress?

No, just egress to https://update.domino.tech.

.Is it enabled by default?

No, it's not enabled by default. The following keys are required to be set up on the install configuration to enable:

[source,yaml]
----
fleetcommand.api_token = [Token provided by Domino]
fleetcommand.url = https://update.domino.tech/
----

.Will this incur additional cloud costs?

The outgoing traffic will increase due to the telemetry leaving the cluster, but it will be minimal from a cloud costs perspective.

== How is the data stored in Fleetcommand?

.Where is the collected information stored?
The collected information is stored on a MongoDB instance. The information is persisted every time the Fleetcommand server receives a new update from the agent or reporter.

.How often is the information reported?
The information is transmitted to Fleetcommand server every 30 seconds (or 15 seconds for Domino >5.3).

----- admin_guide/configuration/email-configuration.txt -----
:page-version: 6.1
:page-permalink: aa19d4
:page-title: Email configuration
:page-order: 60

The email configuration page allows a SysAdmin to quickly configure a deployment to send emails.

To access the email settings page, go to the *Admin portal > Platform settings > Email*.

== Modes

Email configuration has 3 modes: SMTP, SES, and Logging.

=== SMTP

[cols="1a,2a,2a",options="header"]
|===
|Field Name|Description|Example
|Host Name|The host of the SMTP server.|`email-smtp.us-west-2.amazonaws.com`
|Port|The port of the SMTP server.|`465`
|Username|Username to authenticate against the SMTP server.|`AKIAIOSFODNN7EXAMPLE`
|Password|Password to authenticate against the SMTP server.|`wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY`
|Secure Communication Protocol|SMTP Security method.|`Plaintext`, `TLS`, `STARTTLS`
|===


NOTE: SMTP configuration will be synced to the deployment's Keycloak instance upon saving. This removes the need to log in to Keycloak to update SMTP settings.


=== SES

[cols="1a,2a,2a",options="header"]
|===
|Field Name|Description|Example
|AWS Region|Optional override for AWS SES region. The default is `us-west-2`.|`us-east-1`
|===

NOTE: Ensure nucleus-dispatcher has access to a policy with the `ses:SendRawEmail` permission.

=== Logging

Logs email sending attempts when debug logging is enabled.

== Notifications FROM Address
This required field is the email address that will appear in the from field of emails sent from the deployment.

----- admin_guide/configuration/feature-flags.txt -----
:page-version: 6.1
:page-title: Feature flags
:page-permalink: 6469bf
:page-order: 40

// See https://github.com/cerebrotech/domino/blob/develop/server/app/domino/server/featureflags/domain/FeatureFlags.scala

Use feature flags to control which Domino functionality is available to users.

You can change feature flags in the Admin Portal by choosing *Platform Settings* > *Feature Flags*.

Feature flags are categorized as follows:

User-Scoped Overrides:: Flags in this section override the values set in the Deployment Global section.
+
NOTE: You cannot override all flags at the user level. Therefore, this section doesn't list all feature flags.

Deployment Global:: Flags in this section override Domino's system default value.

Default:: If no value is set for the flag in the Deployment Global section, then Domino uses a system default value.

TIP: When you change the value of a feature flag, you do not have to restart the server.

[cols="2a,^2a,3a,10a",options="header"]
|===

|Feature Flag |Default Value |Category |Description

4+|`AppPublishingEnabled`
|
|`true`
|Deployment Global
|Allow app publishing. Set to `false` to disable the *Anyone, including anonymous users* and *Anyone with an account access permissions*. See link:71635d#Domino-App-features[Grant Access to Domino Apps] for more information about these permissions.

4+|`EnableApps`
|
|`true`
|User-Scoped Overrides

Deployment Global
|Apps host web applications and dashboards. When this flag is set to `true`, users can publish apps from inside a project, view a list of globally discoverable apps, view apps to which they have access, and view apps in the Assets dashboard.
When this flag is set to `false`, the only difference is that users cannot publish an app or web application from their project.
See link:71635d#Publish-an-app[Publish a Domino App].

4+|`EnableContentSecurityPolicyForApps`
|
|`false`
|Deployment Global
|The content security policy allows Domino web apps to access specific whitelisted external resources like images and APIs.

This flag is set to `false` by default which allows all requests to all resources. Set this flag to `true` to block requests to resources that are not whitelisted and allow requests to whitelisted resources.

See link:aec000[Identify Resources to Whitelist] for more information.

4+|`EnableDownloadingSupportBundle`
|
|`true`
|Deployment Global
|This flag is set to `true` by default which allows non-admins to download link:055b74[Support Bundles] if they are the owner of the Project.

When set to `false`, the following changes take place:

* Disables the support button in Workspace detail's *Support Tab*.
* Hides the support bundle row in the Job details.
* Hides the support bundle row in the App details.

[[enableInWorkspaceBranchSelection]]
4+|`enableInWorkspaceBranchSelection`
|
|`false`
|User-Scoped Overrides
|Enable branch selection within workspaces for Git code repositories. When `true`, users can select which files to commit in Git-based projects. By default, this is turned off and Domino commits all changes to the project. See link:0d2247[Use Git in your Workspace].

4+|`EnableLaunchers`
|
|`true`
|User-Scoped Overrides

Deployment Global
|Launchers are simple web forms that allow you to run templatized scripts.
By default, from inside a project, users can publish a Launcher, view and use Launchers to which they have, and can view apps in the Assets dashboard.
When `false`, users cannot publish a Launcher from inside the project. See link:f4e1e3[Launchers].

[[enablelegacyjwt]]
4+|`EnableLegacyJwtTooling`
|
|`false`
|Deployment Global
|Indicates whether to use the API Proxy or expose a (legacy) JWT access token in the container to authenticate users to the Domino API.
If `true`, Domino starts the periodic refresh process and delays the run start until the first token is acquired and propagated.
The token is mounted as a file into the run container. The API Proxy process is also running.
If `false`, only the API Proxy process is running, the token is not refreshed periodically, and it’s not mounted into the run.
*This legacy JWT tooling behavior will be deprecated in upcoming Domino versions.*

4+|`EnableMetricAlertsApi`
|
|`true`
|User-Scoped Overrides

Deployment Global

|Enable custom metrics notification API.
See `/api/metricAlerts/v1` in the link:8c929e#_sendMetricAlert[Domino Platform API reference] for more information.

4+|`EnableModelAPIs`
|
|`true`
|User-Scoped Overrides

Deployment Global
|You can use the Domino endpoint feature to publish a model. When `false`, the Domino endpoints feature is disabled so the user cannot publish Domino endpoints.
See link:8dbc91[Model Publishing].

4+|`EnableModelMonitoringForModelAPIs`
|
|`true`
|User-Scoped Overrides

Deployment Global
|When `false`, this hides the Monitoring tab in the Domino endpoint. When `true`, the user can monitor their Domino endpoint.
See link:d42ae6/[Use Monitoring].

4+|`EnableScheduledJobs`
|
|`true`
|User-Scoped Overrides

Deployment Global
|Use Scheduled Jobs to plan jobs in advance and execute them on a regular cadence.
When `false`, the Scheduled Jobs feature is disabled so users cannot use this feature in a project.
However, they can still see Scheduled Jobs from the Assets Portfolio.
See link:5dce1f[Scheduled Jobs] and link:cc299e[See the Assets for Your Project].

4+|`enableSelectFileSync`
|
|`true`
|Deployment Global
|When a user is in a workspace and has made changes to multiple files, this flag activates the Select and Sync button so users can select the files that they want to commit and Sync to Git. This only happens when workspace comes from a project created as a Git project, not a Domino File System (DFS) project.

4+|`ExportsWorkflowEnabled`
|
|`true`
|Deployment Global
|Enables the Exports page and related functionality. See link:02ec6d[Export to Sagemaker]
and link:5cef47[Export to NVIDIA]
, link:5cef47[Export to NVIDIA], and link:a1b168[Use Models in Snowflake Queries]
for more information about exports.

4+|`KubernetesContainerMetricsEnabled`
|
|`true`
|Deployment Global
|Domino retrieves CPU and memory statistics from Prometheus. If something is wrong with Prometheus, the link:111eae[Jobs dashboard] and link:e6e601[Workspaces] might run slowly. Set this flag to `false` to stop retrieving data from Prometheus. The user will see blank values in tables and CPU and memory graphs will not be shown.

4+|`GpuMetricsEnabled`
|
|`true`
|Deployment Global
|Domino retrieves GPU and memory statistics from Prometheus. Metrics are populated to Prometheus from Nvidia DCGM. If something is wrong with Prometheus, the link:111eae[Jobs dashboard] and link:e6e601[Workspaces] might run slowly. Set this flag to `false` to stop retrieving data from Prometheus. GPU and GPU memory graphs will not be shown.

4+|`SecureIdentityPropagationToAppsEnabled`
|
|`true`
|Deployment Global
|When `true`, Domino Apps receive a token as part of their incoming requests that can be used to identify the requesting Domino user.

4+|`ShortLived.DaskAutoscalingEnabled`
|
|`true`
|User-Scoped Overrides

Deployment Global
|This enables the autoscale features on the Launch New Workspace page so users can set the Auto-scale workers check box and then set a minimum and maximum number of workers. See link:aaa2c1#dask_cluster_settings[Understand your cluster settings.]

4+|`ShortLived.DaskClustersEnabled`
|
|`true`
|User-Scoped Overrides

Deployment Global
|Enables support for Dask clusters in workspaces. See link:747a51[On-demand Dask].

4+|`ShortLived.DisableDFSBasedProjects`
|
|`false`
|User-Scoped Overrides

Deployment Global
|Prevents users from creating classic Domino File System (DFS) based projects. See link:a8e081[Projects] for more about DFS projects.

NOTE: If this is set to `true`, but ShortLived.EnableGitBasedProjects is set to `false`, new projects cannot be created.

4+|`ShortLived.EnableAsyncModels`
|
|`true`
|User-Scoped Overrides

Deployment Global
|When false, the async Domino endpoints feature is disabled so users cannot publish async Domino endpoints. See link:user_guide/8dbc91[Model Publishing].

4+|`ShortLived.EnableCostBudgetsAndAlerts`
|
|`false`
|Deployment Global
|Enables cost budgets and alerts. Requires an Enterprise license. See link:157618[Analyze costs] for an example.

4+|`ShortLived.EnableDatasetMultiStorageSupport`
|
|`true`
|Deployment Global
|Enables multi-account storage support for datasets; see link:4f5b2a[Set Up Multiple Storage Accounts] for complete instructions.

4+|`ShortLived.EnableDataSources`
|
|`true`
|
|No longer supported.

4+|`ShortLived.EnableDiskUsageVolumeCheck`
|
|`true`
|User-Scoped Overrides

Deployment Global
|When `true`, Domino measures disk usage and makes volume size recommendations when users create new workspaces and jobs.

4+|`ShortLived.EnableExternalDataVolumes`
|
|`true`
|User-Scoped Overrides
|Enables the integration of External Data Volumes (EDV) with Domino.

WARNING: Do not set this to `false` as you will disable functionality.

4+|`ShortLived.EnableGitBasedProjects`
|
|`true`
|User-Scoped Overrides

Deployment Global
|Allows users to create Git-based projects (GBP). See link:910370[Git-based Projects].

NOTE: If this is set to `false` but ShortLived.DisableDFSBasedProjects is set to `true`, new projects cannot be created.

4+|`ShortLived.EnableInWorkspaceBranchSelection`
|
|`true`
|User-Scoped Overrides

Deployment Global
|Allows users to search for and switch branches in their Git repositories in a workspace. When set to `false`, users can see the branch name but cannot interact with it. See link:0d2247[Using Git in Your Workspace].

4+|`ShortLived.EnableReadWriteDatasets`
|
|`true`
|User-Scoped Overrides

Deployment Global
|Enables the Dataset functionality.

WARNING: Do not set this to `false` as you will disable important functionality.

4+|`ShortLived.EnableRepoCreationInNewProjectModal`
|
|`true`
|User-Scoped Overrides

Deployment Global
|If using GitHub or GitLab PAT, allows users to create a Git repository when creating a Git-based project. See link:eaab17[Create a Git-based Project].

4+|`ShortLived.EnableUserNotifications`
|
|`true`
|Deployment Global
|If this is set to `false`, Notifications are hidden from users.

4+|`ShortLived.EnableWorkspaceReproduction`
|
|`true`
|Deployment Global
|From the Workspaces page, the user can click *History*. If this flag is enabled, the user will see the Workspace History with a tab that lists commits and a tab that lists *Sessions*. On the *Commits* tab, the user can hold their mouse over a row. At the end of the row, they can click *Open* to create a workspace from the commit. When the new workspace is created, a new branch is also created. This is referred to as link:74f8ed#reproduce-the-environment[reproducing a workspace]. If this is disabled, the user sees a list of Workspace Sessions.

4+|`ShortLived.EnhancedMergeConflictsEnabled`
|
|`true`
|User-Scoped Overrides

Deployment Global
|Allows users to use the application to resolve merge conflicts when syncing to a Git repository.

When set to `false`, a merge conflict with remote will create a new branch with the user's changes. The user must manually merge the changes onto their original branch. See link:acd236[Resolve Merge Conflicts].

4+|`ShortLived.ExternalDataVolumes`
|
|`true`
|Deployment Global
|Set to `true` so your users can use external data sources (such as hard drives and flash drives). When `true`, the user can open a project and click Data in the navigation pane. Then, they can click the External Data Volumes tab to add external data volumes to their project and retrieve data from that source.
See link:ee8d01[External Data Volumes].

[[ExternalDataVolumesFullCensor]]
4+|`ShortLived.ExternalDataVolumesFullCensor`
|
|`false`
|User-Scoped Overrides

Deployment Global
|When `true`, the External Data Volumes section on the Data page shows only those external data volumes that are available to the user. Additional volumes might have been mounted, but will not be listed.
If this is set to `false`, this is considered partial volume censorship and all volumes mounted to the project are listed, but some might be grayed out to refuse a user's access.
See link:eb09fa[Configure Censorship].

4+|`ShortLived.GitReferencesCustomizableEnabled`
|
|`true`
|User-Scoped Overrides

Deployment Global
|Allows users to select a branch, commit, tag, or custom Git reference when importing a Git repository.

When set to `false`, imported repositories default to the default branch. See link:314004#add-a-repo[Add a repository to a project].

4+|`ShortLived.GitRepoValidationEnabled`
|
|`true`
|Deployment Global
|Allows automated user validation to Git repositories when a Git-based project is created, imported, or a job or workspace is started. This validation prevents workspaces from failing due to invalid credentials to clone a Git repository. See link:314004[Import Git Repositories].

4+|`ShortLived.HardwareTierCapacityFetchingEnabled`
|
|`true`
|Deployment Global
|This flag controls whether the time estimate (such as <1 min and <7 min) is shown for the Hardware tier on the Project settings page.

4+|`ShortLived.JobsListAutoUpdateDisabled`
|
|`false`
|User-Scoped Overrides

Deployment Global
|Use this flag to turn off auto-update on the Job page to improve job performance. If this is turned off, the user must click the refresh button on the Jobs page to get updates on the information in the Jobs Timeline such as CPU (%), Memory, Duration, Status, and Comments. See link:111eae[Jobs].

4+|`ShortLived.LauncherFileUploadPathEnabled`
|
|`false`
|User-Scoped Overrides

Deployment Global
|If `true`, shows the *Upload file to field* on the New Launcher page when the *Launcher Type* is set to *Upload file(s)*.
This lets users set an upload path for the launcher.
See link:f4e1e3[Launchers].

4+|`ShortLived.ModelRegistry`
|
|`true`
|User-Scoped Overrides

Deployment Global
|When set to `false`, the Model Registry feature is disabled so users cannot link:19df62[register and govern their models] in the model registry.

4+|`ShortLived.RayAutoscalingEnabled`
|
|`true`
|User-Scoped Overrides

Deployment Global
|This enables the autoscale features on the Launch New Workspace page so users can set the Auto-scale workers check box and then set a minimum and maximum number of workers. See link:c50248#ray_cluster_settings[Set your cluster settings].

4+|`ShortLived.RayClustersEnabled`
|
|`true`
|User-Scoped Overrides

Deployment Global
|Enables support for Ray clusters in workspaces. See link:d13903[On-demand Ray].

[[sl-repoclonerimagebuilds]]
4+|`ShortLived.RepoclonerImageBuilds`
|
|`true`
|Deployment Global

|When `true`, the ImageBuilder uses the Repocloner binary to download Domino projects and Git repositories on the build context.
Set it to `false` to use Replicator instead.
If `true`, Replicator access is not required.

NOTE: If your deployment uses Istio, this flag must be set to `false`; Repocloner is incompatible with Istio.

4+|`ShortLived.ShowFeatureStore`
|
|`false`
|Deployment Global
|

4+|`ShortLived.ShowV1DataProjects`
|
|`false`
|User-Scoped Overrides

Deployment Global
|No longer supported. Do not change the default value.

4+|`ShortLived.SparkAutoscalingEnabled`
|
|`true`
|User-Scoped Overrides

Deployment Global
|This enables the autoscale features on the Launch New Workspace page so users can set the Auto-scale workers check box and then set a minimum and maximum number of workers. See link:f11f6a[Understand your cluster settings].

4+|`ShortLived.SparkClustersEnabled`
|
|`true`
|User-Scoped Overrides

Deployment Global
|Enables support for Spark clusters in workspaces. See link:482ec5[On-demand Spark].

4+|`ShortLived.iFrameRequired`
|
|`true`
|Deployment Global
|Limits external access to Domino Apps via iFrame to decrease the risk of attacks from malicious apps. Changes to this setting apply only to Apps published afterwards.

4+|`ShortLived.EnableModelDeploymentsApi`
|
|`true`
|Deployment Global
|Enables the model deployments to AWS SageMaker feature. See link:8c929e[Domino Platform API reference].

4+|`ShortLived.DominoHome`
|
|`true`
|Deployment Global
|Enables the new Domino Home page. When set to `false`, the user sees the Projects page instead of the Domino Home page when they log in.

4+|`ShortLived.EnableBearerAuthenticationForDominoEndpoints`
|
|`false`
|Deployment Global
|Enables the use of Bearer Authentication for Domino Endpoints (formerly Model APIs), disables existing Basic Authentication, will require restarting running Domino Endpoints.

4+|`ShortLived.HybridSuppressHelm`
|
|`false`
|Deployment Global
|Suppresses generation of Helm command for Nexus deployment, effectively switching to generate the bootstrap token needed for the new way of installing and registering Data Planes.

|===

----- admin_guide/configuration/index.txt -----
:page-version: 6.1
:page-title: Configuration
:page-permalink: 30fc1c
:page-order: 60

The `Domino` custom resource definition (link:https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources[CRD^]), reconciled by the link:88f534[Platform Operator], contains a configuration field in its specification that is used to store your Domino cluster configuration.

link:d284e6[Catalog services]::
Information about the core and extended catalog services.

link:7f4331[Install configuration reference schema]::
YAML schema for the Domino CRD's `.spec.config` field.

link:06c620[Deployments telemetry]::
Detailed description of the data sent back from the deployments to Domino.

link:6469bf[Feature flags]::
Control functionality available to your users.

link:71d6ad[Configuration records]::
Set or update global settings for your Domino installation.

link:aa19d4[Email configuration]::
Configure a deployment to send emails.

link:d05707[Customize the Domino UI]::
Customize the Domino interface with your organization's custom banner, custom footer, Contact Us link, and more.

----- admin_guide/configuration/installer-configuration.txt -----
:page-version: 6.1
:page-title: Install configuration reference
:page-sidebar: Schema
:page-permalink: 7f4331
:page-order: 20


[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`schema`
| |YAML schema version. |✓ |`1.14`

4+|`name`
| |Unique deployment name. This must contain the name of the deployment owner. |✓ |`[a-zA-Z0-9_-]+`

NOTE: On data plane installs, this will be the subdomain of the data plane, i.e. the `data-plane` of `data-plane.domino.example.com`.

4+|`version`
| |Domino version to install. |✓ |Supported versions: `6.1.0`, `6.1.1`, `6.1.2`

4+|`hostname`
| |Hostname Domino application will be accessed at. |✓ |Valid fully qualified domain name (FQDN)

NOTE: On data plane installs, this is the `hostname` of the control plane.

4+|`ssl_enabled`
| |Should Domino only be accessible with HTTPS. |✓ |`true`, `false`

4+|`ssl_redirect`
| |Should Domino only be accessible with HTTPS. |✓ |`true`, `false`

4+|`request_resources`
| |Create Kubernetes https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/[resource requests and limits^] for services. |✓ |`true`, `false`

4+|`enable_network_policies`
| |Use https://kubernetes.io/docs/concepts/services-networking/network-policies/[network policies^] for fine-grained service access. |✓ |`true`, `false`

NOTE: Requires a compatible CNI plugin, for example, Calico.

4+|`external_docker_registry`
| |A URL to ECR (Amazon Elastic Container Registry), ACR (Azure Container Registry), GCR (Google Container Registry) or GAR (Google Artifact Registry). The node must have permissions to the registry.| | NOTE: Either `external_docker_registry` or `internal_docker_registry` must be configured.

4+|`custom_certificates`
| |link:4caa42[Custom Certificates]|✓ |

4+|`minimal_resources`
| |Use the minimum amount of nodes and resources to run Domino. This may not be a stable configuration and should be used with caution.| |`false`

4+|`cluster_domain`
| |The Kubernetes cluster domain name. | |`cluster.local`

4+|`aws_account_id`
| |AWS account ID in which Domino is deployed. | |

4+|`operator_role_arn`
| |IAM role arn. | |

4+|`operator_service_account_name`
| |Kubernetes service account name. | |
|===

[[control-and-data-planes]]
== Control and data planes

This section configures whether Domino is a "control plane" or "data plane" install.

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`control_plane`
| |If `true`, install the control plane. This currently always includes a local data plane. | |`true`, `false`

4+|`data_plane.type`
| |Which type of data plane to install. When performing a data plane-only install, this should be `remote`. | |`local`, `remote`
|===

[NOTE]
====
* To perform a standard "control plane" install, you can omit both of these sections. The defaults are a "control plane" and a `local` data plane type.
* To perform a "data plane" install, you should configure the following:

[source,YAML]
----
control_plane: false
data_plane:
  type: remote
----
====

[[istio]]
== Istio

This section configures how and if an https://istio.io[Istio Service Mesh^] is deployed by or integrated to Domino.
A Domino-deployed Istio is for Domino use only.
These configurations must only be installed and/or enabled if link:dafa95[intra-cluster encryption in transit] is required.

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`istio.enabled`
| |Enable Istio in deployment (that is, sidecar injection). |✓ |`true`, `false`

4+|`istio.install`
| |Install Istio service with Domino. |✓ |`true`, `false`

4+|`istio.cni`
| |Configures whether Istio installation is done with a CNI.

If `true`, the installation is done with a CNI and requires fewer
permissions; this is our preferred and recommended setting. If `false`,
the installation will add required capabilities to every pod security
policy: `NET_ADMIN`, and `NET_RAW`. |✓ |`true`, `false`
|===

== Ingress controller

This section configures the https://github.com/kubernetes/ingress-nginx[NGINX ingress controller^] deployed by the `fleetcommand-agent`.

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`ingress_controller.install`
| |Install the NGINX ingress controller. |✓ |`true`, `false`

4+|`ingress_controller.gcp_static_ip_name`
| |On Google Cloud Platform (GCP), use a static IP address to expose nginx. | |The `name` of the IP address

4+|`ingress_controller.class_name`
| |Name of the ingress class for Domino. |✓ |`nginx`

4+|`ingress_controller.istio_annotations`
| | Include annotations related to Istio.|✓ |`true`,`false`

4+|`ingress_controller.allowed_cidrs`
| | List of allowed ingress CIDR blocks.| |
|===

[[namespaces]]
== Namespaces

Use https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/[Namespaces^] to virtually segment Kubernetes executions.
Domino creates namespaces according to the specifications in this section.
The installer requires that these namespaces do not exist at installation.

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`namespaces.platform.name`
| |Namespace to place Domino services. |✓ |link:https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names[Kubernetes Names^]

4+|`namespaces.compute.name`
| |Namespace for user executions. |✓ |link:https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names[Kubernetes Names^]

NOTE: Might be the same as the platform namespace.

4+|`namespaces.system.name`
| |Namespace for deployment metadata. |✓ |link:https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names[Kubernetes Names^]

4+|`namespaces.istio.name`
| |Namespace for Istio. |✓ |link:https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names[Kubernetes Names^]

4+|`namespaces.*.annotations`
| |Optional annotations to apply to each namespace. | |link:https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations[Kubernetes Annotation^]

4+|`namespaces.*.labels`
| |Optional labels to apply to each namespace. | |link:https://kubernetes.io/docs/concepts/overview/working-with-objects/labels[Kubernetes Labels and Selectors^]
|===

[[storage-classes]]
== Storage classes

https://kubernetes.io/docs/concepts/storage/storage-classes/[Storage Classes^] are a way to abstract the dynamic provisioning of volumes in Kubernetes.

Domino requires the following storage classes:

. `block` storage for Domino services and user executions that need fast I/O.
. `shared` storage that can be shared between multiple executions.

Domino supports pre-created storage classes, although the installer can create a `shared` storage class backed by NFS or a cloud NFS analog as long as the cluster can access the NFS system for read and write, and the installer can create several types of `block` storage classes backed cloud block storage systems like Amazon EBS.

=== Block

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`storage_classes.block.create`
| |Whether to create the block storage class. |✓ |`true`, `false`

4+|`storage_classes.block.name`
| |The block storage class name. |✓ |https://kubernetes.io/docs/concepts/overview/working-with-objects/names/[Kubernetes Name^]

NOTE: Always required due to platform limitations.
Cannot be "" which indicates the default storage class.

4+|`storage_classes.block.type`
| |Type of the block storage class to use. |✓ |For example,

* `ebs`
* `hostpath`
* `gce`
* `azure-disk`
* `ceph-rdb`

4+|`storage_classes.block.base_path`
| |Base path to use on nodes with `hostpath` volumes. | |

4+|`storage_classes.block.default`
| |Whether to set this https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/[storage class as the default^]. |✓ |`true`, `false`
|===

=== Shared

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`storage_classes.shared.create`
| |Whether to create the shared storage class. |✓ |`true`, `false`

4+|`storage_classes.shared.name`
| |The shared storage class name. |✓ |https://kubernetes.io/docs/concepts/overview/working-with-objects/names/[Kubernetes Name^]

4+|`storage_classes.shared.type`
| |Type of the shared storage class to use. |✓ |For example,

* `efs`
* `nfs`
* `azure-file` Azure File requires outbound port 445 to be open from your Azure cluster.
* `cephfs`

4+|`storage_classes.shared.efs.region`
| |EFS store AWS region. | |For example, `us-west-2`

4+|`storage_classes.shared.efs.filesystem_id`
| |EFS filesystem ID. | |For example, `fs-7a535bd1`

4+|`storage_classes.shared.efs.access_point_id`
| |Access Point ID. | |For example, `fsap-01234567`

4+|`storage_classes.shared.nfs.server`
| |NFS server IP or hostname. | |

4+|`storage_classes.shared.nfs.mount_path`
| |Base path to use on the server when you create shared storage volumes. | |

4+|`storage_classes.shared.nfs.mount_options`
| |YAML List of additional NFS mount options. | |For example, `- mfsymlinks`

4+|`storage_classes.shared.azure_file.storage_account`
| |Azure storage account to create file stores. | |
|===

[[blob-storage]]
== Blob storage

Domino can store long-term, unstructured data in blob storage buckets. Only the `shared` storage class described previously (NFS and S3) are supported for `logs`.

`projects` also support Azure storage, and `backups` also supports both Azure and Google Cloud Storage (GCS) storage.

=== S3

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`blob_storage.*.s3.region`
| |AWS region of the S3 bucket store. | |For example, `us-west-2`

4+|`blob_storage.*.s3.bucket`
| |S3 bucket name. | |For example, `domino-bucket-1`

4+|`blob_storage.*.s3.sse_kms_key_id`
| |KMS Key ARN. | |For example, `arn:aws:kms:us-west-2:890728157128:key/mrk-eb66487f9d674b46b3ca3c8b934ddd4b`
|===

=== Azure

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`blob_storage.*.azure.account_name`
| |Azure storage account name. | |For example, `mystorage`

4+|`blob_storage.*.azure.account_key`
| |Access key for the storage account. | |For example, `domino-bucket-1`

4+|`blob_storage.*.azure.container`
| |Name of the container in the storage account. | |For example, `backups`
|===

=== GCS (Google Cloud Storage)

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`blob_storage.backups.gcs.bucket`
| |The bucket name. | |For example, `domino-bucket-1`

4+|`blob_storage.backups.gcs.service_account_name`
| |The service account name with write access to the bucket. | |For example, `bucket-sa`

4+|`blob_storage.backups.gcs.project_name`
| |The service account's project name. | |For example, `myproject`
|===

== Autoscaler

For Kubernetes clusters without native cluster scaling in response to new user executions, Domino supports the use of the https://github.com/kubernetes/autoscaler[cluster autoscaler^].

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`autoscaler.cloud_provider`
| |Cloud provider Domino is deployed with. | |For example, `aws`

4+|`autoscaler.aws.region`
| |AWS region Domino is deployed into. | |For example, `us-west-2`

4+|`autoscaler.azure.resource_group`
| |Azure resource group Domino is deployed into. | |https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/deploy-to-resource-group?tabs=azure-cli[Azure resource group^]

4+|`autoscaler.azure.subscription_id`
| |Azure subscription ID Domino is deployed with. | |https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/deploy-to-resource-group?tabs=azure-cli[Azure subscription ID^]
|===

[[aws-auto-discovery]]
=== AWS Auto Discovery

The cluster autoscaler supports https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler/cloudprovider/aws#auto-discovery-setup[Auto Discovery^] on AWS.
Without any explicit configuration of specific autoscaling groups, it detects all ASGs that have the appropriate tags and refreshes them if their settings are updated directly.
All ASGs must be listed with accurate min/max settings (or not listed at all) is not required as referenced below in the Groups section.
ASG settings can be updated directly in AWS.
The cluster-autoscaler configuration doesn't need to be updated, and you don't need to rerun the installer.

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`autoscaler.auto_discovery.cluster_name`
| |K8s Cluster Name. | |Must *exactly* match the name in AWS

4+|`autoscaler.auto_discovery.tags`
| |Optional: If filled in, `cluster_name` is ignored. | |For example, `- my.tag` or `[]`

4+|`autoscaler.groups`
| | | |Must be set to `[]` if using `auto_discovery`
|===

By default, if no `autoscaler.groups` and `autoscaler.auto_discovery.tags` are specified, the `cluster_name` will be used to look for the following AWS tags:

* `k8s.io/cluster-autoscaler/enabled`
* `k8s.io/cluster-autoscaler/{{ cluster_name }}`

The `tags` setting can be used to explicitly specify which resource tags the autoscaler service must look for.

To disable auto-discovery and use specific `groups`, ensure that `auto_discovery.cluster_name` is an empty value.

=== Groups

Autoscaling groups are not dynamically discovered.
Each autoscaling group must be individually specified, along with the minimum and maximum scaling size.

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`autoscaler.groups.*.name`
| |Autoscaling group name. | |Must *exactly* match the name in the cloud provider

4+|`autoscaler.groups.*.min_size`
| |Minimum scaling size. | |For example, `0`

4+|`autoscaler.groups.*.max_size`
| |Maximum scaling size. | |For example, `10`
|===

== External DNS

Domino can automatically configure your cloud DNS provider.
See https://github.com/kubernetes-sigs/external-dns[external-dns^] for more information.

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`external_dns.provider`
| |Cloud DNS provider. | |For example, `aws`

4+|`external_dns.domain_filters`
| |Only allow access to domains that match this filter. | |For example, `my-domain.example.com`

4+|`external_dns.zone_id_filters`
| |Only allow updates to specific https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zones-working-with.html[hosted zones^]. | |

4+|`txt_owner_id`
| | The owner ID in the TXT record. | |
|===

== Email notifications

Domino supports SMTP to send email notifications in response to user actions and run results.

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`email_notifications.enabled`
| |Whether Domino must send email notifications. |✓ |`true`, `false`

4+|`email_notifications.server`
| |SMTP server hostname or IP. | |

4+|`email_notifications.port`
| |SMTP server port. | |

4+|`email_notifications.enable_ssl`
| |Whether the SMTP server uses SSL encryption. |✓ |`true`, `false`

4+|`email_notifications.from_address`
| |Email address to send emails from Domino with. | |For example, `domino @example.com`

4+|`email_notifications.authentication.username`
| |If you use SMTP authentication, the username. | |

4+|`email_notifications.authentication.password`
| |If you use SMTP authentication, the password. | |
|===

== Monitoring

Domino supports in-cluster monitoring with Prometheus as well as more detailed, external monitoring through New Relic APM and Infrastructure.

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`monitoring.prometheus_metrics`
| |Install Prometheus monitoring. |✓ |`true`, `false`

4+|`monitoring.node_exporter`
| |Enable Node Exporter to generate and expose hardware and OS metrics. |  | `true`, `false`

4+|`monitoring.kube_state_metrics`
| |Enable Kube-State-Metrics to generate and expose cluster-level metrics. |  | `true`, `false`

4+|`monitoring.newrelic.apm`
| |Enable New Relic APM. |✓ |`true`, `false`

4+|`monitoring.newrelic.infrastructure`
| |Enable New Relic Infrastructure.|✓ |`true`, `false`

4+|`monitoring.newrelic.license_key`
| |New Relic account license key. | |

4+|`monitoring.newrelic.managed_controlplane`
| |Configure New Relic for Kubernetes clusters with managed control planes, i.e. EKS, AKS, and GKE.| |

4+|`monitoring.gpu_metrics`
| |Enable GPU metrics.| |
|===

== Grafana Alerts

Domino supports in-cluster alerting with Grafana.
To enable in-cluster alerting, a Grafana contact point needs to be defined to receive alert notifications.

[cols="1a,8a,^2a",options="header"]
|===
|Key |Description |Required

3+|`grafana_alerts.slack.name`
| |The display name of the Slack contact point in the Grafana UI. |

3+|`grafana_alerts.slack.channel`
| |The name of the Slack channel to send alerts to. |

3+|`grafana_alerts.slack.token`
| |The Slack app token used to authenticate to your Slack workspace. |
|===

== Helm

Configuration for the https://helm.sh/[Helm^] repository that stores Domino's charts.

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`helm.version`
| |Which version of Helm to use. |✓ |`2` or `3`

4+|`helm.host`
| |Hostname of the chart repository. |✓ |For Helm 2 this must be `quay.io` or the address of your private app server. For Helm 3 it must be `gcr.io`.

4+|`helm.namespace`
| |Namespace to find charts in the repository. | |Helm repo namespace. When you use official Domino repositories this must be `domino`. For Helm 3 with `gcr.io` or `mirrors.domino.tech`, use `domino-eng-service-artifacts`.

4+|`helm.username`
| |Username for chart repository if authentication is required. When you use Helm 3 with charts hosted in GCR this must be `_json_key`. | |Username

4+|`helm.password`
| |Password for chart repository if authentication is required. | |For Helm 3 this is the base64 encoded JSON key that was provided by Domino.
|===

=== Image registries

List of https://www.docker.com/[Docker^] registries for Domino components.

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`helm.image_registries.*.server`
| |Docker registry host. |✓ |For example,

* `quay.io`
* `mirrors.domino.tech`

4+|`helm.image_registries.*.username`
| |Docker registry username. |✓ |

4+|`helm.image_registries.*.password`
| |Docker registry password. |✓ |
|===

[[internal-docker-registry]]
== Internal Docker registry

The recommended configuration for the internal Docker registry deployed with Domino.
Use override values to allow the registry to use S3, GCS, or Azure blob store as a backend store.
GCS requires a service account already be bound into the Kubernetes cluster with configuration to ensure the `docker-registry` service account is properly mapped.

NOTE: Either `internal_docker_registry` or `external_docker_registry` must be configured.

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`internal_docker_registry.s3_override.region`
| |AWS region of the S3 bucket store. | |For example, `us-west-2`

4+|`internal_docker_registry.s3_override.bucket`
| |S3 bucket name. | |For example, `domino-bucket-1`

4+|`internal_docker_registry.s3_override.sse_kms_key_id`
| |KMS Key ARN. | |For example, `arn:aws:kms:us-west-2:890728157128:key/mrk-eb66487f9d674b46b3ca3c8b934ddd4b`

4+|`internal_docker_registry.gcs_override.bucket`
| |GCS bucket name. | |For example, `domino-bucket-1`

4+|`internal_docker_registry.gcs_override.service_account_name`
| |GCS service account with access to the bucket. | |

4+|`internal_docker_registry.gcs_override.project_name`
| |GCP project name that Domino is deployed into. | |

4+|`internal_docker_registry.azure_blobs_override.account_name`
| |Azure blobstore account name. | |

4+|`internal_docker_registry.azure_blobs_override.account_key`
| |Azure blobstore account key. | |

4+|`internal_docker_registry.azure_blobs_override.container`
| |Azure blobstore container name. | |
|===

== Telemetry

Domino supports user telemetry data to help improve the product.

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`intercom.enabled`
| |Enable Intercom onboarding. |✓ |`true`, `false`

4+|`mixpanel.enabled`
| |Enable MixPanel. |✓ |`true`, `false`

4+|`mixpanel.redacted_mode`
| |When enabled, telemetry data will be redacted to remove potentially sensitive information.|✓ |`true`, `false`
|===

== Support

Domino supports customer service tools to improve customer experience.

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`zendesk.enabled`
| |Enable Zendesk. |✓ |`true`, `false`

4+|`zendesk.token`
| |Zendesk API token. |✓ |
|===

== Domino Governance

Configuration for Domino Governance.

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`ai_governance.enabled`
| |Enable Domino Governance. |✓ |`true`, `false`
|===

== Audit Trail

Domino's Audit Trail feature captures user-generated events to be queried and viewed in the UI.

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`audit_trail.enabled`
| |Enable Audit Trail. |✓ |`true`, `false`
|===


== GPU

If using NVIDIA-based GPU compute nodes, enable the following configuration setting to install the required components:

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`gpu.enabled`
| |Enable GPU support. |✓ |`true`, `false`
|===


== Neuron

If using Neuron Trainium GPU compute nodes, enable the following configuration setting to install the required components:

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`neuron.enabled`
| |Enable GPU support. |✓ |`true`, `false`
|===


== Fleetcommand

Domino supports minor patch upgrades through an internal tool named *Fleetcommand*.
To learn more about the telemetry being sent back to Domino, see link:06c620[Deployments Telemetry].

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`fleetcommand.enabled`
| |Enable ability for Domino staff to apply minor patches. |✓ |`true`, `false`

4+|`fleetcommand.url`
| |The URL to fleetcommand, Domino staff will provide this.| |

4+|`fleetcommand.api_token`
| |Deployment-specific API token (Domino staff will provide this). | |
|===

== Node selectors

Domino will by default deploy some DaemonSets on all available nodes in the host cluster.
When you run Domino in a multi-tenant Kubernetes cluster, where some nodes must not be used by Domino, you can label nodes for Domino with a single, consistent label.
Then, provide that label to `fleetcommand-agent` with the below configuration to apply a selector to all Domino resources for that label.

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`global_node_selectors`
| |List of key/value pairs to use as the label for the selector. |Optional |Example:

[source,YAML]
----
global_node_selectors:
  domino-owned: "true"
----

This example applies a selector for `domino-owned=true` to all Domino deployment resources.
|===

[[pod-configurations]]
== Pod configurations

Global pod configuration that applies to all pods which Domino deploys.

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`global_pod_annotations`
| |List of key and value pairs to use as annotations that apply to all pods. |Optional |Example:

[source,YAML]
----
global_pod_annotation:
  my-annotation: "abc"
----

This example adds an annotation `my-annotation` to all Domino pod resources.

4+|`global_pod_labels`
| |List of key and value pairs to use as labels that apply to all pods. |Optional |Example:

[source,YAML]
----
global_pod_labels:
  my-label: "abc"
----

This example adds a label `my-label` to all Domino pod resources.

4+|`global_pod_environment`
| |List of name and value pairs to use as environment variables that apply to all pods. |Optional |Example:

[source,YAML]
----
global_pod_environment:
  - name: MY_ENV
    value: VALUE
----

This example adds an environment variable `MY_ENV` to all Domino pod resources.
|===

== Image caching

These settings control the Domino image caching service, which runs as a privileged pod and uses the host Docker socket to pre-pull popular Domino environment images onto compute workers.
It can be disabled.

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`image_caching.enabled`
| |Whether or not to deploy the image caching service. |✓ |`true`, `false`
|===

== Certificate management

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`certificate_management.install`
| |Whether to install cert-manager. Domino requires cert-manager, but only one instance of cert-manager can be installed in any given Kubernetes cluster. If your Kubernetes cluster already has cert-manager installed, this should be false.|✓ |`true`, `false`
|===

== Teleport Kubernetes agent

No teleport support is installed if `teleport_kube_agent` is not present.

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`teleport_kube_agent.proxyAddr`
| |The teleport address. |✓ |

4+|`teleport_kube_agent.authToken`
| |The authentication token for Teleport. |✓ |
|===

== Elasticsearch

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`elasticsearch.enable_sysctl`
| | To disable the `sysctlInitContainer` (which runs as root), set the `sysctl vm.max_map_count` correctly through some other method.|✓ |`true`, `false`
|===

== Image building

A builder will use a storage cache as large as the amount specified by the `cache_storage_size`.
When a build completes, the storage cache is reduced to the `cache_storage_retention` amount.
The `cache_storage_retention` value must not be greater than the `cache_storage_size`.

When the underlying OS does not support user namespace mapping, like EKS, you might have to disable rootless building for deployment targets.
Running rootless in an environment that does not support it defaults to using the native filesystem snapshotter and causes image building performance to drop significantly.

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`image_building.verbose`
| | Enable verbose logging.|✓ |`true`, `false`

4+|`image_building.rootless`
| | Run as a non-root user. |✓ |`true`, `false`

4+|`image_building.concurrency`
| | Maximum number of concurrent image builds. | |

4+|`image_building.cache_storage_size`
| | Size of each image builder's cache.| |

4+|`image_building.cache_storage_retention`
| | Amount of storage to keep during garbage collection.| |

4+|`image_building.builder_node_selector`
| | List of key/value pairs to use as the label for the selector. | |

4+|`image_building.pool_sync_wait_time`
| | Duration after which builder cluster is inspected for idle pods.| |Examples: `1h`, `5m`, `30s`

4+|`image_building.pool_max_idle_time`
| | Duration after which idle build pods are terminated. | |Examples: `1h`, `5m`, `30s`

4+|`image_building.fetch_and_extract_timeout`
| | Duration the build waits to fetch and extract the remote Docker context. | |Examples: `1h`, `5m`, `30s`

4+|`image_building.pool_endpoint_watch_timeout`
| | Number of seconds the worker pool waits for a buildkit pod to become ready for traffic.| |
|===

=== Registries

Configures buildkit/hephaestus to support HTTP-only and/or self-signed registries.

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`image_building.registries.*.http`
| | Registry only supports HTTP.|✓ |`true`, `false`

4+|`image_building.registries.*.insecure`
| | Registry uses a self-signed certificate and is considered insecure.|✓ |`true`, `false`
|===

=== Cloud registry authentication

Set up cloud authentication so that the image builder can pull images from and push images to cloud container registries.

==== Azure

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`image_building.cloud_registry_auth.azure.tenant_id`
| | Azure AD Directory (tenant) ID. |✓ |

4+|`image_building.cloud_registry_auth.azure.client_id`
| | Azure AD Application  (client) ID. |✓ |

4+|`image_building.cloud_registry_auth.azure.client_secret`
| | Azure AD Application secret, required when using a Service Principal. |  |

4+|`image_building.cloud_registry_auth.azure.workload_identity`
| | Azure AD Application (client) ID represents a Workload Identity. |✓  | `true`, `false`
|===

==== Google Cloud Platform (GCP)

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`image_building.cloud_registry_auth.gcp.service_account`
| | Service account with access to target GCR/GAR registry. |✓ |
|===


== Service host prefixes

Certain services (currently only apps) can be configured with a separate subdomain.

[cols="3a,4a,^1a,4a",options="header"]
|===
|Key |Description |Required |

4+|`service_host_prefixes.app`
| | Optional subdomain prefix for apps. | |
|===


== Admin Toolkit

The Domino Admin Toolkit compiles troubleshooting data for debugging and support.

[cols="3a,4a,^1a,4a",options="header"]
|===
|Key |Description |Required |

4+|`admin_toolkit.enable_report_uploads`
| | Upload toolkit report to Domino. | | `true`, `false`
|===



== FIPS

Control whether Domino is FIPS 140-2 compliant.

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`fips`
| |Enables a FIPS compliant configuration.|Optional |`true`, `false`
|===

[NOTE]
====
Enabling FIPS compliance will require the following:

. The Kubernetes node image must have FIPS enabled.
. To ensure compliance with the fips-140-2 policy, a FIPS enabled version of Istio must be installed and enabled.
  This can be achieved by setting `fips=true` and `istio.install=true`.
  Otherwise, if you're providing your own version of Istio, it must be FIPS compliant.
. For EKS, the following must be added to link:#pod-configurations[global_pod_environment].

[source,YAML]
----
global_pod_environment:
  - name: AWS_USE_FIPS_ENDPOINT
    value: "true"
----
====

== Domino Flows

Domino Flows enables efficient orchestration and monitoring of multi-step processes while ensuring full lineage and reliable reproducibility.
Domino Flows is implemented on top of Flyte, which is an open-source task orchestrator that facilitates building production-grade data and ML pipelines.
The orchestrator stores task data (inputs and outputs) and task metadata in blob storage.
Configure the following so that Domino Flows has appropriate access to blob storage.

=== AWS

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`flyte.metadata_storage.s3.region`
| |AWS region of the Flyte metadata S3 bucket. | |For example, `us-west-2`

4+|`flyte.metadata_storage.s3.bucket`
| |Flyte metadata S3 bucket name. | |For example, `domino-flyte-metadata-bucket-1`

4+|`flyte.metadata_storage.s3.sse_kms_key_id`
| |KMS key ID associated with the Flyte metadata S3 bucket. | |For example, `2f38143f-a42e-4f11-9ebb-162f11a6c350`

4+|`flyte.data_storage.s3.region`
| |AWS region of the Flyte data S3 bucket. | |For example, `us-west-2`

4+|`flyte.data_storage.s3.bucket`
| |Flyte data S3 bucket name. | |For example, `domino-flyte-data-bucket-1`

4+|`flyte.data_storage.s3.sse_kms_key_id`
| |KMS key ID associated with the Flyte data S3 bucket. | |For example, `2f38143f-a42e-4f11-9ebb-162f11a6c350`

4+|`flyte.cluster_config.eks.aws_account_id`
| |AWS account ID associated with the Flyte S3 buckets. | |For example, `012345678901`

4+|`flyte.cluster_config.eks.controlplane_role_arn`
| |ARN for the AWS role that has access to the Flyte metadata S3 bucket. | |For example, `arn:aws:iam::012345678901:role/domino-flyte-controlplane`

4+|`flyte.cluster_config.eks.dataplane_role_arn`
| |ARN for the AWS role that has access to the Flyte data S3 bucket. | |For example, `arn:aws:iam::012345678901:role/domino-flyte-dataplane`
|===

=== Azure

[cols="1a,8a,^2a,6a",options="header"]
|===
|Key |Description |Required |Values

4+|`flyte.metadata_storage.azure.account_name`
| |Azure storage account name for the Flyte metadata container. | |For example, `domino-flyte-account-name`

4+|`flyte.metadata_storage.azure.account_key`
| |Access key for the Flyte metadata storage account. | |For example, `[to come]`

4+|`flyte.metadata_storage.azure.container`
| |Name of the Flyte metadata container in the storage account. | |For example, `domino-flyte-metadata-container`

4+|`flyte.data_storage.azure.account_name`
| |Azure storage account name for the Flyte data container. | |For example, `[to come]`

4+|`flyte.data_storage.azure.account_key`
| |Access key for the Flyte data storage account. | |For example, `[to come]`

4+|`flyte.data_storage.azure.container`
| |Name of the Flyte data container in the storage account. | |For example, `domino-flyte-data-container`

4+|`flyte.cluster_config.aks.tenant_id`
| |ID of the tenant associated with the storage containers. | |For example, `fc04e151-a39c-40a1-96d9-3913aa8a4a46`

4+|`flyte.cluster_config.aks.subscription_id`
| |Azure subscription ID associated with the storage containers. | |For example, `f914720b-d639-4832-bc9c-4d6f2cdebce4`

4+|`flyte.cluster_config.aks.controlplane_client_id`
| |Azure client ID associated with the metadata storage. | |For example, `3831e719-05b8-470f-a171-a3b34ca50af9`

4+|`flyte.cluster_config.aks.dataplane_client_id`
| |Azure client ID associated with the data storage. | |For example, `01770221-a803-49fb-bda4-94033a87638e`
|===

----- admin_guide/domino-infrastructure/index.txt -----
:page-version: 6.1
:page-title: Domino infrastructure
:page-permalink: 97ba97
:page-order: 20

link:2941b1[Size the Infrastructure]::
Domino application a platform and compute workload in Kubernetes, and each has its own principles for sizing the infrastructure.

link:eca4b2[Manage Compute Resources]::
link:9d16e5[Manage hardware tiers], link:3854c7[scale compute capacity], link:526762[set user executions quota] and link:fa4fa3[execution queue limits], link:7bb387[view platform and compute nodes details], link:2359da[use Intel Habana accelerators], link:fb7da2[manage nodes, node pools], and link:cd38c2[persistent volumes], and more to make sure your users have enough compute power to run Domino clusters.

----- admin_guide/domino-infrastructure/manage-compute-resources/execution-queue-limits.txt -----
:page-version: 6.1
:page-title:  Execution queue limits
:page-permalink: fa4fa3
:page-order: 40

// As an admin, I can configure the number of queue slots for executions per user so that additional user execution requests over that queue limit will be rejected and fail.
[[tr24]]
// As a Domino user, any execution I attempt to start beyond the limit of executions available to me and beyond the number of queue slots available per user will be rejected and fail.
[[tr25]]
// As an admin, I can configure the number of global queue slots for executions on the deployment so that additional execution requests over that queue limit will be rejected and fail.
[[tr26]]
// As a Domino user, any execution I attempt to start beyond the limit of executions available to me and beyond the number of global queue slots available will be rejected and fail.

To prevent too many queued executions from overwhelming a Domino deployment, limit the number of execution slots that can be queued globally or per user.

To do this, use the `com.cerebro.domino.computegrid.userExecutionsQuota.globalExecutionQueueLimit` or `com.cerebro.domino.computegrid.userExecutionsQuota.userExecutionQueueLimit` configuration parameter. See link:71d6ad#compute-grid[Configuration records] for details.

Once a queue limit is reached, additional executions are rejected with a failure status.
The count includes executions for Domino workspaces, jobs, web applications, and any executions that make up an on-demand distributed compute cluster.
For example, an on-demand Spark cluster consumes an execution slot for the master and each Spark executor.

----- admin_guide/domino-infrastructure/manage-compute-resources/hardware-tiers/advanced-hardware-tier-settings.txt -----
:page-version: 6.1
:page-title:  Advanced hardware tier settings
:page-permalink: 4f943d
:page-order: 30

The following advanced hardware tier settings provide you more control.
//more control over what?

[[compute-cluster-specific-hardware-tiers]]
== Restrict to a compute cluster

[[tr11]]
// As an admin, I can set a hardware tier to be restricted to one or more compute clusters supported by the Distributed Compute Operator, and hardware tiers restricted in this way will be the only tiers available to use for the specified type(s) of clusters and will be unavailable to other executions and cluster types.
You can dynamically orchestrate on-demand compute clusters on top of the Domino managed infrastructure.
These workloads often have unique resource requirements, and you might want to create dedicated hardware tiers that will be available only for compute cluster workloads.
In this case, use the *Restrict to compute cluster* setting to specify the compute cluster types, such as Spark or Ray, that must exclusively use a given hardware tier.

When configuring resources for a cluster, the options will be limited to hardware tiers where the *Restrict to compute cluster* setting is selected for the cluster type.
Hardware tiers configured this way will not be available when selecting resources outside of cluster configuration.

[[tr12]]
// As a Domino user, I can use any available unrestricted hardware tier for a compute cluster unless and until one or more hardware tiers have been restricted to that type of cluster.
[NOTE]
====
For a given cluster type, hardware tier choices will be filtered only after at least one restricted hardware tier is configured.
====

[[maximum-simultaneous-executions]]
== Maximum simultaneous executions

[[tr13]]
// As an admin, I can set a hardware tier's Maximum Simultaneous Executions field, and this value will be honored for workloads executed on the tier.

If you want to limit the deployment-wide capacity for a hardware tier, but you do not want to create a dedicated node pool for the hardware tier, you can specify the *Maximum Simultaneous Executions*.

This ensures that no more than the specified number of executions can use the selected hardware tier at the same time.
Additional executions beyond the limit will be queued.

[[overprovison-pod]]
== Overprovision pods

[[tr14]]
// As an admin, I can set a hardware tier's Overprovisioning Pods field, and this will pre-provision an adequate number of nodes required to service the specified number of executions for this hardware tier.
[[tr15]]
// As an admin, I can set a hardware tier's Overprovisioning Pods and Overprovisioning Schedule fields, and this will pre-provision an adequate number of nodes required to service the specified number of executions for this hardware tier during the period(s) specified by the schedule values.

On cloud deployments enabled for autoscaling, when a capacity request is made, new nodes are provisioned.
This option minimizes cost, but provisioning a new node can take several minutes causing users to wait.
The situation can be particularly time-consuming in the mornings when many users first log onto a system that has scaled down overnight.

You can address this problem by overprovisioning several warm slots for popular hardware tiers.
Domino will automatically pre-provision nodes that might be necessary to accommodate the specified number of overprovisioned executions using this hardware tier.
This minimizes the chance that a user must wait for a new node to spin up.

To keep costs under control, you can apply overprovisioning on a scheduled basis for periods when many new users are expected.

To do this:

. Set the number of *Overprovisioning Pods*.
. Select the *Enable Overprovisioning Pods On a Schedule* checkbox.
. Specify the schedule as needed.

[[custom-gpu]]
== Use a custom GPU resource name

[[tr16]]
// As an admin, I can set a hardware tier's Use custom GPU resource name field in order to create and use GPU hardware tiers other than the default type "nvidia.com/gpu".

By default, Domino requests GPU resources of type `nvidia.com/gpu`.
This works well for most NVIDIA GPU-enabled devices, but when your deployment is backed by different GPU devices (for example, NVIDIA MIG GPUs, AMD GPUs, AWS vGPUs, or Xilinx FPGA), you must use a different name for the GPU resources.

Select *Use custom GPU resource name* and enter the GPU resource name that corresponds to the name of the GPU devices being discovered and reported by Kubernetes.

For example, with an NVIDIA A100 GPU configured in MIG Mixed Mode, you can use resources like `nvidia.com/mig-1g.5gb`, `nvidia.com/mig-2g.10gb`, or `nvidia.com/mig-3g.20gb`.

== Allow executions to exceed the default shared memory limit

[[tr17]]
// As an admin, I can set a hardware tier's Allow executions to exceed the default shared memory limit option, and this will allow workloads executed on the tier to exceed the default shared memory limit (64MB).

You can allow hardware tiers to exceed the default limit of 64MB for shared memory.
This is especially beneficial for applications that can use shared memory.

To enable this, select the *Allow executions to exceed the default shared memory limit* checkbox.
This overrides the `/dev/shm` (shared memory) limit, and any shared memory consumption will count toward the overall memory limit of the hardware tier.

Incorporate the size of `/dev/shm` in any memory usage calculations for a hardware tier with this option enabled.
If this option is enabled, you must input the shared memory limit.
For example, suppose the hardware tier has an overall memory limit of 4 GB and you want to limit shared memory to 2 GB, then you should input "2.0" for the shared memory limit.

Setting a shared memory limit greater than the overall memory limit will only allow usage of shared memory up to the overall limit.
For example, if you set a shared memory limit of 6 GB when the hardware tier has an overall memory limit of 4 GB, an execution on the hardware tier can still only use up to 4 GB of shared memory.

[WARNING]
====
`/dev/shm` is considered part of the overall memory footprint of an execution container.
If you allow the hardware tier to exceed the default limit of 64 MB shared memory, be sure that the container's shared memory usage plus regular memory usage is below the overall memory limit, or Kubernetes will terminate the container.
====

== Advanced pod customization

[[tr18]]

When creating or editing a hardware tier in the Admin UI, there is a small *Advanced* link near the bottom of the form.
Clicking this link will reveal text inputs for the following attributes:

* Pod resource requests
* Pod resource limits
* Pod annotations
* Pod labels
* Hugepages
* Capabilities

These attributes apply to all Execution pods, including Workspaces, Jobs, and Apps, as well as Compute Cluster pods.
However, they do not apply to Domino Endpoint pods.

[NOTE]
====
These attributes cannot override any existing Domino-managed fields and will be ignored.
====

=== Pod resource requests / Pod resource limits

Resource requests and limits are applied to the `run` container of Execution pods and the master/worker containers of Compute Cluster pods.
The inputs should consist of newline-delimited key-value pairs in the form `key: value`.

For example:

```
ephemeral-storage: 10Gi
```

[NOTE]
====
Administrators should not specify any CPU or memory requests/limits here, as these values will be overridden by the
existing form inputs: `Cores Requested`, `Cores Limit`, `Memory Requested (GiB)`, `Memory Limit (GiB)`.
The same applies to GPU resources and hugepages.
====

=== Pod annotations

Annotations are applied to all Execution pods and Compute Cluster pods.
The input should consist of newline-delimited key-value pairs in the form `key: value`.

For example:

```
prometheus.io/scrape: "true"
prometheus.io/path: "/metrics"
prometheus.io/port: "9090"
```

This translates to the following pod specification:

```yaml
metadata:
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/path: "/metrics"
    prometheus.io/port: "9090"
```

=== Pod labels

Labels are applied to all Execution pods and Compute Cluster pods.
The input should consist of newline-delimited key-value pairs in the form `key: value`.

For example:

```
example.com/application: my-app
example.com/environment: production
```

This translates to the following pod specification:

```yaml
metadata:
  labels:
    example.com/application: my-app
    example.com/environment: production
```

=== Hugepages

Hugepages are applied to the `run` container of Execution pods and the master/worker containers of Compute Cluster pods.
The input should consist of a key-value pair in the form `page_size: total_size`.

For example:

```
2Mi: 100Mi
```

This translates to the following pod specification:

```yaml
    containers:
      resources:
        limits:
          hugepages-2Mi: 100Mi
        requests:
          hugepages-2Mi: 100Mi
      volumeMounts:
      - mountPath: /hugepages-2Mi
        name: hugepages-2mi
    volumes:
    - name: hugepages-2mi
      emptyDir:
        medium: HugePages-2Mi
```

=== Capabilities

Capabilities are applied to the `run` container of Execution pods and the master/worker containers of Compute Cluster pods.
The input should consist of a newline-delimited list of capabilities.

For example:

```
IPC_LOCK
NET_BIND_SERVICE
```

This translates to the following pod specification:

```yaml
    securityContext:
      capabilities:
        add:
        - IPC_LOCK
        - NET_BIND_SERVICE
```

[NOTE]
====
In order for the capabilities to be added, the Configuration record
`com.cerebro.domino.computegrid.kubernetes.nonRootExecutions.enabled`
must not be set to `true`.
====

----- admin_guide/domino-infrastructure/manage-compute-resources/hardware-tiers/archive-hardware-tier.txt -----
:page-version: 6.1
:page-permalink: 1b34b6
:page-title: Archive a hardware tier
:page-order: 40

WARNING: An archived hardware tier cannot be un-archived.

. In the admin panel, go to *Manage Resources* > *Hardware Tiers*.
. Click *Archive* next to the hardware tier you want to archive.
. Click *Archive Hardware Tier*.

----- admin_guide/domino-infrastructure/manage-compute-resources/hardware-tiers/create-hardware-tier.txt -----
:page-version: 6.1
:page-title:  Create a hardware tier
:page-permalink: c3aaf3
:page-order: 10


[[tr31]]
// As an admin, I can create a new hardware tier.

You must create hardware tiers specifying the CPU, Memory, and GPU for the pods hosting Domino executions.

// As a Domino user, I can specify the hardware tier used to execute a workload.
Users select a hardware tier when launching an execution, which determines the available resources and the node type for the execution.

. Go to *Manage Resources* > *Hardware Tiers*.
. Click *New*.
[[tr2]]
// As an admin, I can create a hardware tier, specifying its resource request and limit attributes.

. Set the following fields to configure the resource requests and limits for execution pods.
+
[[tr3]]
// As an admin, I can set a hardware tier's Cores Requested field, and this value will be honored for workloads executed on the tier.
Name:: The name of the hardware tier that shows to users.
Cores Requested::
The number of requested CPUs.
+
[[tr6]]
// As an admin, I can set a hardware tier's Memory Requested field, and this value will be honored for workloads executed on the tier.
Memory Requested (GiB)::
The amount of requested memory.
+
[[tr7]]
// As an admin, I can set a hardware tier's Memory Limit field, and this value will be honored for workloads executed on the tier.
Memory Limit (GiB)::
The maximum amount of memory.
Domino recommends that this is the same as the request.
+
[[tr8]]
// As an admin, I can set a hardware tier's Allow executions memory limit to exceed a request option, and this will allow workloads executed on the tier to exceed the set value.
If *Allow executions memory limit to exceed request* is not selected, Domino will automatically apply a memory limit equal to the request.
+
[WARNING]
====
Use the option to allow the memory limit to exceed the request  with caution since it can make executions more likely to be evicted under memory pressure.
See https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/[the Kubernetes documentation^] to learn how memory requests and limits influence Kubernetes eviction decisions.
====
+
[[tr4]]
// As an admin, I can set a hardware tier's Cores Limit field, and this value will be honored for workloads executed on the tier.
Cores Limit::
The maximum number of CPUs.
Domino recommends that this is the same as the request.
+
[[tr5]]
// As an admin, I can set a hardware tier's Allow executions to exceed request when unused CPU is an available option, and this will allow workloads executed on the tier to exceed the set value.
If *Allow executions to exceed request when unused CPU is available* is not selected, Domino will automatically apply a CPU limit equal to the request.
[[tr9]]
// As an admin, I can set a hardware tier's Number of GPUs field, and this value will be honored for workloads executed on the tier.
Number of GPUs::
The number of requested GPUs.
The request values, CPU Cores, Memory, and GPUs, are thresholds used to determine whether a node has the capacity to host an execution pod.
These requested resources are effectively reserved for the pod.
The limit values control the amount of resources a pod can use above and beyond the amount requested.
If there's additional headroom on the node, the pod can use resources up to this limit.
+
However, if resources are in contention, and a pod is using resources beyond those it requested and thereby causing excess demand on a node, the offending pod might be evicted from the node by Kubernetes, and the associated Domino Run is terminated.
For this reason, Domino recommends that you set the requests and limits to the same values.
+
Ensure that the CPU, memory, and GPU requests in your hardware tier do not exceed the available resources in the target node pool, considering overhead.
Otherwise, an execution using such a hardware tier will never start.
If you need more resources than are available on existing nodes, you might have to add a new node pool with different specifications.
This might mean adding individual nodes to a static cluster or configuring new auto-scaling components that provision new nodes with the required specifications and labels.
Cents Per Minute Per Run:: Enter how much it will cost (in cents) per minute to run each machine in the hardware tier.
You might notice that when users select a hardware tier, they see the cost in dollars.
The cost is converted from cents to dollars (rounded to the fourth floating digit).
Node Pool:: Specify the underlying machine type on which a Domino execution will run.
+
[[tr10]]
// As an admin, I can set a hardware tier's Node Pool field for multiple hardware tiers, and those tiers will all use the same node pool.
+
Nodes that have the same value for the `dominodatalab.com/node-pool`
https://kubernetes.io/docs/concepts/configuration/assign-pod-node/[Kubernetes node label^] form a *Node Pool*.
Executions with a matching value in the *Node Pool* field will then run on these nodes.
+
As an example, in the previous screenshot, the `large-k8s` hardware tier is configured to use the `default` node pool.
+
The following diagram shows a cluster configured with two node pools for Domino, one named `default` and one named `default-gpu`.
You can label node pools with the same scheme `dominodatalab.com/node-pool=<node-pool-name>` to make additional node pools available to Domino.
The arrows in the diagram represent Domino requesting that a node with a given label be assigned to an execution.
Kubernetes will then assign the execution to a node in the specified pool that has sufficient resources.
+
image::/images/4.x/admin_guide/node-pools.png[alt="Node pools", width=1000]
+
By default, Domino creates a node pool with the label
`dominodatalab.com/node-pool=default` and all compute nodes Domino
creates in cloud environments are assumed to be in this pool.
In cloud environments with automatic node scaling, you configure scaling components like AWS Auto Scaling Groups or Azure Scale Sets with these labels to create elastic node pools.

. Click *Insert*.

NOTE: See link:4f943d[Advanced hardware tier settings]

[[domino-endpoint-tier]]
== Domino endpoint hardware tier

You can define specific hardware tiers for machine learning models deployed as link:a54b4d[Domino endpoints]. This allows you to cater your hardware tiers to the unique demands of machine learning model deployment. Domino endpoint tiers and regular hardware tiers cannot be interchanged, meaning Domino endpoints can only use Domino endpoint tiers, and Domino endpoint tiers cannot be used outside of Domino endpoints.

== Next steps

To learn more, see link:a54b4d[Scale models with Domino endpoint hardware tiers].


----- admin_guide/domino-infrastructure/manage-compute-resources/hardware-tiers/edit-hardware-tier.txt -----
:page-version: 6.1
:page-title: Edit a hardware tier
:page-permalink: 6c2d20
:page-order: 20

[[tr32]]
// As an admin, I can edit a hardware tier.

. In the admin panel, go to *Manage Resources* > *Hardware Tiers*.
. Click *Edit*.

. Make the changes needed and click *Update*.

----- admin_guide/domino-infrastructure/manage-compute-resources/hardware-tiers/hardware-tier-best-practices.txt -----
:page-version: 6.1
:page-title: Hardware tier best practices
:page-permalink: 908bd9
:page-order: 50

Domino Hardware Tiers define Kubernetes requests and limits and link them to specific node pools.
Domino recommends the following best practices.

* Account for overhead
* Isolate workloads and users using node pools
* Isolate compute cluster workloads
* Set resource requests and limits to the same values

[[accounting-for-overhead]]
== Account for overhead


When designing hardware tiers, consider what resources will be available on a given node when Domino submits your workload for execution.
Not all physical memory and CPU cores of your node will be available due to system overhead.

Consider the following overhead components:

. Kubernetes management overhead
. Domino daemon-set overhead
. Domino execution sidecar overhead

Overhead is relevant if you want to define a hardware tier dedicated to one execution at a time per node, such as for a node with a single physical GPU.
It is also relevant if you absolutely must maximize node density.

== Kubernetes management overhead

Kubernetes typically reserves a portion of each node's capacity for daemons and pods that are required for Kubernetes itself.
The amount of reserved resources usually scales with the size of the node, and also depends on the Kubernetes provider or distribution.

See the following for information about reserved resources by cloud-provider managed Kubernetes providers:

* https://github.com/awslabs/amazon-eks-ami/blob/main/templates/al2/runtime/bootstrap.sh[AWS EKS^]
* https://docs.microsoft.com/en-us/azure/aks/concepts-clusters-workloads#resource-reservations[Azure AKS^]
* https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-architecture#node_allocatable[Google GKE^]

=== Get available resources for your instance

. Run the following command to check one of your compute nodes:
+
----
kubectl describe nodes
----
. Find the `Allocatable` section of the output.
This shows the memory and CPU available for Domino.

== Domino daemon-set overhead

Domino runs a set of management pods that reside on each of the compute nodes.
Use these for log aggregation, monitoring, and environment image caching.

The overhead of these daemon-sets is roughly 0.5 CPU core and 0.5 Gi RAM.
This overhead is taken from the allocatable resources on the node.

=== Domino execution overhead

For each Domino execution, there is a set of supporting containers in the execution pod that manages authentication, handling request routing, loading files, and installing dependencies.
These supporting containers make CPU and memory requests that Kubernetes considers when scheduling workspace, job, and app pods.

The supporting container overhead currently is roughly 1 CPU core and 1.5 GiB RAM.
You can configure this so it might vary for your deployment.

.Example

Consider an `m5.2xlarge` EC2 node with a raw capacity of 8 CPU cores and 32 GiB of RAM.

When used as part of an EKS cluster, the node reports the following allocatable capacity of ~27GiB of RAM and 7910m CPU cores.

[source,yaml]
----
Capacity:
  attachable-volumes-aws-ebs:  25
  cpu:                         8
  ephemeral-storage:           104845292Ki
  hugepages-1Gi:               0
  hugepages-2Mi:               0
  memory:                      32120476Ki
  pods:                        58
Allocatable:
  attachable-volumes-aws-ebs:  25
  cpu:                         7910m
  ephemeral-storage:           95551679124
  hugepages-1Gi:               0
  hugepages-2Mi:               0
  memory:                      28372636Ki
  pods:                        58
----

Also, account for 500m CPU and 0.5GiB RAM for the Domino and EKS daemons.

Lastly, for a single execution add 1000m CPU and 1.5GiB RAM for sidecars, and you are left with roughly 6410m CPU and 25GiB RAM that you can use for a single large hardware tier.

If you want to partition the node into smaller hardware tiers, you must account for the sidecar overhead for every execution that you want to co-locate.

As a general rule, larger nodes allow for more flexibility as Kubernetes will take care of efficiently packing your executions onto the available capacity.

[[tr1]]
// As an admin, I can see which pods are running on a specific node by clicking on the node's name on the Admin Infrastructure page.
.See which pods are running on a specific node

. In the Admin application, click *Infrastructure*.
. Click the name of a node.
In the following image, there is a box around the execution pods.
The other pods handle logging, caching, and other services.

image::/images/4.x/admin_guide/pod-info.png[alt="execution pod resource utilization"]



[[isolating-workloads-and-users-using-node-pools]]
== Isolate workloads and users using node pools

.Define node pools

. Go to *Advanced > Hardware Tiers*.
. Create or edit a hardware tier.
. In the *Node Pool* field, enter `your-node-pool` which must match the node pool label such as: `dominodatalab.com/node-pool=<your-node-pool>`.
You can name a node pool anything you like, but Domino recommends naming them something meaningful given the intended use.

[[tr2]]
// As an admin, I can create a hardware tier that uses the Domino "default" node pool.
[[tr3]]
// As an admin, I can create a hardware tier that uses the Domino "default-gpu" node pool.
Domino typically comes pre-configured with `default` and `default-gpu`
node pools, with the assumption that most user executions will run on nodes in one of those pools.
As your compute requirements become more sophisticated, you might want to keep certain users separate from one another or provide specialized hardware to certain groups of users.

For example, if there's a data science team in New York City that needs a specific GPU machine that other teams don't need it, you can use the following label for the appropriate nodes: `dominodatalab.com/node-pool=nyc-ds-gpu`.
In the hardware tier form, you would specify `nyc-ds-gpu`.
To ensure only that team has access to those machines, create a `NYC` organization, add the correct users to the organization, and give that organization access to the new hardware tier that uses the `nyc-ds-gpu` node pool label.


== Isolate compute cluster workloads

Domino on-demand compute clusters often require pooling a large amount of compute resources on specialized hardware (for example, using larger nodes compared to your other workloads).

[[tr4]]
// As an admin, I can restrict a hardware tier to a specific type (or multiple types) of compute cluster.
Consider a use case where you want a set of extra large nodes to be available for on-demand Ray workloads but not available for regular workloads.

. In the admin panel, go to *Manage Resources* > *Hardware Tiers*.
. Create or edit a hardware tier.
. From *Restrict to compute cluster*, select the *Ray* checkbox.

For clusters that are comprised of multiple workload types (for example, Ray head and worker), Domino recommends that you create a separate dedicated Hardware Tier that match the requirements for each type.
In the Ray example, you can create `ray-head` and `ray-worker` Hardware Tiers.


== Set resource requests and limits to the same values

With Kubernetes, resource limits must be greater than or equal to resource requests.
So, if your memory request is 16 GiB, your limit must be greater than or equal to 16 GiB.
But, although setting a request greater than limits can be useful, and there are cases where allowing bursts of CPU or memory can be useful, this is also dangerous.
Kubernetes might evict a pod using more resources than initially requested.
For Domino workspaces or jobs, this would cause the execution to be terminated.

For this reason, Domino recommends setting memory and CPU requests equal to limits.
In this case, Python and R cannot allocate more memory than the limit, and execution pods will not be evicted.

On the other hand, if the limit is higher than the request, a user can use resources that another user's execution pod must be able to access.
This is the noisy neighbor problem that you might have experienced in other multi-user environments.
But, instead of allowing the noisy neighbor to degrade performance for other pods on the node, Kubernetes will evict offending pods when necessary to free up resources.

User data on disk will not be lost, because Domino stores user data on a persistent volume that can be reused.
But, anything in memory will be lost and the execution will have to be restarted.

----- admin_guide/domino-infrastructure/manage-compute-resources/hardware-tiers/index.txt -----
:page-version: 6.1
:page-title: Manage hardware tiers
:page-permalink: 9d16e5
:page-order: 10

Domino allows you to create and manage hardware tiers that your users will use to host executions.

link:c3aaf3[Create a hardware tier]::

Information about how to create hardware tiers and specify the CPU, Memory, and GPU for the pods hosting Domino executions.

link:6c2d20[Edit a hardware tier]::

Steps to edit a hardware tier.

link:4f943d[Advanced settings]::

Information about advanced hardware tier settings that give you more control.

link:1b34b6[Archive a hardware tier]::

Steps to archive a hardware tier.

link:908bd9[Best practices]::

Domino's recommendations on best practices for when you work with hardware tiers.

----- admin_guide/domino-infrastructure/manage-compute-resources/index.txt -----
:page-version: 6.1
:page-title: Manage compute resources
:page-permalink: eca4b2
:page-order: 20

//Need intro material for later versions

link:9d16e5[Manage Hardware Tiers]::
Create, edit, and archive hardware tiers that your users will use to host executions.

link:3854c7[Scale Compute Capacity]::
Learn how Domino relies on Kubernetes to autoscale environments for executions to run on compute resources.

link:526762[User Executions Quota]::
Set a limit on the number of simultaneous executions that a user can have running concurrently.

link:fa4fa3[Execution Queue Limits]::
You can limit the number of execution slots that can be queued to prevent queued executions from overwhelming a Domino deployment.

link:7bb387[View Details of Platform and Compute Nodes]::
View a list and the details of the current platform and compute nodes in your compute grid.

link:2359da[Use Intel Habana Accelerators]::
Add Habana Gaudi accelerators to a hardware tier.

link:fb7da2[Node Pools and Nodes]::
Add, remove, and manage nodes and node pools.

link:cd38c2[Manage Persistent Volumes]::
Manage the persistent volumes that store Domino project files during executions.

----- admin_guide/domino-infrastructure/manage-compute-resources/manage-persistent-volumes/configure-volume-size.txt -----
:page-version: 6.1
:page-title: Configure volume size
:page-permalink: d9ae24
:page-order: 50


Users can configure volume size on a per-project basis on the *Project Settings* page.
You can set the default volume size for new projects, as well as minimum and maximum volume size allowed.
You can edit the following link:71d6ad#workspaces[Configuration records key values] to control the minimum, maximum, and default volume size for new volumes:

* `com.cerebro.domino.workbench.project.defaultVolumeSizeGiB`
* `com.cerebro.domino.workbench.project.minVolumeSizeGiB`
* `com.cerebro.domino.workbench.project.maxVolumeSizeGiB`

----- admin_guide/domino-infrastructure/manage-compute-resources/manage-persistent-volumes/durable-workspace-volume-backups.txt -----
:page-version: 6.1
:page-title: Durable Workspace volume backups on AWS
:page-sidebar: Durable Workspace volume backups
:page-permalink: 097c84
:page-order: 10

Since the data in durable workspace volumes is not automatically written back to the Domino File System, users can lose work if the volume is lost or deleted.
When Domino is running on AWS, it safeguards against this by backing up the EBS volume that backs up the workspace PV with EBS snapshotting to S3.

EBS snapshots can be expensive, especially if you have a lot of users.
Use the following config keys to manage snapshots for your organization.

See link:71d6ad#workspaces[Configuration records] for details about the following config keys to manage snapshots for your organization:

* `com.cerebro.domino.workbench.workspace.volume.enableSnapshots`
* `com.cerebro.domino.workbench.workspace.volume.snapshotCleanupFrequency`
* `com.cerebro.domino.workbench.workspace.volume.numSnapshotsToRetain`

If you accidentally deleted or lost a durable workspace volume that contains data you want to recover, contact Domino support for assistance in restoring from the snapshot.

----- admin_guide/domino-infrastructure/manage-compute-resources/manage-persistent-volumes/enable-automatic-workspace-deletion.txt -----
:page-version: 6.1
:page-title: Enable automatic Workspace deletion
:page-permalink: b66109
:page-order: 40

Domino can automatically delete unused workspaces to make more storage available.
Set link:71d6ad#workspaces[com.cerebro.domino.workbench.workspace.enabledPVCleanup] to `true` to enable this feature.

When a workspace has been stopped for 30 days, Domino marks it for deletion and notifies the workspace user.
Use the `com.cerebro.domino.workbench.workspace.idlePeriodThresholdDays` key to adjust this period.

The user is notified several times during the 14-day grace period.
Use `com.cerebro.domino.workbench.workspace.unusedGracePeriodDays` to adjust the grace period.
Domino notifies the user once more when the workspace is deleted.

----- admin_guide/domino-infrastructure/manage-compute-resources/manage-persistent-volumes/enable-volume-provisioning-recommendations.txt -----
:page-version: 6.1
:page-title: Enable volume provisioning recommendations
:page-sidebar: Enable volume provisioning
:page-permalink: 7be849
:page-order: 30

When a user starts a link:e6e601#launch[workspace] or a link:942549[job], Domino can provide volume provisioning recommendations based on project size and previous usage to reduce storage costs and maximize performance.
This feature is enabled by default.
//how do you turn it off?

----- admin_guide/domino-infrastructure/manage-compute-resources/manage-persistent-volumes/garbage-collection.txt -----
:page-version: 6.1
:page-title: Garbage collection
:page-permalink: c29bdf
:page-order: 20


[NOTE]
====
This feature applies to Idle PVs used for jobs and scheduled jobs, not durable workspaces.
====

Domino has configurable values to help you tune your cluster to balance performance with cost controls.
The more idle volumes you allow, the more likely it is that users can reuse a volume and avoid having to copy project files from the blob store.
However, this comes at the cost of keeping additional idle PVs.

By default, Domino will:

* Limit the total number of idle PVs to 32.
You can modify the `com.cerebro.domino.computegrid.kubernetes.volume.maxIdle` in the link:71d6ad#compute-grid[Configuration records] to adjust this.

* Terminate any idle PV that has not been used in `7` days.
You can modify the `com.cerebro.domino.computegrid.kubernetes.volume.maxAge` in the link:71d6ad#compute-grid[Configuration records] to adjust this.


// legacy content below!


----- admin_guide/domino-infrastructure/manage-compute-resources/manage-persistent-volumes/index.txt -----
:page-version: 6.1
:page-title: Manage persistent volumes
:page-permalink: cd38c2
:page-order: 80

When not in use, Domino project files are stored and versioned in the Domino blob store.
When a Domino execution is started from a project, the project files are copied to a Kubernetes persistent volume (PV) that is attached to the compute node and mounted in the Run.

You can run the following command to see all https://kubernetes.io/docs/concepts/storage/persistent-volumes/[current PVs^], sorted by last-used:

[source,shell]
----
kubectl get pv --sort-by='.metadata.annotations.dominodatalab.com/last-used'
----

== Storage workflow for Jobs

When a user starts a new Job, Domino assigns a new execution pod to the cluster.
This pod will have an associated https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims[Persistent Volume Claim (PVC)^] which defines what type of storage it requires from Kubernetes.

If an idle PV exists that matches the PVC, Kubernetes mounts that PV on the node it assigns to host the pod, and the Job or Workspace starts.
If an appropriate idle PV does not exist, Kubernetes creates a new PV according to the https://kubernetes.io/docs/concepts/storage/storage-classes/[storage class^].

When the user completes their Job or Workspace, the PV data is written to the Domino File System, and the PV unmounts and sits idle until it is either reused for the user's next Job or until link:c29bdf[garbage collection].
By reusing PVs, users who are actively working in a project do not have to repeatedly copy data from the blob store to a PV.

A Job only matches with a fresh PV or one previously used by that project.
PVs are not reused between projects.

== Storage workflow for Workspaces

Workspace volumes are handled differently than volumes for Jobs.
Workspaces are potentially long-lived development environments that users will stop and resume repeatedly without writing data back to the Domino File System each time.
As a result, the PV for the Workspace is a similarly long-lived resource that stores the user's working data.

These workspace PVs are durably associated with the durable Workspace they are initially created for.
Each time that Workspace is stopped, the PV is detached and preserved so that it's available the next time the user starts the Workspace.
When the Workspace starts again, it reattaches its PV and the user will see all their working data saved during the last session.

The contents of the project files in the workspace PV will be written back to the Domino File System only when a user chooses to initiate a sync.
A durable workspace PV will only be deleted if the user deletes the associated workspace.

== Next steps

* link:097c84[Durable Workspace volume backups on AWS]
* link:c29bdf[Garbage collection]
* link:7be849[Enable volume provisioning recommendations]
* link:b66109[Enable automatic Workspace deletion]
* link:d9ae24[Configure volume size]

----- admin_guide/domino-infrastructure/manage-compute-resources/nodes-and-node-pools/add-node-pool.txt -----
:page-version: 6.1
:page-title: Add a node pool
:page-permalink: 3813fa
:page-order: 10

To make a node groups available to Domino, add new Kubernetes worker nodes with a distinct `dominodatalab.com/node-pool` label.
Then, reference the value of that label when you create new
link:908bd9[hardware tiers] or link:9dece2[model resource quotas]
to configure Domino to assign
executions to those nodes.

See below for an example of creating a scalable node pool in EKS.


== Creating a scalable node pool in EKS

This example shows how to create a new
https://eksctl.io/usage/nodegroup-managed/[node group^] with https://eksctl.io/[eksctl^] and expose it to the cluster autoscaler as a
labeled Domino node pool.

. Create a `new-nodegroup.yaml` file like the one below, and configure
it with the properties you want the new group to have. All values shown
with a `$` are variables that you must modify.
+
[source,yaml]
----
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: $CLUSTER_NAME
  region: $CLUSTER_REGION
nodeGroups:
  - name: $GROUP_NAME # this can be any name you choose, it will be part of the ASG and template name
    instanceType: $AWS_INSTANCE_TYPE
    minSize: $DMINIMUM_GROUP_SIZE
    maxSize: $DESIRED_MAXIMUM_GROUP_SIZE
    volumeSize: 400 # important to allow for image caching on Domino workers
    availabilityZones: ["$YOUR_CHOICE"] # this should be the same AZ (or the same multiple AZ's) as your other node pools
    ami:
      $AMI_ID
    labels:
      "dominodatalab.com/node-pool": "$NODE_POOL_NAME" # this is the name you'll reference from Domino
      # "nvidia.com/gpu": "true" # uncomment this line if this pool uses a GPU instance type
    tags:
      "k8s.io/cluster-autoscaler/node-template/label/dominodatalab.com/node-pool": "$NODE_POOL_NAME"
      # "k8s.io/cluster-autoscaler/node-template/label/nvidia.com/gpu": "true" # uncomment this line if this pool uses a GPU instance type
----
+
The AWS tag with key
`k8s.io/cluster-autoscaler/node-template/label/dominodatalab.com/node-pool`
is important for exposing the group to your cluster autoscaler.
+
You cannot have compute node pools in separate, isolated
AZs, as this creates volume affinity errors.
. Once your configuration file describes the group you want to create,
run `eksctl create nodegroup --config-file=new-nodegroup.yaml`.
. Take the names of the resulting ASG and add them to the
`autoscaling.groups` section of your `domino.yml`
link:7f4331[installer configuration].
. Run the Domino installer to update the
autoscaler.
. Create a new
link:908bd9[hardware tier] or link:9dece2[model resource quota]
in Domino that references the new labels.

When finished, you can start Domino executions that use the new Hardware Tier and those executions will be assigned to nodes in the new group, which will be scaled as configured by the cluster autoscaler.

[[create-node-pool-spot-instance]]
== Creating a node pool with spot instances (PREVIEW)

With spot instances, customers’ can leverage the cloud provider's unused capacity and receive a compute environment at a significantly (50-90%) discounted rate. See link:https://aws.amazon.com/ec2/spot/[AWS spot instance webpage^] to learn more. 

One disadvantage of spot instances is that AWS can revoke the allocation of spot instances with just a 2-minute notice when there's high demand for on-demand instances. Typical frequency of interruption by instance type and region can be found on link:https://aws.amazon.com/ec2/spot/instance-advisor/[AWS spot Instance Advisor webpage^]. 

However, due to their cost benefit, customers may want to leverage spot instances for workloads such as Domino endpoints, Domino Apps and small duration Jobs.

NOTE: We recommend using on-demand instances for default and platform node pools.

Follow these steps to use spot instances:

. Add a link:3813fa[new node pool] with spot instances. 
.. Select *Spot instances*.
.. In the https://github.com/dominodatalab/terraform-aws-eks/blob/6ff5ab208565c5149223b9d14756ddd98a63bd46/variables.tf#L111C2-L111C2/[Terraform template^], select *spot = True*.
+
image::/images/5.7/admin-guide/node-pool-spot-instances.png[width=700, alt="Terraform template for spot instances"]
+
. Create a link:9d16e5[new hardware tier] that will map the new node pool with spot instances. We recommend using 'Spot' to clearly differentiate between spot and On-demand hardware tiers. 

=== Spot instance best practices 

*Be flexible in terms of instance types and availability zones* 
A spot instance pool consists of unused EC2 instances with the same instance type and availability zone. You should be flexible about which instance types you request and in which Availability Zones. 

*Use the capacity-optimized allocation strategy* 
Allocation strategies in auto-scaling groups help you deploy your target capacity without manually searching for the spot instance pools with free capacity. 

*Use proactive capacity balancing* 
Capacity rebalancing helps maintain availability by adding a new spot instance to the fleet before a running spot instance receives the 2-minute notification. It balances the capacity-optimized allocation strategy and the policy of mixed entities.

If the node pool is allowed to use spot instances in multiple availability zones (AZs) and one of the AZs gets its spot instance interrupted, the workload will still not be able to come up again, because the EBS volume of the failed workload is tied to the AZ. If you encounter this issue, use the hardware tier that uses non-spot node pool. 

----- admin_guide/domino-infrastructure/manage-compute-resources/nodes-and-node-pools/index.txt -----
:page-version: 6.1
:page-title: Node pools and nodes
:page-permalink: fb7da2
:page-order: 70

link:3813fa[Add a Node Pool]::
Make node groups available to Domino.

link:7965b6[Remove a Node from Service]::
Remove nodes temporarily or permanently.

link:808704[Manage Long-Running Workloads]::
Move pods off of the cordoned node.

----- admin_guide/domino-infrastructure/manage-compute-resources/nodes-and-node-pools/manage-long-running-workloads.txt -----
:page-version: 6.1
:page-title: Manage long-running workloads
:page-permalink: 808704
:page-order: 30


.Move pods off a cordoned node

. For the long-running workloads governed by a https://kubernetes.io/docs/concepts/workloads/controllers/deployment/[Kubernetes deployment^], use the following command to move the pods off of the cordoned node:
+
[source,shell]
----
$ kubectl rollout restart deploy model-5e66ad4a9c330f0008f709e4 -n domino-compute
----
+
The name of the deployment is the same as the first part of the name of the pod in the previous section.
. To see a list of all deployments in the compute namespace, run:
+
----
kubectl get deploy -n domino-compute
----
+
Whether the associated app or model experiences any downtime depends on the update strategy of the deployment.
For the previously described example workloads in a test deployment, one App and one Domino endpoint, you have the following describe output (filtered for brevity):
+
[source,shell]
----
$ kubectl describe deploy run-5e66b65e9c330f0008f70ab8 -n domino-compute | grep -i "strategy|replicas:"
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
RollingUpdateStrategy:  1 max unavailable, 1 max surge

$ kubectl describe deploy model-5e66ad4a9c330f0008f709e4 -n domino-compute | grep -i "strategy|replicas:"
Replicas:               2 desired | 2 updated | 2 total | 2 available | 0 unavailable
StrategyType:           RollingUpdate
RollingUpdateStrategy:  0 max unavailable, 25% max surge
----
+
This App would experience some downtime, since the old pod will be terminated immediately (`1 max unavailable` with only 1 pod currently running).
The model will not experience any downtime since the termination of the old pod will be forced to wait until a new pod is available (`0 max unavailable`).
You can edit the deployments to change these settings and avoid downtime.

== Manage older versions of Kubernetes

Earlier versions of kubernetes do not have the `kubectl rollout restart` command, but you can achieve a similar effect by patching the deployment with a throwaway annotation like this:

[source,shell]
----
$ kubectl patch deploy run-5e66b65e9c330f0008f70ab8 -n domino-compute -p '{"spec":{"template":{"metadata":{"annotations":{"migration_date":"'$(date +%Y%m%d)'"}}}}}'
----

The patching process respects the same update strategies as the previously mentioned restart command.

== Sample commands to retire several nodes

If you have to retire several nodes, you might want to loop over many nodes and/or workload pods in a single command.
To do this, you can customize the output format of `kubectl` commands, filter them, and combine them with `xargs`.

When constructing commands for larger maintenance, always run the first part of the command by itself to verify that the list of names being passed to `xargs` and to the final `kubectl` command are what you expect.

.Cordon all nodes in the default node pool

[source,shell]
----
$ kubectl get nodes -l dominodatalab.com/node-pool=default -o custom-columns=:.metadata.name --no-headers | xargs kubectl cordon
----

.Filter labels to view apps running on a particular node

[source,shell]
----
$ kubectl get pods -n domino-compute -o wide -l dominodatalab.com/workload-type=App | grep <node-name>
----

.Do a rolling restart of all model pods (over all nodes)

[source,shell]
----
$ kubectl get deploy -n domino-compute -o custom-columns=:.metadata.name --no-headers | grep model | xargs kubectl rollout restart -n domino-compute deploy
----

----- admin_guide/domino-infrastructure/manage-compute-resources/nodes-and-node-pools/remove-node-from-service.txt -----
:page-version: 6.1
:page-title: Remove a node from service
:page-permalink: 7965b6
:page-order: 20

There might be times when you have to remove a specific node (or multiple nodes) from service, either temporarily or permanently.
This might include cases of troubleshooting nodes that are in a bad state, or retiring nodes after an link:e79fe6[update to the AMI] so that all nodes are using the new AMI.

This topic describes how to temporarily prevent new workloads from being assigned to a node, as well as how to safely remove workloads from a node so that it can be permanently retired.

== Temporarily remove a node from service

The `kubectl cordon <node>` command prevents additional pods from being scheduled onto the node, without disrupting any of the pods currently running on it.
For example, let's say a new node in your cluster has come up with some problems, and you want to cordon it before launching any new runs to ensure they will not land on that node.
The procedure might look like this:

[source,shell]
----
$ kubectl get nodes
NAME                                          STATUS   ROLES    AGE   VERSION
ip-192-168-0-221.us-east-2.compute.internal   Ready    <none>   12d   v1.14.7-eks-1861c5
ip-192-168-17-8.us-east-2.compute.internal    Ready    <none>   12d   v1.14.7-eks-1861c5
ip-192-168-24-46.us-east-2.compute.internal   Ready    <none>   51m   v1.14.7-eks-1861c5
ip-192-168-3-110.us-east-2.compute.internal   Ready    <none>   12d   v1.14.7-eks-1861c5
$ kubectl cordon ip-192-168-24-46.us-east-2.compute.internal
node/ip-192-168-24-46.us-east-2.compute.internal cordoned
$ kubectl get no
NAME                                          STATUS                     ROLES    AGE   VERSION
ip-192-168-0-221.us-east-2.compute.internal   Ready                      <none>   12d   v1.14.7-eks-1861c5
ip-192-168-17-8.us-east-2.compute.internal    Ready                      <none>   12d   v1.14.7-eks-1861c5
ip-192-168-24-46.us-east-2.compute.internal   Ready,SchedulingDisabled   <none>   53m   v1.14.7-eks-1861c5
ip-192-168-3-110.us-east-2.compute.internal   Ready                      <none>   12d   v1.14.7-eks-1861c5
----

Notice the `SchedulingDisabled` status on the cordoned node.

You can undo this and return the node to service with the command:

[source,shell]
----
kubectl cordon <node>
----

== Permanently remove a node from service

. Identify user workloads
+
Before removing a node from service permanently, you must ensure there are no workloads still running on it that must not be disrupted.
For example, you might see the following workloads running on a node (notice the specification of the compute namespace with `-n` and wide output to include the node hosting the pod with `-o`):
+
[source,shell]
----
$ kubectl get po -n domino-compute -o wide | grep ip-192-168-24-46.us-east-2.compute.internal
run-5e66acf26437fe0008ca1a88-f95mk               2/2     Running     0          23m     192.168.4.206    ip-192-168-24-46.us-east-2.compute.internal    <none>           <none>
run-5e66ad066437fe0008ca1a8f-629p9               3/3     Running     0          24m     192.168.28.87    ip-192-168-24-46.us-east-2.compute.internal    <none>           <none>
run-5e66b65e9c330f0008f70ab8-85f4f5f58c-m46j7    3/3     Running     0          51m     192.168.23.128   ip-192-168-24-46.us-east-2.compute.internal    <none>           <none>
model-5e66ad4a9c330f0008f709e4-86bd9597b7-59fd9  2/2     Running     0          54m     192.168.28.1     ip-192-168-24-46.us-east-2.compute.internal    <none>           <none>
domino-build-5e67c9299c330f0008f70ad1            1/1     Running     0          3s      192.168.13.131   ip-192-168-24-46.us-east-2.compute.internal    <none>           <none>
----
+
Different types of workloads must be treated differently.
. To see the details of a specific workload, run the following command:
+
[source,shell]
----
kubectl describe po run-5e66acf26437fe0008ca1a88-f95mk -n domino-compute
----
The labels section of the describe output is particularly useful to distinguish the type of workload, as each of the workloads named as `run-...` will have a label like `dominodatalab.com/workload-type=<type of workload>.`.
+
The previous example contains one each of the major user workloads:

* `run-5e66acf26437fe0008ca1a88-f95mk` is a link:942549[Job], with label `dominodatalab.com/workload-type=Batch`.
It will stop running on its own once it is finished and disappear from the list of active workloads.
* `run-5e66ad066437fe0008ca1a8f-629p9`, is a link:e6e601[Workspace], with label `dominodatalab.com/workload-type=Workspace`.
It will keep running until the user who launched it shut it down.
You can contact users to shut down their workspaces, waiting a day or two for them to shut them down, or remove the node with the workspaces still running.
+
CAUTION: The last option is not recommended unless you are certain there is no un-synced work in any of the workspaces and have communicated with the users about the interruption.
* `run-5e66b65e9c330f0008f70ab8-85f4f5f58c-m46j7`, is an link:71635d[App], with the label `dominodatalab.com/workload-type=App`.
It is a long-running process, and is governed by a kubernetes deployment.
It will be recreated automatically if you destroy the node hosting it, but will experience whatever downtime is required for a new pod to be created and scheduled on another node.
See below for methods to proactively move the pod and reduce downtime.---------------
* `model-5e66ad4a9c330f0008f709e4-86bd9597b7-59fd9`, is a link:8dbc91[Domino endpoint].
It does not have a `dominodatalab.com/workload-type` label, and instead is easily identifiable by the pod name.
It is also a long-running process, similar to an app, with similar concerns.
See below for methods to proactively move the pod and reduce downtime.---------------
* `domino-build-5e67c9299c330f0008f70ad1` is an link:f51038[Environment].
It will finish on its own and go into a `Completed` state.

----- admin_guide/domino-infrastructure/manage-compute-resources/scale-compute-capacity.txt -----
:page-version: 6.1
:page-title:  Scale compute capacity
:page-permalink: 3854c7
:page-order: 20

[[tr18]]
// As a Domino user, I can set my compute cluster to scale capacity when exceeding a resource threshold, up to the limit of available executions.
[[tr19]]
// As an admin, I can set the duration of time after which an autoscaled cluster will shut idle nodes down.

//WHERE do you set this?

The amount of compute power required for your Domino cluster will fluctuate as users start and stop executions.
Domino relies on Kubernetes to find space for each execution on existing compute resources.
In cloud autoscaling environments, if there's not enough CPU or memory to satisfy a given execution request, the Kubernetes cluster autoscaler will start new compute nodes to fulfill that increased demand.
In environments with static nodes, or in cloud environments where you have reached the autoscaling limit, the execution request will be queued until resources are available.

Autoscaling Kubernetes clusters will shut nodes down when they are idle for more than a configurable duration.
This reduces your costs by ensuring that nodes are used efficiently, and terminated when not needed.

[[tr20]]
// As a Domino user, any execution I start that exceeds the available execution resources will be put in a queue to be executed when those resources become available.
Cloud autoscaling resources have properties like the minimum and maximum number of nodes they can create.
You must set the node maximum to whatever you are comfortable with given the size of your team and expected volume of workloads.
If everything else is equal, it is better to have a higher limit than a lower one, as compute node cost is cheap to start up and shut down, while your users' time is valuable.
If the cluster cannot scale up further, your users' executions will wait in a queue until the cluster can service their request.

----- admin_guide/domino-infrastructure/manage-compute-resources/use-intel-habana-accelerators.txt -----
:page-version: 6.1
:page-title:  Use Intel Habana accelerators
:page-permalink: 2359da
:page-order: 60

// As an admin, I can configure Domino to use Habana accelerators.

Gaudi accelerators from Habana Labs (an Intel company) deliver low-cost-to-train deep learning models for natural language processing, object detection, and image recognition use cases.

AWS currently offers a dl1.24xlarge EC2 image with eight Gaudi accelerators available. See https://aws.amazon.com/ec2/instance-types/dl1/[Amazon EC2 DL1 Instances^].

== 1. Add node pool and hardware tiers

. Create or add a Gaudi-enabled node to an existing link:3813fa[node-pool] in your Domino cluster.
. Create a link:c3aaf3[hardware tier] so users can use this resource in Domino.
.. Select the *Use custom GPU resource name* checkbox.
.. In *GPU Resource Name* enter `habana.ai/gaudi`.

== 2. Install the Habana Device Plugin for Kubernetes

*Prerequisite:* Admin `kubectl` permissions to your cluster

. Use the kubectl command to add the https://docs.habana.ai/en/latest/Installation_Guide/Additional_Installation/Kubernetes_Installation/Intel_Gaudi_Kubernetes_Device_Plugin.html[Habana device plugin for Kubernetes^].
+
----
kubectl create -f
https://vault.habana.ai/artifactory/docker-k8s-device-plugin/habana-k8s-device-plugin.yaml
----
. Run the following command to verify the plugin is running:
+
----
kubectl get pods -n habana-system
----

== 3. Use Gaudi-enabled containers

Many Intel Habana environment containers work natively in Domino.
To use a custom image to create a new environment, paste the Docker registry path into the *FROM* field.
----
vault.habana.ai/gaudi-docker/1.5.0/ubuntu20.04/habanalabs/tensorflow-installer-tf-cpu-2.9.1
----

See the https://developer.habana.ai/catalog_categories/container/[Habana Developer Catalog^] for more pre-built container images that can be used in Domino.

----- admin_guide/domino-infrastructure/manage-compute-resources/user-executions-quota.txt -----
:page-version: 6.1
:page-title:  Set user executions quota
:page-permalink: 526762
:page-order: 30

[[tr21]]
// As an admin, I can configure the executions limit per user so that additional user execution requests over that threshold will wait in a queue until that user's executions fall below the threshold.
[[tr22]]
// As a Domino user, I cannot start an execution if doing so would exceed the available execution resources per user.

To prevent a single user from monopolizing a Domino deployment, limit the number of simultaneous executions a user can run concurrently.

To do this, use the `com.cerebro.domino.computegrid.userExecutionsQuota.maximumExecutionsPerUser` configuration parameter. See link:71d6ad#compute-grid[Configuration records] for details.

After the limit of simultaneously running executions is reached for a user, additional executions are queued.
The count includes executions for Domino workspaces, jobs, web applications, and any executions that make up an on-demand distributed compute cluster.
For example, an on-demand Spark cluster consumes an execution slot for the master and each Spark executor.

----- admin_guide/domino-infrastructure/manage-compute-resources/view-details-of-platform-and-compute-nodes.txt -----
:page-version: 6.1
:page-title:  View platform and compute nodes details
:page-sidebar: Platform and compute nodes
:page-permalink: 7bb387
:page-order: 50

[[tr27]]
// As an admin, I can view the list and details of current platform and compute nodes in my compute grid on the Admin Infrastructure page.

You can view a list and the details of the current platform and compute nodes in your compute grid.
//DEFINE COMPUTE GRID SOMEWHERE!

. In the Admin panel, click *Reports > Infrastructure*.
A list of Platform and Compute nodes opens.

+
Non-Platform nodes with a value in the *Node Pool* column are compute nodes that can be used for Domino executions by configuring a Hardware Tier to use the pool.
. Click the name of a node.
A complete `kubectl describe` description for the node opens, including all applied labels, available resources, and currently hosted pods.

----- admin_guide/domino-infrastructure/sizing-the-infrastructure/index.txt -----
:page-version: 6.1
:page-title: Size the infrastructure
:page-permalink: 2941b1
:page-order: 10

Domino runs in https://kubernetes.io/[Kubernetes^], which is an orchestration framework for delivering applications to a distributed compute cluster.
The Domino application runs the following types of workloads in Kubernetes; each has different principles to sizing infrastructure:

Data plane agent::

Domino recommends increasing resource requests and limits on certain platform pods for larger link:c65074[Domino Nexus] deployments.
The data plane agent is included in this recommendation.
See link:350856[Size the data plane agent] for more information.

Domino Platform::

These always-on components provide user interfaces, the Domino API server, orchestration, metadata and supporting services.
The standard link:a0b173[architecture] runs the platform on a stable set of nodes for high availability, and the capabilities of the platform are principally managed through vertical scaling. This means changing the CPU and memory resources available on those platform nodes and changing the resources requested by the platform components.
See link:a3ed46[Size the Domino Platform].

Asynchronous Domino endpoints::
Asynchronous Domino endpoints may impose additional storage requirements for MongoDB and RabbitMQ.  See link:bfd8e5[Asynchronous Domino endpoints Capacity Planning] for sizing information.

Domino Compute Grid::

These on-demand components run users' data science, engineering, and machine learning workflows.
Compute workloads run on customizable collections of nodes organized into node pools.
The number of these nodes can be variable and elastic, and the capabilities are principally managed through horizontal scaling.
This means changing the number of nodes.

However, when there are more resources present on compute nodes, they can handle additional workloads, and therefore there are benefits to vertical scaling.

Domino uses https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/[Kubernetes requests and limits^] to manage the CPU and memory resources that Domino pods use.
These requests and limits can be scaled to adjust resource consumption and performance.
Container workloads such as databases and search systems, whose data integrity is affected by the enforcement of limits, do not have limits added to their configuration and you must not add limits to them.
See link:eca4b2[Manage Compute Resources] for sizing information.

----- admin_guide/domino-infrastructure/sizing-the-infrastructure/size-async-domino-endpoint.txt -----
:page-version: 6.1
:page-permalink: bfd8e5
:page-title: Asynchronous Domino endpoints capacity planning
:page-order: 30

Asynchronous Domino endpoints may impose additional storage requirements on MongoDB and RabbitMQ depending on the number of API invocations and other factors.

Here are the default volume sizes for MongoDB and RabbitMQ in a new Domino deployment:

* MongoDB storage volume size: `100 GiB`
* RabbitMQ storage volume size: `25 GiB`

The default configuration can support asynchronous Domino endpoints based on the following parameters:

[cols="2,^1",options="header"]
|===
| Parameter | Value
| Number of asynchronous Domino endpoints | 100
| Number of API requests per endpoint per day | 5,000
| MongoDB document size | 10 KiB
| MongoDB document retention | 2 days
| RabbitMQ request message size | 10 KiB
| RabbitMQ response message size | 10 KiB
| RabbitMQ request input queue retention | 2 days
| RabbitMQ response output queue retention | 2 days
|===

The sections below show how to calculate MongoDB and RabbitMQ storage requirements for asynchronous Domino endpoints using the above parameters.

== MongoDB storage requirements

* Number of documents per endpoint per day
+
`Number of API requests per endpoint per day`

* Additional MongoDB storage
+
`Number of asynchronous Domino endpoints` x `Number of documents per endpoint per day` x `MongoDB document size` x `MongoDB document retention`

.Example:

100 endpoints x 5,000 documents per endpoint per day x 10 KiB x 2 days =~ `10 GiB`

== RabbitMQ storage requirements

* Number of API responses per endpoint per day
+
`Number of API requests per endpoint per day`

* RabbitMQ request input queue storage
+
`Number of asynchronous Domino endpoints` x `Number of API requests per endpoint per day` x `RabbitMQ request message size` x `RabbitMQ request input queue retention`

* RabbitMQ response output queue storage
+
`Number of asynchronous Domino endpoints` x `Number of API responses per endpoint per day` x `RabbitMQ response message size` x `RabbitMQ response output queue retention`

* Additional RabbitMQ storage
+
`RabbitMQ request input queue storage` + `RabbitMQ response output queue storage`

.Example:

(100 endpoints x 5,000 requests per endpoint per day x 10 KiB x 2 days) + (100 endpoints x 5,000 responses per endpoint per day x 10 KiB x 2 days) =~ `20 GiB`

== Minimum available storage

To prevent asynchronous Domino endpoints from filling the disks, Domino monitors MongoDB and RabbitMQ disk storage metrics and stops accepting new asynchronous Domino endpoint requests when the available disk storage falls below the following configurable thresholds:

[cols="2,^1",options="header"]
|===
| Configuration Records Key | Default Value
| `com.cerebro.domino.modelApis.async.mongoDb.minimumAvailableBytes` | 500000000
| `com.cerebro.domino.modelApis.async.rabbitmq.availableAsyncLimitBytes` | 1000000000
|===

----- admin_guide/domino-infrastructure/sizing-the-infrastructure/size-data-plane-agent.txt -----
:page-version: 6.1
:page-permalink: 350856
:page-title: Size the data plane agent
:page-order: 10

For larger link:c65074[Domino Nexus] deployments, Domino recommends increasing resource requests and limits on certain platform pods.
The data plane agent is included in this recommendation.
If you are deploying a remote link:5781ea[data plane], you should also consider the resource sizing for that agent.

== Size the data plane agent

You can use Helm values like these to size the data plane agent:
[source,yaml]
----
agent:
  deployment:
    resources:
      requests:
        cpu: 2000m
        memory: 2Gi
      limits:
        cpu: 4000m
        memory: 4Gi
----

== Sizing tips

If the Domino control plane is not hosted in AWS, remote data planes upload DFS artifacts via a proxy hosted by the `nucleus-frontend` deployment in the control plane.
//How does this statement relate to sizing?

If there are a large number of remote jobs syncing a large number of files, performance may be improved by increasing the number of replicas for this deployment and/or the resource requests and limits.

----- admin_guide/domino-infrastructure/sizing-the-infrastructure/size-the-domino-platform/index.txt -----
:page-version: 6.1
:page-permalink: a3ed46
:page-title: Size the Domino Platform
:page-order: 20


The resource needs of the Domino Platform depend primarily on the peak user demand the deployment aims to support.
User demand is the number of users and the amount of work they're starting in Domino.
You can measure this by the peak number of concurrent executions.
The resources used by those executions do not substantially change the effect on the Domino Platform, therefore, you must scale around the total number of executions.

The following diagram shows the Domino Platform supplying essential lifecycle, data, and support services to user executions running on the Domino Compute Grid.
Regardless of the size of the compute grid infrastructure or resources requested by each compute pod, when the compute grid manages concurrent executions, it will max out its capacity to supply the essential services.

image::/images/5.2/admin_guide/platform-sizing.png[alt="Sizing the platform", width=500]

For the default size shown, the link:2941b1[maximum number of concurrent executions is 300]. See link:33c18d[Standard Domino Platform] for more information.

For more sizing options, see link:b9350c[Large Domino Platform] and link:519e3e[XL Domino Platform].

----- admin_guide/domino-infrastructure/sizing-the-infrastructure/size-the-domino-platform/large-size.txt -----
:page-permalink: b9350c
:page-title: Large Domino Platform
:page-version: 6.1
:page-order: 20

If the concurrent execution capability of the standard size is insufficient, use the below recommendations to deploy a large size.

Consider this resource sizing a base recommendation. You may need to change resources relative to the recommendation to support each customer's unique use of Domino. It is critical to pair this sizing with what you observe in link:d30657[monitoring] and make other adjustments accordingly.

Domino recommends that you consult your Customer Success Manager for customizations and advice before you deploy this model.

== Platform infrastructure details

[cols="^2a,^1a", options="header"]
|===
| Parameter                   | Value
| Number of platform nodes    | 5
| CPU per node                | 16 cores
| Memory per node             | 64 GB
| Maximum concurrent executions | 600
|===

== Deploy the platform

If you use this sizing model, ensure that your platform node pool can scale up to the number of platform nodes above.
Then, add the following resource request and limit overrides to the link:7f4331[fleetcommand-agent configuration file]:

[source,yaml]
----
include::../../../../../../static/attachments/6.1/domino-sizing-large.yaml[]
----

To check the status of your available platform resources and allocation, run the following:

----
kubectl describe nodes -l dominodatalab.com/node-pool=platform | grep -A 10 Allocated
----

== Next steps

Learn how to link:eca4b2[manage compute resources].

----- admin_guide/domino-infrastructure/sizing-the-infrastructure/size-the-domino-platform/standard-size.txt -----
:page-version: 6.1
:page-permalink: 33c18d
:page-title: Standard Domino Platform
:page-order: 10

== Sizing strategy

Consider the following resource sizing recommendation a base recommendation. You might need to change resources relative to the recommendation. It is critical to pair this sizing with what you observe in link:d30657[monitoring] and make other adjustments accordingly.

== Platform infrastructure details

[cols="^2a,^1a", options="header"]
|===
| Parameter                   | Value
| Number of platform nodes    | 3
| CPU per node                | 16 cores
| Memory per node             | 64 GB
| Maximum concurrent executions | 300
|===

== Deploy the platform

The Domino fleetcommand-agent installer assumes the default sizing model. Merge any overrides to the
link:7f4331[fleetcommand-agent configuration file] to apply them and then run the installer.

[IMPORTANT]
====
It is critical to preserve values by running the installer. Applying _ad hoc_ `kubectl edit` changes can result in a loss of configuration on upgrade!
====


== Next steps

Learn how to link:eca4b2[manage compute resources].
----- admin_guide/domino-infrastructure/sizing-the-infrastructure/size-the-domino-platform/xl-size.txt -----
:page-version: 6.1
:page-permalink: 519e3e
:page-title: XL Domino Platform
:page-order: 30

== Sizing strategy

If the concurrent execution capability of the standard size is insufficient, use the below recommendations to deploy a XL size platform.

Consider this resource sizing a base recommendation. You may need to change resources relative to the recommendation to support each customer's unique use of Domino. It is critical to pair this sizing with what you observe in link:d30657[monitoring] and make other adjustments accordingly.

Domino recommends that you consult your Customer Success Manager for customizations and advice before you deploy this model.

== Platform infrastructure details

[cols="^2a,^1a", options="header"]
|===
| Parameter                   | Value
| Number of platform nodes    | 6
| CPU per node                | 16 cores
| Memory per node             | 64 GB
| Maximum concurrent executions | 1500
|===

If you are running more than 1500 executions, consult your Customer Success Manager for customizations and advice.

== Deploy the platform

If you use this sizing model, ensure that your platform node pool can scale up to the number of platform nodes shown above. Merge the following resource request and limit overrides to the link:7f4331[fleetcommand-agent configuration file] to apply them and then run the installer.

[IMPORTANT]
====
It is critical to preserve values by running the installer. Applying _ad hoc_ `kubectl edit` changes can result in a loss of configuration on upgrade.
====

[source,yaml]
----
include::../../../../../../static/attachments/6.1/domino-sizing-xl.yaml[]
----

To check the status of your available platform resources and allocation, run the following:

----
kubectl describe nodes -l dominodatalab.com/node-pool=platform | grep -A 10 Allocated
----

== Next steps

Learn how to link:eca4b2[manage compute resources].
----- admin_guide/feedback.txt -----
:page-version: 6.1
:page-permalink: cc8a91
:page-title: Send feedback
:page-order: 150

Your input helps us prioritize development work that makes a difference to our customers.
You can send feedback to Domino Data Lab directly from the Domino UI.

You can suggest new features or improvements, tell us about your experience using the product, and include screenshots to illustrate your feedback.
Optionally, you can consent to being contacted by Domino for further discussion.

Click *Account > Feedback* to send your feedback to the Domino team.

NOTE: The feedback feature does not open a support ticket.
For help with technical issues, link:80328c[contact our Technical Support team].

As a Domino admin, you can set some options for the feedback feature in the link:71d6ad#_feedback[Configuration records]:
// The TR anchor is suppressing the human-readable anchor name for that section; to do = fix this.

* Enable the *Feedback* button in Domino.
+
The default is `true`. If you want to disable the *Feedback* button, set it to `false`.

* The name of the feedback sender
+
The default is `Domino feedback`.
You can leave this as-is for anonymity or change it to the name of your organization.

* The sender's email address
+
This is the email address from which the feedback is sent.
You can change it to the email address of one of your organization's representatives.
Users can also choose to include their own email addresses in the feedback.

* The recipient's email address
+
Do not change this value without guidance from Domino Data Lab.

----- admin_guide/get-help/additional-resources.txt -----
:page-version: 6.1
:page-permalink: 78d384
:page-title: Additional resources
:page-order: 10

For more information about Domino, use these resources:

https://domino.ai/[Domino Data Lab website^]:: Explore the website.
You will find https://www.dominodatalab.com/solutions[solutions^], including use cases, https://www.dominodatalab.com/resources/[resources^] such as white papers and reports, information about our https://www.dominodatalab.com/partners[partners^], and more.

https://support.domino.ai/[Knowledge Base^]::
Search articles written by our support team.

https://domino.ai/blog/[Data Science blog^]::
You can read data science tips and tutorials from leading Data Scientists.
Subscribe to have the content sent directly to your inbox.

https://university.domino.ai/[Training^]::
Use Domino University to learn about Domino.

NOTE: You must get an access code from your Customer Service Manager to use the Domino Academy.

If you still need assistance, contact link:80328c[Domino Technical Support].

//Would be nice to tell them how to submit doc suggestions!

----- admin_guide/get-help/browser_support.txt -----
:page-version: 6.1
:page-permalink: 75302e
:page-title: Browser support
:page-order: 50

As part of our effort to make data science teams more productive by
providing a secure, central system of record, Domino strives to deliver a consistent, equal user experience across whichever platforms you prefer.

Domino supports the following browsers on Windows and MacOS desktop:

* Google Chrome (latest 3 major versions)
* Firefox (latest 3 major versions)
* Safari (latest 3 major versions)
* Microsoft Edge (latest 3 major versions)

The following browsers are officially not supported:

* Internet Explorer
* Mobile browsers
* Beta, preview, or other pre-release versions of desktop browsers

----- admin_guide/get-help/contact_support.txt -----
:page-version: 6.1
:page-permalink: 1be375
:page-title: Contact Technical Support
:page-order: 30

You can submit a ticket to the Domino technical support team in the following ways:

. Submit a ticket through our ticketing portal at https://support.domino.ai[support.domino.ai^].
. Email support@dominodatalab.com.

For critical issues, always submit a ticket through the web form or Help in the Domino application.

== Guidelines

* Provide as much information as you can.
* Provide steps to reproduce your problem.
* For issues with a specific Run, include a link to the run and a screenshot of the error.
* For issues with the CLI, include the contents of `domino.log`  from the folder in which you ran the command.
* For issues with projects, include a link to the project.
* For issues with environments, include a link to the environment.

----- admin_guide/get-help/index.txt -----
:page-version: 6.1
:page-permalink: 90423b
:page-title: Get help
:page-order: 140

link:78d384[Additional resources]::
Refers you to more information about Domino.

link:1be375[Contact Technical Support]::
How to create a support ticket.

link:12e54b[Support bundles]::
How to retrieve a ZIP file with logs and reports to share with Domino support.

link:75302e[Browser support]::
Lists the browsers that Domino supports.

----- admin_guide/get-help/support_bundle.txt -----
:page-version: 6.1
:page-permalink: 12e54b
:page-title: Support Bundles
:page-order: 40


When working with the Domino support team, you might be asked to retrieve a support bundle ZIP file that contains logs and reports from Domino components with information about a Domino execution.

Support bundles are available for Jobs, Workspaces, and Apps.

== Download Job support bundle from the UI

For Jobs, you can simply click the download link by going to: *Jobs detail* > *Logs* > *Support bundle* in the Domino UI.

== Download the support bundle programmatically using execution ID

You can retrieve a support bundle with an execution ID.
To find the execution ID, look in the execution assignment messages in the *Setup Output* in the logs panel for the execution.

The execution assignment message uses the following template:

[source,shell]
----
Successfully assigned <node-pool>/run-<execution-id>-<pod-id> to <node-name>
----

After you have the execution ID, go to the following URL to retrieve that execution's support bundle:

[source,shell]
----
<domino-url>/v4/admin/supportbundle/<execution-id>
----


For example, if you have an execution assignment message like `Successfully assigned aws-staging-compute/run--5e17a24d74904f0007099b9b-tq582 to ip-...`, the execution ID is 
`-5e17a24d74904f0007099b9b`.

To retrieve the support bundle, go to:


[source,shell]
----
<domino-url>/v4/admin/supportbundle/<5e17a24d74904f0007099b9b>
----

----- admin_guide/index.txt -----
:page-version: 6.1
:page-title: Admin Guide
:page-permalink: b35e66

Domino unifies infrastructure and data to drive innovation, enhance control, and lower costs for all data science and AI initiatives.

It lets you empower data science teams with the tools they love and with the governance guardrails to ensure safe, compliant, and cost-effective AI. Domino helps IT deliver the promise of AI value with the most flexible, scalable, and safe architecture for your AI operations.

* *Improve productivity*: Deliver self-serve tools, secure access to data anywhere, and standardized computing in IT-governed sandboxes.
* *Run AI anywhere*: On hybrid, multi-cloud, or on-prem to optimize costs, ensure compliance, and avoid vendor lock-in.
* *Slash operational costs*: Simplify AI management by unifying commercial and open-source tools, disparate stacks, and data silos on one future-ready platform.

image::/images/6.1/ecosystem-new.png[alt="Domino machine learning ecosystem", role=noshadow, width=1200]

== How do we do it? 
Domino is an open system that provides self-service access to data and tools, enables the reuse of materials, and facilitates collaboration with other teams in your organization while enforcing best practices, enhancing knowledge, and improving efficiency.

* link:fbb41f[Data]: Domino connects to external data sources such as databases, data warehouses, and data lakes. The complete list of supported data sources is available under link:fbb41f[Data Source Connectors].
* link:d2ba79[LLMs]: Our deep learning models are trained on extensive datasets for language processing tasks. Based on their training data, they generate new text that mimics human language.
* link:a8e081[Software]: Domino can be connected to link:0792cf[Jira], link:00f51f[GitHub], link:cce362[MLflow], and link:57edf2[Sagemaker]. This enables seamless integration of your data science workflows and allows users to track progress on data science projects.
* link:08a636[Languages]: Domino allows data scientists to use their preferred languages and tools, such as Python, SAS, Matlab, and R.
* link:867b72[IDEs and Tools]: You can use IDEs and tools like Jupyter Notebook, JupyterLab, RStudio, VS Code, MATLAB, and SAS with Domino.
* link:3164f4[Packages and Libraries]: We support a range of packages and libraries, including open-source options such as Python, R, TensorFlow, PyTorch, and others.
* link:799193[Compute Environments]: You can add different environments to any Domino installation by visiting https://quay.io/[quay.io^]. Our link:fa8137[documentation] has step-by-step instructions for setting up these environments. 

== Administering Domino

These sections provide instructions for installing, operating, administering, and configuring Domino in your Kubernetes cluster. This content applies to Domino systems with self-installation licenses.

* link:4bb6a6[Installation]: Domino is a Kubernetes native platform. You can install it on many different kinds of infrastructure.
* link:30fc1c[Configuration]: The Domino custom resource definition (CRD), reconciled by the Platform Operator, contains a configuration field in its specification used to store your Domino cluster configuration.
* link:3164f4[Platform management]: Administer and manage your Domino platform to make sure it fulfills the needs of your organization.
* link:c91e77[Security and compliance]: Domino can be configured with various security and compliance services.
* link:3539d1[Authentication]: Domino uses https://www.keycloak.org/[Keycloak^] authentication service that runs in a pod on your Domino cluster to view, create, and manage groups of users.
* link:de48cc[User management]: Learn how to manage users in Domino to control permissions, set up accounts, and track activity.
* link:43a1b8[Operations]: Monitor and optimize the performance of Domino systems.
* link:5522a2[Upgrade Domino]: Learn how to upgrade your Domino systems.

== Domino Cloud

If you want to run Domino as a managed service in your cloud or in a single-tenant vendor cloud,
https://www.dominodatalab.com/contact-us/[contact Domino^].

Professional Services will handle installation, operations, and administration for customers running Domino as a managed service and the content of this guide is not applicable.

----- admin_guide/installation/bootstrap.txt -----
:page-permalink: b09679
:page-version: 6.1
:page-title: Bootstrap a Domino Cluster
:page-sidebar: Bootstrap a Domino Cluster
:page-order: 30

IMPORTANT: Before following this guide, you must first review and satisfy any target environment-specific requirements covered under link:e6ea06[Install Domino].

For release tags and versions, please refer to the link:5246aa[releases] page.

To bootstrap a Domino cluster:

. link:598b40[Download and install] the platform operator's companion command line tool, `ddlctl`.
. Export your `quay.io` credentials to your shell as `QUAY_USERNAME` and `QUAY_PASSWORD` (if you do this, `ddlctl` will use them automatically).
. Make sure Docker is installed and the daemon is running on your machine.
. Make sure you have access to the target cluster with sufficient permissions to create namespaces, secrets, and deploy resources.
. Get the `fleetcommand-agent` image tag for your target Domino version on the link:5246aa[releases] page.
. Run the following command:
+
[source,shell,subs="attributes"]
----
ddlctl bootstrap
----

NOTE: It is not strictly necessary to supply flags to `bootstrap` as it will interactively prompt you for any missing information.

The `bootstrap` subcommand will walk you through a series of operations:

* Creating the `domino-operator` namespace.
* Creating the registry secret to be used by the platform operator for pulling images from `quay.io`.
* Installing the platform operator Helm chart.
* Creating a configuration for your Domino cluster that conforms to the link:7f4331[schema].
* Generating a `Domino` custom resource (CR) for your Domino cluster.
* Applying the `Domino` CR to your Kubernetes cluster on confirmation.
* Tailing the logs of the ensuing operator job until its completion.

== Bootstrap an existing Domino cluster

The above instructions can be used to bootstrap an entirely new Domino cluster or to migrate an existing cluster to use the platform operator. The principal distinction between the two is:

* For a new cluster, the `bootstrap` subcommand will generate a new, empty configuration file for you to fill in.
* With an existing cluster, you will need to supply a path containing the existing configuration to add to the `Domino` custom resource.

== Skip generating and applying a Domino custom resource

If you prefer to approach things piecemeal, you can bootstrap the operator independently of generating and applying a `Domino` custom resource. Just supply `bootstrap` with the `--skip-domino-resource` flag, i.e.:

[source,shell,subs="attributes"]
----
ddlctl bootstrap --skip-domino-resource
----

To then create a `Domino` custom resource separately (whether with a new or existing configuration), follow the link:e6ea06[Install Domino] guide.

----- admin_guide/installation/index.txt -----
:page-version: 6.1
:page-title: Installation
:page-permalink: 4bb6a6
:page-order: 40
:page-separator: true
:page-section: System administration

Domino is a Kubernetes native platform.
You can install it on many different kinds of infrastructure.

link:8fe42a[Requirements]::
Review the requirements to install Domino.

link:b09679[Bootstrap a Domino cluster]::
Bootstrap a Domino cluster with the Platform Operator.

link:88f534[Platform Operator]::
Install, upgrade, and manage Domino with the Platform Operator.

link:e6ea06[Install Domino]::
Install Domino with the Platform Operator.

* *link:e3bf0a[Amazon Web Services (AWS) deployments]*
+
Install Domino on Amazon Elastic Kubernetes Service (EKS).

* *link:bf888c[Azure deployments]*
+
Install Domino on Azure Kubernetes Service (AKS).

* *link:e96f4c[Google Cloud Platform (GCP) deployments]*
+
Install Domino on Google Kubernetes Engine (GKE).

* *link:87b601[Private or offline installation]*
+
Install Domino without a connection to the Internet.

* *link:3fb861[OpenShift]*
+
Install Domino using OpenShift.

link:8d3953[Set up custom certificates]::
Configure Domino to connect to services that use custom certificates that are external to the Domino cluster.

link:0fa80e[User acceptance testing]::
Verify that Domino features work as expected.

link:5246aa[Releases]::
View latest releases.

----- admin_guide/installation/manage-deployments/aws/architecture.txt -----
:page-permalink: 91566e
:page-version: 6.1
:page-title: AWS deployment architecture
:page-sidebar: Architecture
:page-order: 10

These topics describe how to install Domino on link:https://aws.amazon.com/eks/[Amazon Elastic Kubernetes Service (EKS)^].
EKS is hosted on link:https://aws.amazon.com/what-is-aws/[Amazon Web Services (AWS)^].
When you deploy Domino to EKS, you must have the following services and components:

image::/images/5.1/aws-arch-new.png[alt="A map of the Amazon Web Services architecture you'll need to set up a Domino deployment", width=1200, role=noshadow]

.Runtime platform
* *A*: link:https://docs.aws.amazon.com/eks/latest/userguide/clusters.html[EKS cluster^] deployed in three Availability Zones (AZ).
The cluster must match our link:7b2cbe[compatible Kubernetes versions].

.Nodes/instances: link:https://docs.aws.amazon.com/eks/latest/userguide/eks-compute.html[EKS nodes^]
* *B*: Platform nodes: 4-6 nodes, m7i-flex.2xlarge
* *C*: Compute nodes: m6i.2xlarge
* *D*: GPU compute nodes: g5.2xlarge

.Storage
* *E*: Shared file system, Datasets: link:https://aws.amazon.com/efs/[Amazon Elastic File System (EFS)^].
* *F*: Project files, Logs, and Backups: https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingBucket.html[Amazon Simple Storage Service (S3) Buckets^].
* *G*: Environment and model images: link:https://aws.amazon.com/ecr/[Amazon Elastic Container Registry (ECR)^].

.Networking
* *H*: Ingress load balancer: link:https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/what-is-load-balancing.html[Elastic load balancer^].
* *I*: Cluster network: link:https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html[Amazon Virtual Private Cloud (VPC)^].

----- admin_guide/installation/manage-deployments/aws/deploy.txt -----
:page-permalink: c1eec3
:page-version: 6.1
:page-title: Deploy Domino on AWS Elastic Kubernetes Services (EKS)
:page-sidebar: Deploy Domino on AWS EKS
:page-order: 40

:preset-defined:

This topic describes how to deploy Domino components on link:https://aws.amazon.com/eks/[Amazon Elastic Kubernetes Service (EKS)^].
EKS is hosted on link:https://aws.amazon.com/what-is-aws/[Amazon Web Services (AWS)^].

== Create an EKS configuration file

. Get the `$FLEETCOMMAND_AGENT_TAG` for your target release from the link:5246aa[releases] page.

. Use environment variables to set some values used by the `ddlctl` CLI.
This simplifies the commands you'll run while installing Domino components:
+
[source, shell]
----
unset HISTFILE
export QUAY_USERNAME=<`quay.io` username provided by Domino>
export QUAY_PASSWORD=<`quay.io` password provided by Domino>
export FLEETCOMMAND_AGENT_TAG=<Tag that corresponds to the version of Domino deployed>
----

. Generate an EKS configuration file.
+
For a standard control plane install:
+
[source,shell]
----
ddlctl create config --agent-version $FLEETCOMMAND_AGENT_TAG --preset eks
----
+
For a data plane install:
+
[source,shell]
----
ddlctl create config --agent-version $FLEETCOMMAND_AGENT_TAG --preset data-plane-eks
----
+
IMPORTANT: Changing the defaults in the generated configuration can affect the deployment. If you must adjust its parameters, contact a Domino representative.

. Gather Terraform values to update your configuration file:
+
* If you deployed your infrastructure using the link:https://github.com/dominodatalab/terraform-aws-eks[terraform-aws-eks^] module version v3.0.1 or above, you can use the included link:https://github.com/dominodatalab/terraform-aws-eks/tree/main/examples/deploy#usage[tf.sh^] script:
+
[source,shell]
----
./tf.sh infra output domino_config_values
----
+
* Otherwise use:
+
[source,shell]
----
terraform output domino_config_values
----

. For both control plane and data plane installs, open the `domino.yml` file and edit the attributes as follows:

* `name`: The name of the deployment. This can't be changed post-deployment.
* `hostname`: The hostname for the Domino install (for example, `domino.example.com`).

* `storage_classes.block.parameters.kmsKeyId`: KMS key for block storage.

* `storage_classes.shared.efs.region`: AWS region for the EFS system.
* `storage_classes.shared.efs.filesystem_id`: EFS file system ID.
* `storage_classes.shared.efs.access_point_id`: EFS access point ID.
+
For control plane installs, also configure:

* `autoscaler.auto_discovery.cluster_name`: Name of the k8s cluster.
* `autoscaler.aws.region`: The AWS deployment region.
+
NOTE: Configure only either `external_docker_registry` or `internal_docker_registry`. The `external_docker_registry` should only be set during new installations. If you are upgrading and have previously configured `internal_docker_registry`, you must continue to use it.

* `external_docker_registry`: Specifies the ECR Repository URL.
* `internal_docker_registry.s3_override.region`: AWS region for the S3 registry bucket.
* `internal_docker_registry.s3_override.bucket`: S3 bucket name for internal Docker registry.
* `internal_docker_registry.s3_override.sse_kms_key_id`: KMS key for S3 internal Docker registry bucket.

* `blob_storage.projects.s3.region`: S3 bucket region for projects.
* `blob_storage.projects.s3.bucket`: S3 bucket name for projects.
* `blob_storage.projects.s3.sse_kms_key_id`: KMS key for S3 projects bucket.

* `blob_storage.logs.s3.region`: S3 bucket region for logs.
* `blob_storage.logs.s3.bucket`: S3 bucket name for logs.
* `blob_storage.logs.s3.sse_kms_key_id`: KMS key for S3 logs bucket.

* `blob_storage.backups.s3.region`: S3 bucket region for backups.
* `blob_storage.backups.s3.bucket`: S3 bucket name for backups.
* `blob_storage.backups.s3.sse_kms_key_id`: KMS key for S3 backups bucket.

. Configure the load balancer by adding the code below to the end of the file. Replace `<SSL certificate arn>` and `<Monitoring bucket name>` with the values for your installation. Note that the CIDR `0.0.0.0/0` for `loadBalancerSourceRanges` can be updated to restrict access to certain CIDR blocks.
+
[source,yaml]
----
release_overrides:
  nginx-ingress:
    chart_values:
      controller:
        kind: Deployment
        hostNetwork: false
        service:
          enabled: true
          externalTrafficPolicy: "Local"
          type: LoadBalancer
          annotations:
            service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy: 'ELBSecurityPolicy-TLS-1-2-2017-01'
            service.beta.kubernetes.io/aws-load-balancer-ssl-cert: '<SSL certificate arn>'
            service.beta.kubernetes.io/aws-load-balancer-type: 'nlb'
            service.beta.kubernetes.io/aws-load-balancer-internal: 'false'
            service.beta.kubernetes.io/aws-load-balancer-backend-protocol: 'ssl'
            service.beta.kubernetes.io/aws-load-balancer-ssl-ports: '443'
            service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: '3600'
            service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: 'true'
            service.beta.kubernetes.io/aws-load-balancer-access-log-enabled: 'true'
            service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval: '5'
            service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name: '<Monitoring bucket name>'
            service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix: 'ELBAccessLogs'
          loadBalancerSourceRanges:
            - 0.0.0.0/0 # should always be 0.0.0.0/0, networkPolicy does the enforcement
        config:
          use-forwarded-headers: 'false'
          ssl-ciphers: 'ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA384:AES128-GCM-SHA256:AES128-SHA256:AES256-GCM-SHA384:AES256-SHA256:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!PSK:!aECDH:!EDH-DSS-DES-CBC3-SHA:!EDH-RSA-DES-CBC3-SHA:!KRB5-DES-CBC3-SHA'
          ssl-protocols: 'TLSv1.2 TLSv1.3'
      networkPolicy:
        ipCidrs:
          - 10.0.0.0/16   # node network
          - 100.64.0.0/16 # pod network
          - 0.0.0.0/0     # access list starts here
----

NOTE: We have switched to Network Load Balancers. Existing installs should either continue to use the existing Classic Load Balancer configuration or otherwise be prepared to adjust DNS for the replacement Network Load Balancer on upgrade, as Kubernetes will destroy the currently provisioned Classic Load Balancer when changing the configuration.

For reference, this is the old Classic Load Balancer configuration:

[source,yaml]
----
release_overrides:
  nginx-ingress:
    chart_values:
      controller:
        kind: Deployment
        hostNetwork: false
        service:
          enabled: true
          type: LoadBalancer
          annotations:
            service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy: ELBSecurityPolicy-TLS-1-2-2017-01
            service.beta.kubernetes.io/aws-load-balancer-ssl-cert: <SSL certificate arn>
            service.beta.kubernetes.io/aws-load-balancer-internal: false
            service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp
            service.beta.kubernetes.io/aws-load-balancer-ssl-ports: '443'
            service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: '3600'
            service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: '*'
            service.beta.kubernetes.io/aws-load-balancer-access-log-enabled: 'true'
            service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval: '5'
            service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name: <Monitoring bucket name>
            service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix: ELBAccessLogs
          loadBalancerSourceRanges:
            - 0.0.0.0/0
          targetPorts:
            http: http
            https: http
        config:
          use-proxy-protocol: 'true'
----

== Install Domino

.Use the following command:

[source,shell]
----
ddlctl create domino --config {filepath-of-config-created-in-previous-step}  --agent-version $FLEETCOMMAND_AGENT_TAG
----

include::../../../../../../static/content-reuse/deploy-domino.adoc[tag=ingress]

include::../../../../../../static/content-reuse/deploy-domino.adoc[tag=dns]

IMPORTANT: Create a canonical name (CNAME) to this host in your DNS, not an address record (A record).

include::../../../../../../static/content-reuse/deploy-domino.adoc[tag=validation]

----- admin_guide/installation/manage-deployments/aws/index.txt -----
:page-permalink: e3bf0a
:page-version: 6.1
:page-title: AWS
:page-order: 10

These topics describe how to install Domino on link:https://aws.amazon.com/eks/[Amazon Elastic Kubernetes Service (EKS)^].
EKS is hosted on link:https://aws.amazon.com/what-is-aws/[Amazon Web Services (AWS)^].

link:91566e[AWS Deployment Architecture]::
See an overview of the services and components you need to deploy Domino in AWS.

link:4ff9e1[Prerequisites]::
Review the prerequisites before you install Domino on an EKS cluster.

link:e0f2ff[Provision infrastructure with Terraform]::
Provision your infrastructure with Domino's recommended Terraform module.

link:c1eec3[Deploy Domino on AWS EKS]::
Deploy Domino to the cloud.

link:13423d[Upgrade Kubernetes in AWS EKS]::
Upgrade Kubernetes to keep it compatible with your Domino deployment.

----- admin_guide/installation/manage-deployments/aws/infrastructure.txt -----
:page-permalink: e0f2ff
:page-version: 6.1
:page-title: Provision Terraform infrastructure for AWS
:page-sidebar: Provision infrastructure
:page-order: 30

IMPORTANT: A Domino install can't be hosted on a subdomain of another Domino install.
For example, if you have Domino deployed at `data-science.example.com`, you can't deploy another instance of Domino at `acme.data-science.example.com`.

:tf-mod-version: v3.27.0

== Provision your infrastructure with Domino's recommended Terraform module

Domino strongly recommends that you use our specialized link:https://github.com/dominodatalab/terraform-aws-eks/tree/{tf-mod-version}[Terraform module^] designed for AWS, which is publicly accessible.

This module offers an efficient method for provisioning your environment, and it can also serve as a reference if you opt for manual infrastructure setup.

[NOTE]
====
If you encounter the following error when running link:https://github.com/dominodatalab/terraform-aws-eks/blob/{tf-mod-version}/examples/deploy/set-mod-version.sh[set-mod-version.sh^]:

`Error: The MOD_VERSION {tf-mod-version} is not a suitable tag for the modules source.`

set the `MOD_VALIDATION_OFF` variable in your shell, for example:
```
export MOD_VALIDATION_OFF=true
```
====

== Validated module versions

For Domino version `{version}`, we have validated compatibility with Terraform module version `{tf-mod-version}`. When following the deployment instructions, please use `MOD_VERSION={tf-mod-version}`. Detailed instructions can be found at link:https://github.com/dominodatalab/terraform-aws-eks/tree/{tf-mod-version}#bootstrap-module[Deploy using a module^].

== Karpenter support

Starting with module version v3.26.1, the Domino Terraform module supports Karpenter. Instructions can be found in the link:https://github.com/dominodatalab/terraform-aws-eks/tree/{tf-mod-version}#karpenter[module documentation^].
----- admin_guide/installation/manage-deployments/aws/kubernetes-upgrade.txt -----
:page-permalink: 13423d
:page-version: 6.1
:page-title: Amazon Elastic Kubernetes Services (EKS) upgrade guide
:page-sidebar: EKS upgrade guide
:page-order: 50

This topic describes how to upgrade Kubernetes in your link:https://aws.amazon.com/eks/[Amazon Elastic Kubernetes Service (EKS)^] Domino deployment.
EKS is hosted on link:https://aws.amazon.com/what-is-aws/[Amazon Web Services (AWS)^].

[IMPORTANT]
====
* Immediately after the Kubernetes upgrade, you must link:5522a2[upgrade Domino] to a link:7b2cbe[compatible version of Kubernetes].
Domino will not work as expected until this is completed.
+
For example, after upgrading Kubernetes to v1.22, you must upgrade to Domino v5.2 or later. Kubernetes v1.22 is not compatible with older versions of Domino.
Similarly, after upgrading to Kubernetes 1.23 or 1.24, you must upgrade to Domino v5.3 or later.

* The CDK automation at link:https://github.com/dominodatalab/cdk-cf-eks[cdk-cf-eks^] has been deprecated. If you have infrastructure provisioned with CDK you will need to migrate to Terraform using the link:https://github.com/dominodatalab/cdk-cf-eks/tree/master/convert[CDK to Terraform convert^] utility, before upgrading to Kubernetes 1.25.
====

== Terraform

If you deployed your infrastructure using the link:https://github.com/dominodatalab/terraform-aws-eks[terraform-aws-eks^] module version v3.0.1 or above, follow link:https://github.com/dominodatalab/terraform-aws-eks/tree/main/examples/deploy#kubernetes-upgrade[Upgrading K8s^]. Otherwise, there are 2 options:

. Upgrade the module to version v3 using link:https://github.com/dominodatalab/terraform-aws-eks/tree/main/bin/state-migration[State Migration^], then follow link:https://github.com/dominodatalab/terraform-aws-eks/tree/main/examples/deploy#kubernetes-upgrade[Upgrading K8s^].
. Follow the instructions below to perform the update with prior versions.

.Review the prerequisites

To upgrade Kubernetes on a Terraform-provisioned cluster you must have the following files used/created during the cluster creation:

* Terraform state file: `terraform.tfstate`
* Variables file: `domino-terraform.auto.tfvars`
* Terraform configuration file: `main.tf`

You'll also need a Unix or Linux terminal with the following:

* link:https://learn.hashicorp.com/tutorials/terraform/install-cli[Terraform^] installed with an active session (see the link:https://github.com/dominodatalab/terraform-aws-eks#requirements[Terraform module requirements^] for compatible versions).
* link:https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html[Amazon Web Services Command Line Interface (AWS CLI)^] installed.


.Update cluster and node_groups
. Set AWS credentials in the environment:
+
[source,shell]
----
export AWS_ACCESS_KEY_ID='_FILL_ME_IN_'
export AWS_SECRET_ACCESS_KEY='_FILL_ME_IN_'
export AWS_REGION='_FILL_ME_IN_'
----

. Validate that there are no pending changes:
+
[source,shell]
----
terraform plan
----
+
The following message indicates no pending changes:
+
[source,shell]
----
No changes. Your infrastructure matches the configuration.
----

. Open the `domino-terraform.auto.tfvars` file and add/edit the `k8s_version` attribute with the desired Kubernetes version:
+
[source,shell]
----
k8s_version = '_FILL_ME_IN_'
----
+
NOTE: If you are using custom images for the node groups you will need to provide the appropriate Amazon Machine Image (AMI).

. Validate the desired changes:
+
[source,shell]
----
terraform plan -out=terraform.plan
----

. Update the cluster and `node_groups`:
+
[source,shell]
----
terraform apply terraform.plan
----

The upgrade takes some time as `terraform apply` performs these actions:

* The control plane is upgraded to the desired version.
* The latest `amazon-eks-node` version is retrieved and applied to the `managed-node-groups`. The update is detailed in link:https://docs.aws.amazon.com/eks/latest/userguide/managed-node-update-behavior.html[Managed nodes update behavior^].

----- admin_guide/installation/manage-deployments/aws/prerequisites.txt -----
:page-permalink: 4ff9e1
:page-version: 6.1
:page-title: Prerequisites
:page-order: 20

== Review the prerequisites to provision with Terraform

To install and configure Domino in your AWS account, you must have:

* An AWS account.
* `quay.io` credentials provided by Domino.
* A SSL certificate for your domain in link:https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html[AWS Certificate Manager (ACM)^].
* A Unix or Linux terminal with the following:
** link:https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html[Amazon Web Services Command Line Interface (AWS CLI)^] installed.
** link:https://learn.hashicorp.com/tutorials/terraform/install-cli[Terraform^] installed with an active session (see the link:https://github.com/dominodatalab/terraform-aws-eks#requirements[Terraform module requirements^] for compatible versions).
** link:https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/[Kubectl^] installed.
** link:https://docs.docker.com/engine/install/[Docker^] installed.
** link:https://helm.sh/docs/intro/install/[Helm^] installed.
** The link:c3298b[`ddlctl` command line] installed.
* The link:88f534[Platform Operator] installed.

== Validate the prerequisites

Run the following:

[source, shell]
----
aws --version
terraform version
kubectl version --client=true --short=true
docker version
helm version
----

== Validate AWS credentials

Validate AWS credentials by making sure that the following command returns the `UserId`, `Account`, and `Arn`:

[source,shell]
----
aws sts get-caller-identity
----

== Verify that the required binaries are installed

Ensure you have a Unix or Linux terminal with the following installed:

NOTE: If you already have cluster access in your local kubeconfig, the `aws` CLI steps are not necessary.

[source, shell]
----
# ddlctl
ddlctl version

# aws-cli
aws --version

# kubectl
kubectl version --client=true --short=true

# docker daemon is installed and running
docker --version
docker ps
----

Otherwise, follow these steps to install those that are missing:

. Install the `aws-cli` by following link:https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html[these instructions^].

. Install `kubectl` by following link:https://kubernetes.io/docs/tasks/tools/[these instructions^].

. Install Docker and exit the terminal.
When you log in again, the modifications that you just made will become active.

. Install link:598b40[ddlctl].

== Configure cluster access

. Run the following:
+
[source, shell]
----
unset HISTFILE
export DEPLOY_NAME=<Name of deployment>
export AWS_REGION=<The region to deploy the resources>
export AWS_ACCESS_KEY_ID=<Your AWS access key ID>
export AWS_SECRET_ACCESS_KEY=<Your AWS secret key>
----

. To retrieve the credentials for your Kubernetes cluster, check your local kubeconfig with:
+
[source,shell]
----
export KUBECONFIG=$(pwd)/kubeconfig
----

. Update the kubeconfig:
+
[source,shell]
----
aws eks update-kubeconfig --kubeconfig $KUBECONFIG --region $AWS_REGION --name $DEPLOY_NAME --alias $DEPLOY_NAME
----

----- admin_guide/installation/manage-deployments/azure/architecture.txt -----
:page-permalink: 4a4332
:page-version: 6.1
:page-title: Azure deployment architecture
:page-sidebar: Architecture
:page-order: 10

This topic describes the architecture for a Domino deployment on link:https://docs.microsoft.com/azure/aks/intro-kubernetes[Azure Kubernetes Service (AKS)^].
AKS is hosted on link:https://azure.microsoft.com/overview/[Azure^].
When you deploy Domino to AKS, you must have the following services and components:

image::/images/diagrams/azure-architecture.png[alt="A map of the Azure architecture to set up a Domino deployment", width=1000, role=noshadow]
// Original diagram in Lucid: https://lucid.app/lucidchart/a578cdea-1d3b-4319-becb-fd5e40a7c99d/edit?invitationId=inv_4d333cfd-bf6a-418b-b38f-d01a255ecff4

*Runtime platform (https://azure.microsoft.com/en-us/products/kubernetes-service[Azure Kubernetes Service^]):* +
*A -* AKS cluster deployed in three https://learn.microsoft.com/en-us/azure/reliability/availability-zones-overview[Availability Zones^].
The cluster must match Domino's compatible Kubernetes versions.

*Nodes/instances (https://learn.microsoft.com/en-us/azure/aks/create-node-pools[AKS node pools^]):* +
*B -* System Pool: Scales 1-3 per zone, Standard_DS4_v2. +
*C -* Platform Pool: Scales 1-3 per zone, Standard_D8S_v4. +
*D -* Compute Pool: Scales 0-10 per zone, Standard_D8S_v4. +
*E -* GPU Pool: Scales 0-3 per zone, Standard_NC6s_v3.

*Networking:* +
*F -* https://learn.microsoft.com/en-us/azure/load-balancer/load-balancer-overview[Ingress load balancer^].

*Storage:* +
*G -* Environment and model images; https://azure.microsoft.com/en-us/products/container-registry[Azure Container Registry^]. +
*H -* https://learn.microsoft.com/en-us/azure/storage/common/storage-account-overview[Storage Account^]; Azure Files access for shared file system; https://azure.microsoft.com/en-us/products/storage/files[Datasets^]; https://azure.microsoft.com/en-us/products/storage/blobs[Azure Blob^] API access for Project files, Logs, and Backups.

----- admin_guide/installation/manage-deployments/azure/deploy.txt -----
:page-permalink: ebc28f
:page-version: 6.1
:page-title: Deploy Domino on AKS
:page-sidebar: Deploy Domino on AKS
:page-order: 40
:preset-defined:

== Set up an HTTPS certificate

. Run the following to create the `domino-platform` namespace:
+
[source,shell]
----
kubectl create namespace domino-platform
----

. To make your application available through HTTPS, use the certificate for the project's domain name to create a secret:
+
[source, shell]
----
kubectl -n domino-platform create secret tls my-cert --key=<path to your private key> --cert=<path to your cert>
----

== Create an AKS configuration file

. Get the `$FLEETCOMMAND_AGENT_TAG` for your target release from the link:5246aa[releases] page.

. Use environment variables to set some values used by the `ddlctl` CLI.
This simplifies the commands you'll run while installing Domino components:
+
[source, shell]
----
unset HISTFILE
export QUAY_USERNAME=<`quay.io` username provided by Domino>
export QUAY_PASSWORD=<`quay.io` password provided by Domino>
export FLEETCOMMAND_AGENT_TAG=<Tag that corresponds to the version of Domino deployed>
----

. Generate an AKS configuration file.

* Gather the required parameters which you will add to the generated configuration file when you link:#env-params[enter the environment parameters in the configuration template]:
** TENANT_ID: ID of the tenant where AKS was deployed.
** IMAGE_BUILD_CLIENT_ID: The image building client id created by terraform.
** IMAGE_BUILD_WORKLOAD_IDENTITY: Whether the image build client id is a workload identity.
** REG_DNS_NAME: The DNS name of the container registry created by terraform in the AKS resource group.
** STORAGE_ACCOUNT_NAME: The name of the storage account created by terraform in the AKS resource group.
** STORAGE_ACCOUNT_KEY: The key of the storage account created by terraform in the AKS resource group.
** STORAGE_ACCOUNT_CONTAINER_NAME: The name of the container in the storage account created by terraform in the AKS resource group.

* Run the following:
+
[source,shell]
----
ddlctl create config --agent-version $FLEETCOMMAND_AGENT_TAG --preset aks
----
+
IMPORTANT: Changing the defaults in the generated configuration can affect the deployment. If you must adjust its parameters, contact a Domino representative.

. Review your generated configuration file and edit the attributes as follows, referencing the environment variables you collected earlier.
+
[[env-params]]

* `name`: The name of the deployment. This can't be changed post-deployment.
* `hostname`: The hostname for the Domino install (for example, `domino.example.com`).
* `storage_classes.block.type`: `azure-disk`
* `storage_classes.shared.type`: `azure-file`
* `storage_classes.shared.azure_file.storage_account`: `""`
+
IMPORTANT: `storage_classes.shared.azure_file.storage_account` must be an empty string to correctly default to the AKS cluster's default file store.
+
* `blob_storage.projects.azure.account_name`: STORAGE_ACCOUNT_NAME value.
* `blob_storage.projects.azure.account_key`: STORAGE_ACCOUNT_KEY value.
* `blob_storage.projects.azure.container`: STORAGE_ACCOUNT_CONTAINER_NAME value.
* `blob_storage.logs.type`: `shared`
* `blob_storage.backups.type`: `shared`
* `blob_storage.backups.azure.account_name`: STORAGE_ACCOUNT_NAME value.
* `blob_storage.backups.azure.account_key`: STORAGE_ACCOUNT_KEY value.
* `blob_storage.backups.azure.container`: STORAGE_ACCOUNT_CONTAINER_NAME value.
* `helm.image_registries.*.username`: Your `quay.io` username.
* `helm.image_registries.*.password`: Your `quay.io` password.
* `image_building.cloud_registry_auth.azure.tenant_id`: TENANT_ID value.
* `image_building.cloud_registry_auth.azure.client_id`: IMAGE_BUILD_CLIENT_ID value.
* `image_building.cloud_registry_auth.azure.workload_identity`: IMAGE_BUILD_WORKLOAD_IDENTITY value.
* `image_building.cloud_registry_auth.azure.client_secret`: optional CLIENT_SECRET value.
* `internal_docker_registry` : `null`
* `external_docker_registry`: The container registry DNS name.
+
NOTE: If you have DFS project files stored in Azure File Storage, you can contact Domino's Customer Success team for assistance migrating that data to an Azure Blob Storage deployment.

. Add the following code to the end of the file:
+
[source,yaml]
----
release_overrides:
  nginx-ingress:
    chart_values:
      controller:
        kind: Deployment
        hostNetwork: false
        service:
          enabled: true
          type: LoadBalancer
          annotations:
            service.beta.kubernetes.io/azure-load-balancer-health-probe-request-path: "/healthz"
        extraArgs:
          default-ssl-certificate: domino-platform/my-cert
----

== Install

With your configuration file ready, you can create a `Domino` custom resource using `ddlctl`:

[source,shell]
----
$ ddlctl create domino --config {path-to-config-yaml} --agent-version $FLEETCOMMAND_AGENT_TAG
----

If you would prefer to just generate the `Domino` custom resource YAML, you can supply the `--export` flag and pipe the result to a file.

When the installation completes successfully, you should see a message that says:

[source,console]
----
2019-11-26 21:20:20,214 - INFO - fleetcommand_agent.Application - Deployment complete.
Domino is accessible at $YOUR_FQDN
----

include::../../../../../../static/content-reuse/deploy-domino.adoc[tag=ingress]

include::../../../../../../static/content-reuse/deploy-domino.adoc[tag=dns]

[NOTE]
====
* You must enable WebSockets so Domino can sync files and workspaces.
In most cases, WebSockets are enabled by default.
However, some content delivery networks (CDNs) don't support WebSockets.

* If you use https://learn.microsoft.com/azure/frontdoor/front-door-overview[Azure Front Door^] or a similar CDN that doesn't support WebSockets, you must route incoming traffic so that it skips the CDN.

* As an alternative, https://learn.microsoft.com/azure/application-gateway/application-gateway-websocket[Application Gateway^] has native WebSocket support.
====

include::../../../../../../static/content-reuse/deploy-domino.adoc[tag=validation]

----- admin_guide/installation/manage-deployments/azure/index.txt -----
:page-permalink: bf888c
:page-version: 6.1
:page-title: Azure
:page-order: 20

These topics describe how to install Domino on link:https://docs.microsoft.com/azure/aks/intro-kubernetes[Azure Kubernetes Service (AKS)^].
AKS is hosted on link:https://azure.microsoft.com/overview/[Azure^].

link:4a4332[Azure deployment architecture]::
See an overview of the services and components you must have to deploy Domino.

link:f691db[Prerequisites]::
Review the prerequisites before you install Domino.

link:bfdaf6[Provision Infrastructure with Terraform]::
Provision your infrastructure with Domino's recommended Terraform module.

link:ebc28f[Deploy Domino]::
Deploy Domino to the cloud.

link:d41d8c[Kubernetes upgrade guide in AKS]::
Upgrade Kubernetes to keep it compatible with your Domino deployment.

----- admin_guide/installation/manage-deployments/azure/infrastructure.txt -----
:page-permalink: bfdaf6
:page-version: 6.1
:page-title: Provision Terraform infrastructure for Azure
:page-sidebar: Provision infrastructure
:page-order: 30

Use this topic to provision infrastructure with link:https://azure.microsoft.com/overview/what-is-azure/[Microsoft Azure^].
After the infrastructure is in place, you can deploy Domino on link:https://azure.microsoft.com/services/kubernetes-service/[Azure Kubernetes Service (AKS)^].

[IMPORTANT]
====
* A Domino install can't be hosted on a subdomain of another Domino install.
For example, if you have Domino deployed at `data-science.example.com`, you can't deploy another instance of Domino at `acme.data-science.example.com`.
* Do not run commands using AZ Command Line Interface (CLI) version 2.33.0.
This includes shells that run AZ CLI version 2.33.0, such as Azure Cloud Shell.
That version of AZ CLI contains a bug that prevents `vm create` from working.
====

== Orchestrate the installation

. To start a VM in your Azure environment before you have a dedicated bastion or Azure VM, use link:https://docs.microsoft.com/azure/azure-resource-manager/management/manage-resource-groups-portal[Azure Portal^] or https://docs.microsoft.com/azure/azure-resource-manager/management/manage-resources-cli[Azure CLI^] to run the following:
+
[source,shell]
----
az vm create \
--resource-group $RG_NAME \
--name bastion \
--image UbuntuLTS \
--admin-username azureuser \
--generate-ssh-keys
----

. Sign in to the VM.
. Install the packages:
+
[source,shell]
----
apt update
curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -
curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash
curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
mv kubectl /usr/local/bin
chmod +x /usr/local/bin/kubectl
apt-add-repository "deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main"
apt-get update && sudo apt-get install terraform
apt install -y containerd docker.io
----

. Add `azureuser` to the Docker group:
+
[source,shell]
----
+usermod -a -G docker azureuser
----

. Sign out to apply the group change:
+
[source,shell]
----
logout
----

. Sign in to the VM again.

. Sign in to Azure:
+
[source,shell]
----
az login
----

== Provision your infrastructure with Domino's Terraform module

Domino recommends that you use our public-facing link:https://github.com/dominodatalab/terraform-azure-aks[Azure-specific Reference Terraform module^].
You can also reference this module to manually provision the environment and infrastructure.

. Create a resource group in your Azure subscription.

. Open the `main.tf` link:/attachments/{version}/azure-main.tf[file] and edit the following attributes:
+
* `domino-aks.api_server_authorized_ip_ranges`: The IP ranges to allow incoming traffic into the server nodes.
Enter `null` to allow all traffic.
* `domino-aks.resource_group`: The name of the resource group that you created in the previous step.
* `domino-aks.deploy_id`: The name of the AKS cluster to create.
* `domino-aks.kubernetes_version`: The Kubernetes version.
* Optional: `domino-aks.namespaces`: Update if you don't want to use domino-platform and domino-compute.
* Optional: `domino-aks.node_pools.compute.vm_size`: Update if you don't want to use DS8_v2 SKU.
* Optional: `domino-aks.node_pools.compute.gpu`: Update if you don't want to use the NC6s_v3 SKU.
* Optional: `domino-aks.node_pools.compute.platform`: Update if you don't want to use the DS8_v2 SKU.

. To initialize the modules, run the following command in the same folder as `main.tf`:
+
[source,shell]
----
terraform init
----

. To start the infrastructure deployment, run the following commands:
+
[source,console]
----
terraform plan -out=terraform.plan
terraform apply terraform.plan
----

[TIP]
====
* You can ignore any deprecation warnings.
* If you receive resource quota errors, link:https://docs.microsoft.com/azure/azure-portal/supportability/view-quotas[check^] that the vCPU quotas in your Azure subscription are large enough to fulfill the `node_pools` values.
====

----- admin_guide/installation/manage-deployments/azure/kubernetes-upgrade.txt -----
:page-permalink: d41d8c
:page-version: 6.1
:page-title: Azure Kubernetes Services (AKS) upgrade guide
:page-sidebar: AKS upgrade guide
:page-order: 50

[IMPORTANT]
====
You must link:5522a2[upgrade Domino] to a link:7b2cbe[compatible version of Kubernetes] immediately after the Kubernetes upgrade.
Domino will not work as expected until this is completed.

For example, after upgrading Kubernetes to v1.22, you must upgrade to Domino v5.2 or later. Kubernetes v1.22 is not compatible with older versions of Domino.
Similarly, after upgrading to Kubernetes 1.23 or 1.24, you must upgrade to Domino v5.3 or later.
====

== Prerequisites

To upgrade Kubernetes on Azure, you must have a Unix or Linux terminal with the following:

- link:https://docs.microsoft.com/cli/azure/install-azure-cli[Azure CLI^] installed.
- link:https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/[Kubectl^] installed.

Use environment variables to set the values of IDs, names, and labels.
This simplifies the commands you'll run when you upgrade:

[source,shell]
----
export SUB_ID=<Id of the subscription where AKS was deployed>
export RG_NAME=<Name of the resource group where AKS was deployed>
export CLUSTER_NAME=<The name of the cluster where domino is deployed>
----

== Get cluster credentials

. If you aren't already signed in to Azure CLI, run:
+
[source,shell]
----
az login
----

. Protect your current kubeconfig from being overwritten:
+
[source,shell]
----
export KUBECONFIG=$(pwd)/kubeconfig
----

. Get the remote kubeconfig:
+
[source,shell]
----
az aks get-credentials --subscription $SUB_ID --resource-group $RG_NAME --name $CLUSTER_NAME -f $KUBECONFIG
----

== Disable the Disruption Budgets

You must temporarily disable the `PodDisruptionBudget` (PDB) for certain services.
To do this, back up the PDBs, delete the original PDBs, then update Kubernetes and restore the PDBs from the backup after the update is complete.

. Back up the PDBs:
+
[source,shell]
----
kubectl get pdb -n domino-platform -o yaml mongodb-arbiter > mongodb-arbiter.yml
kubectl get pdb -n domino-platform -o yaml mongodb-primary  > mongodb-primary.yml
kubectl get pdb -n domino-platform -o yaml mongodb-secondary > mongodb-secondary.yml
----

. Delete the PDBs:
+
[source,shell]
----
kubectl delete pdb -n domino-platform mongodb-arbiter
kubectl delete pdb -n domino-platform mongodb-primary
kubectl delete pdb -n domino-platform mongodb-secondary
----

== Update the cluster

. Check for available AKS cluster upgrades:
+
[source,shell]
----
az aks get-upgrades --subscription $SUB_ID --resource-group $RG_NAME --name $CLUSTER_NAME --output table
----

. Run the cluster update (this might take several minutes to complete):
+
[source,shell]
----
az aks upgrade --subscription $SUB_ID --resource-group $RG_NAME --name $CLUSTER_NAME --kubernetes-version <version-number>
----

[#1-24]
== Kubernetes v1.24

If you upgrade to Kubernetes 1.24, add a path annotation for the `nginx-ingress-controller` probe:

[source, shell]
----
kubectl annotate svc -n domino-platform nginx-ingress-controller "service.beta.kubernetes.io/azure-load-balancer-health-probe-request-path=/healthz"
----

IMPORTANT: You must annotate this path to avoid `504 Gateway Time-out` errors.
This is because, in AKS version 1.24, the path no longer defaults to `/healthz`.

== Restore the disruption budgets

Run these commands from the same folder as the backup:

[source,shell]
----
kubectl apply -f mongodb-arbiter.yml
kubectl apply -f mongodb-primary.yml
kubectl apply -f mongodb-secondary.yml
----

----- admin_guide/installation/manage-deployments/azure/prerequisites.txt -----
:page-permalink: f691db
:page-version: 6.1
:page-title: Prerequisites
:page-order: 20

To install and configure Domino in your Azure account, you must have:

* An Azure subscription with enough quota to create:
** 4 D8s_v4 virtual machines or a link:https://docs.microsoft.com/azure/virtual-machines/sizes[similar power^] option.
** NC6s_v3 or similar level virtual machines, if you want to use GPU nodes.
* `quay.io` credentials provided by Domino.
* An SSL certificate for your domain.
* A Unix or Linux terminal with the following:
  ** https://docs.microsoft.com/cli/azure/install-azure-cli[Azure CLI^] installed and signed into your Azure account, with a user that has an link:https://docs.microsoft.com/azure/role-based-access-control/rbac-and-directory-admin-roles#azure-roles[owner role^] on the subscription.
  ** https://learn.hashicorp.com/tutorials/terraform/install-cli[Terraform^] installed.
  ** https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/[Kubectl^] installed.
  ** https://docs.docker.com/engine/install/[Docker^] installed.
  ** The link:c3298b[ddlctl command line] installed.
* link:88f534[Platform Operator] installed.

== Configure cluster access

. Run the following commands:
+
[source,shell]
----
export SUB_ID=<ID of the subscription where AKS was deployed>
export RG_NAME=<Name of the resource group where AKS was deployed>
export CLUSTER_NAME=<AKS cluster name>
----

. To retrieve credentials for the Kubernetes cluster, run the following command to add the AKS credentials to your `kubectl config` file:
+
[source,shell]
----
az aks get-credentials --subscription $SUB_ID --resource-group $RG_NAME --name $CLUSTER_NAME
----

----- admin_guide/installation/manage-deployments/gcp/architecture.txt -----
:page-permalink: bb7224
:page-version: 6.1
:page-title: GCP deployment architecture
:page-sidebar: Architecture
:page-order: 10

These topics describe how to install Domino on link:https://cloud.google.com/kubernetes-engine/docs/concepts/kubernetes-engine-overview[Google Kubernetes Engine (GKE)^].
GKE is hosted on link:https://cloud.google.com/docs/overview[Google Cloud Platform (GCP)^].

When you deploy Domino to GKE, you must have the following services and components:

image::/images/5.1/gcp-arch.png[A map of the Google Cloud architecture you'll need to set up a Domino deployment,role=noshadow]

.Runtime platform
* *A*: link:https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-regional-cluster[GKE cluster^] deployed in three zones.
The cluster must match our link:7b2cbe[compatible Kubernetes versions].

.Node/instances: link:https://cloud.google.com/kubernetes-engine/docs/concepts/node-pools[Node pools^]
* *B*: Platform nodes: Node pool (4-6) n2-highmem-8, 128Gb root disk.
* *C*: Compute nodes: Node pool (1–20) n2-highmem-8, 400Gb root disk.
* *D*: GPU compute nodes: Node pool (0–5) (Optional) n2-highmem-8, 400Gb root disk.

.Storage
* *E*: Shared filesystem and datasets: link:https://cloud.google.com/storage/docs/quickstart-console[Google Cloud storage^].
* *F*: Backups: link:https://cloud.google.com/storage/docs/quickstart-console[Google Cloud storage^].
* *G*: Environment and model images: link:https://cloud.google.com/artifact-registry/docs/overview[Google Artifact Registry^].

.Networking
* *H*: Ingress load balancer: link:https://cloud.google.com/load-balancing/docs/https[External HTTPS load balancing^].
* *I*: Cluster network: link:https://cloud.google.com/vpc/docs/vpc[Virtual Private Cloud (VPC) network^].

----- admin_guide/installation/manage-deployments/gcp/deploy.txt -----
:page-permalink: d108fc
:page-version: 6.1
:page-title: Deploy Domino on GCP
:page-sidebar: Deploy Domino on GCP
:page-order: 40
:preset-defined:

This topic describes how to deploy Domino components on link:https://cloud.google.com/kubernetes-engine/docs/concepts/kubernetes-engine-overview[Google Kubernetes Engine (GKE)^].
GKE is hosted on link:https://cloud.google.com/docs/overview[Google Cloud Platform (GCP)^].

== Set up HTTPS certificate

. Run the following to create the `domino-platform` namespace:
+
[source,shell]
----
kubectl create namespace domino-platform
----

. To make your application available through HTTPS, use the certificate for the project's domain name to create a secret:
+
[source, shell]
----
kubectl -n domino-platform create secret tls my-cert --key=<path to your private key> --cert=<path to your cert>
----

== Create a GKE configuration file

. Get the `$FLEETCOMMAND_AGENT_TAG` for your target release from the link:5246aa[releases] page.

. Use environment variables to set some values used by the `ddlctl` CLI.
This simplifies the commands you'll run while installing Domino components:
+
[source, shell]
----
unset HISTFILE
export QUAY_USERNAME=<`quay.io` username provided by Domino>
export QUAY_PASSWORD=<`quay.io` password provided by Domino>
export FLEETCOMMAND_AGENT_TAG=<Tag that corresponds to the version of Domino deployed>
----

. Generate a GKE configuration file:
+
[source,shell]
----
ddlctl create config --agent-version $FLEETCOMMAND_AGENT_TAG --preset gke
----
+
IMPORTANT: Changing the defaults in the generated configuration can affect the deployment. If you must adjust its parameters, contact a Domino representative.

. You must reference the Terraform output from the infrastructure deployment described in link:5c4b9c[Provision infrastructure and runtime environment] to complete the configuration.
If you don't have the output saved, run `terraform output` to retrieve it.

. Open the generated configuration file and edit the following attributes:

* `name`: The name of the deployment. This can't be changed post-deployment.
* `hostname`: The host name for the Domino install (for example, `domino.example.com`).
* `storage_class.block.type`: `gce`
* `storage_class.shared.type`: `nfs`
* `storage_class.shared.nfs.server`: The `google_filestore_ip_address` from the Terraform output.
* `storage_class.shared.nfs.mount_path`: `/share1` (This must match the `google_filestore_file_share` Terraform output).
* `blob_storage.projects.type`: `shared`
* `blob_storage.logs.type`: `shared`
* `blob_storage.backups.type`: `gcs`
* `blob_storage.backups.gcs.bucket`: The `google_bucket_name` from the Terraform output.
* `blob_storage.backups.gcs.service_account_name`: The `google_platform_service_account` from the Terraform output.
* `blob_storage.backups.gcs.project_name`: The `google_project` from the Terraform output.
* `helm.image_registries.*.username`: Your `quay.io` username.
* `helm.image_registries.*.password`: Your `quay.io` password.
* `image_building.cloud_registry_auth.gcp.service_account`: The `google_gcr_service_account` from the Terraform output.
* `internal_docker_registry` : `null`
* `external_docker_registry`: The `google_artifact_registry` from the Terraform output.
+
[[env-params]]
. Add the following code to the end of the file:

[source, shell]
----
release_overrides:
  nginx-ingress:
    chart_values:
      controller:
        kind: Deployment
        hostNetwork: false
        service:
          enabled: true
          type: LoadBalancer
          annotations:
            cloud.google.com/backend-config: '{"ports": {"80":"nginx-ingress-controller","443":"nginx-ingress-controller"}}'
        extraArgs:
          default-ssl-certificate: domino-platform/my-cert
----

Domino recommends that you keep a backup copy of your final configuration file.
To do this, use the following command:

[source, shell]
----
cp domino.yml{,.backup-$( date +%s )}
----

== Install

With your configuration file ready, you can create a `Domino` custom resource using `ddlctl`:

[source,shell]
----
$ ddlctl create domino --config {path-to-config-yaml} --agent-version $FLEETCOMMAND_AGENT_TAG
----

If you would prefer to just generate the `Domino` custom resource YAML, you can supply the `--export` flag and pipe the result to a file.

When the installation completes successfully, you should see a message that says:

[source,console]
----
2019-11-26 21:20:20,214 - INFO - fleetcommand_agent.Application - Deployment complete.
Domino is accessible at $YOUR_FQDN
----

include::../../../../../../static/content-reuse/deploy-domino.adoc[tag=ingress]

include::../../../../../../static/content-reuse/deploy-domino.adoc[tag=dns]

include::../../../../../../static/content-reuse/deploy-domino.adoc[tag=validation]

----- admin_guide/installation/manage-deployments/gcp/index.txt -----
:page-permalink: e96f4c
:page-version: 6.1
:page-title: GCP
:page-order: 30

These topics describe how to install Domino on link:https://cloud.google.com/kubernetes-engine/docs/concepts/kubernetes-engine-overview[Google Kubernetes Engine (GKE)^].
GKE is hosted on link:https://cloud.google.com/docs/overview[Google Cloud Platform (GCP)^].

link:bb7224[GCP deployment architecture]::
See an overview of the services and components you must have to deploy Domino in GCP.

link:9de45f[Prerequisites]::
Review the prerequisites before you install Domino on a GKE cluster.

link:cca053[Provision infrastructure with Terraform]::
Provision your infrastructure with Domino's recommended Terraform module.

link:d108fc[Deploy Domino on GCP]::
Deploy Domino to the cloud.

link:b6b37d[Upgrade Kubernetes in GCP]::
Upgrade Kubernetes to keep it compatible with your Domino deployment.

----- admin_guide/installation/manage-deployments/gcp/infrastructure.txt -----
:page-permalink: cca053
:page-version: 6.1
:page-title: Provision GCP infrastructure
:page-sidebar: Provision infrastructure
:page-order: 30

IMPORTANT: A Domino install can't be hosted on a subdomain of another Domino install.
For example, if you have Domino deployed at `data-science.example.com`, you can't deploy another instance of Domino at `acme.data-science.example.com`.

Use this topic to provision infrastructure with link:https://cloud.google.com/docs/overview[Google Cloud Platform (GCP)^].
After the infrastructure is in place, you can deploy Domino on link:https://cloud.google.com/kubernetes-engine/docs/concepts/kubernetes-engine-overview[Google Kubernetes Engine (GKE)^].

== Orchestrate the installation

. In the GCP console, go to *Identity and Access Management services (IAM)* and select *Include Google-provided role grants*.
Filter by *Kubernetes Engine Service Agent* and click the pencil icon.
+
image::/images/5.1/gcp-iam.png[alt="A screenshot of the GCP console, with the edit icon highlighted in red"]

. Add the *Cloud KMS CryptoKey Encrypter/Decrypter* role to the service principal.
+
image::/images/5.1/gcp-add-permission.png[alt="A screenshot of the edit permissions page, with the role text field highlighted in red"]

== Provision your infrastructure with Domino's Terraform module

Domino recommends that you use our public-facing link:https://github.com/dominodatalab/terraform-gcp-gke[GCP-specific Reference Terraform module^].
You can also reference this module to manually provision the environment and infrastructure.

. Open the `main.tf` link:/attachments/{version}/gke-main.tf[file] then go to the `gke_cluster` module, and edit the attributes as follow:

* `gke_cluster.project`: Name of the project where the cluster will be deployed.
* `gke_cluster.deploy_id`: Name of the cluster to create.
* `gke_cluster.location`: Location of the cluster.
* `gke_cluster.master_authorized_networks_config.cidr_block`: External network that can access Kubernetes master through HTTPS.
Must be specified in Classless Inter-Domain Routing (CIDR) notation.
* `gke_cluster.node_pool_overrides.gpu.node_locations`: Location of the GPU nodes.
+
. To initialize the modules, run the following command in the same folder as `main.tf`:
+
[source,shell]
----
gcloud init
gcloud auth application-default login
terraform init
----

. To start the infrastructure deployment, run the following commands (you can ignore deprecation warnings):
+
[source,shell]
----
terraform plan -out=terraform.plan
terraform apply terraform.plan
----

IMPORTANT: Save the Terraform output from the previous commands because you'll need it to complete the Domino deployment.

----- admin_guide/installation/manage-deployments/gcp/kubernetes-upgrade.txt -----
:page-permalink: b6b37d
:page-version: 6.1
:page-title: Google Kubernetes Engine (GKE) upgrade guide
:page-sidebar: GKE upgrade guide
:page-order: 50

This topic describes how to upgrade Kubernetes in your link:https://cloud.google.com/kubernetes-engine/docs/concepts/kubernetes-engine-overview[Google Kubernetes Engine (GKE)^] Domino deployment.
GKE is hosted on link:https://cloud.google.com/docs/overview[Google Cloud Platform (GCP)^].

[IMPORTANT]
====
Immediately after the Kubernetes upgrade, you must link:5522a2[upgrade Domino] to a link:7b2cbe[compatible version of Kubernetes].
Domino will not work as expected until this is completed.

For example, after upgrading Kubernetes to v1.22, you must upgrade to Domino v5.2 or later. Kubernetes v1.22 is not compatible with older versions of Domino.
Similarly, after upgrading to Kubernetes 1.23 or 1.24, you must upgrade to Domino v5.3 or later.
====

== Prerequisites

To upgrade Kubernetes in GKE, you must have a Unix or Linux terminal with the following:

- link:https://cloud.google.com/sdk/docs/install?hl=fr#deb[gcloud CLI^] installed.
- link:https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/[Kubectl^] installed.

Use environment variables to set the values of IDs, names, and labels.
This simplifies the commands you'll run while upgrading:

[source,shell]
----
export CLUSTER_PROJECT=<The project name of the cluster>
export CLUSTER_NAME=<The name of the cluster>
export CLUSTER_REGION=<The region where the cluster is located>
----

== Get cluster credentials

. If you aren't already signed in with gcloud, run `gcloud init`.

. To protect your current configuration file from being overwritten, run `export KUBECONFIG=$(pwd)/kubeconfig`.

. Get the remote `kubeconfig`:
+
[source,shell]
----
gcloud container clusters get-credentials $CLUSTER_NAME --region $CLUSTER_REGION --project $CLUSTER_PROJECT
----

== Disable the disruption budgets

You must temporarily disable the `PodDisruptionBudget` (PDB) for certain services.
To do this, back up the PDBs, delete the original PDBs, then update Kubernetes and restore the PDBs from the backup after the update is complete.

. Back up the PDBs:
+
[source,shell]
----
kubectl get pdb -n domino-platform -o yaml mongodb-arbiter > mongodb-arbiter.yml
kubectl get pdb -n domino-platform -o yaml mongodb-primary > mongodb-primary.yml
kubectl get pdb -n domino-platform -o yaml mongodb-secondary > mongodb-secondary.yml
----

. Delete the PDBs:
+
[source,shell]
----
kubectl delete pdb -n domino-platform mongodb-arbiter
kubectl delete pdb -n domino-platform mongodb-primary
kubectl delete pdb -n domino-platform mongodb-secondary
----

== Determine the upgrade version

. Get the available GKE versions:
+
[source, shell]
----
gcloud container get-server-config \
  --region=${CLUSTER_REGION} \
  --flatten="channels" \
  --format="yaml(channels.channel,channels.validVersions)"
----

. Get the current master version:

[source, shell]
----
gcloud container clusters describe ${CLUSTER_NAME} --region=${CLUSTER_REGION} --format="value(currentMasterVersion)"
----

NOTE: You can only upgrade by one minor version at a time.
For example, if you are on Kubernetes version 1.22, you can upgrade to version 1.23.

== Update the cluster

. Upgrade the control plane.
This might take several minutes to complete.
Monitor the GCP console for the status of the cluster upgrade.
+
[source,shell]
----
gcloud container clusters upgrade $CLUSTER_NAME --project $CLUSTER_PROJECT --master --region $CLUSTER_REGION --cluster-version <version-number>
----

. Upgrade the node pools:
+
[source,shell]
----
gcloud container clusters upgrade $CLUSTER_NAME --project $CLUSTER_PROJECT --region $CLUSTER_REGION --node-pool=platform
gcloud container clusters upgrade $CLUSTER_NAME --project $CLUSTER_PROJECT --region $CLUSTER_REGION --node-pool=compute
gcloud container clusters upgrade $CLUSTER_NAME --project $CLUSTER_PROJECT --region $CLUSTER_REGION --node-pool=gpu
----

== Restore the disruption budgets

Run these commands from the same folder as the backup:

[source,shell]
----
kubectl apply -f mongodb-arbiter.yml
kubectl apply -f mongodb-primary.yml
kubectl apply -f mongodb-secondary.yml
----

----- admin_guide/installation/manage-deployments/gcp/prerequisites.txt -----
:page-permalink: 9de45f
:page-version: 6.1
:page-title: Prerequisites
:page-order: 20

Use this topic to prepare to deploy Domino on link:https://cloud.google.com/kubernetes-engine/docs/concepts/kubernetes-engine-overview[Google Kubernetes Engine (GKE)^].
GKE is hosted on link:https://cloud.google.com/docs/overview[Google Cloud Platform (GCP)^].

== Download the infrastructure module

* link:/attachments/{version}/gke-main.tf[main.tf] defines your infrastructure.

== Review the prerequisites

To install and configure Domino in your GCP account, you must have:

* A GCP project.
* `quay.io` credentials provided by Domino.
* An SSL certificate for your domain.
* A Unix or Linux terminal with the following:
** link:https://cloud.google.com/sdk/docs/install[gcloud CLI^] installed.
** link:https://learn.hashicorp.com/tutorials/terraform/install-cli[Terraform^] installed.
** link:https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/[Kubectl^] installed.
** link:https://docs.docker.com/engine/install/[Docker^] installed.
** The link:c3298b[ddlctl command line] installed.
* link:88f534[Platform Operator] installed.

== Validate the prerequisites

To verify that the tools were installed correctly, run the following:

[source, shell]
----
gcloud version
terraform version
kubectl version
docker version
ddlctl version
----

== Configure cluster access

. To retrieve the credentials for your Kubernetes cluster, check your local `kubeconfig` with:
+
[source,shell]
----
export KUBECONFIG=$(pwd)/kubeconfig
----

. If you aren't already signed in with gcloud, run `gcloud init`.

. To protect your current configuration file from being overwritten, run `export KUBECONFIG=$(pwd)/kubeconfig`.

. Get the remote `kubeconfig`:
+
[source,shell]
----
gcloud container clusters get-credentials $CLUSTER_NAME --region $CLUSTER_REGION --project $CLUSTER_PROJECT
----

----- admin_guide/installation/manage-deployments/index.txt -----
:page-permalink: e6ea06
:page-version: 6.1
:page-title: Install Domino
:page-sidebar: Install Domino
:page-order: 60

IMPORTANT: This guide assumes you have already provisioned your target infrastructure and satisfied the prerequisites outlined in the link:8fe42a[Requirements] page.

Installing Domino can be broken down into two principal tasks:

1. Filling out the configuration file.
2. Creating the Domino custom resource.

If you are comfortable tackling all of these tasks in one workflow, you can go through the `bootstrap` command covered in link:b09679[Bootstrap a Domino Cluster].

Otherwise, you can address each of these tasks in turn.

TIP: This guide is intended for use when you install Domino for the first time. If you are upgrading an existing Domino deployment or migrating an existing Domino cluster to use the Platform Operator, please see the link:5522a2[Upgrade Domino] guide.

== Create the configuration file

. Get the `$FLEETCOMMAND_AGENT_TAG` for your target release from the link:5246aa[releases] page.

. Use environment variables to set some values used by `ddlctl` CLI.
This simplifies the commands you'll run while installing Domino components:
+
[source, shell]
----
unset HISTFILE
export QUAY_USERNAME=<`quay.io` username provided by Domino>
export QUAY_PASSWORD=<`quay.io` password provided by Domino>
export FLEETCOMMAND_AGENT_TAG=<Tag that corresponds to the version of Domino deployed>
----

. Generate a configuration file:
+
[source,shell]
----
ddlctl create config --agent-version $FLEETCOMMAND_AGENT_TAG
----
+
TIP: If you are creating a configuration file for a target cloud provider, refer to the specific guide for that provider to get the correct value for the `--preset` flag when running this subcommand. For data planes, these are also the `data-plane` and `data-plane-eks` presets.

. Running this command will write a configuration file to the current working directory with the name:
+
[source,shell]
----
config-$FLEETCOMMAND_AGENT_TAG.{timestamp}.yaml
----

== Populate the configuration file

In order to install a functional Domino cluster, you will need to update the generated configuration file with the appropriate values for your target environment.

You can refer to the link:30fc1c[configuration reference] pages for more details on the values you can set.

IMPORTANT: Changing the defaults in the generated configuration can affect the deployment. If you must adjust its parameters, contact a Domino representative.

[[data-plane-deployments]]
=== Data plane deployments

Domino now supports installing data planes with the operator. You should use the `data-plane` or `data-plane-eks` presets to generate an initial data plane configuration.

The primary difference between the default preset and the `data-plane` preset is the following stanza:

[source,yaml,subs="attributes"]
----
control_plane: false
data_plane:
  type: remote
----

`email_notifications`, `telemetry`, `blob_storage`, `internal_docker_registry`, `external_docker_registry` and `monitoring` are omitted from the configuration.

Additionally, `storage_classes` is _optional_ on data plane installs.

=== Recommended install configurations

We recommend enabling the *New Relic integration* while updating the install configuration:

[source,yaml,subs="attributes"]
----
monitoring:
prometheus_metrics: true
newrelic:
  apm: true
  infrastructure: true
  license_key: '[Token provided by Domino]'
----

The New Relic integration enables Domino application monitoring via New Relic. This helps Domino monitor and proactively assist with platform infrastructure issues. Enabling the New Relic integration can reduce the time it takes to resolve support tickets.

== Create the Domino custom resource

Once you are satisfied with the configuration file, you can create the Domino custom resource. This will get picked up by the link:88f534[Platform Operator] which will then handle the installation of all Domino components.

[source,shell]
----
ddlctl create domino --config config-$FLEETCOMMAND_AGENT_TAG.{timestamp}.yaml --agent-version $FLEETCOMMAND_AGENT_TAG
----

If you would prefer to just generate the `Domino` custom resource YAML, you can supply the `--export` flag and pipe the result to a file.

When the installation completes successfully, you should see a message that says:

[source,console]
----
2019-11-26 21:20:20,214 - INFO - fleetcommand_agent.Application - Deployment complete.
Domino is accessible at $YOUR_FQDN
----

=== Dry runs

. If you would prefer to dry run the installation, you should modify the `Domino` custom resource to put it in dry run mode:
+
[source,shell]
----
ddlctl create domino --config config-$FLEETCOMMAND_AGENT_TAG.{timestamp}.yaml --agent-version $FLEETCOMMAND_AGENT_TAG --export > domino-cluster.yaml
----

. In the `domino-cluster.yaml` file, modify the `spec.agent.spec` to put the operator in a dry run mode:
+
[source,yaml]
----
spec:
  agent:
    spec:
      dryRunMode: "true"
----

. Now apply the resource to the cluster:
+
[source,shell]
----
kubectl apply -f domino-cluster.yaml -n domino-operator
----

Dry running an upgrade of Domino will report on the changes that will be made to the Domino cluster compared to what is (or is not) already represented in the Helm manifests in Helm storage.

Once you change `dryRunMode` back to `"false"` (or remove the field entirely), a reconciliation will be triggered and the Domino cluster will be installed.

== Next steps

* Install Domino on link:e3bf0a[Amazon Elastic Kubernetes Service (EKS)].
* Install Domino on link:bf888c[Azure Kubernetes Service (AKS)].
* Install Domino on link:e96f4c[Google Kubernetes Engine (GKE)].
* Install Domino link:87b601[offline], without a connection to the Internet.
* Install Domino using link:3fb861[OpenShift].

----- admin_guide/installation/manage-deployments/openshift.txt -----
:page-permalink: 3fb861
:page-version: 6.1
:page-title: OpenShift
:page-order: 60

:openshift-doc-base-url: https://docs.openshift.com/container-platform/4.11

[[tr1]]

== Requirements for Domino installation

=== Namespaces

You don't have to configure namespaces prior to install.
Domino will create three namespaces in the cluster during installation, according to the following specifications:

[cols="1a,3a",options="header"]
|===
|Namespace |Contains
|`platform` |Durable Domino application, metadata, and platform services required for platform operation.

|`compute` |Ephemeral Domino execution pods launched by user actions in the application.

|`domino-system` |Domino installation metadata and secrets.
|===

[[node-pools]]
=== Node pools

The OpenShift cluster must have worker nodes with the following specifications and distinct node labels. It includes optional pools:

* OpenShift Container Storage (OCS). This pool runs the storage nodes as part of the OCS Operator which is part of the OpenShift Data Foundation (ODF) Operator.
* GPU. Nodes in this pool contain Nvidia GPUs.

[cols="2a,^1a,^1a,^1a,^1a,3a",options="header"]
|===
|Pool |Min-Max |vCPU |Memory |Disk |Labels
|`platform` | 4-6 |8 |32G |128G
|`dominodatalab.com/node-pool: platform`

|`default` |1-20 |8 |32G |128G |`dominodatalab.com/node-pool: default`

|Optional: `default-gpu` |0-5 |8 |32G |128G
|`dominodatalab.com/node-pool: default-gpu` `nvidia.com/gpu: true`

|Optional: `ocs` |3-3 |8 |32G |128G
|`node.ocs.openshift.io/storage: 'true'`
|===

Generally, the `platform` worker nodes need an aggregate minimum of 24 CPUs and 96G of memory.
Domino recommends that you spread the resources across multiple nodes with proper failure isolation (for example, availability zones).

We recommend deploying to at least three availability zones (AZs) for high availability and tolerance.
You must create a machineset per AZ per node pool, as shown in {openshift-doc-base-url}/machine_management/creating_machinesets/creating-machineset-aws.html[this AWS MachineSet Example^].

==== Node autoscaling

For clusters on top of an elastic cloud provider like AWS, you must create
{openshift-doc-base-url}/machine_management/applying-autoscaling.html[ClusterAutoscaler^],
{openshift-doc-base-url}/machine_management/applying-autoscaling.html#machine-autoscaler-about_applying-autoscaling[MachineAutoscaler^] and {openshift-doc-base-url}/machine_management/deploying-machine-health-checks.html[MachineHealthCheck^]
resources to achieve node autoscaling.

=== GPU support

In order to run GPU workloads in OpenShift, the following must be installed:

. Node Feature Discovery (NFD) Operator
. NFD Instance
. Nvidia GPU Operator
. ClusterPolicy
. GPU Enabled MachineSet

You can use the https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/contents.html[GPU Operator on OpenShift^] guide.

To confirm that you are able to schedule GPU workloads, you can create a pod that requires a GPU node:

[source, yaml]
----
spec:
    resources:
      limits:
        nvidia.com/gpu: 1
----

=== Storage

See the link:25b6dc#storage-classes[storage requirements] for your infrastructure.

We recommend using the https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.11/html-single/red_hat_openshift_data_foundation_architecture/index[Openshift Data Foundation (ODF) Operator^] to handle the storage.

In order to create a storage cluster for ODF, the following must be installed:

. OCS Dedicated MachineSet (optional but recommended)
. ODF Operator
. StorageSystem
. StorageCluster

You can use the https://red-hat-storage.github.io/ocs-training/training/ocs4/odf4-install-no-ui.html[ODF CLI Install^]  guide.

Confirm the following `storageclasses` are created:

. `ocs-storagecluster-ceph-rbd`
. `ocs-storagecluster-cephfs`

=== Networking

==== Domain

[[tr3]]
Domino must be configured to serve from a specific FQDN.
To serve Domino securely over HTTPS, you also need an SSL certificate that covers the chosen name.

IMPORTANT: A Domino install can't be hosted on a subdomain of another Domino install.
For example, if you have Domino deployed at `data-science.example.com`, you can't deploy another instance of Domino at `acme.data-science.example.com`.

==== Network plugin

Domino relies on https://kubernetes.io/docs/concepts/services-networking/network-policies/[Kubernetes network policies^] to manage secure communication between pods in the cluster.
By default, OpenShift uses the {openshift-doc-base-url}/networking/cluster-network-operator.html[Cluster Network Operator^] to deploy the OpenShift SDN default CNI network provider plugin, which supports network policies and hence should just work.

==== Ingress

[[tr4]]
Domino uses the https://github.com/kubernetes/ingress-nginx[NGNIX ingress controller maintained by the Kubernetes project^] instead of (but _does not replace_) the {openshift-doc-base-url}/networking/ingress-operator.html[OpenShift implemented HAProxy-based ingress controller^] and deploys the ingress controller as a
https://kubernetes.io/docs/concepts/services-networking/service/#nodeport[node port service^].

[[tr5]]
By default, the ingress listens on node ports `443` (HTTPS) and `80` (HTTP).

==== Load balancer

[[tr6]]
A load balancer must be set up to use your DNS name.
For example, in AWS, you must set up the DNS so it points a CNAME at an https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-internet-facing-load-balancers.html[Elastic Load Balancer^].

[[tr7]]
After you complete the link:4bb6a6[installation process], you must configure the load balancer to balance across the link:3fb861#node-pools[platform nodes] at the ports specified by your ingress.

==== External resources

[[tr8]]
If you plan to connect your cluster to other resources like data sources or authentication services, pods running on the cluster must have network connectivity to those resources.

=== Container registry

[[tr9]]
Domino deploys its own container image registry instead of using the {openshift-doc-base-url}/registry/index.html[OpenShift built in container image registry^].
During installation, the OpenShift cluster image configuration is {openshift-doc-base-url}/registry/configuring-registry-operator.html#images-configuration-cas_configuring-registry-operator[modified to trust the Domino certificate authority (CA)^].
This is done to ensure that OpenShift can run pods using Domino's custom built images.
In the `images.config.openshift.io/cluster` resource, you can find a reference to a ConfigMap that contains the Domino CA.

[source,yaml]
----
spec:
  additionalTrustedCA:
    name: domino-deployment-registry-config
----

== Bootstrapping

If you opt to use the `ddlctl bootstrap` subcommand to install the platform operator and Domino, make sure to pass the `--openshift` flag.

For more information on bootstrapping, see link:b09679[Bootstrap a Domino Cluster].

== Create a configuration file

. Get the `$FLEETCOMMAND_AGENT_TAG` for your target release from the link:5246aa[releases] page.

. Use environment variables to set some values used by the `ddlctl` CLI.
This simplifies the commands you'll run while installing Domino components:
+
[source, shell]
----
unset HISTFILE
export QUAY_USERNAME=<`quay.io` username provided by Domino>
export QUAY_PASSWORD=<`quay.io` password provided by Domino>
export FLEETCOMMAND_AGENT_TAG=<Tag that corresponds to the version of Domino deployed>
----

. Generate an OpenShift configuration file:
+
[source,shell]
----
ddlctl create config --agent-version $FLEETCOMMAND_AGENT_TAG --preset openshift
----
+
IMPORTANT: Changing the defaults in the generated configuration can affect the deployment. If you must adjust its parameters, contact a Domino representative.

== Install

You can proceed with the installation as outlined in link:e6ea06[Install Domino].

----- admin_guide/installation/manage-deployments/private-offline.txt -----
:page-permalink: 87b601
:page-version: 6.1
:page-title: Offline installation
:page-order: 50

[[tr1]]
Domino provides bundles of offline installation media for when you run the Domino Platform Operator without internet access to upstream sources of images and charts.

To serve these resources, you must have a Docker registry accessible to your cluster.

== Downloads

URLs of available offline installation bundles are available on the link:5246aa[releases] page.

You can download these bundles using the https://curl.se/[curl^] command with basic authentication.
If there are network connectivity issues, you can use the `wget` command with the `--continue` or `-c` option.

Contact your Domino account team for credentials.

This page contains examples you can use in your command line for specific use cases, where `${RELEASE_TAG}` denotes the fleetcommand-agent information for your specific Domino version, as specified on the on the link:5246aa[releases] page.

For instance:

[source,shell,subs="attributes"]
----
export RELEASE_TAG={version}.x.catalog-xxxxxxx
----

[NOTE]
====
* The versioned collection of images (named `docker-images-{version}.x.tar`) contains all required images for this type of deployment and must be the only downloaded file.
* If you have not yet installed the link:88f534[Platform Operator] and link:c3298b[ddlctl] command line, you will need to do so first as it is not included in the offline bundle.
====

Example `curl` download:

[source,shell,subs="attributes"]
----
curl -u username:password -#SfLOJ https://mirrors.domino.tech/s3/domino-artifacts/offline/fleetcommand-agent-${RELEASE_TAG}/docker-images-{version}.x.tar
----

Example `wget` download:

[source,shell,subs="attributes"]
----
wget -c --user domino-registry --password xxxxxxx https://mirrors.domino.tech/s3/domino-artifacts/offline/fleetcommand-agent-${RELEASE_TAG}/docker-images-{version}.x.tar
----

If you are unable to download the full bundle, a partitioned bundle is available. To use the partitioned bundle, download all parts to recreate the full tarball and then follow the rest of the instructions.

Download the list of newline-separated part filenames:

[source,shell,subs="attributes"]
----
curl -u username:password -#SfLOJ https://mirrors.domino.tech/s3/domino-artifacts/offline/fleetcommand-agent-${RELEASE_TAG}/docker-images-{version}.x.parts.txt
----

Download each part of the bundle using the provided filenames:

[source,shell,subs="attributes"]
----
while read part; do curl -u username:password -#SfLOJ "https://mirrors.domino.tech/s3/domino-artifacts/offline/fleetcommand-agent-${RELEASE_TAG}/$part"; done < docker-images-{version}.x.parts.txt
----

Use the downloaded parts to recreate the full bundle:

[source,shell,subs="attributes"]
----
cat docker-images-{version}.x.tar[0-9]* > docker-images-{version}.x.tar
----

== Verification

SHA-256 checksums of files are available to verify the contents of downloaded bundles once downloaded.

Example verifications:

[source,shell,subs="attributes"]
----
curl -u username:password -#SfLOJ https://mirrors.domino.tech/s3/domino-artifacts/offline/fleetcommand-agent-${RELEASE_TAG}/docker-images-{version}.x.tar.sha256sum
sha256sum -c docker-images-{version}.x.tar.sha256sum
----

If `<filename>: OK` is not displayed, ensure the downloaded bundles were not tampered with before loading and installing them.

For partitioned bundles, use the following:

[source,shell,subs="attributes"]
----
curl -u username:password -#SfLOJ https://mirrors.domino.tech/s3/domino-artifacts/offline/fleetcommand-agent-${RELEASE_TAG}/docker-images-{version}.x.parts.sha256sum
sha256sum -c docker-images-{version}.x.parts.sha256sum
----

== Images list

If you require a list of images contained in a Domino release, they can be downloaded alongside the installation bundle.

[source,shell,subs="attributes"]
----
curl -u username:password -#SfLOJ https://mirrors.domino.tech/s3/domino-artifacts/offline/fleetcommand-agent-${RELEASE_TAG}/images.json
----

The resulting JSON file contains a key-value pairing of Domino version and list of images.

Below is an example of extracting image names for Domino 6.x.x using https://stedolan.github.io/jq/[jq^]:

[source,shell,subs="attributes"]
----
jq -r '.["{version}.x"][].name' images.json
----

== Extract and load

The images bundle is a `.tar` archive that must be extracted before you use it.

[source,shell,subs="attributes"]
----
tar -xvf docker-images-{version}.x.tar
----

In the `docker-images` bundle there will be:

* a collection of individual Docker image `.tar` files
* an `images.json` metadata file
* a `domino-load-images.py` script

The `domino-load-images.py` file is a script to ingest the `images.json` metadata file and load the associated Docker images for a specific Domino version into the given remote Docker registry.

To load images into your private registry, run `domino-load-images.py` and pass in the URL of your registry as an argument. The script expects to run in the same directory as the `images.json` metadata file and the `.tar` image files.

Example:

[source,shell]
----
python domino-load-images.py your-registry-url.domain:port
----

Once images have been loaded into your private registry you're ready to install Domino.


== Install

[[tr2]]
To install Domino with a custom registry, the image references must be modified to reference the upstream registry.
Use the `--image-registry` argument on the `ddlctl create config` subcommand to modify all image references to the external registry for authentication purposes.
Use the `--agent-repository` argument to specify the custom repository for the `fleetcommand-agent` image (the default is `quay.io/domino/fleetcommand-agent`)

NOTE: If you are using `ddlctl` version `0.4.4` you must specify dummy values for `--username` and `--password` if you are using the `--offline` flag to circumvent validations. This will not be required in future versions of `ddlctl`.

[source,shell,subs="attributes"]
----
ddlctl create config --offline --agent-version ${RELEASE_TAG} --agent-repository "your-registry-url.domain:port/custom-repository" --image-registry "your-registry-url.domain:port" --username "your-username" --password "your-password"
----

If your registry requires authentication, ensure the `image_registries` section of your generated configuration is filled in with the correct credentials:

[source,yaml]
----
helm:
  image_registries:
  - server: your-registry-url.domain:port
    username: '<username>'
    password: '<password>'
----

Helm charts come prepackaged within the `fleetcommand-agent` image that runs reconciliation jobs for the Platform Operator.
The `helm` configuration object doesn't need to contain additional fields besides `image_registries`.

With your configuration file ready, you can proceed with the installation as outlined in link:e6ea06[Install Domino].

----- admin_guide/installation/platform-operator/ddlctl/docker-engine.txt -----
:page-permalink: 12a907
:page-version: 6.1
:page-title: Configure the Docker Engine API for ddlctl
:page-sidebar: Docker Engine API
:page-order: 30

Certain `ddlctl` operations require access to the Docker Engine API.

== Docker Engine and Docker Desktop

. Verify that you have Docker installed and running.
. Make sure you can access the Docker daemon:
+
[source,bash]
----
docker info
----
+
If the `Server:` section does not return an error, Docker is working and `ddlctl` will function correctly.

IMPORTANT: Newer versions of Docker Desktop changed a default setting that blocks access to the default Docker socket.
Verify that you have enabled *Allow the default Docker socket to be used* under Docker Desktop's *Advanced Settings* before continuing.

== Podman

Podman installs the Docker socket in an unconventional place. You can use Podman for `ddlctl` operations by setting
the `DOCKER_HOST` environment variable link:https://podman-desktop.io/docs/migrating-from-docker/using-the-docker_host-environment-variable#procedure[as described in the Podman documentation].


----- admin_guide/installation/platform-operator/ddlctl/index.txt -----
:page-permalink: c3298b
:page-version: 6.1
:page-title: Work with the ddlctl command line
:page-sidebar: ddlctl command line
:page-order: 20

[source,shell,subs="attributes"]
----
Command line utility for interacting with the Domino Data Lab platform operator, which reconciles
the state of a Domino cluster against the root configuration, manages installations
and upgrades, and provides an interface to monitor the overall health of the cluster.
Usage:
  ddlctl [command]
Available Commands:
  bootstrap   Bootstrap the domino-operator
  completion  Generates ddlctl completion scripts for supported shells
  create      Create or update Kubernetes resources and required artifacts for applying a valid Domino
  diff        Diff Domino platform resources
  get         Get Domino platform resources
  help        Help about any command
  logs        Display logs for Domino platform resources
  reconcile   Reconcile a Domino platform resource
  resume      Resume a suspended platform resource
  suspend     Suspend reconciliation of Domino platform resource
  upgrade     Upgrade a Domino resource
  version     Print the client and server-side components version information
----

For help text on specific commands and subcommands, just add the `--help` flag.

== Next steps

* link:598b40[Install the ddlctl command line]
* link:73c6b3[Upgrade the ddlctl command line]
* link:12a907[Configure the Docker Engine API]

----- admin_guide/installation/platform-operator/ddlctl/install.txt -----
:page-permalink: 598b40
:page-version: 6.1
:page-title: Install the ddlctl command line
:page-sidebar: Installation
:page-order: 10

First, you will need to determine the version of `ddlctl` you want to install. You can find the latest version on the link:5246aa[releases] page.

Next, follow these steps:

. Set some environment variables to make the download process easier:
+
[source,bash]
----
export VERSION={version from release page}
export OS={darwin|linux}
export ARCH={arm64|x86_64}
----

. Download the `ddlctl` binary for your operating system and architecture with the following script:
+
[source,bash]
----
wget https://mirrors.domino.tech/s3/domino-artifacts/ddlctl/${VERSION}/ddlctl_${VERSION}_${OS}_${ARCH}.tar.gz
----
+
For example, if you are installing `ddlctl` for Domino `6.0.0`, per the link:5246aa[releases] page, you would run the following command in order to fetch the command line tool (`0.5.4`) for that version (assuming you are on a Linux machine with an ARM64 architecture):
+
[source,shell,subs="attributes"]
----
wget https://mirrors.domino.tech/s3/domino-artifacts/ddlctl/0.5.4/ddlctl_0.5.4_linux_arm64.tar.gz
----

. Extract the binary from the tarball:
+
[source,bash]
----
tar -xvf ddlctl_${VERSION}_${OS}_${ARCH}.tar.gz
----

. Move the binary to a directory in your PATH, such as `/usr/local/bin`:
+
[source,bash]
----
mv ddlctl_${VERSION}_${OS}_${ARCH}/ddlctl /usr/local/bin
----

IMPORTANT: The Docker Engine API is required for certain `ddlctl` operations, such as `ddlctl create config`. link:12a907[Configure the Docker Engine API] to ensure `ddctl` operates correctly.

----- admin_guide/installation/platform-operator/ddlctl/upgrade.txt -----
:page-permalink: 73c6b3
:page-version: 6.1
:page-title: Upgrade the ddlctl command line
:page-sidebar: Upgrade
:page-order: 20

First, you will need to determine the version of `ddlctl` you want to upgrade to. You can find the latest version on the link:5246aa[releases] page.

Next, follow these steps:

. Set some environment variables to make the download process easier:
+
[source,bash]
----
export VERSION={version from release page}
export OS={darwin|linux}
export ARCH={arm64|x86_64}
----

. Download the `ddlctl` binary for your operating system and architecture with the following script:
+
[source,bash]
----
wget https://mirrors.domino.tech/s3/domino-artifacts/ddlctl/${VERSION}/ddlctl_${VERSION}_${OS}_${ARCH}.tar.gz
----
+
For example, if you are installing `ddlctl` for Domino `6.0.0`, per the link:5246aa[releases] page, you would run the following command in order to fetch the command line tool (`0.4.4`) for that version (assuming you are on a Linux machine with an ARM64 architecture):
+
[source,shell,subs="attributes"]
----
wget https://mirrors.domino.tech/s3/domino-artifacts/ddlctl/0.4.4/ddlctl_0.4.4_linux_arm64.tar.gz
----

. Extract the binary from the tarball:
+
[source,bash]
----
tar -xvf ddlctl_${VERSION}_${OS}_${ARCH}.tar.gz
----

. Move the binary to a directory in your PATH, such as `/usr/local/bin`:
+
[source,bash]
----
mv ddlctl_${VERSION}_${OS}_${ARCH}/ddlctl /usr/local/bin
----

IMPORTANT: The Docker Engine API is required for certain `ddlctl` operations, such as `ddlctl create config`. link:12a907[Configure the Docker Engine API] to ensure `ddctl` operates correctly.

----- admin_guide/installation/platform-operator/drift.txt -----
:page-permalink: f6776a
:page-version: 6.1
:page-title: Drift Detection
:page-order: 50

The `fleetcommand-agent` image that runs operator jobs creates definitions for an additional custom resource definition managed by the platform operator called `HelmRelease`. These resources map one-to-one to the Helm releases that are deployed to the cluster and are managed by Domino. They abide by a separate reconciliation loop than the `Domino` resource and are continuously evaluated for drift between the deployed manifest of the Helm release and the live state of the cluster.

While the operator is capable of correcting drift, this behavior is not yet enabled globally or configurable by service through the `Domino` resource. By default, it will warn of drift on the `HelmRelease` resource conditions directly. In a future release, this will be surfaced as a configurable option.

Using `ddlctl` is the best way to inspect the state of `HelmRelease` resources in your cluster:

[source,shell,subs="attributes"]
----
# Get all HelmRelease resources in the cluster across namespaces
ddlctl get helmrelease --all

# Get all HelmRelease resources in the domino-platform namespace
ddlctl get helmrelease --namespace domino-platform

# Get all HelmRelease resources in the cluster that are marked as Stalled
ddlctl get helmrelease --all --status stalled=true
----

A `HelmRelease` is marked as `Stalled` when the operator detects that:

* the Helm release has drifted from the desired state,
* the Helm release is in a `Failed` state,
* the Helm release is locked in a pending state,
* the Helm release was deleted, or
* the latest Helm revision does not match the desired revision of the current `Domino` generation.

To get all HelmRelease resources in the cluster that are marked as `Ready`, run the following:

[source,shell,subs="attributes"]
----
ddlctl get helmrelease --all --status ready=true
----

`HelmRelease` resources are deployed with a default `5` minute interval, meaning if a release were to get out of sync in the cluster it will not necessarily register as drift immediately, but get picked up on the next reconciliation.

If you want to *force* a reconciliation, you can do this through the `ddlctl` command line:

[source,shell,subs="attributes"]
----
ddlctl reconcile helmrelease nucleus -n domino-platform
----

== Investigating drift

Discovering what has drifted on a `HelmRelease` resource can be done in a few ways.

`ddlctl` offers a subcommand for inspecting the diff of a Helm release against the live state of the cluster:

[source,shell,subs="attributes"]
----
ddlctl diff helmrelease nucleus -n domino-platform
----

If the resource has drifted, you can expect to see something similar to the following:

[source,shell,subs="attributes"]
----
NAME                	READY	REASON 	MESSAGE                                           	DRIFT DETECTION MODE	SUSPENDED
domino-data-importer	False	Drifted	cluster has drifted from desired helmrelease state	warn                	false
----

For resources that are in sync, you can expect to see something more like the following:

[source,shell,subs="attributes"]
----
NAME   	READY	REASON	MESSAGE                                  	DRIFT DETECTION MODE	SUSPENDED
nucleus	True 	InSync	helmrelease is in sync with cluster state	warn                	false
----

The operator also writes information on the nature of drift to events, which can be inspected with `kubectl describe`, i.e.:

[source,shell,subs="attributes"]
----
# Inspect the events of a Helm release
kubectl describe helmrelease nucleus -n domino-platform
----

The `Warning` event will report on the resource where drift was detected, the type of drift, and include the JSON patch (either in full or in part) that would be applied if `correct` mode were enabled on the `HelmRelease` resource rather than `warn`.

NOTE: As there is a character limit on Kubernetes events, the JSON patch will be truncated to 500 characters max, but the full patch can be found in the operator logs, which can also be accessed with `ddlctl` by running `ddlctl logs operator`.

----- admin_guide/installation/platform-operator/index.txt -----
:page-permalink: 88f534
:page-version: 6.1
:page-title: Platform Operator
:page-sidebar: Platform Operator
:page-order: 40

Starting with Domino 6.0.0, the Platform Operator is the *only* supported path for managing your Domino cluster's service layer.

This includes:

* Installing Domino.
* Upgrading Domino.
* Keeping your Domino cluster in sync with the desired state.

The Platform Operator is constituted of a few components:

* The Operator itself, which is packaged as a Helm chart.
* The `ddlctl` command line interface.

The `ddlctl` command line can be used for:

** Bootstrapping a new (or existing) Domino cluster with the operator and Domino custom resource.
** Installing and/or upgrading the operator.
** Installing and/or upgrading Domino.
** Interacting with Domino resources.

Detailed information can be found in these topics:

* The Domino Platform Operator is a Kubernetes operator that manages the lifecycle of Domino on Kubernetes. Find out link:e43374[how it works].
* Learn how to link:c3298b[work with the ddlctl command line], including how to link:598b40[install] and link:73c6b3[upgrade] it.
* Get the steps to link:be2d4f[install the Platform Operator].
* Get the steps to link:5ecdf6[upgrade the Platform Operator].
* Find out more about link:f6776a[drift detection].

----- admin_guide/installation/platform-operator/install.txt -----
:page-permalink: be2d4f
:page-version: 6.1
:page-title: Install the Platform Operator
:page-order: 30

IMPORTANT: This guide assumes you have already installed the `ddlctl` command line tool. If you have not done so yet, please follow the link:598b40[ddlctl installation guide] first and ensure you have the correct `ddlctl` version for your target Domino version per the link:5246aa[releases] page.

From there, the installation is straightforward and you can just run the following command:

[source,shell,subs="attributes"]
----
ddlctl bootstrap --skip-domino-resource
----

NOTE: The `--disable-webhook` flag is available in `ddlctl` version `0.4.7` and later. While we recommend keeping the default behavior of the Platform Operator chart, you can opt to disable the validating webhook with this flag.

== Offline installation

If you are installing the Domino Platform Operator on a system that is not connected to the internet, you will need to download the Helm chart independently and point `ddlctl` to it in your local environment, rather than relying on the default behavior of it pulling it for you.

=== Download the Helm Chart

Find the version of the Platform Operator that corresponds to your Domino version on the link:5246aa[releases] page and substitute it in the command below:

[source,shell,subs="attributes"]
----
wget https://mirrors.domino.tech/s3/domino-artifacts/platform-operator/platform-operator-<version>.tgz
----

For example, if you are installing the Platform Operator for Domino `6.0.0`, per the link:5246aa[releases] page, you would run the following command in order to fetch the Operator (`0.4.4`) for that version:

[source,shell,subs="attributes"]
----
wget https://mirrors.domino.tech/s3/domino-artifacts/platform-operator/platform-operator-0.4.4.tgz
----

=== Unarchive the Helm chart

You can either click on the downloaded file to unarchive it or run the following command:

[source,shell,subs="attributes"]
----
tar -xvf platform-operator-<version>.tgz
----

This will result in a directory named `platform-operator`.

=== Run the bootstrap command

[source,shell,subs="attributes"]
----
ddlctl bootstrap --chart-repo-url platform-operator --skip-domino-resource
----

If you downloaded and unarchived the Helm chart in a different directory than your current working directory, make sure to specify that in the `--chart-repo-url` flag.

----- admin_guide/installation/platform-operator/overview.txt -----
:page-permalink: e43374
:page-version: 6.1
:page-title: How it works
:page-sidebar: How it works
:page-order: 10

The Domino Platform Operator is a Kubernetes operator that manages the lifecycle of Domino on Kubernetes.

It reads from a custom resource definition (CRD) to understand the desired state of the Domino Platform. It then orchestrates Kubernetes Jobs to install and upgrade Domino using a `fleetcommand-agent` image that is coupled with every release version of Domino.

Unlike `fleetcommand-agent`, the Platform Operator's reconciliation logic is not tied to a specific Domino version, so it can be used to upgrade across multiple versions of Domino unless otherwise specified in the release notes. Upgrading the operator itself, however, does not necessitate any downtime for a Domino cluster.

Whenever the `Domino` custom resource definition is updated in the cluster, a reconciliation is triggered that evaluates whether the change materially alters the state of the Domino platform (i.e. the version of the agent was changed, the version of the cluster was changed, configuration values were changed, etc.).

If the change is found to be material, the operator will provision a new Kubernetes job that uses the `fleetcommand-agent` version set in the `Domino` specification with the configuration values mounted from the `.spec.config` field.

NOTE: You may recognize the structure of the `.spec.config` field from previous Domino versions as the `domino.yml` file or `agent.yaml` file. More on the configuration options available in the `Domino` custom resource definition can be found in the link:30fc1c[configuration reference] topic.

The operator ships with a companion command line tool named `ddlctl` which can be used not only to interact with the `Domino` custom resource in your cluster, but also to create and manage the definitions. More information on `ddlctl` can be found in the link:c3298b[ddlctl] section.

As `Domino` is reconciled like any other Kubernetes resource, the success or failure of its reconciliation will be reflected in the status.

Let's look at an example of what this looks like using `ddlctl`.

* With `ddlctl`, you can easily fetch the status of `domino` in your cluster:
+
[source,shell,subs="attributes"]
----
$ ddlctl get domino
----

* This will parse the conditions of the `domino` resource within the `domino-operator` namespace and report on it in a human readable format:
+
[source,shell,subs="attributes"]
----
VERSION   	READY	MESSAGE                              	JOB STATE   	AGENT VERSION                	DRY RUN	SUSPENDED
6.0.0   	True 	job platform-operator-s5lfx succeeded	JobSucceeded	6.0.0.catalog-xxxx          	false  	false
----

----- admin_guide/installation/platform-operator/upgrade.txt -----
:page-permalink: 5ecdf6
:page-version: 6.1
:page-title: Upgrade the Platform Operator
:page-order: 40

IMPORTANT: This guide assumes you have already installed (or upgraded) the `ddlctl` command line tool. If you have not done so yet, please follow the link:598b40[ddlctl installation guide] first and ensure you have the correct `ddlctl` version for your target Domino version per the link:5246aa[releases] page.

From there, upgrading is straightforward and you can just run the following command:

[source,shell,subs="attributes"]
----
ddlctl bootstrap --skip-domino-resource --skip-namespace --skip-registry-secret
----

NOTE: The `--disable-webhook` flag is available in `ddlctl` version `0.4.7` and later. While we recommend keeping the default behavior of the Platform Operator chart, you can opt to disable the validating webhook with this flag.

== Offline upgrades

If you are upgrading the Domino Platform Operator on a system that is not connected to the internet, you will need to download the Helm chart independently and point `ddlctl` to it in your local environment, rather than relying on the default behavior of it pulling it for you.

=== Download the Helm Chart

Find the version of the Platform Operator that corresponds to your Domino version on the link:5246aa[releases] page and substitute it in the command below.

[source,shell,subs="attributes"]
----
wget https://mirrors.domino.tech/s3/domino-artifacts/platform-operator/platform-operator-<version>.tgz
----

=== Unarchive the Helm chart

You can either click on the downloaded file to unarchive it or run the following command:

[source,shell,subs="attributes"]
----
tar -xvf platform-operator-<version>.tgz
----

This will result in a directory named `platform-operator`.

=== Run the bootstrap command

[source,shell,subs="attributes"]
----
ddlctl bootstrap --chart-repo-url platform-operator --skip-domino-resource --skip-namespace --skip-registry-secret
----

If you downloaded and unarchived the Helm chart in a different directory than your current working directory, make sure to specify that in the `--chart-repo-url` flag.

----- admin_guide/installation/releases.txt -----
:page-permalink: 5246aa
:page-version: 6.1
:page-title: Releases
:page-order: 90

:agent-version: 6.1.2
:fcmd-tag: 6.1.2.catalog-c8f8be5
== {agent-version} (September 2025)

=== ddlctl

`0.5.9`

- link:598b40[ddlctl installation guide]
- link:73c6b3[ddlctl upgrade guide]

=== fleetcommand-agent


`{fcmd-tag}`

- link:e6ea06[Domino installation guide]
- link:5522a2[Domino upgrade guide]

=== Offline Installation Bundle

link:https://mirrors.domino.tech/s3/domino-artifacts/offline/fleetcommand-agent-{fcmd-tag}/docker-images-{agent-version}.tar[Download]


:agent-version: 6.1.1
:fcmd-tag: 6.1.1.catalog-2bfaba5
== {agent-version} (July 2025)

=== ddlctl

`0.5.6`

- link:598b40[ddlctl installation guide]
- link:73c6b3[ddlctl upgrade guide]

=== fleetcommand-agent


`{fcmd-tag}`

- link:e6ea06[Domino installation guide]
- link:5522a2[Domino upgrade guide]

=== Offline Installation Bundle

link:https://mirrors.domino.tech/s3/domino-artifacts/offline/fleetcommand-agent-{fcmd-tag}/docker-images-{agent-version}.tar[Download]


:agent-version: 6.1.0
:fcmd-tag: 6.1.0.catalog-b50f131

== {agent-version} (June 2025)

=== ddlctl

`0.5.4`

- link:598b40[ddlctl installation guide]
- link:73c6b3[ddlctl upgrade guide]

=== fleetcommand-agent


`{fcmd-tag}`

- link:e6ea06[Domino installation guide]
- link:5522a2[Domino upgrade guide]

=== Offline Installation Bundle

link:https://mirrors.domino.tech/s3/domino-artifacts/offline/fleetcommand-agent-{fcmd-tag}/docker-images-{agent-version}.tar[Download]

----- admin_guide/installation/requirements.txt -----
:page-version: 6.1
:page-permalink: 8fe42a
:page-title: Requirements
:page-order: 20

IMPORTANT: This is not a universal list of requirements. Please be sure to also review the documentation for your target deployment environment for any additional prerequisites.

To install Domino, you must have the following:

* https://quay.io[quay.io^] credentials provided by Domino.
* https://www.docker.com[Docker^] installed and the daemon running on your local machine.
* https://helm.sh[Helm^] installed.
* https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/[Kubectl^] installed.
  ** https://kubernetes.io/docs/reference/access-authn-authz/authentication/[Kubectl access^] to the cluster where you'll deploy Domino.
* Access to Docker images for Domino components.
* Access to Helm charts.
* The link:c3298b[ddlctl command line] installed.

IMPORTANT: Newer versions of Docker Desktop changed a default setting that blocks access to the default Docker socket, which `ddlctl` depends on for some operations.
Verify that you have enabled *Allow the default Docker socket to be used* under *Advanced Settings* before continuing.

[[tr2]]
To retrieve images of Domino components, the target cluster must have access to the following domains:

* `quay.io`
* `domino.tech`
* `k8s.gcr.io`
* `docker.elastic.co`
* `docker.io`
* `gcr.io`

[[tr3]]
If you intend to install Domino offline, configure your `Domino` custom resource definition specification to point to a private Docker registry and application registry.
Offline installation packages are downloaded from the `mirrors.domino.tech` domain.

Domino recommends that you use a partition with at least ten times more disk space than the total volume of your container images.
For example, if your container images occupy several dozen gigabytes, use a partition with several hundred gigabytes of space available.
This ensures that the partition has enough free space for container images that Domino users create and manage.
See link:https://tickets.dominodatalab.com/hc/en-us/articles/360060906852-Why-did-my-execution-fail-with-the-error-No-space-left-on-device-[Why did my execution fail with the error 'No space left on device'?^] for advice if you exceed your available disk space.

----- admin_guide/installation/set-up-custom-certicates.txt -----
:page-version: 6.1
:page-permalink: 8d3953
:page-title: Set up custom certificates
:page-order: 70

You can configure Domino to connect to services that use custom certificates that are external to the Domino cluster.
In addition to public services like AWS S3, you might want to use private services in your security domain that are secured with custom certificates or a custom certificate authority.

The following are examples of private services:

* Docker registry
* Git server
* S3 service
* LDAPs
* OIDC
* Data sources


Domino recommends that you add certificates for private services to the installation configuration file (`domino.yml`) as described in this topic. This ensures that Domino propagates the certificates throughout the system and maintains them, even when you upgrade Domino.

NOTE: If you add custom certificates to `etc/ssl/certs` in a link:f51038[compute environment], the system overwrites them at runtime. If you cannot use Domino's recommended method or must configure the certificates for a specific compute environment, see https://support.domino.ai/support/s/article/Configure-Certificates-for-A-Specific-Compute-Environment-1718868040165[Configure Certificates for A Specific Compute Environment^].

Start with link:77bb0e[Domino-custom-certificates ConfigMap] for complete details about how to set up custom certificates.

----- admin_guide/installation/user-acceptance-testing.txt -----
:page-version: 6.1
:page-permalink: 0fa80e
:page-title: User acceptance testing
:page-order: 80

You can use this user acceptance testing (UAT) list as a guide to check for issues after you install or upgrade Domino.
You can also use this list to help establish your instance's baseline functionality before an upgrade.

Each item in the list pertains to a Domino feature and its expected behavior.
Treat these items as suggested tests to confirm that the feature works correctly.

NOTE: Some of the test items might not apply to your Domino instance.
For example, you might only have Domino File System projects, but the list includes items about Git-based projects.
Modify the list to address your organization's needs.

== Administrative account features

* link:eb97ae[Access the executions page] to view pods, node details, and deployment logs for an execution.
  ** Stop a test execution.
* link:7bb387[Access the infrastructure page] to view a list of currently active compute and platform nodes.
  ** Verify that the number of compute and platform nodes on the infrastructure page matches the intended solution design.
* Verify that link:6e50e8[user activity reports] are set up.
* link:71d6ad[Set Configuration records] that match your organization's needs. For example:
  ** Send a license usage report is sent to usage@dominodatalab.com.
  ** Configure the maximum number of simultaneous executions per user.
* link:0002fb[Access the workspaces page] as an administrator.
* link:d5d2ac[Access the control center] and verify that the metrics display as expected.
* Verify that users and organizations receive link:ba9786[in-app system notifications].
* Verify that the following events trigger an email notification:

  ** link:5b84c5[Run completion].
  ** link:06ceeb[Username @-mentions in comments].
  ** link:4b6411[User added as a project collaborator].

== User account features

* Create a new user account or sign in with single sign on (SSO) to your Domino instance.
* Access your Domino instance from a user account and verify that pages load correctly.
* Verify that you can access the following as a user:

  ** link:13a1a4[Project portfolio].
  ** link:cc299e[Assets portfolio].

== Command Line Interface (CLI)

* link:e21e55[Install] the Domino CLI.
* link:9355a5#to-synchronize-the-files-on-your-computer-with-the-server[Sync a project] from the Domino CLI.
* link:9355a5#_run_your_code[Execute a batch run] from the Domino CLI, and verify that results sync with your local machine.

== Compute Environment

* link:fa8137[Create a new compute environment] and verify that it builds successfully and pushes the associated image to Domino's Docker registry.
* link:bfa148[Add a new package] and build a new compute environment.
* link:5dd2c1/[Add pre-run, post-run, pre-setup, and post-setup scripts] and verify that they execute when you perform a run.
* Start a session in each of the following and execute code:
+
  ** link:064d93[RStudio].
  ** link:93aef2[Jupyter].
  ** link:93aef2[Jupyterlab].
  ** link:fbb1db[Visual Studio Code].

== Datasets

* Upload files to link:0a8d11[create Datasets] and link:dbdbff[snapshots].
* Verify that link:6942ab[shared Datasets and snapshots] are mounted in a project.

== Data Sources

link:fbb41f[Connect to Data Sources] that your organization uses.

== External Data Volumes

* link:ee8d01#_add_an_edv_to_a_project[Mount] an external data volume.
* link:ee8d01#_view_all_mounted_volumes_in_a_project[View] external data volumes from the data page.
* link:ee8d01#_use_an_edv_in_a_project[Access] external data volumes.
* link:ee8d01#_remove_an_edv_from_a_project[Unmount] an external data volume.

== Git-based Projects

* link:eaab17[Create] a Git-based project.
* link:63ac71#change-branhces[Switch branches] in the workspace via file changes workspace UI option.
* link:b51a09[Resolve merge conflicts manually] in Domino and push code to a remote repository.

== Logs

link:a9e507[View] the following:

* Workspace logs.
* Execution logs.
* Domino endpoint logs.
* link:164503[Setup logs] for workspaces and runs.
* link:164503[User output logs] for workspaces and runs.

== Domino endpoints

* link:8dbc91#Deploy-a-model[Publish] a Domino endpoint.
* link:8dbc91#Request-a-prediction[Send test calls] to the Domino endpoint.
* link:74f8ed#reproduce-the-environment[Use workspace checkpoints] to open a previously published Domino endpoint version in a workspace.

== Projects

* link:a8e081[Create] a new project.
* link:7a0fee[Upload files] to a project from a project's *Files Tab*.
  ** View the files page and verify that files render.
* link:029c37[Revert a project's files] to a previous commit.
* link:262fef[Verify that new files can sync].
* link:314004[Add a Git repository] to the project.
* link:d6c084[Commit, push, and sync] with a Git repository.
* link:0ea71e[Edit the volume size] and launch new workspaces with the updated volume.
* Verify that results from the following actions sync back to Domino:
  ** link:9a69d9[Batch run Python code to train a model].
  ** link:c5ce58[Batch run R code to train a model].

== Supported clusters

The following items require that you first create the respective workspace and cluster compute environments.

Attach the following to a workspace or job:

* link:d13903[Ray cluster].
* link:482ec5[Spark cluster].
* link:747a51[Dask cluster].

== Web applications

* link:2039f2[Start a Python Flask app and publish it from Domino].
  ** Verify that Domino creates a shareable link to the app.
* link:e92082[Start a R Shiny app and publish it from Domino].
  ** Verify that Domino creates a shareable link to the app.

== Workspaces

* link:02d166[Resume a workspace].
* Verify that link:6ac5a1[environment variables are visible and available] in workspaces.
* link:93aef2[Start a Jupyter workspace] and link:288e42[execute Python code].
  ** Verify that results sync back to Domino.
* View the following from the workspace settings page:
+
  ** link:164503[Logs].
  ** link:0002fb[Details].
  ** link:d5800f[Resources usage].

* link:ca786d[Start a workspace in a Domino File System project], sync a new file, and stop the workspace. Verify that you can perform the following:
+
  ** link:9453b6[View the history] of the workspace and the file state.
  ** link:f86aa5[Open a new workspace with a new branch from a previous commit].
    *** In the files section of the project, view the branch name you created in the previous test.

----- admin_guide/kubernetes-infrastructure/cluster-compatibility.txt -----
:page-version: 6.1
:page-title: Compatibility with clusters
:page-permalink: 1e06ac
:page-order: 20

Domino has been tested and verified to run on the following types of
clusters:



[cols="^2a,^2a,3a",options="header"]
|===
|Vendor |Partner |Cluster information

^|image:/images/logos/aws.png[aws, link="https://aws.amazon.com/", width="100px", role=noshadow]
|✓
|link:b5da89[Amazon Elastic Kubernetes Service Deployment Guide^]

link:https://aws.amazon.com/eks/[EKS Website^]

link:https://aws.amazon.com/marketplace/pp/prodview-faq6kqootwqsw[Domino on AWS Marketplace^]

^|image:/images/logos/azure.png[azure, link="https://azure.microsoft.com/", width="140px", role=noshadow]
|✓
|link:7d0b3e[Azure Kubernetes Service Deployment Guide^]

link:https://aws.amazon.com/eks/[AKS Website^]

link:https://azuremarketplace.microsoft.com/en-in/marketplace/apps/dominodatalab.domino-azure[Domino on Azure Marketplace^]

^|image:/images/logos/gcp.png[gcp, link="https://cloud.google.com/", width="140px", role=noshadow]
|✓
|link:6e290a[Google Kubernetes Engine Deployment Guide^]

link:https://cloud.google.com/kubernetes-engine[GKE Website^]

^|image:/images/logos/redhat.png[redhat, link="https://www.redhat.com/", width="100px", role=noshadow]
|✓
|link:3fb861[Red Hat OpenShift Deployment Guide^]

https://www.redhat.com/[Red Hat Website^]

|===





[[tr7]]
If you have questions about cluster compatibility, https://www.dominodatalab.com/contact-us/[contact Domino^].

----- admin_guide/kubernetes-infrastructure/cluster-requirements.txt -----
:page-version: 6.1
:page-title: Cluster requirements
:page-permalink: 25b6dc
:page-order: 30

You can deploy Domino into a Kubernetes cluster that meets the following requirements.

//DOCS-1092

See link:7b2cbe[Kubernetes Compatibility] for general cluster compatibility.



== Cluster permissions

[[tr4]]
Domino needs permission to install and configure pods in the cluster through Helm.
The Domino installer is delivered as a containerized Python utility that operates Helm through a `kubeconfig` that provides service account access to the cluster.

== Namespaces

Domino creates one dedicated namespace for Platform nodes, one for Compute nodes, and one for installer metadata and secrets.

[[node-pool-requirements]]
== Nodes


The recommended node requirements for Domino are four platform nodes and two compute nodes.

If you are upgrading to 5.0 or higher, and the platform cluster does not support autoscaling, you must increase the number of nodes to support the increased node requirements.

[[storage-requirements]]
== Storage requirements

[[storage-classes]]
=== Storage classes

Domino requires at least two storage classes.

. *Dynamic block storage*
+
Domino requires high-performance block storage for the following types of data:
+
--
* Ephemeral volumes attached to user execution
* High-performance databases for Domino application object data
--
+
This storage must be backed by a storage class with the following properties:
+
--
* Supports dynamic provisioning
* Can be mounted on any node in the cluster
* SSD-backed recommended for fast I/O
* Capable of provisioning volumes of at least 100GB
* Underlying storage provider can support `ReadWriteOnce` semantics
* Are backed by true, fully POSIX-compliance block storage (i.e., NOT NFS)
--
+
NOTE: If this storage does not meet these requirements — or if you override critical
services that rely on block storage (mongo, postgres, Git) to use a different
storage class — you may see performance degradations, catastrophic failures, and
unexpected data loss.
+
By default, this storage class is named `dominodisk`.
+
In AWS, EBS is used to back this storage class.
The following is an example configuration for a compatible EBS storage class:
+
[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: domino-compute-storage
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
  fsType: ext4
----
+
In GCP, compute engine persistent disks are used to back this storage class.
The following is an example configuration for a compatible GCEPD storage class:
+
[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: dominodisk
parameters:
  replication-type: none
  type: pd-standard
provisioner: kubernetes.io/gce-pd
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
----


. *Long-term shared storage*
+
Domino needs a separate storage class for long-term storage for:
+
--
* Project data uploaded or created by users
* Domino Datasets
* Docker images
* Domino backups
--
+
This storage needs to be backed by a storage class with the following properties:
+
--
* Dynamically provisions Kubernetes PersistentVolume
* Can be accessed in ReadWriteMany mode from all nodes in the cluster
* Uses a `VolumeBindingMode` of `Immediate`
--
+
In AWS for example, these storage requirements are handled by a class that is backed by EFS for Domino Datasets, and a class that is backed by S3 for project data, backups, and Docker images.
+

+
By default, this storage class is named `dominoshared`.


=== Native

Domino requires cloud-provider native object storage for a few resources and services:

* *Blob Storage*.
For AWS, the blob storage must be backed by S3 (see link:b5da89#blob-storage[Blob storage]).
For other infrastructure, the `dominoshared` storage class can be used.
* *Logs*.
For AWS, the log storage must be backed by S3 (see
link:b5da89#blob-storage[Blob storage]).
For others, the
`dominoshared` storage class can be used.
* *Costs*.
For AWS only, the log storage must be backed by S3 (see
link:b5da89#blob-storage[Blob storage]).

* *Backups*.
For all supported cloud providers, storage for backups is backed by the native blob store.
For on-prem, backups are backed by the
`dominoshared` storage class.
+
** AWS: https://aws.amazon.com/s3/[S3^]
** Azure:
https://azure.microsoft.com/en-us/services/storage/blobs/[Azure Blob Storage^]
** GCP: https://cloud.google.com/storage[GCP Cloud Storage^]

* *Datasets*.
For AWS, Datasets storage must be backed by EFS (see
link:b5da89#datasets-storage[Datasets storage]).
For other infrastructure, the `dominoshared` storage class can be used.

=== On-Prem

[[tr5]]
In On-Prem environments, a wide variety of block and file-based storage is used by customers. We expected that `dominodisk` is backed by block storage (not NFS), ideally matching the requirements of `Dynamic block storage` defined above. In some cases, host volumes can be used for backing services like Git, Postgres, and MongoDB. Note that Postgres and MongoDB provide state replication. Host volumes can be used for Runs, but network-attached block storage is preferred to leverage files cached in block storage that is portable between nodes.
If host volumes are used for Runs, file caching must be disabled and you will potentially expect slow start-up executions for large projects.

== Node pool requirements

Domino requires a minimum of two node pools, one to host the Domino Platform and one to host Compute workloads.
Additional optional pools can be added to provide specialized execution hardware for some Compute workloads.
[[tr6]]

. *Platform pool requirements*
* Boot Disk: 128GB
* Min Nodes:
4
* Max Nodes:
4
* Spec: 8 CPU / 32GB
* Labels: `dominodatalab.com/node-pool: platform`
* Tags:
** `kubernetes.io/cluster/{{ cluster_name }}: owned`
** `k8s.io/cluster-autoscaler/enabled: true` #Optional for autodiscovery
** `k8s.io/cluster-autoscaler/{{ cluster_name }}: owned` #Optional for autodiscovery


. *Compute pool requirements*
* Boot Disk: 400GB
* Recommended Min Nodes: 1
* Max Nodes: Set as necessary to meet demand and resourcing needs
* Recommended min spec: 8 CPU / 32GB
* Enable Autoscaling: Yes
* Labels: `domino/build-node: true`,
`dominodatalab.com/node-pool: default`
* Tags:
** `k8s.io/cluster-autoscaler/node-template/label/dominodatalab.com/node-pool: default`
** `kubernetes.io/cluster/{{ cluster_name }}: owned`
** `k8s.io/cluster-autoscaler/node-template/label/domino/build-node: true`
** `k8s.io/cluster-autoscaler/enabled: true` #Optional for autodiscovery
** `k8s.io/cluster-autoscaler/{{ cluster_name }}: owned` #Optional for autodiscovery


. *Optional GPU compute pool*
* Boot Disk: 400GB
* Recommended Min Nodes: 0
* Max Nodes: Set as necessary to meet demand and resourcing needs
* Recommended min Spec: 8 CPU / 16GB / One or more NVIDIA GPU Device
* Nodes must be pre-configured with the appropriate NVIDIA driver, NVIDIA-docker2, and set the default docker runtime to nvidia.
For example,
https://docs.aws.amazon.com/eks/latest/userguide/gpu-ami.html[EKS GPU optimized AMI^].
* Labels: `dominodatalab.com/node-pool: default-gpu`,
`nvidia.com/gpu: true`
* Tags:
** `k8s.io/cluster-autoscaler/node-template/label/dominodatalab.com/node-pool: default-gpu`
** `kubernetes.io/cluster/{{ cluster_name }}: owned`
** `k8s.io/cluster-autoscaler/enabled: true` #Optional for autodiscovery
** `k8s.io/cluster-autoscaler/{{ cluster_name }}: owned` #Optional for autodiscovery

[[cluster-networking]]
== Cluster networking

[[tr7]]
To manage secure communication between pods, Domino relies on link:https://kubernetes.io/docs/concepts/services-networking/network-policies/[Kubernetes network policies^].
You must use a networking solution that supports the Kubernetes `NetworkPolicy` resource.
One such solution is link:https://projectcalico.docs.tigera.io/about/about-calico#full-kubernetes-support[Calico^].

== Ingress and SSL

[[tr8]]
Domino must be configured to serve from a specific FQDN, and DNS for that name must resolve to the address of an SSL-terminating load balancer with a valid certificate.

IMPORTANT: A Domino install can't be hosted on a subdomain of another Domino install.
For example, if you have Domino deployed at `data-science.example.com`, you can't deploy another instance of Domino at `acme.data-science.example.com`.

The load balancer must target incoming connections on ports 80 and 443 to port 80 on all nodes in the Platform pool.
This load balancer must support WebSocket connections.

In order for Domino to correctly detect the protocol of incoming requests, the SSL-terminating load balancer must properly set the `X-Forwarded-Proto` header. Domino does not currently support the alternative `Forwarded` header.

[[tr9]]
Health checks for this load balancer must use HTTP on port 80 and check for 200 responses from a path of `/healthz`.

[[tr10]]
NOTE: Domino continues to support Environments with subdomains.
If you are using subdomains for your Domino deployment and need best-practice information, contact your Account Manager.
However, Domino recommends that you do not use them for improved security.

== NTP

[[tr11]]
To support SSO protocols, TLS connections to external services, intra-cluster TLS when using Istio, and to avoid general interoperability issues, the nodes in your Kubernetes cluster must have a valid Network Time Protocol (NTP) configuration.
This will allow for successful TLS validation and operation of other time-sensitive protocols.

----- admin_guide/kubernetes-infrastructure/index.txt -----
:page-version: 6.1
:page-title: Kubernetes infrastructure
:page-permalink: be5e54
:page-order: 30

Domino runs in your https://kubernetes.io/[Kubernetes^] cluster, and the infrastructure can be managed with Kubernetes tools like https://kubernetes.io/docs/reference/kubectl/overview/[kubectl^].

The following are the types of https://kubernetes.io/docs/concepts/architecture/nodes/[Kubernetes nodes^] used by Domino:

Platform nodes::

Platform nodes, https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/[labeled^] with `dominodatalab.com/platform-node: true`, host the always-on components of the Domino application, including the frontends, API server, authentication service, and supporting metadata services.
These nodes host a fixed collection of persistent pods.

Compute nodes::

Compute nodes, https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/[labeled^] with `dominodatalab.com/node-pool:<node name>`, host user jobs and published Domino Models and Apps.
The workload hosted by these nodes will change with user demand, and using an elastic cloud cluster will allow for automatic scaling of this pool to meet the needs of active users.

Domino recommends that you have a separate Node Pool for Platform and Compute nodes.
This is not always possible, but is preferred.
Domino does set resource limits and requests, so they can't overwhelm individual nodes.

See link:a0b173[Domino architecture] to learn more.

== Next steps

* link:7b2cbe[Kubernetes compatibility].
* link:1e06ac[Compatibility with clusters].
* link:25b6dc[Cluster requirements].
* link:d06563[NVIDIA DGX in Domino].

----- admin_guide/kubernetes-infrastructure/kubernetes-compatibility.txt -----
:page-version: 6.1
:page-title: Kubernetes compatibility
:page-permalink: 7b2cbe
:page-order: 10

The table below shows the versions of Kubernetes on which Domino validates its software. Validation is performed for the following providers: AWS, Azure, Google Cloud, Red Hat OpenShift, and Rancher.

For security and stability, Domino encourages customers to keep their Kubernetes infrastructure up to date running a version of Domino we've officially validated. Outdated or unvalidated combinations of Domino and Kubernetes may affect the supportability of your deployment.

== Upgrade guidance

Every 12-14 months, Kubernetes versions reach end-of-life (EOL) and may no longer be supported by the Kubernetes provider. If you have questions about the compatibility and upgrade path, link:https://tickets.dominodatalab.com/hc/en-us[contact Domino customer support for assistance^].

Work with Domino to align your installation and upgrade planning with the guidance below:

* Domino ships major or minor releases approximately every 3 months. Strive to keep your Domino deployments updated with the latest releases by upgrading every 3 to 6 months. Skipping multiple versions of Domino results in larger gaps in Kubernetes version compatibility and more complex upgrades.

* Each Kubernetes provider has a different release schedule for specific versions of Kubernetes. Domino strives to support new Kubernetes releases within approximately 3 months of the generally available release date.

* Domino should be upgraded onto a supported Kubernetes version (see compatibility table below). When you upgrade Kubernetes, you should immediately upgrade to a supported Domino version.

* Domino recommends keeping Kubernetes control plane and data plane at the same version, minimizing link:https://kubernetes.io/releases/version-skew-policy/#supported-version-skew[version skew^].

=== Domino Nexus hybrid deployments

Nexus control planes require the same Kubernetes cluster requirements as any other Domino deployment.

However, Nexus data planes have the following differences:

* No requirement for shared storage (RWX storage class).
* Requires only one (compute) namespace.
* Ingress configuration is different, see link:491fe8[Enable a Data Plane for Workspaces] for more information.
* Kubernetes version compatibility may be different from the control plane (see table below).

== Kubernetes compatibility per platform

The following matrix shows the supported combinations of Domino and Kubernetes platform versions. For inquiry about unlisted Kubernetes distributions, contact your Domino professional services representative.

End of life (EOL) for Kubernetes versions is dictated by the Kubernetes platform provider. For more information on what EOL means for each platform, see the links in the Kubernetes Platform column.

=== Control plane

[cols="2a,>1a,>1a,>1a,>1a,>1a", options="header", stripes="hover"]
|===
| 5+^|Domino Version
|**Kubernetes Platform** ^|**6.1.x** ^|**6.0.x** ^|**5.11.x** ^|**5.10.x** ^|**5.9.x**

.2+|link:https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html#kubernetes-release-calendar[Amazon Elastic Kubernetes Service (EKS)^]
|1.32|1.31|1.28|1.27|1.27
>|1.33^‡^|1.32*|1.29|1.28|1.28

.2+|link:https://learn.microsoft.com/en-us/azure/aks/supported-kubernetes-versions?tabs=azure-cli#aks-kubernetes-release-calendar[Azure Kubernetes Service (AKS)^]
|1.32|1.30|1.28|1.27|1.27
>|1.33^‡^|1.31|1.29|1.28|1.28

.2+|link:https://cloud.google.com/kubernetes-engine/docs/release-schedule[Google Kubernetes Service (GKE)^]
|1.31|1.31|1.28|1.27|1.26
>|1.32|1.32|1.29|1.28|1.27

.2+|link:https://access.redhat.com/support/policy/updates/openshift[Red Hat OpenShift^]
|1.30 (4.17)|1.30 (4.17)|1.28 (4.15)|1.27 (4.14)|1.27 (4.14)
>|1.31 (4.18)|1.30 (4.17)|1.28 (4.15)|1.27 (4.14)|1.27 (4.14)

.2+|Rancher^§^
|||1.27 (2.8.3)|1.26 (2.7.9)|1.26 (2.7.6)
>| | |1.28 (2.8.3)||
|===

*EKS 1.32 is available from Domino 6.0.3. +
^‡^EKS 1.33 and AKS 1.33 are available from Domino 6.1.2. +
^§^link:https://support.domino.ai/support/s/article/Rancher[Rancher deprecation notice]

=== Nexus data plane

The table below shows the minimum supported version of Kubernetes for Nexus data planes by platform and Domino version. Supported versions may differ for Nexus data planes deployed on https://www.vcluster.com/docs[vClusters^].

[cols="2,>1,>1", options="header", stripes="hover"]
|===
| 2+^|Control Plane Domino Version
| **Platform Type** ^|**6.0.x** ^|**5.11.x**

|non-vCluster |1.26|1.26
|vCluster |1.27|1.27

|===

== Docker compatibility

Domino doesn't publish a validated container runtime compatibility matrix and recommends using the default container runtime version provided by your vendor. However, if your Kubernetes nodes use Docker, it should be 20.10.24 or higher.

== Next steps
* link:25b6dc[Kubernetes cluster requirements].

----- admin_guide/kubernetes-infrastructure/nvidia-dgx-in-domino/best-practices.txt -----
:page-version: 6.1
:page-title: NVIDIA DGX best practices
:page-permalink: 1266a4
:page-order: 30

. Build Node
+
[[tr5]]
Domino recommends you _do not_ use a DGX GPU as a build node for environments.
Instead, opt for a CPU resource as part of your overall Domino architecture.

. Splitting GPUs per Tier
+
[[tr6]]
Domino recommends providing several GPU tiers with different numbers of GPUs in each tier.
For example, 1, 2, 4, and 8 GPU hardware tiers as different training jobs can take use of single or parallel GPU usage and consuming a whole DGX box for one workload might not be feasible in your environment.

. Governance
+
[[tr7]]
[[tr8]]
After splitting up hardware tiers, access can be global or, alternatively,  limited to specific organizations.
Domino recommends that you ensure that the right organizations have GPU Hardware Tier access, or are restricted, to ensure availability for critical work, and/or to prevent the unauthorized use of GPU tiers.

----- admin_guide/kubernetes-infrastructure/nvidia-dgx-in-domino/configure-cuda-nvidia-drivers.txt -----
:page-version: 6.1
:page-title: Configure CUDA / NVIDIA drivers
:page-permalink: cf6a52
:page-order: 20

NVIDIA Driver::
Your server administrator must configure the NVIDIA driver at the host level.
Use the https://www.nvidia.com/download/index.aspx?lang=en-us[configuration guide^] to identify the correct NVIDIA driver for your host.
See the https://docs.nvidia.com/dgx/index.html[DGX Systems Documentation^] for more information.

CUDA Version::
The CUDA software version required for a given development framework, such as Tensorflow, is documented on their website.
For example, https://www.tensorflow.org/install/gpu[Tensorflow >=2.1 requires CUDA 10.1^] and some additional software packages, for example,
https://developer.nvidia.com/cudnn[CuDNN^].

CUDA and NVIDIA Driver Compatibility::
After you identify the correct CUDA version, consult the https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility__table-toolkit-driver[CUDA-NVIDIA Driver Compatibility Table^].
+
In the Tensorflow 2.1 example, the CUDA 10.1 requirement means you must be running CUDA >=10.1 and NVIDIA driver >=410.48 on the host.
Table 1 in the previous link will guide your choice of matching CUDA and NVIDIA driver versions.
+
Subsequently, the link:f51038[Domino Compute Environment] must be configured to leverage the _exact_ CUDA version that corresponds to the application.
+
[[tr4]]
Simplifying this constraint, CUDA drivers provide backwards compatibility: the CUDA version on the host can be greater or equal to that which is specified in your Compute Environment.
+
Because the CUDA software installation process often returns unexpected results when attempting to install _an exact CUDA version_, including patch version, the fastest route to a functioning configuration is typically to install the _latest_ available minor release from your required major version of CUDA, and subsequently creating a Docker environment variable (ENV) from within your Compute Environment that constrains compatible sets of CUDA, GPU generations, and NVIDIA drivers.

Need Additional Assistance?::

Consult your Domino customer success engineer for guidance on your specific needs.
Domino can sample configurations that will simplify your configuration process.

----- admin_guide/kubernetes-infrastructure/nvidia-dgx-in-domino/index.txt -----
:page-version: 6.1
:page-title: NVIDIA DGX in Domino
:page-permalink: d06563
:page-order: 40

NVIDIA DGX systems can run Domino workloads if they are added to your Kubernetes cluster as compute (worker) nodes.
This topic covers how to setup and add DGXes to Domino.

image::/images/nvidia_dgx_flow_diagram.png[alt="DGX & Domino Integration Flow Diagram", width=1200, role=noshadow]

The flow chart begins from the top left, with a Domino end user requesting a GPU tier.

If a DGX is already configured for use in Domino's Compute Grid, the Domino platform administrator can define a GPU-enabled Hardware Tier from within the Admin console.

The middle lane of the flow chart outlines the steps required to integrate a provisioned DGX system as a node in the Kubernetes cluster that is hosting Domino, and subsequently configure that node as a GPU-enabled component of Domino's compute grid.

The bottom swim lane outlines that, to leverage a NVIDIA DGX system with Domino, it must be purchased and provisioned into the target infrastructure stack hosting Domino.

== Next steps

* link:d7c03e[Install NVIDIA DGX in Domino]
* link:cf6a52[Configure CUDA / NVIDIA drivers]
* link:1266a4[NVIDIA DGX best practices]

----- admin_guide/kubernetes-infrastructure/nvidia-dgx-in-domino/install-dgx-systems.txt -----
:page-version: 6.1
:page-title: Install NVIDIA DGX in Domino
:page-permalink: d7c03e
:page-order: 10


You can purchase NVIDIA DGX systems through https://www.nvidia.com/en-us/data-center/where-to-buy-dgx-systems/[NVIDIA's Partner Network^].
Install the DGX system in a hosting environment with network access to additional link:25b6dc#node-pool-requirements[host] and link:25b6dc#storage-requirements[storage] infrastructure required to host Domino.

== Configure DGX System for Domino

.Option A: New Kubernetes cluster & Domino installation

[[tr1]]
If this is a new (greenfield) deployment of Domino:

Install and configure a Kubernetes cluster that meets Domino's link:25b6dc[cluster requirements], including valid configuration of your link:25b6dc#cluster-networking[Kubernetes' network policies] to support secure communication between pods that will host Domino's link:e3500f[platform services] and link:eca4b2[compute grid].

.Option B: Existing Kubernetes cluster and/or Domino installation

[[tr2]]
. https://kubernetes.io/docs/concepts/architecture/nodes/[Add the DGX to your K8s API server^] as a worker node, with a link:908bd9#isolating-workloads-and-users-using-node-pools[node label] consistent with your chosen naming conventions.
The default node label for GPU-based worker nodes is `default-gpu`.

. You must add proper https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/[taints^] to your DGX node.
This facilitates the selection of the DGX for GPU-based workloads running on Domino.
[[tr3]]

.Configure a Domino hardware tier to leverage your configured DGX compute mode

After the DGX is added to your API server and labeled properly, you can configure link:9d16e5[hardware tiers] from within Domino's Admin application.

Domino provides governance features from within this interface, supporting LDAP/AD federation or SSO-based attributes for managed access control and
link:526762[user execution quotas].
Domino has also published a series of link:908bd9[best practices] to manage hardware tiers in your compute grid.

----- admin_guide/manage-users/activate-users.txt -----
:page-version: 6.1
:page-title: Create and manage users
:page-permalink: b2471f
:page-order: 10

Create and activate users so they can use Domino. You can also deactivate users temporarily or permanently from the Admin panel.

Email services must be configured in your deployment for user creation to work.

== Create a user
Use the *Admin panel* to manually add new users by email and assign their roles. They’ll receive an invitation to set up their account after creation.

image::/images/6.1/admin-guide/create-user.png[alt="Add new user", width=800, role=noshadow]

. Go to *Admin panel* > *Users* > *View all*.
. Click *Add User*.
. Enter the user's email address and assign roles.
. Click *Save*.

The user will receive two emails:

* A notification 
* An invitation to set up their account 

They must follow the setup link in the email to complete their profile.

== Activate or deactivate a user
// available for cloud admins

. Go to *Admin panel* > *Users* > *View all*. 
. Find and select the user in the list.
. Click *Activate* or *Deactivate*, depending on their current status.

=== After deactivation
After deactivating a user, complete the following cleanup tasks to prevent orphaned resources and broken executions.

. Stop and delete workspaces: 
.. Go to *Reports* > *Workspaces* and search for any workspaces owned by the user.
.. Delete these workspaces.
. Check for active executions:
.. Go to *Reports* > *Executions* to view any running endpoints or apps owned by the user.
.. Make note of any that must remain active after project transfer — they will need to be restarted.
. Transfer or archive projects:
.. Go to *Reports* > *Projects*.
.. Either archive the user’s projects, or transfer ownership to another user.

IMPORTANT: If you transfer a project that contains running endpoints, apps, or other executions, those executions will stop working until they are manually restarted under the new owner.

== Next steps

* Learn how to link:2611b7[manage and assign user roles].
* See how to link:57f42d[reset a user's password].

----- admin_guide/manage-users/index.txt -----
:page-version: 6.1
:page-permalink: de48cc
:page-title: User management
:page-order: 90

Learn how to manage users in Domino to control permissions and track activity.

link:b2471f[Create and activate users]::
Create and activate users so they can use Domino. You can also deactivate users temporarily or permanently.

link:2611b7[Work with roles]::
Assign roles to users to grant them sets of permissions.

link:6921e5[Manage Domino Service Accounts]::
Create and manage Domino Service Accounts to authenticate and access the Domino API from outside of the Domino cluster.

link:6e50e8[Review user activity]::
View information about a user such as their name, whether they are active, and the projects they own.
You can also download the user information to a CSV file and schedule regularly generated user activity reports.

link:57f42d[Reset password]::
Reset user passwords.

----- admin_guide/manage-users/password-reset.txt -----
:page-version: 6.1
:page-permalink: 57f42d
:page-title: Reset password
:page-order: 60

*SysAdmins* can reset a user's password. To initiate the process, the deployment must have link:aa19d4[email settings configured] and utilize link:1a7dfa[local user management] and not an external identity provider.

To reset a password as an admin:

. Go to *Manage resources* > *Users*.
. Select *Reset Password* for the desired user.

Domino will invalidate the user's current password, and the user will receive an email prompting them to create a new password. 

When using an external identity provider to manage users, this button can be hidden and the feature disabled by setting the link:71d6ad[Configuration record] key `com.cerebro.domino.userManagement.passwordResetEnabled` to `false`.

== Next steps

Learn how to link:6e50e8[run a user activity report].

----- admin_guide/manage-users/roles/data-analyst.txt -----
:page-version: 6.1
:page-title: Enable and assign Data Analyst role
:page-sidebar: Enable and assign Data Analyst role
:page-permalink: 48afc2
:page-order: 20

The Data Analyst role is designed for users who have some technical background and coding experience in Python and R.
Users in the Data Analyst role don't have access to Machine Learning Operations (MLOps) capabilities like Domino endpoints, model monitoring, exports, or expensive compute capabilities such as GPU processing.

A Data Analyst user has the privileges of the Practitioner role, except that they cannot:

* Use model-related features
* Edit the hardware tier
* Edit or manage Environment definitions

Learn how to enable the Data Analyst role, assign users to the role, and monitor their activities.

== Enable the Data Analyst role

. Log in as an administrator.
. Go to *Admin* > *Platform settings* > *Configuration records* > Click *Add Record*.
. Create a new config option `com.cerebro.domino.dataAnalyst.enabled` and set it to `true`.

Select the hardware tier that Domino assigns to all Data Analyst executions:

. Restart the Domino services to apply the new setting.
. Go to *Admin* > *Manage resources* > *Hardware tiers*.
. Next to the hardware tier you want to use for Data Analysts, click *Edit* and select the *Is Tier for Data Analysts* checkbox.
. Click *Update*.

== Assign the Data Analyst role

After you enable the Data Analyst role, Keycloak will have a `data-analyst-user` role in the `domino-play` client. Use either of the following methods to set up a Keycloak client role.

[[manual]]
=== Manually assign the user role

Manually assign a single user the Data Analyst role:

. Log in to Keycloak.
. Go to *DominoRealm* > *Users* > Click the user ID > *Role Mappings*.


[[keycloak]]
=== Use Keycloak groups to assign the role

Assign all members of a Keycloak group to the Data Analyst role.

Regardless of your authentication framework, you must propagate the `data-analyst-user` role in the `domino-play` client and the Practitioner Domino global role. The following steps use the SAML/LDAP authentication method:

. Log in to Keycloak and go to *DominoRealm*.
. Click *Groups* and create a new group, such as `data-analyst-group`.
. In the new group, click *Role Mappings*.
. Click *Assign role* > *Filter by clients* > Select *data-analyst-user* > *Assign*.
+
image::/images/5.9/admin_guide/authentication-authorization/keycloak-assign-group-role.png[Use Keycloak groups to assign the Data Analyst role]

Add users to the Keycloak group:

. Go to *DominoRealm* > *Users* and find the user you want to add.
. Click the user ID > *Groups*.
. Click *Join group* > *Data analyst*.
+
image::/images/5.9/admin_guide/authentication-authorization/join-keycloak-group.png[Use Keycloak groups to assign the Data Analyst role]

== Monitor Data Analysts

View link:6e50e8[user activity reports] to view user information and activity.

After you enable the Data Analyst role, the Users page displays a *License* column with the following values:

* *DataScienceProfessional* (standard Domino license)
* *DataAnalyst* (the new Data Analyst license)
* *LiteUser* (a legacy role)

----- admin_guide/manage-users/roles/index.txt -----
:page-version: 6.1
:page-title: Work with roles
:page-permalink: 2611b7
:page-order: 30

Administrators assign roles to users based on assignments and responsibilities.
Set these roles in the application or link:bc00bd[map them from your identity provider] if you have SSO integration enabled.
If you start with a completely new Domino installation, the first user to log in is assigned the *SysAdmin* and *Practitioner* roles.

The available roles are:

* *SysAdmin* - Administers instance with full administrative access.
* *CloudAdmin* - Administers Domino Cloud instance with limited administrative access (link:5dc7d5[Domino Cloud] only).
* *ProjectManager* - Manages organizations and project tags.
* *SupportStaff* - Manages compute-related functionality.
* *Practitioner* - Uses compute and file storage.
* *ReadOnlySupportStaff* - View compute-related configuration.
* *Librarian* - Manages project library.
* *LimitedAdmin* - SysAdmin without access to projects and data.
* *LicenseReviewer* - Views license-related content.
* *Lite User* - A user with no role. See link:#lite-user[Lite User].
* *GovernanceAdmin* - Manages policies with Domino Governance. See link:#governance-admin[GovernanceAdmin role].

TIP: *LimitedAdmin* and *LicenseReviewer* roles do not grant any permissions to Projects or Data.

// Practitioner for new users, unless CC changed

By default, all new users are assigned the *Practitioner* role.

== Edit roles

// Users with multiple roles: permissions add
// Sysadmins can edit roles
When multiple roles are assigned to a user, permissions are additive.
To grant users roles, you must be a *SysAdmin* or a *CloudAdmin*.
SysAdmins can grant any role to any user.
CloudAdmins can grant the CloudAdmin and Practitioner roles to CloudAdmins and Practitioners.

. In the Admin application, click *Users*.
. Search for the username to grant permissions.
. Click *Edit* and select the roles.
. Click *Save*.

// Lite users can only view the project list
// Practitioners can view, create, fork, archive projects
// Sysadmins can view the project list and archive projects

== Actions for specific roles

=== Project overview actions

[cols="3a,^2a,^2a,^2a,^2a,^2a,^2a,^2a,^2a",options="header"]
|===
|Permission|Practitioner|SysAdmin|CloudAdmin|SupportStaff|ReadOnlySupportStaff|Librarian|Limited Admin|License Reviewer

|Create Project |✓|-|-|-|-|-|-|-
|View Project List |✓|✓|✓|✓|✓|✓|-|-
|Fork Project |✓|-|-|-|-|-|-|-
|Archive Project |✓|✓|✓|✓|-|-|-|-
|===

// Lite users can only list and view files in projects
// Practitioners can list, view, edit, and upload files in projects
// Sysadmins can only list and view files in projects

=== File actions

[cols="3a,^2a,^2a,^2a,^2a,^2a,^2a,^2a,^2a",options="header"]
|===
|Permission|Practitioner|SysAdmin|CloudAdmin|SupportStaff|ReadOnlySupportStaff|Librarian|Limited Admin|License Reviewer

|List and View Files |✓|✓|✓|✓|✓|-|-|-
|Edit Files |✓|-|-|-|-|-|-|-
|Upload Files |✓|-|-|-|-|-|-|-
|===

// Lite users can only view workspace history
// Practitioners can view workspace history, start, stop, open, and delete workspaces
// Sysadmins can only stop or delete workspaces, and view their history

=== Workspace actions

[cols="3a,^2a,^2a,^2a,^2a,^2a,^2a,^2a,^2a",options="header"]
|===
|Permission|Practitioner|SysAdmin|CloudAdmin|SupportStaff|ReadOnlySupportStaff|Librarian|Limited Admin|License Reviewer

|Start Workspace |✓|-|-|-|-|-|-|-
|Stop Workspace |✓|✓|✓|✓|-|-|-|-
|Open Workspace |✓|-|-|-|-|-|-|-
|View Workspace History |✓|✓|✓|✓|✓|✓|-|-
|Delete Workspace |✓|✓|✓|-|-|-|-|-
|===

// Lite users can only view job history
// Practitioners can view job history, start, and stop jobs; or create, edit, and delete scheduled jobs
// Sysadmins can only view job history, stop jobs, edit and delete scheduled jobs

=== Job actions

[cols="3a,^2a,^2a,^2a,^2a,^2a,^2a,^2a,^2a",options="header"]
|===
|Permission|Practitioner|SysAdmin|CloudAdmin|SupportStaff|ReadOnlySupportStaff|Librarian|Limited Admin|License Reviewer

|Start Job |✓|-|-|✓|-|-|-|-
|Stop Job |✓|✓|✓|✓|-|-|✓ (Public projects only)|-
|View Job History |✓|✓|✓|✓|-|-|-|-
|Create Scheduled Job |✓|-|-|-|-|-|-|-
|Edit Scheduled Job |✓|✓|✓|-|-|-|-|-
|Delete Scheduled Job |✓|✓|✓|-|-|-|-|-
|===

// Lite users cannot interact with project settings
// Practitioners can view and edit project settings
// Sysadmins can view and edit project settings

=== Project settings actions

[cols="3a,^2a,^2a,^2a,^2a,^2a,^2a,^2a,^2a",options="header"]
|===
|Permission|Practitioner|SysAdmin|CloudAdmin|SupportStaff|ReadOnlySupportStaff|Librarian|Limited Admin|License Reviewer

|View Project Settings |✓|✓|✓|✓|✓|✓|-|-
|Edit Project Settings |✓|✓|✓|✓|-|✓|-|-
|===

// Lite users can be made collaborators of Domino endpoints
// Practitioners can create, collaborate on, stop, view and edit model settings, and promote model versions to prod
// Sysadmins can by model "editor" collaborators, stop, view and edit model settings

=== Experiment management actions

[cols="3a,^2a,^2a,^2a,^2a,^2a,^2a,^2a",options="header"]
|===
|Permission|Practitioner|SysAdmin|SupportStaff|ReadOnlySupportStaff|Librarian|Limited Admin|License Reviewer

|Register a new experiment or a new run of an experiment |✓|✓|-|-|-|-|-
|View/list/search experiments and runs (including metadata and artifacts) |✓|✓|✓|-|-|-|-
|Delete (archive) an experiment or experiment run |✓|✓|-|-|-|-|-
|Update an experiment or experiment run (includes logging artifacts, adding tags, etc.) |✓|✓|-|-|-|-|-
|===

=== Model registry actions

[cols="3a,^2a,^2a,^2a,^2a,^2a,^2a",options="header"]
|===
|Permission|Launcher user|Results consumer|Contributor|Project owner|SysAdmin|CloudAdmin

|Register a new model or a new version of a model |-|-|✓|✓|✓|✓
|Archive a registered model version |-|-|Owning user only|✓|✓|✓
|Update a registered model version |-|-|✓|✓|✓|✓
|View / list / search registered models and their versions |-|✓|✓|✓|✓|✓
|Deploy a model as a Domino endpoint |-|-|✓|✓|✓|✓
|Export a model as a Domino endpoint Image |-|-|✓|✓|✓|✓
|Download model artifacts |-|-|✓|✓|✓|✓
|===

=== Domino endpoint actions

[cols="3a,^2a,^2a,^2a,^2a,^2a,^2a,^2a,^2a",options="header"]
|===
|Permission|Practitioner|SysAdmin|CloudAdmin|SupportStaff|ReadOnlySupportStaff|Librarian|Limited Admin|License Reviewer

|Create Domino endpoint |✓|-|-|-|-|-|-|-
|Be a Domino endpoint "Owner" |✓|-|-|-|-|-|-|-
|Be a Domino endpoint "Editor" |✓|✓|✓|✓|-|-|-|-
|Be a Domino endpoint "Viewer" |✓|-|-|-|-|-|-|-
|Stop a model version |✓|✓|✓|✓|-|-|-|-
|View model settings  |✓|✓|✓|✓|✓|-|-|-
|Edit model settings  |✓|✓|✓|✓|-|-|-|-
|Promote a model version to Prod |✓|-|-|-|-|-|-|-

|===

// Lite users can only view apps
// Practitioners can publish, stop, and view apps
// Sysadmins can stop and view apps

=== App actions

[cols="3a,^2a,^2a,^2a,^2a,^2a,^2a,^2a,^2a",options="header"]
|===
|Permission|Practitioner|SysAdmin|CloudAdmin|SupportStaff|ReadOnlySupportStaff|Librarian|Limited Admin|License Reviewer

|Publish or Start App |✓|-|-|-|-|-|-|-
|Stop App |✓|✓|✓|✓|-|-|-|-
|View App |✓|✓|✓|✓|-|-|-|-
|===

// Lite users can view and run launchers (if permitted in project settings)
// Practitioners can view, create, edit, delete, and run launchers
// Sysadmins can only view launchers

=== Launcher actions

[cols="3a,^2a,^2a,^2a,^2a,^2a,^2a,^2a,^2a",options="header"]
|===
|Permission|Practitioner|SysAdmin|CloudAdmin|SupportStaff|ReadOnlySupportStaff|Librarian|Limited Admin|License Reviewer

|View Launchers |✓|✓|✓|✓|-|-|-|-
|Create or Edit Launcher |✓|-|-|-|-|-|-|-
|Delete Launcher |✓|-|-|-|-|-|-|-
|Run Launcher |✓|-|-|-|-|-|-|-
|===

// Lite users can only view datasets
// Practitioners can create datasets and snapshots, can mount and view datasets
// Sysadmins can view datasets and delete their snapshots

=== Dataset actions

See link:7876f1#ds-permissions[Dataset permissions]
and link:8f5b7e#_dataset_roles[Dataset Roles]
for more information.

[cols="3a,^2a,^2a,^2a,^2a,^2a,^2a,^2a,^2a",options="header"]
|===
|Permission|Practitioner|SysAdmin|CloudAdmin|SupportStaff|ReadOnlySupportStaff|Librarian|Limited Admin|License Reviewer

|Create Dataset |✓|-|-|-|-|-|-|-
|Mount/Unmount Dataset |✓|-|-|-|-|-|-|-
|Delete Dataset Snapshot |-|✓|✓|✓|-|-|-|-
|List All Datasets on Global Data Page |-|✓|✓|✓|-|-|-|-
|List All Datasets and Snapshots in Admin Application |-|✓|✓|✓|-|-|-|-
|Permanently Delete Datasets and Snapshots from the Admin Application |-|✓|✓|✓|-|-|-|-
|Cancel Delete Requests within the time set by `com.cerebro.domino.dataset.graceTimeForDeletion`.
See link:71d6ad#tr310[Read-write datasets]. |-|✓|✓|✓|-|-|-|-
|Edit Any Dataset Permissions |-|✓|✓|✓|-|-|-|-
|===

// Lite users can list and view environments
// Practitioners can list, view, create, and edit environments
// Sysadmins can only list and view environments

=== Environment actions

NOTE: As a reminder, your organization incurs costs when anyone creates or stores environments.

[cols="3a,^2a,^2a,^2a,^2a,^2a,^2a,^2a,^2a",options="header"]
|===
|Permission|Practitioner|SysAdmin|CloudAdmin|SupportStaff|ReadOnlySupportStaff|Librarian|Limited Admin|License Reviewer

|List and View Environment |✓|✓|✓|✓|✓|-|-|-
|Create Environment |✓|✓|✓|-|-|-|-|-
|Edit Environment* |✓|✓|✓|✓|-|-|-|-
|===

*Only admins can edit global environments. Practitioner users can only edit a global environment if they are an owner.

=== Administrator actions

[cols="3a,^2a,^2a,^2a,^2a,^2a,^2a,^2a,^2a,^2a",options="header"]
|===
|Permission|Lite User|Practitioner|SysAdmin|CloudAdmin|SupportStaff|ReadOnlySupportStaff|Librarian|Limited Admin|License Reviewer

|View Admin UI |-|-|✓|✓|✓|✓|-|✓|✓
|Edit Settings in Admin UI |-|-|✓|✓|-|-|-|✓|-
|Edit Configuration records |-|-|✓|-|-|-|-|-|-
|Edit Users |-|-|✓|✓|-|-|-|-|-
|Edit Feature Flags |-|-|✓|-|-|-|-|✓|-
|Create Global Environments |-|-|✓|✓|-|-|-|-|-
|Edit Global Environments |-|-|✓|✓|-|-|-|-|-
|View Usage Reports |-|-|✓|✓|-|-|-|✓|✓
|Create Notifications |-|-|✓|✓|-|-|-|✓|-
|Edit Hardware Tiers |-|-|✓|*|-|-|-|✓|
|Run MongoDB Queries |-|-|**|-|-|-|-|-|-
|Manage Executions |-|-|-|-|-|-|✓|✓|-
|View Datasets in Admin UI |-|-|✓|✓|✓|-|-|-|-
|Manage Datasets in Admin UI |-|-|✓|✓|✓|-|-|-|-
|Use Cost Monitoring |-|-|✓|✓|-|-|-|-|-
|Configure Cost Budgets and Alerts |-|-|✓|✓|-|-|-|-|-
|===

*CloudAdmins can only manage hardware tiers if the configuration records key `com.cerebro.domino.dominoCloud.cloudAdmin.canManageHwTiers` is set to `true`.

**MongoDB access is disabled in Domino Cloud.

=== Organization actions

[cols="4a,^2a,^2a,^2a,^2a,^2a,^2a,^2a,^2a,^2a",options="header"]
|===
|Permission|Lite User|Practitioner|SysAdmin|CloudAdmin|SupportStaff|ReadOnlySupportStaff|Librarian|Limited Admin|License Reviewer


|Create Organizations |✓|✓|✓|✓|✓|✓|-|✓|-
|Organization Owner Can Add/Remove Members To/From the Organization |✓|✓|✓|✓|✓|✓|✓|✓|-
|Organization Owner Can Make Another User an Owner of the Organization |✓|✓|✓|✓|✓|✓|✓|✓|-
|Add/Remove Members To/From Any Organization |-|-|✓|✓|-|-|-|-|-
|Can Make Another User an Owner of Any Organization |-|-|✓|✓|-|-|-|-|-
|Select Hardware Tiers Available to Members of the Organization |-|-|✓|✓|-|-|-|✓|-

|===

NOTE: You cannot delete organizations after you create them.

== More information about specific roles

=== Project Manager Role

// Project managers get owner-level access to all projects owned by members of their organizations
// Project managers can see assets from their organizations in the projects portfolio and assets portfolio

When Project Managers are members of organizations, their role grants
them owner-level access to all projects that are owned by other members
of the organizations. This allows the Project Manager to see these
projects and their assets in the Projects Portfolio and Assets
Portfolio.

The Project Manager might also have the ability to add users to
these organizations, thereby gaining contributor access to those users'
projects. For this reason, the Project Manager must be treated as a highly
privileged role, similar to System Administrator.

=== CloudAdmin role

CloudAdmins are given most of the access SysAdmins have, but not all. CloudAdmins are only available on link:5dc7d5[Domino Cloud].

.CloudAdmins restrictions:

* Manage configuration records
* Manage feature flags
* Manage email configuration
* Manage search index
* Manage API keys
* Run MongoDB commands
* View Kubernetes dashboard
* Restart Nucleus

CloudAdmins can only manage users with the Practitioner or CloudAdmin roles.
Users with any other roles cannot be managed by CloudAdmins.
CloudAdmins can only assign the Practitioner or CloudAdmin roles.

[[lite-user]]
=== Lite User role

A user with no roles is called a *Lite User* or, in some contexts, a *Results Consumer*. They have restricted feature access and may have a different licensing status.

*Lite Users* have permission to do the following:

* View the project list.
* View files in a project.
* View Workspace history.
* View Job history.
* Be added as collaborators of Domino endpoints.
* View Apps.
* View and run Launchers (if permitted in project settings).
* List and view Environments.
* View experiments.

=== Data Analyst role

The Data Analyst role is for users who have some technical background and coding experience in Python and R, but who do not need access to all the MLOps features of Domino.
For more information, see link:48afc2[Data Analyst role].

[[governance-admin]]
=== GovernanceAdmin role

The GovernanceAdmin role has permission to do the following:

* View bundles.
* View approvals.
* Query audit events.
* View policy overviews.
* Manage policies evidence templates.

For more information, see link:cd4f06[Domino Governance policies].

----- admin_guide/manage-users/run-user-activity-report.txt -----
:page-version: 6.1
:page-title: Review user activity
:page-permalink: 6e50e8
:page-order: 50

You can review user behavior across Domino.

. In the navigation pane, click *Manage resources > Users*.
. The report will show you when they signed up, the number of workloads run, whether they are active, and their roles.

[[user-activity]]
== Generate a User Activity report

You can download a CSV of the user activity information.

. In the navigation pane, click *Admin*.
. Go to *Reports* > *User Activity Report*.
. Enter the following:
* The *Start Date* and *End Date* for the report.
* The number of days to see in the recent activity section.
* The organization for which you want to see data.
* Email addresses to send copies of the report.
. Click *Download Report*.

== Schedule a User Activity report

Configure certain configuration records keys to schedule User Activity reports to be automatically generated and emailed.

. In the navigation pane, click *Admin*.
. Go to *Platform Settings* > *Configuration Records*.
. Set the options below; `com.cerebro.domino.com.cerebro.domino.Usage.ReportRecipients` is required.
+
[cols="1a,^2a,4a",options="header"]
|===
|Key |Default |Description

3+|`com.cerebro.domino.email.usageReportRecipient`
| |`usage@dominodatalab.com` |Sets the default recipient for the user activity report. To access this report, go to *Reports* > *User Activity Report*.

3+|`com.cerebro.domino.email.EmailToDomino`
| |`true` |When `true`, the system sends a scheduled user activity report to usage@dominodatalab.com to help improve Domino.

3+|`com.cerebro.domino.Usage.RecentUsageDays`
| |`30` |Specifies the number of days to report for recent activity in the user activity reports. For example, the default value includes activity within the past 30 days in the Recent Activity section.

IMPORTANT: The cron entry is set in UTC.

3+|`com.cerebro.domino.Usage.ReportFrequency`
| |`0 0 2 * * ?` |Defines the frequency for automatically scheduled user activity reports. The default cron string value is set to daily at 02:00.

NOTE: This value must be a QUARTZ cron string expression. See the http://www.quartz-scheduler.org/documentation/quartz-2.3.0/tutorials/crontrigger.html[Cron Quartz tutorial^].

3+|`com.cerebro.domino.Usage.ReportRecipients`
| |Empty |Identifies a comma-separated list of email addresses that receive automatic scheduled user activity reports. This is not shown in the configuration records unless it is set explicitly. Example values are: email1@domain.com, email2@domain.com.

3+|`com.cerebro.domino.Usage.IncludeUsernameAndEmail`
| |`false` |When `true`, automatically generated and emailed user activity reports include username and email address columns.
|===

----- admin_guide/manage-users/service-accounts.txt -----
:page-version: 6.1
:page-permalink: 6921e5
:page-title: Manage Domino Service Accounts
:page-order: 40

*Domino Service Accounts* provide a managed and secure way to access the link:8c929e[Domino API] from outside of the Domino cluster. Domino *Service Accounts* provide stable, token-based authentication for API calls without compromising the accounts of individual Domino users and the assets they have access to. Service Accounts are especially useful when calling the Domino API as part of an automation pipeline. 

== What are Domino Service Accounts?

Domino Service Accounts are a special type of Domino user. They don't represent a human being behind a browser, but rather a "technical user", "service account", or "automation principal" — all these terms describe a digital identity used by software programs to communicate with other software.

- Each Service Account can have one or more long-lived tokens associated with it which are used to authenticate calls to the Domino API. Each token has its own independent lifespan but points to the same Service Account, for which it was created. Each token created for the same Service Account gives the token bearer access to the same Projects.
- Only Service Accounts can have long-lived credentials for calling Domino APIs. Regular Domino users are not allowed to have long-lived tokens.
- Service Accounts are not allowed to utilize the web interface, Domino, or Domino CLI. 
- Service Accounts are managed by Domino Administrators and are not accessible to regular Domino users. Admins create and archive the Service Accounts, as well as generate and manage their tokens. If compromised, each token can be invalidated individually, or the whole service account can be deactivated, effectively prohibiting any access using its tokens.
- Service Accounts can be invited to link:d7731d[collaborate on a Project], just like a regular user. This gives the Service Account access to code and data in that particular Project.

== Create a Service Account

Domino Service Accounts can only be managed with the Domino APIs. The calling user must have an link:2611b7[Administrator role].

[NOTE]
====
The easiest way for an Administrator to communicate with Domino via API is to link:e6e601[start a Workspace] (e.g. Jupyter) in Domino, start a Terminal inside the Workspace, and take advantage of the link:40b91f[API Proxy], which will automatically authenticate the calls to Domino APIs.

The following examples assume that they are executed from inside a Workspace and use the API Proxy. To make these calls from outside of Domino, authenticate as described by the documentation for link:40b91f[Domino API].
====

To create a Service Account, the Administrator makes a call to the `/v4/serviceAccounts` endpoint, supplying the desired username for the Service Account and a valid email address. Domino Service Accounts, just like regular Domino users, need a valid and unique email address. Domino uses this email address to send notifications, inform about failed jobs, and for other purposes.

[source,bash]
----
$ curl -X POST -H 'Content-Type: application/json' \
  -d '{"username": "demo-sa", "email": "demo-sa@customer.com"}' \
  $DOMINO_API_PROXY/v4/serviceAccounts
----

Result:
....
{
    "email": "demo-sa@customer.com",
    "id": "65cd5ca022e1a963208d7a6d",
    "idpId": "ca72dc97-504b-4b3c-98d6-3a306b30bd24",
    "userName": "demo-sa"
}
....
Save the value of the "idpId" property in the IDPID link:6ac5a1[environment variable] for future use.

=== Create multiple Service Accounts emails with "plus addressing"

To manage multiple unique email addresses, you can use "plus addressing" or "subaddressing" to create multiple email addresses for your Service Accounts without requiring you to create new email accounts.

Many email services, like Gmail, Google Workspace, Microsoft Exchange Online, and others support "plus addressing" (or "subaddressing") — an industry-defined way to support dynamic recipient email addresses.

For example, if you have an email like `riley@gmail.com`, you can create a Service Account for an Apache Airflow pipeline with the email address `riley+airflow@gmail.com`. Domino will send all activity related to the Apache Airflow Service Account to the `riley@gmail.com` inbox tagged with the tag `airflow`. This way, a single user can manage multiple service accounts from one inbox.

Check with your enterprise email provider to see if subaddressing is supported at your company.

== List Service Accounts

Admins can list all the existing Service Accounts in Domino.

[source,bash]
----
$ curl $DOMINO_API_PROXY/v4/serviceAccounts
----

Result:
....
[
   {
      "email" : "demo-sa@customer.com",
      "id" : "65cd5ca022e1a963208d7a6d",
      "idpId" : "ca72dc97-504b-4b3c-98d6-3a306b30bd24",
      "username" : "demo-sa"
   }
]
....

== Create a token

Admins can create a token for a Service Account using the Service Accounts `idpId`.

Each token has a default lifespan of 4 months, which is listed in the response above.

The value of the `+token+` property can be copied and used to authenticate the calls to Domino APIs. The caller will be recognized as the Service Account to which this token belongs. Let's save the value to the `+SA_TOKEN+` environment variable for the next example.

In this example, `IDPID` is an environment variable that contains the idpId of the Service Account.

[source,bash]
----
$ curl -X POST -H 'Content-Type: application/json' \
  -d '{"name": "token-for-circleci"}' \
  $DOMINO_API_PROXY/v4/serviceAccounts/$IDPID/tokens
----

Result:
....
{
   "createdAt" : "2024-02-15T00:39:35.521Z",
   "expiresAt" : "2024-06-14T00:39:35.513201660Z",
   "isValid" : true,
   "name" : "token-for-circleci",
   "serviceAccountIdpId" : "ca72dc97-504b-4b3c-98d6-3a306b30bd24",
   "token" : "eyJhbGciOiJSUzI1N... redacted for brevity ...kNY_DlWXl8WJPvjTJsPeddd40u9dUhDp-FDg"
}
....

== Start a Job with a Service Account

The following example starts a Domino Job using the `token` value from the previous "Create a token" step.

[source,bash]
----
$ curl -H "Authorization: Bearer $SA_TOKEN" \
  --header 'Content-Type: application/json' \
  --data '{ "projectId": "12398gg098", "runCommand": "main.py"}' \
  $DOMINO_API_HOST/api/jobs/v1/jobs
----

NOTE: This call goes to $DOMINO_API_HOST, which does not automatically add authentication.


== List all tokens

List all existing tokens for a Service Account. The "List tokens" response contains the expiration dates of all the tokens. Monitoring this and taking proactive steps helps avoid service interruptions when using the tokens.

[source,bash]
----
$ curl -s $DOMINO_API_PROXY/v4/serviceAccounts/$IDPID/tokens
----

Result:
....
[
   {
      "createdAt" : "2024-02-15T00:39:35.521Z",
      "expiresAt" : "2024-06-14T00:39:35.513Z",
      "isValid" : true,
      "name" : "token-for-circleci",
      "serviceAccountIdpId" : "ca72dc97-504b-4b3c-98d6-3a306b30bd24"
   }
]
....

== Invalidate a token

If a token is compromised or is no longer needed, it can be invalidated. This won't affect other tokens belonging to this or any other Service Account.

[source,bash]
----
$ curl -H 'Accept: application/json' -X POST \
  $DOMINO_API_PROXY/v4/serviceAccounts/$IDPID/tokens/token-for-circleci/invalidate
----

== Deactivate a Service Account

If the whole Service Account is no longer necessary, it can be deactivated, making all its tokens invalid.

[source,bash]
----
$ curl -X POST -H 'Content-Type: application/json' \
  $DOMINO_API_PROXY/v4/serviceAccounts/$IDPID/deactivate
----

[[service-account-best-practices]]
== Service Accounts best practices

- To lower the risk of exposing sensitive Project data and code, Domino recommends minimizing the number of Projects on which a single Service Account collaborates.

- To help the Administrators map Service Accounts to Projects, the username can be crafted to reflect the relationship between the Service Account and the Project.

- Domino recommends that you use one token for a single purpose (for example, in a single automation pipeline). It's helpful if the token name reflects where it's being used. This way, if a token needs to be replaced, it could be done without affecting many scripts or automation pipelines.

- The "List tokens" API response contains the expiration dates of all the tokens. Monitoring this and taking proactive steps helps avoid service interruptions when using the tokens.

== Next steps

Explore the link:8c929e[Domino REST API reference].

See other methods users can link:40b91f[authenticate against the Domino API].

----- admin_guide/operations/audit-logs/ai-gateway-logs.txt -----
:page-version: 6.1
:page-permalink: 984c09
:page-title: Monitor AI Gateway Large Language Model (LLM) logs
:page-sidebar: Monitor AI Gateway LLM logs
:page-order: 80

Domino automatically logs link:cce362[AI Gateway endpoint activity] so you can audit LLM activity in Domino Projects.

To view AI gateway endpoint activity, go to *Admin* > *Advanced* > *MongoDB* and run the following query: 

[source,mongodb]
----
+db.audit_trail.find({ kind: "AccessGatewayEndpoint" })+
----

You can refine this query to filter records by fields such as user, model name, or endpoint name.


== Next steps

See link:f5934f[Data Source audit logs] for data captured by Data Sources in Domino.

----- admin_guide/operations/audit-logs/application-logs.txt -----
:page-version: 6.1
:page-title: Monitor application execution logs
:page-permalink: a9e507
:page-order: 50

[[tr1]]
[[execution-logs]]
== Execution logs
User code running as link:942549[Jobs], link:867b72[Workspaces], link:71635d[Apps], and link:8dbc91[Domino endpoints] output execution logs.

Your users can access these from the link:942549#manage-jobs[Jobs dashboard], link:0002fb[Workspaces Dashboard], link:71635d#App-logs-versions-rsrc[App Dashboard], and Domino endpoint instance logs.
This data is a key part of the Domino reproducibility model and is kept indefinitely in the link:373d37[Domino File Store].

[[tr2]]
The installation configuration file at `blob_storage.logs` defines which system these logs are written to.

[[application-logs]]
== Application logs

All Domino services use the https://kubernetes.io/docs/concepts/cluster-administration/logging/[standard Kubernetes logging architecture^] to output their logs.
Relevant logs are printed to `stdout` or `stderr` as indicated, and Kubernetes captures them.

== Support bundle
A zip file that contains logs and reports from Domino components with information about a Domino execution. See link:12e54b[Support Bundles].


== View your front-end logs
. List your all namespaces to find the name of your platform namespace:
+
----
kubectl get namespace
----

. List all the pods in your platform namespace to find the name of a front-end.
You will likely have more than one frontend pod.
+
----
kubectl get pods  -n <namespace for your platform nodes>
----

. Print the front-end logs for one of your frontends.
+
----
kubectl logs <pod name of your frontend pod> -n <namespace for your platform nodes> -c nucleus-frontend
----

Domino recommends that you attach a Kubernetes log aggregation utility to monitor the following https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/[Kubernetes namespaces^] used by Domino to aggregate the logs.

* Platform namespace
+
[[tr3]]
This namespace hosts the core application components of the Domino application, including API servers, databases, and web interfaces.
The installer configuration file defines the name of this namespace at `namespaces.platform.name`.
The following components running in this namespace produce the most important logs:
+
[cols="1a,4a",options="header"]
|===
^|Component ^|Logs
|`nucleus-frontend` |The nucleus-frontend pods host the frontend API server that routes all requests to the Domino application.
Its logs will contain details about HTTP requests to Domino from the application or another API client.
If you see errors in Domino with HTTP error codes like 500, 504, or 401, you can find corresponding logs here.

|`nucleus-dispatcher` |The nucleus-dispatcher pod hosts the Domino scheduling and brokering service that sends user execution pods to Kubernetes for deployment.
Errors in communication between Domino and Kubernetes result in corresponding logs from this service.

|`keycloak` |The keycloak pod hosts the Domino authentication service.
The logs for this service will contain a record of authentication events, including additional details on any errors.

|`cluster-autoscaler` |This pod hosts the open-source Kubernetes cluster autoscaler, which controls and manages autoscaling resources.
The logs for this service contain records of scaling events, both scaling up new nodes in response to demand and scaling down idle resources, including additional details on any errors.
|===

* Compute grid namespace
+
[[tr4]]
This namespace hosts user executions plus Domino environment builds.
The installer configuration file defines the name of this namespace at `namespaces.compute.name`.
+
Logs in this namespace correspond to ephemeral pods hosting user work.
Each pod contains a user-defined environment container, whose logs are link:#execution-logs[execution logs].
There are additional supporting containers in the pods, and their logs might contain additional information about errors or behavior seen in specific Domino executions.
+
Domino advises that you aggregate and keep at least 30 days of logs to facilitate debugging.
You can harvest these with a variety of Kubernetes log aggregation utilities, including:
+
** https://www.loggly.com/solution/kubernetes-logging/[Loggly^]
** https://docs.splunk.com/Documentation/InfraApp/2.0.2/Admin/AddDataKubernetes[Splunk^]
** https://docs.newrelic.com/docs/logs/enable-logs/enable-logs/kubernetes-plugin-logs[New Relic^]

== Next steps

- link:e34be1[Project audit logs] for project-level auditing including user permissions and Dataset-related actions.

- link:f5934f[Data Source audit logs].

----- admin_guide/operations/audit-logs/audit-trail-glossary.txt -----
:page-permalink: dacaf5
:page-version: 6.1
:page-title: Unified Audit Trail Data Glossary
:page-sidebar: Unified Audit Trail Data glossary
:page-order: 40

The *Unified Audit Trail* records necessary user actions to support compliance and regulatory standards. This glossary has a complete list of events and their descriptions in the Unified Audit Trail.

== Application management

[cols="1,3",options="header"]
|===
|Event name
|Description

|Start App
|Logged when an App is started.

|Stop App
|Logged when an App is no longer running.

|Publish App
|Logged when an App is published and visible to selected users.
|===

== Code/artifact file management

[cols="1,3",options="header"]
|===
|Event Name
|Description

|Upload Files
|Logged when new files are uploaded into a project or dataset.

|View File
|Captured when a user views a file within a project without downloading it.

|Edit File
|Triggered when a user modifies a file within a project.

|Remove File
|Captured when a user deletes a non-git-based repository file.

|View Repo File
|Captured when a user views a repository file within a project without downloading it.

|Download Files
|Captured when a user downloads a repository file within a project without downloading it.
|===

== Configuration record

[cols="1,3",options="header"]
|===
|Event Name
|Description

|Create Configuration Record Setting
|Captured when a new configuration record setting is created to manage system-wide behaviors and settings in Domino, influencing multiple features and functionalities.

|Delete Configuration Record Setting
|Logged when a configuration record setting is removed from the system, potentially reverting to default values or removing a custom configuration affecting the entire platform.

|Update Configuration Record Setting
|Logged when a global configuration setting is updated.
|===

== Data source management

[cols="1,3",options="header"]
|===
|Event name
|Description

|Access Datasource from Code Execution
|Logs when a data source is accessed during code execution (e.g., Job, Workspace, App), capturing details of the user, environment, and timestamp of access.

|Download Data Source Audit Data
|Audit data downloaded for a data source.

|Get Data Source Audit Data
|This is a report.

|Create Data Source
|Data source created, capturing creator and type.

|Delete Data Source
|Data source permanently deleted.

|Change Data Source Ownership
|Ownership transferred to another user.

|Add Data Source Credentials
|New credentials added to a data source.

|Change Data Source Permissions
|Permissions modified for a data source.

|Change Data Source Project IDs
|Project associations for the data source changed.

|Add Data Source to Project
|Data source linked to a project.

|Remove Data Source from Project
|Data source removed from a project.
|===

== Dataset management

=== Snapshots

[cols="1,3",options="header"]
|===
|Event name
|Description

|Create Snapshot
|Captured when a snapshot is created to version data or code for reproducibility purposes.

|Complete Snapshot Creation
|Logged when the process of creating a snapshot is successfully completed, capturing the state of the project or dataset.
|===

=== Dataset files

[cols="1,3",options="header"]
|===
|Event name
|Description

|Delete Dataset Files
|Logged when a dataset file is permanently deleted from the system.

|Download Dataset File
|Triggered when a user downloads a file from a dataset.

|Upload Files to Dataset
|Triggered when new files are added to an existing Domino Dataset folder.

|View Dataset File
|Captured when a user views a dataset file without downloading it.
|===

=== Dataset management

[cols="1,3",options="header"]
|===
|Event name
|Description

|Create Dataset
|Logged when a new dataset is created within the platform.

|Delete Dataset
|Logged when a dataset is permanently deleted from the system.

|Finalize Delete Dataset
|Captured when a dataset is permanently deleted after being marked for deletion.

|Mark Dataset for Deletion
|Captured when a dataset is flagged for deletion but not yet permanently removed.

|Update Dataset Description
|Logged when the metadata or description of a dataset is updated.

|Add Grant For Dataset
|Logged when specific access permissions (grants) are added to a dataset.

|Generate Dataset Usage Report
|This is a report.

|Associate Dataset To Project
|Triggered when a dataset is linked to a project, making it accessible for analysis.

|Dissociate Dataset From Project
|Logged when a dataset is unlinked from a project, removing it from the project’s access.

|Add Tag
|Logged when a tag is added to a dataset or snapshot, used for organizing or versioning.
|===

== Environment management

=== Environment management

[cols="1,3",options="header"]
|===
|Event name
|Description

|Create Environment
|Logged when a new compute environment is created for use in running jobs or workspaces.

|Create Environment Revision
|-
|===

=== Feature flags

[cols="1,3",options="header"]
|===
|Event name
|Description

|Feature Flag changed
|Captured when a feature flag is toggled, either enabling or disabling a particular feature.

|Update Feature Flag for User
|Logged when a feature flag is overridden for a specific user, granting them different access from the global setting.

|Update Feature Flag Globally
|Logged when a feature flag is updated globally for the entire organization, overriding default settings or user-level configurations.
|===

== Goals and tasks

=== Comments

[cols="1,3",options="header"]
|===
|Event name
|Description

|COMMENT_ADDED
|Logged when a new comment is added to a Domino project goal.

|COMMENT_ARCHIVED
|Triggered when a comment in a Domino project goal is archived.
|===

=== Project goals

[cols="1,3",options="header"]
|===
|Event name
|Description

|Project Goal Created
|Logged when a new project goal/task is created to track milestones and progress within a project.

|Project Goal Completed
|Logs when a project goal/task is marked as complete, reflecting the accomplishment of a task or milestone in the project.

|Project Goal Linked To
|Captures the event of linking a project goal to another goal, task, or resource, establishing dependencies or relationships.
|===

== Governance events

=== Bundles

[cols="1,3",options="header"]
|===
|Event Name
|Description

|Create Governance Bundle
|Triggered when a new governance bundle is created.

|Create Governance Bundle Stage Approval Request
|Logged when an approval is requested in a bundle.

|Change Governance Bundle Stage
|Logged when the stage (as defined in the policy) of a bundle changes.

|Accept Governance Bundle Stage Approval Request
|Logged when an approval request is accepted.

|Create Governance Bundle State
|Logged when the state (active or archive) of a bundle changes.

|Add Attachment to Bundle
|Triggered when an attachment is added to a bundle.

|Remove Attachment from Bundle
|Triggered when an attachment is removed from a bundle.

|Submit Results in a Bundle
|Created when a governance result is added or when saving the notebook.

|Copy Governance Bundle Results from Another Bundle
|Triggered when results from one bundle is copied to another bundle.

|===

=== Findings and Comments

[cols="1,3",options="header"]
|===
|Event Name
|Description

|Create Governance Finding
|Logged when a finding is created.

|Change Governance Finding Status
|Triggered when the status of a finding is changed.

|Update Governance Comment Events
|Logged when a comment is added in a bundle.
|===

=== Policies

[cols="1,3",options="header"]
|===
|Event Name
|Description

|Create Governance Policy
|Logged when a new policy is created.

|Delete Governance Policy
|Logged when a policy is deleted.

|Change Governance Policy Status
|Triggered when the policy changes status (draft or published).

|Update Governance Policy Definition
|Logged when any change to a policy is made.
|===

== Job management

=== Jobs

[cols="1,3",options="header"]
|===
|Event name
|Description

|Archive Job
|Logged when a job is archived, meaning it is no longer active or scheduled for future runs.

|Launch Job
|Logged when a job is launched either manually or via a scheduled task.

|Stop Job
|Logged when a running job is manually stopped before completion.

|Set Environment Variable
|Captured when an environment variable is set for a specific environment, influencing the behavior of jobs or workspaces.

|Change Results Branch Setting
|Captured when the results branch setting for a project is changed, affecting where job results are stored or displayed.
|===

=== Runs

[cols="1,3",options="header"]
|===
|Event name
|Description

|Change Run Notification Setting
|Captured when the notification settings for job run statuses are changed (e.g., email alerts for job completion).

|Edit Schedule Run
|Captured when the details of a scheduled job are edited, such as adjusting the frequency or resource allocation.

|Schedule Run
|Logged when a job is scheduled to execute at a later time or on a recurring cadence.

|Start Run
|Triggered when a job or command is run with specific arguments or parameters.

|Unschedule Run
|Logged when a scheduled job is removed from the execution queue and no longer runs automatically.
|===

=== Projects

[cols="1,3",options="header"]
|===
|Event name
|Description

|Archive Project
|Logged when a project is archived, stopping all scheduled and in-progress runs and restricting further uploads/downloads.

|Create Project
|Logged when a new project is created within Domino, typically to organize new data science work.

|Create Restricted Project
|Captured when a restricted project is created, which limits access based on user roles or permissions.

|Update Project Setting Event
|Captured when project settings are modified, such as changing visibility, the name of the project, or the description of the project.
|===

== Project and resource management

=== Project management

[cols="1,3",options="header"]
|===
|Event name
|Description

|Archive Project
|Logged when a project is archived, stopping all scheduled and in-progress runs and restricting further uploads/downloads.

|Create Project
|Logged when a new project is created within Domino, typically to organize new data science work.

|Create Restricted Project
|Captured when a restricted project is created, which limits access based on user roles or permissions.

|Set Environment Variable
|Captured when an environment variable is set for a specific environment, influencing the behavior of jobs or workspaces.

|Update Project Setting Event
|Captured when project settings are modified, such as changing visibility, the name of the project, or the description of the project.
|===

=== Resource management

[cols="1,3",options="header"]
|===
|Event name
|Description

|Generate Resource Usage Report
|Logged when a resource usage report is generated, showing memory, CPU, and storage consumption for auditing or monitoring.
|===

== User management

=== User access

[cols="1,3",options="header"]
|===
|Event name
|Description

|New User Added
|Logged when a user is added via Keycloak.

|User Log In
|Logged when a user logs into Domino.

|User Log Out
|Logged when a user logs out of Domino.
|===

=== User activity report

[cols="1,3",options="header"]
|===
|Event name
|Description

|Generate User Activity Report
|Logged when a report is generated that tracks user actions within the platform.
|===

=== User roles and permissions

[cols="1,3",options="header"]
|===
|Event name
|Description

|Add Users To Organization
|Captured when one or more users are added to the organization, giving them access to projects and resources.
This could be done individually or in bulk.

|Change Organization Owner
|Logged when the ownership of the organization is transferred to another user, giving them administrative control over all projects, environments, and users within the organization.

|Remove User From Organization
|Captured when a user is removed from the organization, revoking their access to all projects, datasets, and collaborative workspaces within the organization.
|===

=== Global user roles and permissions

[cols="1,3",options="header"]
|===
|Event name
|Description

|Update User Global Roles
|Logged when the role of a user in a project or workspace is updated.

|User Activated
|Captured when a previously deactivated user is reactivated, granting them access again to the organization and its resources.

|User Deactivated
|Captured when a user is deactivated from the system, removing their access.

|User Marked As Domino Employee
|A user has been designated as a Domino employee, indicating their internal role and permissions within the platform.
|===

=== Project user roles and permissions

[cols="1,3",options="header"]
|===
|Event name
|Description

|Add Collaborator
|Logged when a new collaborator is added to a project.

|Change User Role In Project
|Captured when a user’s role is changed within a specific project.
|===

== Workspace management

=== Environment management

[cols="1,3",options="header"]
|===
|Event name
|Description

|Change Organization Default Environment
|Logged when the default environment for all projects in the organization is changed, impacting the computational setup across multiple projects.

|Restrict Environment Revision
|Logged when environment revisions are restricted to ensure that only a specific version of the environment can be used.
|===

=== Workspace management

[cols="1,3",options="header"]
|===
|Event name
|Description

|Delete a Workspace
|Logged when a workspace is deleted, removing all files and resources tied to it.

|Start a Workspace Session
|Logged when a workspace session is started, allowing interactive development work in Domino.

|Start InteractiveSessionRunCommand Run
|Captured when an interactive session command is executed within a workspace.

|Stop a Workspace Session
|Captured when a workspace session is stopped, either manually or due to inactivity.

|Stopped Workspace Marked For Deletion
|Triggered when a workspace that is no longer in use is marked for deletion.

|Synchronize Working Directory
|Logged when a working directory is synchronized with the latest version of the codebase, ensuring reproducibility.
|===

=== Workspace volume

[cols="1,3",options="header"]
|===
|Event name
|Description

|Workspace Deleted by Volume Cleaner
|Triggered when a workspace is automatically deleted by Domino's volume cleaner to manage storage.

|Workspace Volume Start Settings
|Captured when the settings for workspace volume are initialized at the start of a session.
|===

=== Hardware management

[cols="1,3",options="header"]
|===
|Event name
|Description

|View Hardware Tier
|Records when a user views the hardware tier configuration, detailing the compute resources allocated for project execution.

|Edit Hardware Tier
|Tracks modifications to the hardware tier settings for a project, affecting the computational resources used for jobs or workspaces.
|===

----- admin_guide/operations/audit-logs/audit-trail-overview.txt -----
:page-permalink: 85fbb1
:page-version: 6.1
:page-title: Domino Unified Audit Trail
:page-order: 10

The *Unified Audit Trail* provides a clear record of important user actions to support compliance and regulatory standards. It tracks key events like dataset updates, project setups, user role changes, and data access, ensuring accountability and data integrity. With features like dynamic search and sorting, it’s easy to use and requires no programming knowledge. Examples of audit trail events include:

* Added Collaborator
* Updated User Role
* Created Dataset

The link:dacaf5[Unified Audit Trail Data Glossary] has a complete list of Events that are captured within the Unified Audit Trail.

image::/images/6.0/admin-guide/AuditTrail.png[alt="Domino Unified Audit Trail", width=800, role=noshadow]

The audit trail interface can be opened only by the following roles: *SysAdmin* or *GovernanceAdmin*.

Domino can also help customers access *System Logs*. These system logs are *_different_* than the *Unified Audit Trail*; they capture detailed events primarily used for debugging, troubleshooting, and monitoring performance. Examples of system logs include: 

* Execution logs from user code running as Jobs or Workspaces
* Kubernetes service logs
* Detailed login and logout activity

Exploring these logs typically requires skills in log management tools such as Splunk, scripting such as Python, and querying languages, like SQL. Familiarity with cloud systems and platforms like Kubernetes is also usually required. 

== Next Steps

* link:208cc3[Create filters and view audit events] with a dashboard for flexible search options, quick insights, and detailed historical analysis.
* link:a157c4[Export audit trail data] using the Audit trail API. The data is JSON but can be parsed and stored in other formats, like CSV.
* The link:dacaf5[Audit trail data glossary] contains Domino audit event names along with their descriptions.



----- admin_guide/operations/audit-logs/data-source-log.txt -----
:page-version: 6.1
:page-title: Monitor Data Source logs
:page-permalink: f5934f
:page-order: 60

Domino automatically creates an audit trail for link:ae2e6b[Domino Data Source] activity that records the WHO, WHERE, WHEN & WHAT for user activity. Audit trails can provide a means to help accomplish several security-related objectives, including the following:

- Individual accountability
- Reconstruction of events or security breaches
- Intrusion Detection
- Problem Analysis
- Ensuring compliance with industry regulations

TIP: We recommend you use at least `dominodatalab-data==5.7.2` link:https://pypi.org/project/dominodatalab-data/5.7.2/[PyPI version] for the most thorough metadata event tracking. For R users, we recommend the latest `0.2.4` link:https://github.com/dominodatalab/DominoDataR/releases/tag/v0.2.4[DominoDataR release].

Domino logs the following Data Source events:

* Create Data Sources
* Delete Data Sources
* Edit Data Source permissions
* Edit Data Source ownership
* Access Data Sources from Domino executions
* Add Data Sources to projects
* Remove Data Sources from projects

NOTE: Data Source access events are logged for all executions (Workspaces, Jobs, Scheduled Jobs, Apps, and Launchers), except Domino endpoints.


Access the audit logs via the web UI or link:8c929e[Public REST API endpoint], `/api/datasource/v1/audit`.

//== Download audit logs

//Use the web UI to download Data Source audit logs.

//. Go to the  *Admin console*.
//. Click *Download Logs* > *Data* > *Data Sources*.

//This will download a `txt` file with all the Data Source events in the past six (6) months.

== Data Source API

Use the Data Source audit link:8c929e[Public REST API] endpoint, `/api/datasource/v1/audit`, which takes several optional filter parameters, to access Data Source logs programmatically.

All requests must contain the header `X-Domino-Api-Key` with a value corresponding to a *SysAdmin* API key.

=== Unfiltered Data Source audit API sample
The following is a request with no filter parameters, it returns all events within the last 24 hours.
```json
// request
https://<domino-url>/api/datasource/v1/audit

// response: with no filter parameters, all events in the last 24 hours are returned
[
    {
        "dataSourceId": "64e67c6005e8b2388f992545",
        "dataSourceName": "snowflake-ds",
        "dataSourceType": "SnowflakeConfig",
        "eventKind": "DeleteDataSource",
        "metadata": {
            "performedByUsername": "sample-admin"
        },
        "performedBy": "64e6363605e8b2388f9924c8",
        "timestamp": "2023-08-23T21:40:21.669Z"
    },
    {
        "dataSourceId": "64e67c6005e8b2388f992546",
        "dataSourceName": "s3-ds",
        "dataSourceType": "S3Config",
        "eventKind": "AccessDataSource",
        "metadata": {
            "runType": "Workspace",
            "projectId": "64e6371205e8b2388f9924d4",
            "dataPlaneId": "000000000000000000000000",
            "performedByUsername": "sample-user",
            "runId": "64e6632005e8b2388f9924eb"
        },
        "performedBy": "64e6371105e8b2388f9924d2",
        "timestamp": "2023-08-21T21:30:05.439Z"
    }
]
```

=== Filtered Data Source audit API sample
The following is a request with several filter parameters, including `startTime`, `endTime`, multiple `dataSourceNames`, and `eventKinds`, for a more granular response.
```json
// request
https://<domino-url>/api/datasource/v1/audit/?startTime=2022-08-17T23:09:24.921Z&endTime=2023-08-25T23:09:24.921Z&dataSourceNames=snowflake-ds&dataSourceNames=s3-ds&eventKinds=AccessDataSource

// response
[
    {
        "dataSourceId": "64e67c6005e8b2388f992546",
        "dataSourceName": "s3-ds",
        "dataSourceType": "S3Config",
        "eventKind": "AccessDataSource",
        "metadata": {
            "runType": "Workspace",
            "projectId": "64e6371205e8b2388f9924d4",
            "dataPlaneId": "000000000000000000000000",
            "performedByUsername": "sample-user",
            "runId": "64e6632005e8b2388f9924eb"
        },
        "performedBy": "64e6371105e8b2388f9924d2",
        "timestamp": "2023-08-21T21:30:05.439Z"
    },
    {
        "dataSourceId": "64e67c6005e8b2388f992545",
        "dataSourceName": "snowflake-ds",
        "dataSourceType": "SnowflakeConfig",
        "eventKind": "AccessDataSource",
        "metadata": {
            "runType": "Workspace",
            "projectId": "64e6371205e8b2388f9924d4",
            "dataPlaneId": "000000000000000000000000",
            "performedByUsername": "sample-user",
            "runId": "64e6632005e8b2388f9924eb"
        },
        "performedBy": "64e6371105e8b2388f9924d2",
        "timestamp": "2023-02-05T21:40:01.449Z"
    },
]
```

== Next steps

See link:e34be1[Project audit logs] for information on project-level auditing including user permissions and Dataset-related actions.

----- admin_guide/operations/audit-logs/esignature.txt -----
:page-permalink: 47ddd5
:page-version: 6.1
:page-title: Require e-signature for critical actions
:page-sidebar: Require e-signature
:page-order: 55

Admins can require an e-signature by enabling the configuration record `com.cerebro.domino.eSignatureWorkflowEnabled`. E-signatures can be viewed in the Unified Audit Trail.

== Options for requiring e-signature

There are three options in Domino for requiring an e-signature:

* *Global*: Global requirement enables the e-signature workflow across the entire Domino deployment. Critical actions outlined below will require an e-signature, regardless of the project or user.
* *PerProject*: Projects will have a new setting that specifies whether the critical actions within the project will require an e-signature.
* *Disabled*: No e-signature is required for any actions.

== Critical actions that require e-signature

If `Global` or `PerProject` is enabled, the following actions will require an e-signature:

* Dataset events:
** Download a file or dataset
** Edit dataset name and description
** Edit permissions to datasets (both users and admins)
** Create a snapshot
** Delete dataset
* Project actions:
** Download artifact

== View e-signatures

E-signature events can be viewed in the link:85fbb1[Unified Audit Trail]. In the Audit Trail logs, admins can add columns to see the reason for the changes, the user who e-signed, and any additional information entered by the user.

== Next steps

* The link:dacaf5[Audit Trail Data Glossary] has a complete list of events being tracked. You can use these in the event filter to search for events.
* link:a157c4[Export audit trail data] using the Audit trail API. The data is in JSON format but can be parsed and stored in other formats like CSV.
* link:f5934f[Data Source audit logs] create an audit trail for Data Source activity and records the WHO, WHERE, WHEN, and WHAT for user activity.

----- admin_guide/operations/audit-logs/filter-and-view-events.txt -----
:page-permalink: 208cc3
:page-version: 6.1
:page-title: Filter and View Events
:page-order: 20

Admins can access this data through a dashboard, CSV export, or link:a157c4[API]. The dashboard offers flexible search options for quick insights and detailed historical analysis.

The audit trail interface can be opened only by the following roles: *SysAdmin* or *GovernanceAdmin*.

* *SysAdmin*: Go to *Account* > *Admin Panel* > *Reports* > *Audit trail*
* *GovernanceAdmin* or *SysAdmin*:  Go to *Govern* > *Audit trail*

Once open, the interface for the audit trail displays in a window. Actions are listed from the most recent to the least recent:

* *Timestamp*: The date and time of the action adjusted to your timezone.
* *User Name*: User's login name for the action performed.
* *Event*: The description of the action that occurred; for example, a User Permission was changed.
* *Target*: The ID or Object that received the Event action, such as John Doe, whose permission was changed.
* *Project Name*: Project name where the action took place. If the action was outside a project (e.g., an Admin changing user configurations), the Project Name will be blank.

== Create filters to locate records

Once you open the audit trail window, you can use different filters to get a clearer view of events related to specific projects. You can use a pre-populated commonly used query from the dropdown, such as:

* *Added a Collaborator to a project*
* *Viewed a dataset file*
* *Downloaded a dataset file*

You can also craft a specific filter using different elements. Some common examples are:

* *user_name*: Login name of the user who performed the action.
* *event*: Name of the action that occurred.
* *target_name*: Name of the object that receives the action.
* *project_name*: Name of the project where the action took place.

NOTE: Each filter can only be used once. If you include multiple filters, the results will be intersected (AND) instead of joined (OR). You won’t be able to save a filter.

Filter the audit trail by:

. Select an option from the drop-down, or hover over an event and click the triangle icon to filter by the exact term.
. Enter the exact name of the item you’d like to filter by within quotations: `user_name="john.doe"`
. To search with multiple filters, add a space between the two elements: `user_name="john.doe" project_name="projectElrond"`
. *Optional*: Select a date range for filtering results.
. To refresh the table, click the hourglass icon.

=== View event details
Click an event name in the audit trail to open a panel with more information about that event.

You can show or hide columns like *Dataset Name*, *File Name*, *Old Value*, *New Value*, and *Job*.

=== Download as CSV
Download audit trail data from the Audit Trail UI by clicking *Download as CSV* to export the data. The CSV file will include your selected filters and columns.

== Next steps

* link:a157c4[Export audit trail data] using the Audit trail API. The data is JSON but can be parsed and stored in other formats, like CSV.
* The link:dacaf5[Audit Trail Data Glossary] has a complete list of events being tracked. You can use these in the event filter to search for events.




----- admin_guide/operations/audit-logs/index.txt -----
:page-version: 6.1
:page-title: Audit trail and execution logs
:page-permalink: 785186
:page-order: 30

Domino’s Unified Audit Trail and system logs help you track user interactions and system events, meet the security and audit compliance needs of your business, and troubleshoot issues.

Domino Unified Audit Trail::

link:85fbb1[Audit trails] capture broader compliance events, such as changes to datasets, project configurations, user roles, and data access.

Filter and View Events::

link:208cc3[Create filters and view audit events] with a dashboard for flexible search options, quick insights, and detailed historical analysis.

Audit Trail API::

link:a157c4[Export audit trail data] using the Audit trail API. The data is JSON but can be parsed and stored in other formats, like CSV.

Audit Trail Data Glossary::

The link:dacaf5[Audit Trail Data Glossary] has a complete list of events being tracked. You can use these in the event filter to search for events.

Application logs::

link:a9e507[Application logs] capture application-level logging including user execution from Jobs, Workspaces, and front-end logs. These can also be aggregated in enterprise logging platforms such as Splunk & New Relic.

Require e-signature for critical actions::

Admins can link:47ddd5[require an e-signature] for critical actions.

Data Source logs::

link:f5934f[Data Source logs] capture events like Data Source creation, access, deletion, and permission setting.

Project logs::

link:e34be1[Project logs] capture project-scoped events including user permission setting and Dataset-related events.

AI Gateway logs::

link:984c09[AI Gateway Large Language Model (LLM) logs] capture AI Gateway endpoint activity to track LLM usage.

----- admin_guide/operations/audit-logs/project-log.txt -----
:page-version: 6.1
:page-permalink: e34be1
:page-title: Monitor Project logs
:page-order: 70

Domino automatically logs actions performed in Domino Projects. You can use this data to meet certain audit requirements requested by your regulators. Project actions include, but are not limited to, the following:

* Create datasets/snapshots
* Delete datasets/snapshots
* Restore datasets/snapshots
* Edit dataset permissions
* Add/remove dataset to/from a project
* User permission changes

Domino endpoint data is not included in the Domino Project audit log.

This report cannot be modified.

For advanced audit procedures that track data uploads and downloads to and from Domino Datasets, contact support@dominodatalab.com.

== Download log

. Go to the project for which you want to download the audit log.
. Go to *Settings* > *Access & Sharing*.
. In the *Collaborators and permissions* section, click *Download Audit Log* to download the PDF.
+
The PDF includes all link:8f5b7e[permission] changes made to users that belong to the project.
It records who made the change to what user, when the change was made, and the specific attributes that were changed.
It also records changes to datasets:
+
* When a user marks a dataset for deletion.
* When a user restores or deletes a dataset.
* When a user creates a dataset.
* When a user mounts or unmounts a shared dataset.
+
The audit log lists a timestamp and the name of the user who made the changes.
For shared datasets, the logs also list the project from which the changes originate.

NOTE: If you started your project while using Domino 5.3, the start of the date range is the project start date.
If you started your project prior to using Domino 5.3 and then upgraded to Domino 5.3, the start date of the date range is the date on which your organization deployed Domino 5.3.


== Next steps

See link:f5934f[Data Source audit logs] for data captured by Data Sources in Domino.

----- admin_guide/operations/audit-logs/use-audit-trail-apis.txt -----
:page-permalink: a157c4
:page-version: 6.1
:page-title: Using the Audit Trail API
:page-order: 30

The audit trail service provides an API to programmatically query and retrieve various audit logs.
The endpoint to get this information is:
[source,shell]
----
<DOMINO-HOST>/api/audittrail/v1/auditevents
----

== API Parameters

The API supports a list of parameters that can be used to filter for audit events:

* *startTimestamp*: Unix timestamp in milliseconds.
* *endTimestamp*: Unix timestamp in milliseconds.
* *actorId*: Login ID of the user who performed the action.
* *actorName*: Login name of the user who performed the action.
* *event*: Name of the action that occurred.
* *targetType*: The type of the object that receives the action.
* *targetId*: The ID of the object that receives the action.
* *targetName*: The name of the object that receives the action.
* *withinProjectId*: ID of the project where the action took place.
* *withinProjectName*: Name of the project where the action took place.
* *sort*: A comma-separated list of fields to sort the data.
* *limit*: Maximum number of events to retrieve.
* *Offset*: Number of matching events to not include for pagination.

== Export the audit trail data
You can export audit trail data through the API. The export requires several parameters, including a required hostname and authentication details like JWT tokens or API keys.

The data is JSON but can be parsed and stored in a CSV file containing all events that match the query.

=== Sample export script
Domino provides a https://github.com/dominodatalab/audit-trail-export-script[sample script^] that runs on Python 3. You can use the script to customize for your own use to export data into CSV format. The script requires a hostname and one way to authenticate: either a JWT or an API key.

There are a few things to be aware of before you get started:

* The `--hostname` and an authentication method (either `--jwt` or `--api-key`) must be provided, or the export script will fail.
* Optional parameters can be combined to customize the behavior and context of the export action.
* Events are exported in time-descending order.
* The date and time of events are displayed in UTC.

== Arguments

NOTE: The `--hostname` and an authentication method (either `--jwt` or `--api-key`) must be provided, or the export script will fail.

[cols="<1,<1,<2",options="header"]
|===
|Argument |Required |Description

|`--hostname` |Yes* |The hostname to connect to.
|`--jwt` |Yes* |The JWT token for authentication.
|`--api-key` |Yes* |The Domino API key for authentication.
|`--event` |No |The event name.
|`--user_name` |No |The name of the user performing the export action.
|`--target_name` |No |The object that received the action.
|`--project_name` |No |The name of the project.
|`--start_date` |No |Timestamp for the beginning of a date range for data.
Format: `YYYY-MM-DD HH:MM:SS`
|`--end_date` |No |Timestamp for the end of a date range for data.
Format: `YYYY-MM-DD HH:MM:SS`
|===

== Script templates
There are a few common scripts that you can edit for use in your environment.

=== Use JWT for authentication
[source,shell]
----
python3 export_audit_trail.py --hostname https://domino_instance.com --jwt my-jwt-token
----

=== Use API key for authentication and specify an event
[source,shell]
----
python3 export_audit_trail.py --hostname https://domino_instance.com --api-key my-api-key --event "Create Dataset"
----

=== Provide parameters to filter export data
[source,shell]
----
python3 export_audit_trail.py --hostname https://domino_instance.com --jwt my-jwt-token --user_name "Alice" --event "Create Dataset"
----

=== Use start- and end- timestamps
[source,shell]
----
python3 export_audit_trail.py --hostname https://domino_instance.com --jwt my-jwt-token --start_date "2024-10-18 14:10:04" --end_date "2024-10-22 09:04:33"
----

=== Export all available data through an `env` file
[source,shell]
----
DOMINO_HOSTNAME=<A-DOMINO-INSTANCE>
JWT=<A-JWT>
----

== Next steps
* link:208cc3[Create filters and view audit events] with a dashboard for flexible search options, quick insights, and detailed historical analysis.
* The link:dacaf5[Audit Trail Data Glossary] has a complete list of events being tracked. You can use these in the event filter to search for events.
* link:a9e507[Application logs] capture application-level logging including user execution from Jobs, Workspaces, and front-end logs.

----- admin_guide/operations/backup-restore/backup-and-restore-overview.txt -----
:page-version: 6.1
:page-permalink: 08fbb4
:page-title: Backup and restore overview
:page-order: 10

== Backup structure

[[tr1]]
Backups are bundled into tarballs containing the mongo, Git, keycloak, vault, experiment management (MLflow) tracking database, and model secrets, as well as config files for the backup.

The bundles use the filename convention `YYMMDD-HHMMSS.tar.gz`, and all content in the tarball is in a directory with the same timestamp format. If you created a backup on September 22nd, 2023 at 7:02:27 pm, the tarball
`20230922-190227.tar.gz` contains the following:

* 20230922-190227/
** 20230922-190227/config.local.yaml (Config file for local backup)
** 20230922-190227/config.remote.yaml (Config file for remote backup)
** 20230922-190227/config.transfer.yaml (Additional config file for remote backup when using the `--transfer` option)
** 20230922-190227/git.tar.gz (tarball of Git repos and critical user project metadata/version control)
** 20230922-190227/keycloak_scripts.tar.gz
** 20230922-190227/keycloak-postgres_archive_local-backup.gz (Postgres database dump)
** 20230922-190227/mlflow-postgres_archive_local-backup.gz (MLflow Postgres database dump)
** 20230922-190227/mongo_archive_local-backup.gz (Mongo archive dump of primary Domino application database)
** 20230922-190227/vault-k8s-secrets_local-backup.yaml (Kubernetes secrets and critical secrets to unlock Vault)
** 20230922-190227/vault-postgres_archive_local-backup.gz (Postgres database dump)
** 20230922-190227/model_secrets-k8s-secrets_local-backup.yaml (Kubernetes secrets and Environment variables for models)

[CAUTION]
====
The backup bundle only contains data from the Kubernetes cluster. This process does not back up data that is stored in object storage (for example, AWS S3 buckets, Azure containers, GCP buckets) on shared storage like NFS or CIFS/SMB or in an external Docker Registry. Typically, the infrastructure management function operates object storage externally. This is reliable and often large. The infrastructure admin is responsible for the backup/restore strategy for this data.
====

== Backup location

The default backup location varies depending on the infrastructure and which Domino was installed:

* On AWS, Azure, and GCP a dedicated backup storage bucket or container is configured during Domino installation.
* For on-premises deployments, backups are delivered to the shared storage mount.

The backup system delivers its backup bundle tarballs (`YYYY/MM/YYMMDD-HHMMSS.tar.gz` ) to the backup storage backend that is in use for a deployment. For on-premises deployments, use a shell into the importer pod in `/opt/shared/backups` to access this location.
----- admin_guide/operations/backup-restore/customize-backups.txt -----
:page-version: 6.1
:page-permalink: 565ef3
:page-title: Customize backups
:page-order: 20

The following primary customizations are available through values fed to the `domino-data-importer Helm chart`:

* Backup schedule
* Importer command-line arguments (The importer is the program that performs the backup. Typically, you do not have to customize this component.)


[[tr2]]
== Customize backup schedule

Use the Helm chart value `backups.schedule` to change the interval at which backups are performed. This is fed to a Kubernetes CronJob object.

The default value is `@daily`, and any valid Kubernetes CronJob schedule string will work, for example: `0 */4 * * *`. The string mimics standard cron syntax.

== Customize importer command-line arguments

Use the `backupJobScript Helm chart` value to customize the command that creates the backup bundles. The default command line arguments are:


[source,shell]
----
importer backup --upload --archive --delete --alert-on-fail
----


The available options for the importer are:

----
  --strategy [standard|large]  Strategy for backing up large-storage services
                               (blobs, etc.)  standard: Backs up databases and
                               git large: Backs up everything (blobs, etc.),
                               pulling them locally from nfs/s3. Not
                               appropriate for production use.  [default:
                               standard]
  --archive                    Archive backups into single `.tar.gz` archive
  --archive-delay INTEGER      Delay in seconds before creating archive,
                               helpful with slow shared storage updates
                               [default: 5]
  --delete                     Delete backup after creation
  --upload                     Upload (or copy) backup archives into s3/shared
                               storage directory (implies `--archive`)
  --alert-on-fail              Trigger a Domino notification on failure
                               targeting System Admins.
  -i, --include TEXT           Include migrations by name (comma-delimited,
                               excludes still apply)
  -x, --exclude TEXT           Exclude migrations by name (comma-delimited,
                               will exclude items in --include)
  -d, --dry                    Dry run
  -h, --help                   Show this message and exit.
----

You can remove `--delete` for testing or debugging purposes. The backups are typically removed from local storage after upload via this argument. Removing it will leave the backup's working directory intact, allowing you to troubleshoot the state of it prior to upload.

You can also remove `--alert-on-fail` for testing or debugging purposes to prevent sending any further alerts to the Domino Admin dashboard.

----- admin_guide/operations/backup-restore/index.txt -----
:page-version: 6.1
:page-title: Backup and restore
:page-permalink: 72ff13
:page-order: 60

The Domino backup system unifies all Domino user data (Workbench and Domino app configuration) into a single `cron` job, producing a single tarball backup. This cron job is called `domino-workbench-backup`.

link:08fbb4[Backup and restore overview]::
Backups are bundled into tarballs containing the mongo, Git, keycloak, vault, experiment management (MLflow) tracking database, and model secrets, as well as config files for the backup.

link:565ef3[Customize backups]::
Customizations are available through values fed to the `domino-data-importer Helm chart`.

link:2722ce[Run a manual backup]::
You can create a backup manually, on-demand.

link:b73853[Restore backups]::
Import data from the backup into current or new deployments.

----- admin_guide/operations/backup-restore/restore-backups.txt -----
:page-version: 6.1
:page-title: Restore backups
:page-permalink: b73853
:page-order: 40

== Prerequisite

* Before you restore a backup you must put Domino into maintenance mode.
See the https://github.com/dominodatalab/domino-maintenance-mode/blob/main/README.md[domino-maintenance-mode readme^].

[[tr1]]
== Restore a backup

. Run the following to edit the `domino-data-importer` statefulset:
+
[source,shell]
----
kubectl -n domino-platform edit sts domino-data-importer
----
+
Set the replicas to 1.

. When the domino-data-importer pod is up, run the following code to exec into it:
+
[source,shell]
----
kubectl -n domino-platform exec -it domino-data-importer-0 /bin/bash
----
+
This command can take a long time, depending on the size of your backup. To prevent a timeout interrupting the command, Domino recommends using a terminal multiplexer tool like `/app/tmux` so that if your session is disconnected, the command continues running in the background and you can return to the session when you reconnect. For instructions about how to use `tmux`, see their website or man page `man tmux`.

. Create the directory where the restore process expects to find the backup tarball:
+
[source,shell]
----
mkdir -p /opt/scratch/migration-sessions/
----

. Copy the backup tarball into the new directory `/opt/scratch/migration-sessions/`. If you're restoring directly into the original deployment, copy straight from the backup location (for example, from aws s3 cp).
. Untar the backup
+
[source,shell]
----
cd /opt/scratch/migration-sessions && tar xvf YYYYMMDD-HHMMSS.tar.gz
----

. From the main app directory, run the importer with the path to the backup configuration. You can run it in dry mode first as a confidence check:
+
[source,shell,subs="attributes"]
----
{importer-path} restore --into-cluster same --backup-dir /opt/scratch/migration-sessions/YYYYMMDD-HHMMSS --dry
----
+
Then run it for real:
+
[source,shell,subs="attributes"]
----
{importer-path} restore --into-cluster same --backup-dir /opt/scratch/migration-sessions/YYYYMMDD-HHMMSS
----
+
The importer imports data from the backup into the current deployment.

[IMPORTANT]
====
Remember that backup bundles only contain mongo, Git, keycloak, vault, experiment management (MLflow) tracking database, and model secrets.

* [[tr2]] If you are restoring into the same deployment from which the backup was generated (that is, not on a newly provisioned infrastructure or a torn-down and rebuilt Domino install), this is typically enough for you to restore a deployment into a previous state.
* [[tr3]] If you are restoring into a new deployment, at a minimum you must also copy the blobs, datasets, and docker-registry data for a functional Domino install. Log data (that is, historical run, build, and model logs) is optional. You can use the importer pod as a hub to create the copies because it will have access to all local data.
====


== Restore data that was not backed up
[[tr4]]
You can restore the data elements that are not included in the backup bundle (object storage, shared storage, and external Docker registries) by putting the files in their original place. However, you cannot do this with project "blobs" when backed by S3.

Project blobs stored in S3 are unique because Domino uses S3 metadata. Losing this metadata renders the blob store non-operable. Because of this, ideally, backups for the blobs S3 bucket must be propagated in S3. You must use the Importer's S3-to-S3 copy mechanism for transfers of blobs from one S3 bucket to another to preserve the metadata.

----- admin_guide/operations/backup-restore/run-a-manual-backup.txt -----
:page-version: 6.1
:page-title: Run a manual backup
:page-permalink: 2722ce
:page-order: 30

[[tr1]]
You can create a backup manually, on-demand. This is useful during “lift-and-shift” migrations or prior to upgrades.

== Prerequisite

* Before you perform a backup you must put Domino into maintenance mode.
See the https://github.com/dominodatalab/domino-maintenance-mode/blob/main/README.md[domino-maintenance-mode readme^].

== Manually generate a backup

. Run the following command to scale the domino-data-importer statefulset up to 1 replica:
+
[source,shell]
----
platform_ns="$(kubectl get namespace -l domino-platform -o name)"
platform_ns="${platform_ns#namespace/}"
kubectl scale sts -n $platform_ns domino-data-importer --replicas=1
----

. When the domino-data-importer pod is up, run the following code to exec into it:
+
[source,shell]
----
kubectl -n $platform_ns exec -it domino-data-importer-0 /bin/bash
----


. In the `domino-data-importer` pod, run the backup command:
+
[source,shell]
----
importer backup --upload --archive --delete
----
[[tr2]] Remove the `--delete` flag if you want the backup bundle to remain on the local filesystem.

This command can take a long time, depending on the size of your backup. To prevent a timeout interrupting the command, Domino recommends using a terminal multiplexer tool like the container's `/app/tmux` so that if your session is disconnected, the command continues running in the background and you can return to the session when you reconnect. For instructions about how to use `tmux`, see their website or man page `man tmux`.

Find the backup bundle in the link:08fbb4[backup location] when it is generated automatically.

When the backup is done, scale the `domino-data-importer` statefulset back to `0`.

----- admin_guide/operations/control-center/export-control-center-data.txt -----
:page-version: 6.1
:page-title: Export Compute and Spend data with the API
:page-sidebar: Export Compute and Spend data
:page-permalink: 9a12a0
:page-order: 50
:source-highlighter: highlightjs

The Compute and Spend report provides several views about deployment usage, broken down by hardware tier, project, or user.
However, if you want to do a more detailed, custom analysis, you can use the API to export Control Center data for examination with Domino's data science features or external business intelligence applications.

The endpoint that serves this data is `/v4/gateway/runs/getByBatchId`.

== Prerequisites

* The API key for your account.
* An administrator account to access the full deployment's Control Center data.

== Get the API key

. Log in as an administrator, then click *Account* > *Account Settings*.

. Under *Account Settings*, click *API Key*. Click *Regenerate* if you have not generated a key before.  
. Copy the key and store it carefully as you will need it to make requests to the API.
+
CAUTION: Anyone with this key can authenticate to the Domino API as you so treat it like a sensitive password.

Domino recommends repeating this process every 60-days to maintain security best practices.

[[tr1]]
// curl access to the data gateway endpoint
== Use the data gateway endpoint

The following is a basic call to the data export endpoint, executed with https://curl.haxx.se/[curl^]:

[source,shell]
----
curl --include -H "X-Domino-Api-Key: <your-api-key>" 
'https://<your-domino-url>/v4/gateway/runs/getByBatchId'
----

By default, the endpoint starts with the oldest available run data, starting from January 1st, 2018.
Older data isn't available.
The command also has a default limit of 1000 runs worth of data.
As written, the preceding call returns data on the oldest 1000 runs available.

To try this example, enter `your-api-key` and `your-domino-url` in the previous command.

The standard JSON response object you receive has the following scheme:

[source,json]
----
{
  "runs": [
    {
      "batchId": string,
      "runId": string,
      "title": string,
      "command": string,
      "status": string,
      "runType": string,
      "userName": string,
      "userId": string,
      "projectOwnerName": string,
      "projectOwnerId": string,
      "projectName": string,
      "projectId": string,
      "runDurationSec": integer,
      "hardwareTier": string,
      "hardwareTierCostCurrency": string,
      "hardwareTierCostAmount": number,
      "queuedTime": date-time ,
      "startTime": date-time,
      "endTime": date-time,
      "totalCostCurrency": string,
      "totalCostAmount": number,
      "computeClusterDetails : {
        "computeClusterType": string,
        "masterHardwareTierId": string,
        "masterHardwareTierCostPerMinute": number,
        "workerCount": integer,
        "workerHardwareTierId": string,
        "workerHardwareTierCostPerMinute": number
      }
    }
 ],
  "nextBatchId": string
}
----

The Control Center gives each run recorded a `batchId`, which is an incrementing field that can be used as a cursor to fetch data in multiple batches.
You can see in the previous response, after the array of `runs` objects, a `nextBatchId` parameter points to the next run to include.
Use that ID as a query parameter in a subsequent request to get the next batch:

// TR: batchId can be used; this is covered by tr3, and curl is covered by tr1

[source,shell]
----
curl --include 
-H "X-Domino-Api-Key: <your-api-key>" 
'https://<your-domino-url>/v4/gateway/runs/getByBatchId?batchId=<your-batchId-here>'
----

You can also include a header with `Accept: text/csv` to request the data as CSV.
On the Unix shell, you can write the response to a file with the `>` operator.
This is a quick way to get data suitable for import into analysis tools:

[[tr2]]
// Fetch with text/csv returns a CSV
[source,shell]
----
curl --include 
-H "X-Domino-Api-Key: <your-api-key>" 
-H 'Accept: text/csv' 
'https://<your-domino-url>/v4/gateway/runs/getByBatchId' > your_file.csv
----

== Get all Workspaces

Use the following API call to retrieve all workspaces.

`GET /controlCenter/utilization/runsPerDay`

NOTE: The link:71d6ad#performance[`com.cerebro.domino.controlCenter.cacheTimeToLiveInMinutes`] configuration key specifies the cache refresh as 30 minutes, by default.
This might cause a delay in retrieving some workspaces.

[source,shell]
----
curl -X GET "<your-domino-url>/v4/controlCenter/utilization/runs?startDate=20220503&endDate=20220503" -H  "accept: application/json"
----

[cols="2a,^1a,2a",options="header"]
|===
|Parameter|Required|Description

|`projectId`||
|`startingUserId`||
|`organizationId`||
|`hardwareTierId`||
|`startDate`|✓| Range must be in `YYYYMMDD` format
|`endDate`|✓| Range must be in `YYYYMMDD` format
|===

[[tr3]]
.Example: Get all data

The following code shows a Python script that fetches all Control Center data from the earliest available to a configurable end date, and writes it to a CSV file.
Enter the date of the last known completed run to fetch all available historical data.

[source,python]
----
import requests
import json
import pandas as pd
import os
from datetime import datetime
from datetime import timedelta

URL = "https://<your-domino-url>/v4/gateway/runs/getByBatchId"
headers = {'X-Domino-Api-Key': '<your-api-key>'}
last_date = 'YYYY-MM-DD'

last_date = datetime.strftime(
    datetime.strptime(last_date, '%Y-%m-%d') + timedelta(days = 1),
    '%Y-%m-%d',
)

try:
  os.remove('output.csv')
except:
  pass

batch_ID_param = ""
while True:
    batch = requests.get(url = URL + batch_ID_param, headers = headers)
    parsed = json.loads(batch.text)
    batch_ID_param = "?batchId=" + parsed['nextBatchId']
    df = pd.DataFrame(parsed['runs'])
    df[df.endTime <= last_date].to_csv(
        'output.csv',
        mode="a+",
        index=False,
        header=True)
    if len(df.index) < 1000 or len(df.index) > len(df[df.endTime <= last_date].index):
        break
----

You can run a script like this periodically to import fresh data into your tools for custom analysis.
Work with the data in a Domino project, or make it available to third-party tools like Tableau.

----- admin_guide/operations/control-center/index.txt -----
:page-version: 6.1
:page-title: Use the Control Center
:page-permalink: d5d2ac
:page-order: 40

[[tr1]]
// You can view deployment-wide usage of compute resources by hours
[[tr2]]
// You can view deployment-wide usage of compute resources by spend in USD
[[tr3]]
// You can export control center data
Use the Control Center to see important data about your Domino deployment.

[[tr4]]
IMPORTANT: Only Domino Admins can access the Control Center.
If you need access to the Control Center, contact your local Domino Administrator or email support@dominodatalab.com.

For more information on how to use the Control Center, have a look at these topics:

* link:eb42e4[View Control Center metrics], including deployment-wide usage of compute resources by hours of runtime or spend in USD.
* Drill down into statistics about link:98d13a[hardware tiers], link:c16e55[Projects], and link:c8f7be[user activity].
* link:9a12a0[Export Compute and Spend data] to create your own reports or analyses.
* Use the Projects Portfolio to link:13a1a4[view the status of all your Projects].

----- admin_guide/operations/control-center/view-hardware-tier-information.txt -----
:page-version: 6.1
:page-permalink: 98d13a
:page-title: View hardware tier execution information
:page-sidebar: View hardware tier execution information
:page-order: 20

IMPORTANT: Domino recommends using link:d4b465[FinOps] features to view and control cloud compute costs. With FinOps, you can set budgets, alerts, store usage data for analysis, and directly integrate with cloud billing APIs to provide the most accurate representation of your cloud bills.


[[tr17]]
. In the navigation pane, click *Govern* > *Compute & Spend*.
. In the second chart, switch the data to *Hardware Tiers*.
. Click a bar in the chart.
+
This page shows performance averages for runs that use the specified hardware tier, and tracks completed runs.
The *Run Logs* table lists details about all runs performed on the specified hardware tier.
. Click an entry in the table to view the specified run in the project *Jobs* page.

----- admin_guide/operations/control-center/view-metrics-in-cc.txt -----
:page-version: 6.1
:page-permalink: eb42e4
:page-title: View Control Center metrics
:page-sidebar: View Control Center metrics
:page-order: 10

IMPORTANT: Domino recommends using link:d4b465[FinOps] features to view and control cloud compute costs. With FinOps, you can set budgets, alerts, store usage data for analysis, and directly integrate with cloud billing APIs to provide the most accurate representation of your cloud bills.

[[tr5]]
. In the navigation pane, click *Govern* > *Compute & Spend*.
+
The first chart is a bar chart of deployment compute spend in USD for each day in the current month.
+
[[tr6]]
// Compute spend is based on the "cents per minute" for the hardware tiers
[[tr7]]
// Compute spend does not depend on idle resources or storage
Compute spend is based on settings applied by admins when creating and managing https://tickets.dominodatalab.com/hc/en-us/articles/360058497671-Hardware-tiers-and-cluster-controls[hardware tiers^].
Compute spend data is available in this report only if the *Cents per minute* field is set for the link:9d16e5[hardware tier] in use.
These numbers also only represent active usage and do not reflect other potential spend such as idle cloud resources or storage.
+
[[tr8]]
// Usage chart can be filtered by date range
[[tr9]]
// Usage chart can switch between spend or hours
.. Use the date menu to change the date range.
.. Use the *Compute Spend ($)* menu to change the chart to show compute usage by hours of runtime.

[[tr10]]
// Can see grouping by projects, users, hardware tiers in separate bottom panel
The next chart shows more granular data about projects, users, organizations, and hardware tiers across the selected date range.
You can chart these by metrics such as the following:

* Projects can be charted by compute spend (USD) or compute hours.
* Users can be charted by compute spend (USD) or compute hours.
* Hardware tiers can be charted by average run queue time in minutes.

[[tr11]]
// Bottom panel displays top five results
[[tr12]]
// Bottom panel links to "view all" usage statistics in table for users
This chart shows the top five results for the selected metric. If you set this chart to show data about users, you can click the *View all* link to load a paginated table with detailed usage statistics for all users.

By default, this table shows data for the date range set on the previous page.
Use the menu  to change the date range.

[[tr15]]
== Drill down for more details

You can drill down into most of the tables and charts support for more details.
Click a bar in a bar chart to see a more detailed page about the related project, user, or hardware tier.

[[tr16]]
// Drilldown into items in the lower panel includes a Run Logs table
Some of these pages also show a table of related runs.
You can click an entry in a *Run Logs* table to view the specified run in the project *Runs* page.

----- admin_guide/operations/control-center/view-project-spend.txt -----
:page-version: 6.1
:page-permalink: c16e55
:page-title: View Project spend
:page-order: 30

IMPORTANT: Domino recommends using link:d4b465[FinOps] features to view and control cloud compute costs. With FinOps, you can set budgets, alerts, store usage data for analysis, and directly integrate with cloud billing APIs to provide the most accurate representation of your cloud bills.

[[tr19]]
. In the navigation pane, click *Govern* > *Compute & Spend*.
. In the second chart, switch the data to *Projects*.
. Click a bar in the chart.
+
This page breaks down project spend across Apps, Batch Runs, Endpoints, Launchers, Scheduled Runs, and Workspaces.
The *Run Logs* table lists details about all runs performed on the specified hardware tier.

. Click an entry in the table to view the specified run in the project *Jobs* page.

----- admin_guide/operations/control-center/view-the-status-of-all-projects.txt -----
:page-version: 6.1
:page-permalink: 13a1a4
:page-title: View the status of all Projects
:page-order: 60

Use the Projects Portfolio to see the status of all the projects that your Data Science team is working on.

In the navigation pane, click *Govern* > *Projects Portfolio*.

You can see a list of projects, who is working on them, which projects have goals, and what stage the projects are in.

* Click the buttons to filter the page to see the following:
** Blocked projects.
** Projects in stages.

* Use the tabs to filter the *Active* or *Complete* projects.

* Use the filter to limit the project list with text entered in the *Project Name*, *Owner*, or *Username*.

----- admin_guide/operations/control-center/view-user-activity.txt -----
:page-version: 6.1
:page-permalink: c8f7be
:page-title: View user activity
:page-order: 40


. In the navigation pane, click *Govern* > *Compute & Spend*.
. In the second chart, from the menu, click *Users*.
. Click a bar in the chart.
+
This page shows detailed data on a user's activity in Domino.
The top of the page shows the types of runs this user starts, which projects the user works on, and which hardware tiers the user uses.
You can click bars in the compute hours, project, and hardware tier charts to view the object represented.
The *Run Logs* table lists details about all runs performed on the specified hardware tier.
. Click an entry in the table to view the specified run in the project *Jobs* page.

----- admin_guide/operations/disaster-recovery.txt -----
:page-version: 6.1
:page-permalink: 141306
:page-title: Disaster recovery
:page-order: 50

By default, Domino runs in a High Availability mode, replicating its core components across three or more nodes in the cluster.
Data losses are rare and unlikely to happen.
However, in the event of a service disruption which impacts the entire cluster, Domino recommends recovery point (RPO) targets of 0 to 12 hours depending on the service level requirements of your organization.
For deployments in the cloud, we strongly recommend deployments across multiple availability zones (AZs).
Typically, deployments on multiple AZs can expect an RTO between 0 and 10 minutes.

The following systems are canonical stores of critical Domino data, and they are stored and backed up in AWS as described in the following table.

[cols="2a,4a,4a,4a",options="header"]
|===
|System|Purpose|Store|Backup

|Projects store
|Stores the contents of users' link:a8e081#project-files[project files].
|Stored in https://aws.amazon.com/s3/[S3^] in the bucket specified in the installer configuration at link:7f4331#blob-storage[blob_storage.projects.s3.bucket].
|Relies on the https://docs.aws.amazon.com/AmazonS3/latest/dev/DataDurability.html[inherent durability and automated replication and backups of S3^].

|Log history
|Stores execution logs from Domino jobs and workspaces.
|Stored in https://aws.amazon.com/s3/[S3^] in the bucket specified in the installer configuration at link:7f4331#blob-storage[blob_storage.projects.s3.bucket].
|Relies on the https://docs.aws.amazon.com/AmazonS3/latest/dev/DataDurability.html[inherent durability and automated replication and backups of S3^].

|Docker registry
|Stores Docker images built by Domino to back users' link:f51038[Domino environments] and link:8dbc91[Domino endpoints].
|Stored in https://aws.amazon.com/s3/[S3^] in the bucket specified in the installer configuration at link:7f4331#internal-docker-registry[internal_docker_registry_s3_9verride_bucket].
|Relies on the https://docs.aws.amazon.com/AmazonS3/latest/dev/DataDurability.html[inherent durability and automated replication and backups of S3^].

|Domino Volumes for NetApp ONTAP
|Stores the contents of users' link:06da1b[Domino Volumes].
|Defaults to the shared storage class (`storage_classes.shared`), but can be extended and configured by an admin.
|Domino does not automatically back up Domino Volumes. Disaster recovery relies on NetApp’s native data protection features such as snapshot schedules, SnapMirror replication, or SnapVault. Administrators should configure policies that align with organizational data recovery requirements.

|Datasets
|Stores the contents of users’ Domino Datasets.
|Stored in https://aws.amazon.com/efs/[EFS^] in the file system specified in the installer config at link:7f4331#storage-classes[storage_classes.shared.efs.filesystem_id].
|Relies on the https://aws.amazon.com/efs/faq/#Data_protection_and_availability[inherent durability and automated replication and backups of EFS^].

|MongoDB
|Stores Domino application object data and metadata.
|Stored in EBS-backed Kubernetes persistent volumes attached to pods running the HA MongoDB service in the Domino cluster.
|Domino performs a daily backup that writes an archive containing all MongoDB data to S3 in the bucket specified in the installer configuration at link:7f4331#blob-storage[blob_storage.backups.s3.bucket].

|Domino Git
|Stores Domino project version history.
|Stored in EBS-backed Kubernetes persistent volumes attached to pod running the Domino Git service in the Domino cluster.
|Domino performs a daily backup that writes an archive containing all Git data to S3 in the bucket specified in the installer configuration at link:7f4331#blob-storage[blob_storage.backups.s3.bucket].

|Postgres
|Stores information on users managed by the Keycloak authentication service.
|Stored in EBS-backed Kubernetes persistent volumes attached to pods running the HA Postgres service in the Domino cluster.
|Domino performs a daily backup that writes an archive containing all Postgres data to S3 in the bucket specified in the installer configuration at link:7f4331#blob-storage[blob_storage.backups.s3.bucket].
|===

== Recovery

If you choose to run “hot” backup instances or additional storage facilities for disaster recovery, you can request an increase in quota limits for one or more of the AWS services if needed.
You can check the https://docs.aws.amazon.com/general/latest/gr/aws-service-information.html[default service quotas^] and manage your quotas by logging in to the https://console.aws.amazon.com/servicequotas/home[AWS Service Quotas console^].

If you need to restore a deployment from backups, https://tickets.dominodatalab.com/hc/en-us/articles/360059918151-Domino-Support-Operating-Procedures[submit a maximum severity support request^] and the Domino operations team will assist with the recovery procedure.

In the case of instance or service failure, https://tickets.dominodatalab.com/hc/en-us/articles/360059918151-Domino-Support-Operating-Procedures[submit a maximum severity support request^] and the Domino operations team will assist with restoring the cluster to normal operation.

In the case of critical application failure, https://tickets.dominodatalab.com/hc/en-us/articles/360059918151-Domino-Support-Operating-Procedures[submit a maximum severity support request^] and the Domino operations team will assist with restoring the cluster to normal operation.

By default, Domino runs in a “High Availability” mode, replicating its core components across three or more nodes in the cluster.
Should a fault condition occur, https://tickets.dominodatalab.com/hc/en-us/articles/360059918151-Domino-Support-Operating-Procedures[submit an appropriate severity support request ticket^] and the Domino operations team will assist with restoring the cluster to normal operation.

== Domino backups on-premises

The following systems are canonical stores of critical Domino data, and they are stored and backed on-premises as described below.
These methods can also be applied to other clouds for which Domino does not have native storage integrations.

[cols="2a,4a,4a,4a",options="header"]
|===
|System|Purpose|Store|Backup

|Projects store
|Stores the contents of users’ link:a8e081#project-files[project files].
|Stored in the shared storage class defined at link:7f4331#storage-classes[storage_classes.shared] and referenced at link:7f4331#blob-storage[blob_storage.projects]. The most common backing storage for this class is NFS.
|Domino does not automatically back up this data. Domino recommends configuring a separate backup process that saves or syncs the entire contents of the NFS mount path provided to Domino to another storage system. The volume of project data will grow linearly over time as users produce new projects, so you should account for the growing volume when setting backup frequency and retention policies.


|Log history
|Stores execution logs from Domino jobs and workspaces.
|Stored in the shared storage class defined at link:7f4331#storage-classes[storage_classes.shared] and referenced at link:7f4331#blob-storage[blob_storage.logs].
|This is stored in the same volume as project data, and is therefore on the same filesystem path in the shared storage system. Any backups of the root path provided to Domino will include log data.

|Docker registry
|Stores Docker images built by Domino to back users’ link:bf8b6c[Domino environments] and link:8dbc91[Domino endpoints].
|Stored in a block storage volume provisioned from the storage class defined at link:7f4331#storage-classes[storage_classes.block] and mounted in the Docker registry container.
|Domino does not automatically back up this data. Domino recommends configuring a separate backup process that saves or syncs the entire contents of the NFS mount path provided to Domino to another storage system. The volume of project data will grow linearly over time as users produce new projects, so you should account for the growing volume when setting backup frequency and retention policies.

|Datasets
|Stores the contents of users’ link:0a8d11[Domino Datasets].
|Stored in the shared storage class defined at link:7f4331#storage-classes[storage_classes.shared].
|This is stored in the same volume as project data, and is therefore on the same filesystem path in the shared storage system. Any backups of the root path provided to Domino will include log data.

|MongoDB
|Stores Domino application object data and metadata.
|Stored in a block storage volume provisioned from the storage class defined at link:7f4331#storage-classes[storage_classes.block] and mounted in the Mongo containers.
|Domino performs a daily backup that writes an archive containing all MongoDB data to the shared storage class defined at link:7f4331#storage-classes[storage_classes.shared] and referenced at link:7f4331#blob-storage[blob_storage.backups].

|Domino Git
|Stores Domino project version history.
|Stored in a block storage volume provisioned from the storage class defined at link:7f4331#storage-classes[storage_classes.block] and mounted in the Git container.
|Domino performs a daily backup that writes an archive containing all Git data to the shared storage class defined at link:7f4331#storage-classes[storage_classes.shared] and referenced at link:7f4331#blob-storage[blob_storage.backups].

|Postgres
|Stores information on users managed by the Keycloak authentication service.
|Stored in a block storage volume provisioned from the storage class defined at link:7f4331#storage-classes[storage_classes.block] and mounted in the Postgres containers.
|Domino performs a daily backup that writes an archive containing all Postgres data to the shared storage class defined at link:7f4331#storage-classes[storage_classes.shared] and referenced at link:7f4331#blob-storage[blob_storage.backups].

|===


== Support

Domino support coverage is outlined on our https://tickets.dominodatalab.com/hc/en-us/articles/360059504652-Scope-of-Support-Coverage[Scope of Support Coverage page^].
You can learn more about our support priority codes, ticket statuses, and business hours on our https://tickets.dominodatalab.com/hc/en-us/articles/360060058591-Support-Definitions[Support Definitions page^].

In addition to the support tiers described above, some Domino customers may have unique contractual SLAs defined in the Master Service Agreement.
All customers receive a standard https://tickets.dominodatalab.com/hc/en-us/articles/360060058871-Service-Level-Agreement[service level agreement^].


----- admin_guide/operations/finops/1-budgets-alerts.txt -----
:page-version: 6.1
:page-permalink: 06e4c2
:page-title: Configure Billing Tags, Budgets, and Alerts
:page-order: 10

You can use Domino Cost Center to establish budgets and send alerts to team members when they approach their spending limits on compute and storage resources. The addition of Billing Tags gives you added flexibility to track and manage project costs horizontally across your organization.

These features make it easier to track costs across different projects. Administrators can also set limits on the amount of storage for Datasets.

== Configure billing tags

You can aggregate Cost Center or organization usage by using billing tags within Domino. Billing tags are a way of tagging projects so that they can be grouped by tag for accounting purposes. Projects may have multiple billing tags, and Administrators have the option to require the use of billing tags or not. To create billing tags:

. Go to *Admin* > *Manage resources* > *Cost tracking* > *Billing tags*.
. In the *Configure Billing Tags* section, you can set the *Billing Tag* feature to `Disabled`, `Optional`, or `Required` for new projects. Existing projects are unaffected, but you can still apply billing tags to them manually.
. For Billing Tags, enter one or more names and click *Add Billing Tags*.

image::/images/admin-troubleshooting/configure-billing-tags.png[alt="cost center dashboard", width=900]

After the billing tags are created, you can filter available projects and assign billing tags to them using the *Manually apply Billing Tags* field. This is useful for adding a newly defined billing tag to an existing project.

== Configure budgets to prevent overspending

Budgets help control costs for organizations and projects by preventing overspending. By default, all projects and organizations have no budget limit. To set and configure your budgets:

. Go to *Manage Cost*.
. Go to *Admin* > *Manage resources* > *Cost tracking* > *Budget & alerts*.
. In the *Default Limits* section, set a default monthly (by calendar month) budget that applies to all organizations, projects, or billing tags.
. Use *Budget Overrides* to change the default budget limits for specific organizations, projects, or billing tags.

image::/images/admin-troubleshooting/configure-budgets.png[alt="Configure budgets to prevent overspending", width=900]

== Configure alerts to notify stakeholders

Alerts notify stakeholders when expenses for their organizations are nearing the budget threshold. These alerts are delivered through both Domino notifications and email.

Alerts are not configured by default. Billing tag notifications are only sent by email and require email to be configured to allow alerts. To set and configure alerts:

. Go to *Admin* > *Manage resources* > *Cost tracking* > *Budget & alerts*.
. Under *Alert Settings*, select *Alert on 75% and 100%* of budget thresholds to enable alerts.
. Add additional emails to receive notifications.
. Go to *Platform settings* > *Email* to configure your email and allow email alerts.

Domino sends out daily notifications when 75% of the budget is reached.

== Configure limits per dataset

Admins can also use configuration keys related to read-write Datasets to set limits and quotas on the amount of storage used by Datasets, saving money on storage expenses and monitoring usage.

For more information, please get in touch with Domino Support.

=== Set Dataset snapshot limits

When a Dataset reaches the snapshot limit, users receive an error message when they attempt to create an additional snapshot.

To limit the number of snapshots per Dataset, please reach out to Domino Support.

You must delete old snapshots or increase the limit before writing additional snapshots.

=== Override snapshot limits

You can authorize individual projects to ignore the snapshot limits that you set:

. Go to the *Project* that you want to set snapshot limits on.
. Go to *Settings* > *Hardware & Environment*.
. Select the *Ignore Dataset Limits* checkbox.

Selecting *Ignore Dataset Limits* does not impact any quotas for which hard limits have been set.

=== Dataset sizing

When a user interacts with a dataset by uploading a file, deleting a file, creating a snapshot, or ending a run, the dataset is queued to be sized in the background. This allows large datasets to be sized quickly and accurately without timing out.

To avoid multiple expensive calls to resize the dataset, Domino waits seven days between each sizing request.

The link:71d6ad#tr310[Configuration records] page has more information about Dataset configuration keys.

=== Dataset quota limits

You can also set dataset quotas to notify users if their datasets exceed the allocated storage. Quotas are "soft" quotas, meaning they never block users from creating more datasets or snapshots.

There are two types of dataset quotas. To set quotas:

. Navigate to *Admin* > *Datasets*.
. Choose the type of quota to set:
+
* `Global quota`: Creates a limit that applies to every user in the deployment.
* `Quota override`: Sets individual limits on specific users and overrides the `global quota`.

Quota overrides only work when a global quota is also configured.

=== Dataset quota calculations

Domino calculates quotas based on a user's total dataset storage footprint, which includes all the Datasets they own, except for datasets in specific states:

* `MarkedForDeletion`
* `DeletionInProcess`
* `Deleted`
* `Failed`

The size of a dataset is determined by summing the sizes of each snapshot within the dataset, excluding any deleted snapshots. If a dataset has multiple owners, the size of that dataset is counted towards each owner's quota.

link:8f5b7e#dataset-roles[Dataset Roles] has more information about how the roles work together.

=== Quota notifications

As an administrator, you can configure the thresholds listed in the link:https://docs.dominodatalab.com/en/latest/admin_guide/71d6ad#tr300[Read-write Datasets configuration keys].

When a user reaches certain thresholds of their quota, they receive notifications, emails, and UI warnings on Dataset pages. These actions are triggered by the following events:

* Uploading files to a Dataset (both UI and CLI).
* Creating a snapshot (both UI and CLI).
* Creating a Dataset from a snapshot (both UI and CLI).
* Deleting files from a Dataset in the UI.

When a user receives an email about their Dataset storage quota, administrators automatically receive a similar email. You can also add additional recipients under *Quota Notifications*.

== Next steps

Here are some other ways Domino Cost Center can help you optimize the performance of your AI projects while managing cloud expenditures:

* link:157618[Automatically allocate usage-based costs] to Projects, organizations, and users.
* Optimize link:bc084f[Dataset storage usage] to reduce cloud storage costs.
* Generate link:157618[chargeback/showback reports] and use APIs to facilitate cost recovery across organizations and engage with high-expenditure users to optimize infrastructure usage.

----- admin_guide/operations/finops/2-configuration-options-cost-center.txt -----
:page-version: 6.1
:page-permalink: 624992
:page-title: Configuration Options for Cost Center
:page-sidebar: Configuration Options for Cost Center
:page-order: 20

This guide describes several configuration options that you can customize for the link:d4b465[Domino Cost Center].

You can download Domino Cost Center data in JSON format. You can then connect this JSON file to your link:https://help.tableau.com/current/pro/desktop/en-us/examples_json.htm[Tableau^] or link:https://learn.microsoft.com/en-us/power-bi/connect-data/desktop-quickstart-connect-to-data[Power BI dashboard^]. This allows you to visualize the data and share it with your team members as needed.

Domino also offers the ability to integrate with cloud provider billing APIs, allowing for precise tracking and reporting of Domino's cloud expenses.

== Configure long-term storage for Domino Cost Center

Domino Cost Center lets you retain your cost data in blob storage for long-term analysis. Non-AWS users can manually provision their blob store and set up federated storage for their Cost Center. To provision your blob store and set up Cost Center federated storage:

. In your Domino cluster, create a new file called `federated-store.yaml`.

. Format the contents of the file according to each cloud provider's specifications:

* https://www.ibm.com/docs/en/kubecost/self-hosted/2.x?topic=configuration-gcp-multi-cluster-storage[GCP storage configuration^]
* https://www.ibm.com/docs/en/kubecost/self-hosted/2.x?topic=configuration-azure-multi-cluster-storage[Azure storage configuration^]

. Add a secret using `federated-store.yaml: kubectl create secret generic <secret_name> -n <namespace> --from-file=federated-store.yaml`.

. Set `.Values.kubecostModel.federatedStorageConfigSecret` to use your Kubernetes secret name.

== Cloud integration for Domino Cost Center

Cost Center is preconfigured for AWS. Non-AWS users can manually provision their cost usage report and set up cloud integration for Cost Center. To retrieve billing information from your cloud provider, contact your Domino representative to set up Cost Center Cloud Billing Integration.

== Next Steps

* Prevent overspending by link:06e4c2[setting budgets and sending alerts] to stakeholders, administrators, and team members approaching their limits.
* Learn more about link:157618[automatic allocation of usage-based costs].

----- admin_guide/operations/finops/3-analyze-costs-API.txt -----
:page-version: 6.1
:page-permalink: 157618
:page-title: Analyze costs using API
:page-order: 30

The Domino Cost Center APIs allow aggregation only by labels of Domino domain objects, not by arbitrary Kubernetes objects, such as clusters.

== Access Cost Center through Domino Cost API

Use the link:8c929e[Domino Cost API] to access Cost Center features.

NOTE: Only link:2611b7[SysAdmins] can use the `/allocation` and `/asset` API paths.

=== API samples

To obtain the Domino API key, go to *Account Settings* > *API Key*.

==== Today's total cost with idle cost shared among allocations

[source,shell]
----
curl -H "X-Domino-Api-Key: $API_KEY" -H "Accept: application/json" https://$DOMAIN/api/cost/v2/allocation/summary?window=7d&shareIdle=true -G
----

==== Today's total for a particular Project

[source,shell]
----
curl -H "X-Domino-Api-Key: $API_KEY" -H "Accept: application/json" https://$DOMAIN/api/cost/v2/allocation/summary?window=1d&&aggregate=projectId&aggregate=projectName&filter=projectId:%$PROJECT-ID%22 -G
----

==== Today's total for a particular User

[source,shell]
----
curl -H "X-Domino-Api-Key: $API_KEY" -H "Accept: application/json" https://$DOMAIN/api/cost/v2/allocation/summary?window=7d&shareIdle=true&filter=startingUserId:%$USER-ID%22 -G
----

==== Today's asset costs

[source,shell]
----
curl -H "X-Domino-Api-Key: $API_KEY" -H "Accept: application/json" https://$DOMAIN/api/cost/v2/asset?window=1d -G
----

==== Today's cloud cost

[source,shell]
----
curl -H "X-Domino-Api-Key: $API_KEY" -H "Accept: application/json" https://$DOMAIN/api/cost/v2/cloudCost/accumulated?window=7d&&aggregate=invoiceEntityID -G
----


== API schema

There are a number of field options available with Cost Center APIs. The `window` query parameter is required.

[cols="1a,4a",options="header"]
|===
|Field|Description

|`name`
|Name of each relevant Kubernetes concept described by the allocation, delimited by slashes, e.g. `cluster/node/namespace/pod/container`.

|`properties`
|Map of name-to-value for all relevant property fields, including cluster, node, namespace, controller, controllerKind, pod, container, labels, annotation, etc. Note: Prometheus only supports underscores (`_`) in label names. Dashes (`-`) and dots (`.`), while supported by Kubernetes, will be translated to underscores by Prometheus. This may cause the merging of labels, which could result in aggregated costs being charged to a single label.

|`window`
|Period of time over which the allocation is defined.

|`start`
|Precise starting time of the allocation. By definition must be within the window.

|`end`
|Precise ending time of the allocation. By definition must be within the window.

|`minutes`
|Number of minutes running; i.e. the minutes from start until end.

|`totalCost`
|Total cumulative cost.

|`rawAllocationOnly`
|Object with fields `cpuCoreUsageMax` and `ramByteUsageMax`, which are the maximum usages in the window for the Allocation. If the Allocation query is aggregated or accumulated, this object will be null because the meaning of maximum is ambiguous in these situations. Consider aggregating by namespace: should the maximum be the maximum of each Allocation individually, or the maximum combined usage of all Allocations (at any point in time in the window) in the namespace?
|===

== Next steps

See other ways Domino Cost Center can help you optimize the performance of your projects while managing cloud expenditures:

* Prevent overspending on compute and storage by link:06e4c2[setting budgets and sending alerts] to team members approaching their limits.
* Optimize link:bc084f[Dataset storage usage] to reduce cloud storage costs.
----- admin_guide/operations/finops/4-cost-calculation-methodology.txt -----
:page-permalink: df5806
:page-version: 6.1
:page-title: Cost Calculation Methodology
:page-order: 40

Domino's Cost Center integrates with Kubecost to collect and allocate cost data for your organization. This integration synchronizes hourly cost data and maps it to key Domino entities, including users, projects, and workloads.

== Benefits of using Kubecost

Kubecost provides deep integration with major cloud service providers including AWS, Azure, and GCP. This approach ensures:

* A standardized, transparent cost model across platforms.
* Hourly synchronization with complete billing data from your cloud provider.
* Automatic accounting for pricing discounts negotiated with your CSP.

== Cost allocation to Domino entities
Each hour, Kubecost ingests billing data and attributes it to Domino-specific entities using its allocation engine. These entities include:

* Users
* Projects
* Organizations
* Billing Tags
* Workloads
* Workload Types

This enables you to trace costs directly to specific users, workloads, and teams within Domino.

== Discount handling and cost computation

Compute pricing discounts, covering CPU, GPU, and RAM, are reconciled against the actual billing data from your cloud provider. This ensures accurate reflection of usage-based costs with the following formulas:

* `CPU COST` = `cpuCost` + `cpuCostAdjustment`
* `GPU COST` = `gpuCost` + `gpuCostAdjustment`
* `MEMORY COST` = `ramCost` + `ramCostAdjustment`
* `ALLOC COST` = `totalCost`
* `COMPUTE COST` = `CPU COST` + `GPU COST`
* `STORAGE COST` = `ALLOC COST` - `COMPUTE COST`

Adjustments in each formula represent discounts or credits applied by the CSP.

== Cost representation in the Domino UI

Kubecost also tracks the total cloud billing amount. In Domino,  this is surfaced in the Cost Center's *Total Cost* top-bar metric. Any cloud costs that cannot be mapped to compute or storage usage (such as shared infrastructure expenses) appear under *Cloud Services*.

== Data retention and reconciliation

Cost data is retained for up to one year. Be aware that recent cost data, usually from the past 36-48 hours, may be incomplete due to delays in cloud provider reporting. This data is automatically reconciled once full billing details become available.

== Next Steps

* Prevent overspending by link:06e4c2[setting budgets and sending alerts] to stakeholders, administrators, and team members approaching their limits.
* Learn more about link:157618[automatic allocation of usage-based costs].
* Reduce cloud storage costs by optimizing link:bc084f[dataset storage usage].
----- admin_guide/operations/finops/index.txt -----
:page-version: 6.1
:page-permalink: d4b465
:page-title: Analyze costs with Domino Cost Center
:page-order: 20

Domino's Cost Center dashboard gives administrators clear visibility into cloud costs for your Domino deployment. They can quickly see how resource usage and cost trends over time map to your organization from within Domino.

Administrators can view total expenses and break them down by resource type, such as CPU, GPU, storage, and cloud services. They can also analyze spending by time period and top contributors, including users, projects, organizations, billing tags, workloads, and workload types. This feature helps with both overall cost tracking and detailed analysis of usage patterns. Domino Cost Center helps reduce project costs by:

* link:157618[Automatic allocation of usage-based costs]  to organizational entities (users, projects, etc) and workload.
* Optimization of link:bc084f[dataset storage usage] to reduce cloud storage costs.
* Proactive spend control by link:06e4c2[setting budgets and sending alerts] to stakeholders, administrators, and team members approaching their limits.
* Generation of link:157618[chargeback/showback reports] using APIs to facilitate cost recovery across organizations and engage with high-expenditure users to optimize infrastructure usage.

Domino Cost Center automatically tags all infrastructure usage with essential metadata, associating costs with specific Domino projects, organizations, users, and billing tags, allowing for detailed, usage-based cost allocations.

== Access the Cost Center dashboard
The Cost Center dashboard provides a pre-configured canvas to get you started. Access it from the main Domino page as an Administrator:

. From Domino, go to the top navigation bar.
. Open *Govern* > *Cost center*.
. The Cost Center dashboard loads with the default view.

image::/images/admin-troubleshooting/CostCenterDashboard2.png[alt="cost center dashboard", width=900]

. *Date picker*: Click the date picker in the top left to select a date range, either 7 days, 30 days, 6 months, or a custom range (limited to 1 year).
. *Filter by...* Use the filter controls at the top of the dashboard to narrow your data by category: *Project*, *user*, *billing tag*, or *workload type*.
. *total cost* bar: Shows numerical cost data over the selected date range. Cost is broken down by: *CPU*, *GPU*, *Storage*, *Cloud* services. Note that the cloud costs represent shared infrastructure expenses. This cannot be attributed to specific users, projects, or workloads. Even though they are included in the overall total, cloud costs will not appear in charts and summaries after filters are applied.
. *Total costs by period*: Displays a time-based breakdown of expenditures. Hover over a bar to view detailed costs by resource type.
. *Top* contributors: Scroll down to explore top contributors, organized by category: *Top users*, *Top projects*, *Top organizations*, *Top billing tags*, *Top workloads*, or *Top workload types*. Hovering over a bar reveals the exact spend associated with that contributor.

== Next Steps

* Prevent overspending by link:06e4c2[setting budgets and sending alerts] to stakeholders, administrators, and team members approaching their limits.
* Learn more about link:157618[automatic allocation of usage-based costs].
* Reduce cloud storage costs by optimizing link:bc084f[dataset storage usage].
* Learn about link:df5806[how costs are calculated].
----- admin_guide/operations/image-caching-services.txt -----
:page-permalink: 8a3cb4
:page-version: 6.1
:page-title: Updating the Image Caching Service (ICS)
:page-sidebar: Image Caching Service
:page-order: 45

This document describes the process of updating the Domino image caching service (ICS) with a new compute environment image.

== Image caching service (ICS)

image::/images/6.1/admin-guide/image-caching-service-architecture.png[alt="Image Caching Service Architecture", width=1200, role=noshadow]

ICS has two primary components: the API Service and the Agent.

The ICS Agent runs on each compute node, monitors disk usage, removes unneeded images and calls the API service to get a recommended list of images to cache. It then proactively attempts to download and load these images, within certain constraints.

The ICS API Service provides a list of recommended images to cache. This is built off a base list of images to always cache or never cache, that the administrators can specify. On top of this, a list of images the API Service believes are likely to be used is added, in a priority order. It queries the Domino MongoDB collections to find out the top images used within the last two weeks. The base cache list and other metadata are stored in the Domino Mongo database.

The agent runs under certain constraints:

* It will not download images if the amount of disk space allotted for the cache has been used (10% of the compute node disk).
* It will not download images if the node it is on is currently executing an image pull or a run.
* It will not download images if the cpu is being used passed a certain point. (Docker images are CPU intensive to load).


When the agent runs, pruning occurs before caching.

== ICS update instructions

Here are the steps to modify the list of images used by the image caching service.

. Using `kubectl`, find a running `image-cache-api` pod:
+
[source,bash]
----
image-cache-api-7748fdf4d-8s2tg                              1/1     Running            0               31m
image-cache-api-7748fdf4d-kdbhq                              1/1     Running            0               31m
image-cache-api-7748fdf4d-s42gj                              1/1     Running            0               31m
----
+
. `exec` into one of these pods:
+
[source,bash]
----
kubectl exec -it -n domino-platform image-cache-api-7748fdf4d-8s2tg -- bash
----
+
. Within the pod, verify connectivity to the image caching API endpoint:
+
[source,bash]
----
bash-5.2$ curl -u agent:$ICS_API_TOKEN localhost:5000/base_list
{"base_list":[{"cache":"always","image":"670838783587.dkr.ecr.us-west-2.amazonaws.com/domino/domino-standard-environment:ubuntu22-py3.10-r4.4-domino6.0-standard"}],"status":"Success","tier":"default"}
----
+
The $ICS_API_TOKEN environment variable is pre-populated within the pod and doesn't need to be set.
+
The base list shows the images that should always be in the ICS cache.

. Check the current image cache:
+
[source,bash]
----
bash-5.2$ curl -u agent:$ICS_API_TOKEN localhost:5000/image_cache
{"cache_list":[{"cache":"always","image":"670838783587.dkr.ecr.us-west-2.amazonaws.com/domino/domino-standard-environment:ubuntu22-py3.10-r4.4-domino6.0-standard"},{"cache":"preferred","image":"670838783587.dkr.ecr.us-west-2.amazonaws.com/domino/domino-core-environment:ubuntu22-py3.10-domino6.0-core"},{"cache":"preferred","image":"quay.io/domino/domino-standard-environment:ubuntu22-py3.10-r4.4-domino5.11-standard"},{"cache":"preferred","image":"quay.io/domino/domino-standard-environment:develop.latest"},{"cache":"preferred","image":"quay.io/domino/field:domino-apro-2024-2025"},{"cache":"preferred","image":"670838783587.dkr.ecr.us-west-2.amazonaws.com/domino/domino-standard-environment:ubuntu22-py3.10-r4.4-domino6.0-standard"}],"generated":"2025-06-10 19:34:57.124511","tiers":[]}
----
+
. Updates can be made by sending a POST to this endpoint:
+
Example: Add the `946429944765.dkr.ecr.us-west-2.amazonaws.com/wgamage73590/environment:67c0728cca19ba5c32ecd109-6` into the ICS. Note `-6` refers to the revision number 6 of the compute environment `946429944765.dkr.ecr.us-west-2.amazonaws.com/wgamage73590/environment:67c0728cca19ba5c32ecd109`
+
image::/images/6.1/admin-guide/cache-compute-environment.png[alt="Cache Compute Environment", width=1200, role=noshadow]
+
[source,bash]
----
curl -u bash-5.2$ curl -u agent:$ICS_API_TOKEN -XPOST localhost:5000/base_list/add -H 'Content-Type: application/json' -d'{"image": {"image": "946429944765.dkr.ecr.us-west-2.amazonaws.com/wgamage73590/environment:67c0728cca19ba5c32ecd109-6", "cache": "always"}}'
{"added":"946429944765.dkr.ecr.us-west-2.amazonaws.com/wgamage73590/environment:67c0728cca19ba5c32ecd109-6","status":"Success"}
----
+
. Checking the base list again should now show this entry:
+
[source,bash]
----
bash-5.2$ curl -u agent:$ICS_API_TOKEN localhost:5000/base_list
{"base_list":[{"cache":"always","image":"670838783587.dkr.ecr.us-west-2.amazonaws.com/domino/domino-standard-environment:ubuntu22-py3.10-r4.4-domino6.0-standard"},{"cache":"always","image":"946429944765.dkr.ecr.us-west-2.amazonaws.com/wgamage73590/environment:67c0728cca19ba5c32ecd109-6"}],"status":"Success","tier":"default"}
----
+
. Check the current image cache:
+
[source,bash]
----
bash-5.2$ curl -u agent:$ICS_API_TOKEN localhost:5000/image_cache
{"cache_list":[{"cache":"always","image":"670838783587.dkr.ecr.us-west-2.amazonaws.com/domino/domino-standard-environment:ubuntu22-py3.10-r4.4-domino6.0-standard"},{"cache":"always","image":"946429944765.dkr.ecr.us-west-2.amazonaws.com/wgamage73590/environment:67c0728cca19ba5c32ecd109-6"},{"cache":"preferred","image":"670838783587.dkr.ecr.us-west-2.amazonaws.com/domino/domino-core-environment:ubuntu22-py3.10-domino6.0-core"},{"cache":"preferred","image":"quay.io/domino/domino-standard-environment:ubuntu22-py3.10-r4.4-domino5.11-standard"},{"cache":"preferred","image":"quay.io/domino/domino-standard-environment:develop.latest"},{"cache":"preferred","image":"quay.io/domino/field:domino-apro-2024-2025"},{"cache":"preferred","image":"670838783587.dkr.ecr.us-west-2.amazonaws.com/domino/domino-standard-environment:ubuntu22-py3.10-r4.4-domino6.0-standard"}],"generated":"2025-06-10 19:40:07.215065","tiers":[]}
----

== API syntax

[cols="1a,2a",options="header"]
|===
|Path
|Use

|`GET <host>/health`
|Indicates current status; “ok” means ICS is healthy.

|`GET <host>/image_cache`
|Returns a list of images to prune or cache.

|`GET <host>/base_list`
|Takes the parameter `tier`, and returns the currently configured base list for that tier.

|`POST <host>/base_list`
|
Takes the parameter and a json block. The json is stored as the base list for the service. Note that **always** and **never** can be used to require or block a specific image. **preferred** will have no particular effect. Any number of images may be included.

`tier` is optional and will be **default** if unset.

[source,json]
----
{
    "tier": "default",
    "cache_list": [
        {
            "image": <str of the fully qualified image-with-tag>,
            "cache": "always"
        },
        {
            "image": <str of the fully qualified image-with-tag>,
            "cache": "preferred"
        }
    ]
}
----

|`POST <host>/base_list/add`
|
Takes the parameter `tier` and a json block, adding the single image to the base_list. Note that **always** and **never** can be used to require or block a specific image. **preferred** will have no particular effect.

`tier` is optional and will be **default** if unset.

[source,json]
----
{
    "image": {
        "image": <str of the fully qualified image-with-tag>,
        "cache": <str that is one of 'always', 'never', 'preferred'>
    }
}
----

|`POST <host>/base_list/remove`
|Functions exactly the same as `add`, but will remove an image instead.
|===
----- admin_guide/operations/index.txt -----
:page-permalink: 43a1b8
:page-version: 6.1
:page-title: Operations
:page-order: 100

link:d30657[Monitor Domino and infrastructure]::
Monitoring Domino involves tracking several key application metrics to reveal the health of the application and provide advance warning of any issues or failures of Domino components.

link:d4b465[Analyze costs with Domino Cost Center]::
Domino Cost Center empowers organizations to optimize the performance of their AI projects while managing their cloud expenditures.

link:785186[Audit trail and execution logs]::
Domino audit logs let you meet the security and audit compliance needs of your business while also enabling access to the logs that help troubleshoot issues.

link:d5d2ac[Use the Control Center]::
Use the Control Center to see important data about your Domino deployment.

link:8a3cb4[Update the Image Caching Service (ICS)]::
Update the Domino image caching service (ICS) with a new compute environment image.

link:141306[Disaster recovery]::
Domino Data are stored and backed up in AWS, as well as on-premises.

link:72ff13[Backup and restore]::
The Domino backup system unifies all Domino user data (Workbench and Domino app configuration) into a single cron job.

----- admin_guide/operations/monitoring/grafana/access_grafana.txt -----
:page-permalink: 5cb374
:page-version: 6.1
:page-title: Access Grafana
:page-order: 10

Grafana is available at the URL `\https://<your-domino-domain>/grafana`
 or through the Domino Admin UI under *Advanced* > *Grafana Monitoring*.

Out of the box, Grafana should be configured with SSO so that users with a *SysAdmin* role will automatically be logged in to the Grafana UI if they are logged in to Domino.
If they are not currently logged in to Domino or their session has expired, they will see the Grafana login screen.

image:/images/5.10/grafana_login_sso.png[Grafana login with SSO]

Clicking on **Sign in with Domino Credentials** will take the user to the normal Domino login screen where they can log in using their Domino credentials as normal. Once logged in, Domino will redirect back to the Grafana UI.

All Domino SysAdmin users are automatically granted the Grafana Admin role.

== Login when SSO is unavailable

In situations where SSO is not working (for example, Keycloak is unavailable for some reason), there is a backup method to access Grafana with Admin privileges, as detailed below.

. Get the Grafana password:
+
The Grafana Admin password is stored in a secret called `grafana` in the `domino-platform` namespace.
Retrieve the password with `kubectl`:
+
[source,shell]
----
kubectl get secret -n domino-platform grafana -ojsonpath='{.data.admin-password}'| base64 -d; echo
----
+
NOTE: In some cases, namespaces may vary across deployments.

. Navigate to the Grafana login page:

.. Enter the *Email or username* as `grafana`.
.. Enter the password you retrieved above.
.. Click *Log in*.

== Next steps

* link:57dff6[Standard Grafana dashboards]
* link:53f847[Standard Grafana alerts]

----- admin_guide/operations/monitoring/grafana/alerts.txt -----
:page-version: 6.1
:page-title: Standard Grafana alerts
:page-permalink: 53f847
:page-order: 40

Users are advised to configure alerts to their application administrators if the thresholds listed in the link:d30657[Monitoring] are exceeded.
These alerts indicate potential resourcing issues or unusual usage patterns worth investigating.
See link:a9e507[Application logs], Domino Admin application, and the link:d5d2ac[Control Center] to gather additional information.


== Domino-provisioned alerting

Monitoring your systems proactively is crucial. To assist you, we have provisioned a suite of alerts within Grafana, recommended by Domino. These alerts are based on our extensive experience and insights into common issues and crucial performance metrics. They act as an early warning system and provide immediate insights where more in-depth investigations might be necessary.

=== Benefits of Domino-provided alerts

1. **Instant awareness:** These alerts offer real-time snapshots of your system's health and highlight specific components with issues, if any.
2. **Quick debugging:** Every alert is directly linked to the relevant Grafana dashboard, which enables immediate access to detailed metrics whenever an alert is triggered.
3. **Guided troubleshooting:** Each alert is accompanied by a runbook that offers step-by-step guidance to resolve common issues associated with the alert.

=== Domino alerts overview

Domino alerts are integrated into Grafana. For details on how to access and utilize the provided dashboards in Grafana, refer to the documentation on how to link:169f0f[Use Grafana Dashboards].

Within Grafana, navigate to *Alerting* > *Alert rules* to view a list of alerts provided by Domino.

image::/images/5.8/alerts_overview.png[Alerts Overview, role=noshadow]

Alerts in Grafana can be in one of the following three states:

1. **Normal:** +
   *Representation*: Denoted by a green color in Grafana. +
   *Definition*: Indicates all monitored metrics are within acceptable thresholds, and no action is required. +
   *User Action*: Continue regular monitoring; no immediate action needed.

2. **Pending:** +
   *Representation*: Typically depicted by a yellow or other neutral color. +
   *Definition*: The alert condition has been met, but not for a sufficiently prolonged period to decisively activate the alert. It signals a possible deviation in the metric. +
   *User Action*: Review the alert and identify and rectify any anomalies early. Our alerts are structured to accommodate the dynamic nature of Kubernetes clusters, allowing adjustments before alert state transitions. Observing alerts in “Pending” during such modifications is routine and requires discernment to differentiate between standard adjustments and genuine system irregularities.

3. **Firing:** +
   *Representation*: Denoted by a red color, indicating a critical condition. +
   *Definition*: The alert condition has persisted beyond the predefined duration, necessitating immediate attention and resolution. +
   *User Action*: Address the issue promptly. Refer to the linked dashboard for an in-depth analysis and the associated runbook for resolution steps.

== Utilize alerts for efficient troubleshooting

Domino Alerts are designed to notify you of potential issues and assist in their diagnosis and resolution, while also providing an overview of platform health.

For example, consider the *Domino Workloads* alert:

image::/images/5.8/workloads-alert.png[Domino Workloads Alert, role=noshadow]

In this instance, a top-level alert is marked as `Pending`. A closer look reveals the alert status across various workload types. It indicates that our Apps and Workspaces are healthy, but there might be an issue with our Domino endpoints.

This alert triggers if, during a 10-minute window, less than 80% of any particular workload type is in the running state. From here, you can access the specific runbook and dashboards for a detailed investigation into the potential issue, assisting in quick resolution and deeper understanding.

== Configure alert routing

Proper configuration of alert notifications is crucial to ensure that your team is promptly informed of any issues. By default, alerts in Grafana will only be visible in the Grafana UI, which may not be sufficient in promptly addressing system issues. Therefore, during installation of Domino, an optional Slack notification route can also be configured using the deployer configuration.

=== Configure contact points

A contact point in Grafana is a designated recipient of alert notifications, such as an email address or a Slack channel. The deployer currently allows provisioning of only a Slack contact point. However, other contact points can be manually added after deployment (see link:#configure-additional-contact-points[Configure additional contact points]).

Due to the way Grafana notification routing works, if a Slack contact point is provisioned by the deployer, it will automatically be set as the default notification route's recipient contact point. Due to the fact that Grafana will not allow the modification of provisioned resources, this has the effect of fixing Slack as the default notification recipient. Consequently, it will not be possible to change it after installation without modifying your deployment configuration and re-running the deployer.

==== Slack contact point

The Slack contact point in Grafana is optional. If enabled through the link:7f4331#_monitoring[deployer configuration], alert notifications for the alerts defined in the `Domino Managed` folder will be sent to the Slack contact point. To enable Slack notifications, configure `grafana_alerts.slack.name`, `grafana_alerts.slack.channel` and `grafana_alerts.slack.token` in the `deploy.yaml` file:

* The contact point name as it appears in the Grafana UI using `grafana_alerts.slack.name`.
* The Slack channel designated to receive alert notifications using `grafana_alerts.slack.channel`.
* A `Bot User OAuth Token` used to authenticate to your Slack workspace using `grafana_alerts.slack.token`. This will start with `xoxb-` and is generated by your Slack admin when adding an app to your Slack workspace to receive the alert notifications from Grafana.

Below is an example YAML file, defining a Slack contact point called `slack_contact_point` and sending notifications to the Slack channel *Domino Alerts*:

```yaml
grafana_alerts:
  slack:
    name: slack_contact_point
    channel: 'Domino Alerts'
    token: 'xoxb-1234567890-123456789012-abcdefghijklmnopqrstuvw'
```

When enabled, the Slack contact point is also set as the default notification route's recipient contact point, meaning that any alerts not caught by other routes will be sent to Slack.

[[configure-additional-contact-points]]
==== Configure additional contact points

After deployment of the Domino platform, it is possible to create additional custom contact points by navigating to *Alerting* > *Contact points* in the Grafana sidebar. Refer to the link:https://grafana.com/docs/grafana/latest/alerting/alerting-rules/manage-contact-points/[Grafana documentation^] for more details and additional contact point options.

Any additional contact points added in this way, may not be carried over with Domino upgrades. Therefore, you need to backup the configuration using the export function available under *More* on the contact point definition in the Grafana UI. Furthermore, you cannot modify Grafana resources provisioned at install time through the Deployer (these are marked with the `provisioned` tag in the Grafana UI), so any additional contact points will need to be used in custom notification policies and not in the default provisioned ones.

Note that if an Email contact point is defined, SMTP settings also need to be enabled as described below.

**SMTP settings**

To enable SMTP email notifications, the `grafana.ini` file in the Kubernetes config map needs modification:

```sh
kubectl edit cm grafana -n domino-platform
```

Your `grafana.ini` section should resemble the following:

```yaml
[smtp]
enabled = true
host = smtp.example.com:587
user = myuser
password = mypassword
;cert_file =
;key_file =
skip_verify = false
from_address = admin@example.com
from_name = Grafana
```

Replace the relevant values with your SMTP server details.

=== Notification policies in Grafana

Notification policies manage how and when your notifications should be sent, allowing customization based on alert severity and frequency. The default (root) notification policy waits 30 seconds before sending notifications in order to group them if possible, and waits 5 minutes to send out status changes. Alerts are grouped by the folder name that the alert is created in and the alert name. Any alerts not caught by a child route, are sent to the email contact point by default.

The Deployer provisions at least one child notification policy that sends notifications for any alerts defined in the `Domino Managed` folder to the email contact point. If the Slack contact point is enabled, a similar child notification policy is provisioned to also send alerts to the Slack contact point.

==== Configured additional notification policies

Additional notification policies can be created under the default provisioned root policy. To create a notification policy, navigate to *Alerting* > *Notification Policies* in the Grafana sidebar and modify the contact point to the additional custom policy/policies you created above. Refer to the link:https://grafana.com/docs/grafana/latest/alerting/configure-notifications/create-notification-policy/[Grafana documentation^] for more details and additional notification policy options.

Just as additional custom contact points are not carried over with Domino upgrades, any additional custom notification policies are not either. These will need to be exported as YAML files and re-imported once the upgrade is completed.

== Next steps

* link:917ff5[Customize Grafana alerts]

----- admin_guide/operations/monitoring/grafana/customizing_alerts.txt -----
:page-permalink: 917ff5
:page-version: 6.1
:page-title: Customize Grafana alerts
:page-order: 50

Similarly to customizing dashboards, the standard alerts provided by Domino are immutable. In order to customize an alert, you need to create a copy of the alert and make your changes to the copy. You can add your own new custom alerts and edit them without restriction. Customized alerts will not be overwritten or lost during upgrades of the Domino platform, as long as the Grafana persistent volume `grafana-grafana-0` is not deleted.

Full documentation on all aspects of alerting with Grafana can be found in the https://grafana.com/docs/grafana/latest/alerting/[Grafana documentation^].

To modify an existing alert, log in to Grafana and follow these steps:

. On the left navigation pane, navigate to the alert you want to customize and click **Alerting**, then click on the https://grafana.com/docs/grafana/latest/alerting/fundamentals/alert-rule-evaluation/#evaluation-group[evaluation group^] name containing the alert you wish to edit.
+
The Domino provisioned alerts will show the **Provisioned** graphic next to them.
+
image::/images/6.1/admin-guide/operations/monitoring/grafana/edit_alerts/select_alert.png[alt="Select alert", width=1200, role="noshadow"]

. Click on the **More** button to the right of the row containing the alert you want to customize and then select **Duplicate**.
+
image::/images/6.1/admin-guide/operations/monitoring/grafana/edit_alerts/duplicate.png[alt="Duplicate alert", width=200, role="noshadow"]

. You will be shown the **Copy provisioned alert rule** dialogue box with an explanation that the copied alert will not be marked as provisioned and that it will require a new evaluation group because the original evaluation group is also provisioned and therefore immutable.
+
Click **Copy** to continue.
+
image::/images/6.1/admin-guide/operations/monitoring/grafana/edit_alerts/copy_provisioned.png[alt="Copy provisioned alert", width=400, role="noshadow"]

. Next you will see the **New alert rule** page. Here you can make changes to the alert rule as required. See https://grafana.com/docs/grafana/latest/alerting/fundamentals/alert-rules/[Alert Rules^] in the Grafana documentation for more information.
+
The key thing to edit here is in the **Set evaluation behavior** section where you need to either create a new evaluation group using the **New evaluation group** button (see the https://grafana.com/docs/grafana/latest/alerting/fundamentals/alert-rule-evaluation/[Grafana documentation^] for more details), or select an existing non-provisioned one using the **Select an evaluation group...** dropdown.
+
Click **Save rule and exit** at the top right of the page after you have made your changes.
+
image::/images/6.1/admin-guide/operations/monitoring/grafana/edit_alerts/set_evaluation_behaviour.png[alt="Set behaviour evaluation section", width=600, role="noshadow"]

Your rule will now be saved and active. You can edit it at any time by navigating to the alert in your new evaluation group and clicking on the **Edit** button.

----- admin_guide/operations/monitoring/grafana/customizing_dashboards.txt -----
:page-permalink: 3e7324
:page-version: 6.1
:page-title: Customize Grafana dashboards
:page-order: 30

The standard dashboards provided by Domino are, by default, immutable. To customize a dashboard, you need to create a copy of the dashboard and make your changes to the copy. This will allow you to make changes to the dashboard without affecting the original. You can add your own new custom dashboards without restriction. Customized dashboards will not be overwritten or lost during upgrades of the Domino platform, as long as the Grafana persistent volume `grafana-grafana-0` is not deleted.

Full documentation on all aspects of Grafana dashboards can be found in the https://grafana.com/docs/grafana/latest/dashboards/[Grafana documentation^].

To modify an existing dashboard, log in to Grafana and follow these steps:

. On the left navigation pane, navigate to the dashboard you want to customize by clicking **Dashboards**, then click on the dashboard name in the list of dashboards.
+
image::/images/6.1/admin-guide/operations/monitoring/grafana/edit_dashboard/select_dashboard.png[Dashboard list, 600 , role="noshadow"]

. Click the **Make editable** button in the top right corner of the dashboard.

. Click the down arrow on the **Save dashboard** button and select **Save as copy**. If you accidentally click **Save** instead of **Save as copy**, you will be told that you cannot save a provisioned dashboard and given the option to cancel, copy the dashboard JSON to the clipboard, or save the JSON to a local file.

. Enter a new name for the copied dashboard and click **Save**. You will now be viewing the copied dashboard.

. Click the **Edit** button in the top right corner to start customizing the copy.
+
Once you have finished editing the dashboard, make sure to click the **Save dashboard** button to save your changes. Once a dashboard has been made editable, you can modify it as many times as you like.

== Next steps

* link:53f847[Standard Grafana alerts]

----- admin_guide/operations/monitoring/grafana/dashboards.txt -----
:page-permalink: 57dff6
:page-version: 6.1
:page-title: Standard dashboards
:page-order: 20

The following selection of dashboards are provided with Domino to help you monitor the health of the Domino platform and the underlying Kubernetes cluster and infrastructure. There are other useful dashboards available in Grafana that aren't listed here. They can all be used as-is or modified to suit your specific requirements. Note that in some cases not all panels on the dashboards are shown in the examples below for brevity.

== Domino / Views / Workloads

.Example of the Domino / Views / Workloads dashboard
[%collapsible]
====

image::/images/6.1/admin-guide/operations/monitoring/grafana/dashboards/workload_executions_dashboard.png[alt="Domino / Views / Workloads dashboard", width=1000, role=noshadow]

====

The *Domino / Views / Workloads* dashboard shows the status of all workloads running on the Domino platform. It includes information about the number of workloads in each state, the number of workloads that have failed, and the number of workloads that are currently running. You can use this dashboard to help understand the overall utilization of the Domino platform and to identify any workloads that may be stuck or failing.

*Current Active Domino Executions* |
*Active Executions By Workload Type* |
*Executions Running*

image::/images/6.1/admin-guide/operations/monitoring/grafana/dashboards/workload_executions_dashboard_counts.png[alt="Domino / Views / Workloads dashboard counts", width=300, role="noshadow"]

These three panels, in order, show:

* The number of workloads of all types that are currently running on the Domino platform.
* A breakdown of the workloads by type.
* The ratio of workloads of all types in the running versus non-running state.

*Workload Pod Count By Pod Phase*

image::/images/6.1/admin-guide/operations/monitoring/grafana/dashboards/workload_executions_dashboard_pod_phase.png[alt="Domino / Views / Workloads dashboard pod phase", width=400 role="noshadow"]

This panel shows the number of pods in each phase of the Kubernetes pod lifecycle for all workloads on the Domino platform. The phases are `Pending`, `Running`, `Succeeded`, and `Failed`. If many pods are in the `Pending` state, it might indicate that the Kubernetes cluster is under-provisioned. Lots of pods in the `Failed` state can indicate a problem with the workload itself or with the underlying infrastructure.

*Pod Count of Executions in Running Phase by Workload Type* |
*Pod Count of Executions in Running Phase by Hardware Tier*

image::/images/6.1/admin-guide/operations/monitoring/grafana/dashboards/workload_executions_dashboard_running_by_type_and_tier.png[alt="Domino / Views / Workloads dashboard running by type and tier", width=600, role="noshadow"]

These two panels show the number of workload pods in the `Running` phase for each workload type and hardware tier. The first panel gives an indication of the distribution of workloads across different types, while the second panel shows how the workloads are distributed across different hardware tiers.

*Pod Count of Executions in Pending Phase by Workload Type* |
*Pod Count of Executions in Pending Phase by Hardware Tier*

image::/images/6.1/admin-guide/operations/monitoring/grafana/dashboards/workload_executions_dashboard_pending_by_type_and_tier.png[alt="Domino / Views / Workloads dashboard pending by type and tier", width=600, role="noshadow"]

Similarly to the previous two panels showing workload pods in the `Running` phase, these two panels show the number of workload pods in the `Pending` phase for each workload type and hardware tier. This can help you understand the distribution of workloads that are waiting to be scheduled and detect problems with starting particular workload types, or issues with certain hardware tiers. On busy deployments a workload pod will need to wait for the cluster autoscaler to provision a node before it can be scheduled, so many pods in the `Pending` state can indicate that the cluster is under-provisioned.

*Workload Startup Time By User* |
*Workload Startup Time Filter by User*

image::/images/6.1/admin-guide/operations/monitoring/grafana/dashboards/workload_executions_dashboard_startup_time.png[alt="Domino / Views / Workloads dashboard startup time", width=400, role="noshadow"]

The next couple of panels show the average startup time of workloads on the Domino platform, broken down by user, and the start up times for all workload pods for a particular user as selected using the filter at the top of the dashboard.

image::/images/6.1/admin-guide/operations/monitoring/grafana/dashboards/workload_executions_dashboard_user_filter.png[alt="Workload executions dashboard user filter", width=150]

This can help you identify if slow startup times are affecting a particular user or group of users, and whether it is a recent issue or a long-standing one.


=== Domino Execution Overview

.Example of the Domino Execution Overview dashboard
[%collapsible]

====

image::/images/6.1/admin-guide/operations/monitoring/grafana/dashboards/domino_execution_overview_dashboard.png[alt="Domino Execution Overview dashboard overview", width=1000, role="noshadow"]

====

The *Domino Execution Overview* dashboard, like the *Domino / Views / Workloads* dashboard, provides an overview of the workloads running on the Domino platform. However, it provides more granular detail, including some new Domino metrics, rather than relying on Kubernetes metrics that don't know anything about the application running on top of the cluster.

*Active Executions (by status)* |
*Active Executions (by Execution Type)* |
*Active Executions (by Hardware Tier)*

image::/images/6.1/admin-guide/operations/monitoring/grafana/dashboards/domino_execution_overview_dashboard_executions_by.png[alt="Domino Execution Overview executions by...", width=400, role="noshadow"]

These three panels show the number of active executions on the Domino platform broken down by status, execution type, and hardware tier. This can help you understand the distribution of workloads across types and hardware tiers and spot potential bottlenecks or issues with particular types of workloads.

*Execution Success Rate* |
*Execution Failures (by reason)* |
*Workloads Launched (by Execution type)*

image::/images/6.1/admin-guide/operations/monitoring/grafana/dashboards/domino_execution_overview_dashboard_success_rate.png[alt="Domino Execution Overview success rate", width=400, role="noshadow"]

The *Execution Success Rate* panel shows the percentage of successful executions. An execution can fail for many reasons, not all of which are related to platform health. To determine which execution failures might be a platform issue, the *Execution Failures (by reason)* panel breaks down the failures by specific cause which boils down to whether or not the failure was a system error or a user error.

The *Workloads Launched (by Execution type)* panel shows the number of workloads launched by type over time.

*Node Pool Size*

image::/images/6.1/admin-guide/operations/monitoring/grafana/dashboards/domino_execution_overview_dashboard_node_pool_size.png[alt="Domino Execution Overview node pool size", width=400, role="noshadow"]

The *Node Pool Size* panel shows the number of nodes in each node pool over time. This panel helps you assess and adjust node pool configurations to improve job assignment times and overall cluster performance.

*Compute Environment Pull Statistics*

image::/images/6.1/admin-guide/operations/monitoring/grafana/dashboards/domino_execution_overview_dashboard_pull_stats.png[alt="Domino Execution Overview compute environment pull statistics", width=400, role="noshadow"]

The *Compute Environment Pull Statistics* panel shows the number of times each compute environment image has been pulled and the average maximum time taken for the pulls. This can help you understand the performance of the image pull process and identify any images that are taking a long time to pull. Factors that can influence image pull time are the size of the image, the speed of the network connection, and whether the image is already cached on the node it is required on.

== NGINX Ingress Controller

.Example of the NGINX Ingress Controller dashboard
[%collapsible]
====

image::/images/6.1/admin-guide/operations/monitoring/grafana/dashboards/nginx_ingress_controller.png[alt="NGINX Ingress Controller dashboard", width=1000, role=noshadow]

====

The *NGINX Ingress Controller* dashboard provides an overview of the NGINX Ingress Controller that is used to route traffic to the various services in the Domino platform. The dashboard is divided into several sections that provide information on the health of the NGINX Ingress Controller, the number of requests it is handling, the response times and status of those requests, and the latency of the various routes handled by the controller.

*Dashboard filters*

image::/images/6.1/admin-guide/operations/monitoring/grafana/dashboards/nginx_ingress_controller_filters.png[alt="NGINX Ingress Controller dashboard filters", width=600, role="noshadow"]

The dashboard includes a number of filters that let you select the namespace, ingress class, ingress name, and ingress route that you want to view metrics for. This can be useful if you have multiple ingresses in your deployment and want to focus on a specific one, or to inspect specific routes through the ingress controller.

The `Config Reloads` filter, when selected, displays metrics across all configuration changes for the selected controller. Deselecting this only shows metrics since the last configuration change.

*Controller Request Volume* |
*Controller Connections*

image::/images/6.1/admin-guide/operations/monitoring/grafana/dashboards/nginx_ingress_controller_requests.png[alt="NGINX Ingress Controller dashboard requests", width=500, role="noshadow"]

These two panels show the number of requests and new connections that the NGINX Ingress Controller is handling over time. The *Controller Request Volume* panel shows the number of requests per second, while the *Controller Connections* panel shows the number of connections to the controller. These can be useful for understanding the load coming in to the Domino platform. If the number of requests is high, it might indicate that the platform is under heavy load, while a high number of connections might indicate that the platform is under attack. A breakdown of which routes are receiving the most traffic can be found in the *Ingress Request Volume* panel.

*Ingress Request Volume*

image::/images/6.1/admin-guide/operations/monitoring/grafana/dashboards/nginx_ingress_controller_ingress_requests.png[alt="NGINX Ingress Controller dashboard ingress requests", width=400, role="noshadow"]

The *Ingress Request Volume* panel shows the number of requests that the NGINX Ingress Controller is handling for each route. This can help you understand which routes are receiving the most traffic and might need to be scaled up or down. If a particular route is receiving a lot of traffic, it might be a good idea to check the health of the service behind that route to ensure that it can handle the load.

*Controller Success Rate*

image::/images/6.1/admin-guide/operations/monitoring/grafana/dashboards/nginx_ingress_controller_success_rate.png[alt="NGINX Ingress Controller dashboard success rate", width=400, role="noshadow"]

The percentage of successful requests that the NGINX Ingress Controller is handling is shown in the *Controller Success Rate* panel. A high success rate is generally a good sign, but a low success rate might indicate that there are problems with the services behind the controller, or that the controller itself is having issues. The `Ingress Success Rate` panel shows the success rate per route for more detailed investigation.

*Ingress Success Rate*

image::/images/6.1/admin-guide/operations/monitoring/grafana/dashboards/nginx_ingress_controller_ingress_success_rate.png[alt="NGINX Ingress Controller dashboard ingress success rate", width=400, role="noshadow"]

The *Ingress Success Rate* panel shows the success rate of requests for each route handled by the NGINX Ingress Controller. This can help you understand which routes are experiencing problems and might need to be investigated further.

*Network I/O pressure* |
*Average Memory Usage* |
*Average CPU Usage*

image::/images/6.1/admin-guide/operations/monitoring/grafana/dashboards/nginx_ingress_controller_resources.png[alt="NGINX Ingress Controller dashboard resource usage", width=600, role="noshadow"]

These three panels show the network I/O bandwidth usage, average memory usage, and average CPU usage (measured in CPU cores) of the NGINX Ingress Controller over time. These can be useful for understanding the resource usage of the controller and whether it is under-provisioned or over-provisioned. If the network I/O is high, it might indicate that the controller is under heavy load, while high memory or CPU usage might indicate that the controller is under-provisioned.

*Ingress Percentile Response Times and Transfer Rates*

image::/images/6.1/admin-guide/operations/monitoring/grafana/dashboards/nginx_ingress_controller_response_times.png[alt="NGINX Ingress Controller dashboard response times", width=600, role="noshadow"]

The *Ingress Percentile Response Times and Transfer Rates* panel shows the 50th, 90th, and 99th percentile response times for requests to the NGINX Ingress Controller, as well as the transfer rate of data through the controller for each route. This can help you understand where the bottlenecks are in the Domino platform and which routes might need to be optimized for better performance.

== Kubernetes / System / Cluster Autoscaler

.Example of the Kubernetes / System / Cluster Autoscaler dashboard
[%collapsible]
====

image::/images/6.1/admin-guide/operations/monitoring/grafana/dashboards/cluster_autoscaler_dashboard.png[alt="Kubernetes / System / Cluster Autoscaler dashboard", width=1000, role=noshadow]

====

The *Kubernetes / System / Cluster Autoscaler* dashboard provides an overview of the Kubernetes Cluster Autoscaler that is used to automatically scale the number of nodes in a Kubernetes cluster based on the resource usage of the workloads running on the cluster. The dashboard is divided into several sections that provide information on:

* The current health of the autoscaler.
* The number of nodes in the cluster, including a comparison to the previous week.
* A timeline of the scaling decisions made by the autoscaler.
* The number of needed, unneeded, and unremovable nodes.
* A histogram showing the number of nodes in different phases of the Kubernetes node life-cycle.
* The number of evicted and unschedulable pods.
* AWS autoscaling request durations.
* Total CPU and memory usage of the total maximum available in the cluster.

A more detailed description of the workings of the Kubernetes Cluster Autoscaler can be found in the https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/README.md[official Kubernetes documentation^].

*Cluster Autoscaler - Health Status* |
*Autoscaling direction* |
*Last scaleDown check* |
*Last scaleUp check* |
*Cluster Autoscaler - Pod Status* |
*Cluster able to scale?* |
*# of unschedulable pods*

image::/images/6.1/admin-guide/operations/monitoring/grafana/dashboards/cluster_autoscaler_dashboard_health.png[alt="Kubernetes / System / Cluster Autoscaler dashboard health", width=300, role="noshadow"]

These panels show the current health of the Cluster Autoscaler:

*Cluster Autoscaler - Health Status* is a visual indication of whether the autoscaler pod in the deployment is `UP`.

*Autoscaling direction* shows the direction of the last autoscaling action (`Up`, `Down`, or `Idle`).

*Last scaleDown check* and *Last scaleUp check* show the time of the last check for scaling down and up, respectively. If the time is too long ago, it might indicate that the autoscaler is stuck.

*Cluster Autoscaler - Pod Status* shows the status of the autoscaler pod.

*Cluster able to scale?* shows whether the cluster is able to scale up or down. The autoscaler will not scale the cluster if it detects an issue with the autoscaling groups, or if it has recently scaled up; at which point it enters a cooldown state for a short period of time.

*# of unschedulable pods* shows the number of pods that are currently unschedulable. If this number starts to go up then it can indicate that the cluster is unable to scale up for some reason such as lack of instances in an autoscaling group, or problems with an AWS Availability Zone.

*Nodes reporting ready* |
*Node counts* |
*Now* |
*This time last week*

image::/images/6.1/admin-guide/operations/monitoring/grafana/dashboards/cluster_autoscaler_dashboard_node_count.png[alt="Kubernetes / System / Cluster Autoscaler dashboard node counts", width=600, role="noshadow"]

The *Nodes reporting ready* panel shows the ratio of nodes in the cluster that are currently reporting as ready versus not ready. This should generally be close to 100% most of the time except for short periods when a new node is spinning up as this operation may take some time.

The *Node counts* panel shows the current total number of nodes in the cluster and the current number of `unneeded` nodes. If the number of unneeded nodes is high, it might indicate that the cluster autoscaler is taking too long to scale down the cluster, or that a continuous stream of new workloads in one hardware tier is causing scale-ups, which then puts the autoscaler into a cooldown state, meaning it can't scale down other hardware tiers.

The *Now* and *This time last week* panels show the number of nodes in the cluster over the requested period and a comparison to the same time period one week previously. This can be useful for understanding if the current cluster usage has grown or shrunk over time.

*Cluster Autoscaler Status Timeline*

image::/images/6.1/admin-guide/operations/monitoring/grafana/dashboards/cluster_autoscaler_dashboard_autoscaling_decisions.png[alt="Kubernetes / System / Cluster Autoscaler dashboard timeline", width=1000, role="noshadow"]

This panel shows a timeline of the scaling decisions made by the autoscaler over time. This can be useful for understanding when the autoscaler has scaled the cluster up or down and why.

Green in the `Safe to scale?` line indicates that the autoscaler considers it safe to scale the cluster at that time. Red indicates that it is not safe to scale the cluster.

`Scale up/down requests` shows the scale up/down requests with blue indicating a scale-up and purple a scale-down event.

The `Scaledown in cooldown?` line shows when the autoscaler is in a cooldown state and unable to scale down the cluster. This usually occurs after a scale-up event.

Finally, the `Unneeded nodes present?` line shows when the autoscaler has detected that there are unneeded nodes in the cluster that can be removed by scaling down.

*Node counts*

image::/images/6.1/admin-guide/operations/monitoring/grafana/dashboards/cluster_autoscaler_dashboard_node_necessity.png[alt="Kubernetes / System / Cluster Autoscaler dashboard node counts", width=1000, role="noshadow"]

This panel shows the total number of nodes in green, the number of nodes that the autoscaler thinks are not needed and could be scaled down in yellow, and the number of nodes that are unremovable in light blue.

*Node states over time*

image::/images/6.1/admin-guide/operations/monitoring/grafana/dashboards/cluster_autoscaler_dashboard_node_states.png[alt="Kubernetes / System / Cluster Autoscaler dashboard node states", width=1000, role="noshadow"]

The *Node states over time* panel shows the number of nodes in the cluster in the various states of the Kubernetes node lifecycle, which are `longUnregistered`, `notStarted`, `ready`, `unready`, and `unregistered`. This can be useful for understanding the health of the nodes in the cluster and whether there are any issues with start-up or shutdown of nodes.

If there are many nodes in the `longUnregistered` state, it might indicate that there is a problem with the autoscaling group or the node itself not shutting down after it is unregistered. This can happen if it doesn't properly release AWS resources assigned to it.

A large number of nodes in the `unready` state can indicate that there is a problem with the instance template used to create the node, or that the node is having trouble starting up.

*Evicted and Unschedulable pods*

image::/images/6.1/admin-guide/operations/monitoring/grafana/dashboards/cluster_autoscaler_dashboard_evicted_and_unschedulable.png[alt="Kubernetes / System / Cluster Autoscaler dashboard evicted and unschedulable pods", width=1000, role="noshadow"]

The *Evicted and Unschedulable pods* panel shows the number of pods that have been evicted from nodes in the cluster and the number of pods that are currently unschedulable.

If the number of evicted pods is high, it usually indicates that there is a problem with a pod consuming all the resources on a node.

It is normal for pods to be in the unschedulable state for a short period of time if there are no nodes available for them to fit on and the autoscaler needs to scale up a new node. This can take some time to start and register with the cluster. The pod should move to `pending` once the new node registers.

*Autoscaler AWS Average Request Duration* |
*Autoscaler AWS Request Duration: 95th Percentile*

image::/images/6.1/admin-guide/operations/monitoring/grafana/dashboards/cluster_autoscaler_dashboard_aws_requests.png[alt="Kubernetes / System / Cluster Autoscaler dashboard AWS request duration", width=1000, role="noshadow"]

The *Autoscaler AWS Average Request Duration* and *Autoscaler AWS Request Duration: 95th Percentile* panels show the average and 95th percentile request duration for AWS autoscaling requests. This can be useful for understanding how long it takes AWS to respond to scaling requests. If the duration is high, it might indicate that there is a problem with the AWS Availability Zone that the cluster is in, or that there is a problem with the autoscaling group itself.

*Cluster CPU Limit (cores)* |
*Cluster Memory Limit (GiB)*

image::/images/6.1/admin-guide/operations/monitoring/grafana/dashboards/cluster_autoscaler_dashboard_resource_usage.png[alt="Kubernetes / System / Cluster Autoscaler dashboard resource usage", width=1000, role="noshadow"]

The *Cluster CPU Limit (cores)* and *Cluster Memory Limit (GiB)* panels show the total CPU and memory usage of the cluster over time, compared to the total available according to the size of the nodes added to the cluster by the autoscaler. This can be useful for understanding the resource usage of the cluster and whether it is under-provisioned or over-provisioned.

If the CPU or memory usage keeps reaching the limit for extended periods of time, it might indicate that the cluster is under heavy load, while low usage might indicate that the cluster is over-provisioned.

== Kubernetes / Views / ... and Kubernetes / System / ... dashboards

The *Kubernetes / Views / ...* and *Kubernetes / System / ...* dashboards provide a detailed look at the state of various Kubernetes resources in the cluster. These dashboards can be useful for understanding which parts of the Domino deployment are consuming the most resources, which parts are under-provisioned, and which parts are over-provisioned. They can also be useful for understanding the health of the Kubernetes cluster itself and whether there are any issues with the underlying infrastructure.

*Kubernetes / System / API Server* shows the status of the Kubernetes API servers. The API servers are rarely a problem in most deployments, however, in a multi-tenant Kubernetes environment, you could have a 'noisy neighbor' which you would not know about if you do not have access to the monitoring for that application; monitoring the API Server could help with diagnosing this situation. The API server and related components are described fully in the https://kubernetes.io/docs/concepts/architecture/[official Kubernetes documentation^].

`Kubernetes / System / CoreDNS` shows the status of the CoreDNS pods in the cluster. CoreDNS is the DNS server used by Kubernetes to resolve service names to IP addresses. If CoreDNS is having problems, it can cause issues with service discovery in the cluster. A detailed explanation of CoreDNS can be found in the https://kubernetes.io/docs/tasks/administer-cluster/coredns/[official Kubernetes documentation^].

`Kubernetes / Views / Namespaces` displays resource usage in the cluster by namespace. This allows you to easily separate resource usage of the Domino platform components from the Domino compute workloads.

`Kubernetes / Views / Nodes` shows the status of the nodes in the cluster, which is useful for understanding which nodes are under heavy load.

`Kubernetes / Views / Pods` shows the status of the pods in the cluster. This can be useful for understanding which pods are consuming the most resources or keep restarting.

`Kubernetes / Views / Persistent Volumes` shows the status of the persistent volumes in the cluster. There are a few services that require persistent storage and monitoring, so this is crucial to ensure that the services are running correctly.

== Next steps

* link:3e7324[Customize Grafana dashboards]

----- admin_guide/operations/monitoring/grafana/index.txt -----
:page-version: 6.1
:page-title: Grafana
:page-permalink: 169f0f
:page-order: 10

Domino comes with a pre-configured Grafana instance that provides visibility into the health of the Domino platform and the underlying Kubernetes cluster and infrastructure. It provides a set of basic dashboards and alerts that can be used as-is or customized to suit your specific requirements.

This section provides details on how to access Grafana, an overview of the provided dashboards and alerts, and a basic guide to customizing them. More detailed information on Grafana can be found in the https://grafana.com/docs/grafana/latest/[Grafana documentation^].

== Next steps

For more information about Grafana, have a look at these topics:

* link:5cb374[Access Grafana]
* link:57dff6[Standard Grafana dashboards]
* link:3e7324[Customize Grafana dashboards]
* link:53f847[Standard Grafana alerts]
* link:917ff5[Customize Grafana alerts]

----- admin_guide/operations/monitoring/index.txt -----
:page-version: 6.1
:page-title: Monitoring and alerting
:page-permalink: d30657
:page-order: 10

== Overview

Domino is distributed as a set of containerized https://kubernetes.io/[Kubernetes^] native applications. This means that monitoring Domino involves not only tracking the health of the Domino platform as a whole but also the individual components, the underlying Kubernetes cluster and the infrastructure that cluster runs on. Domino Administrators can use this section of the Admin Guide to understand the key metrics to monitor and the suggested thresholds for alerting. This is not a definitive guide as every environment and deployment is unique; but it should act as a starting point for understanding what to monitor and how to set up alerts to suit your particular requirements.

== Monitoring and alerting components

Domino deployments include several pre-configured components to facilitate monitoring and alerting:

https://prometheus.io/[Prometheus^]::
For collecting and storing metrics locally. Prometheus includes a number of collectors configured to scrape metrics from the various components of the Domino platform.

https://grafana.com/[Grafana^]::
For visualizing metrics and providing alerting features. Grafana is deployed with a number of dashboards that provide visibility into the health of the Domino platform as well as several pre-configured alerts.

https://fluentbit.io/[Fluent Bit^]::
Used for collecting logs from individual components within the Domino platform.

https://www.fluentd.org/[Fluentd^]::
Logs collected by Fluent Bit are aggregated by Fluentd and forwarded on to long term storage such as New Relic, S3 and Elasticsearch.

Currently it is also possible to deploy https://newrelic.com/[New Relic^] components for collecting and storing metrics remotely to facilitate remote monitoring and alerting by Domino Data Lab. Things to note:

* New Relic needs to be specifically enabled in your Domino deployment.
* The New Relic agents are pre-configured to send metrics, APM data and logs to a sub-account of Domino Data Lab's New Relic account that's unique to your deployment.
* This New Relic monitoring is intended solely for use by Domino Data Lab to monitor your deployment remotely and it is not possible to allow 3rd party access to New Relic.
* See link:61791d[Monitor using New Relic] for more detail.

== 3rd party monitoring tools

For local monitoring you can also use several other monitoring tools in addition to, or in replacement of, the pre-configured Grafana instance deployed with Domino to track these metrics, including:

* Exporting to an external Prometheus server.
* Connecting a pre-existing Grafana instance to the Domino platform's Prometheus server. 
* Using your own https://newrelic.com/[New Relic^] account.
* https://docs.splunk.com/Documentation/InfraApp/2.0.2/Admin/AddDataKubernetes[Splunk^]
* https://www.datadoghq.com/blog/how-to-collect-and-graph-kubernetes-metrics/[Datadog^]

Please speak to your Domino Account Manager to discuss these options.

== Domino architecture from an operations perspective

As previously mentioned, the Domino platform is based on a set of containerized services running atop of a Kubernetes cluster. As such there are a few key things to monitor.

image:/images/6.1/admin-guide/operations/monitoring/high_level_architecture.svg[alt="High level architecture", width=1000, role=noshadow]

Domino application::

This is the top layer, representing Domino application components running in containers that are deployed via Helm charts and managed by Kubernetes. At this level you should be monitoring things like count and status of user workloads, status of the individual services that make up the Domino application, connectivity to the platform, and availability of user resources such as external git repositories, databases, etc.

Kubernetes cluster::

This is the Kubernetes software-defined hardware abstraction and orchestration system that manages the deployment and lifecycle of Domino application components.
Cluster operations are handled a layer below Domino, but do have to consider the Domino architecture and cluster requirements.
For detailed guidance about general cluster administration, see https://kubernetes.io/docs/tasks/[the official Kubernetes documentation^].
At this layer you should be monitoring things like pod status, Kubernetes cluster networking, and resource utilization.

Infrastructure layer::

This is the bottom layer that represents the virtual or physical host machines that are doing work as nodes in the Kubernetes cluster.
Information technology owners of the infrastructure are responsible for operations in this layer, including management of compute and storage resources, as well as operating system patching.
Domino does not have any direct requirements in this layer. At this level, it can be useful to monitor things like CPU, memory & disk utilization, as well as networking. However, generally speaking, this is usually handled by the IT team responsible for the infrastructure and, especially in the case of virtualized infrastructure, visibility into this layer may be limited.

== Next steps

For more information about monitoring Domino and infrastructure, have a look at these topics:

* link:169f0f[Grafana]
* link:6b0667[Metrics to monitor]
* link:61791d[Monitor using New Relic]
* link:eb97ae[View active executions]
* link:c736dc[Remote Data Plane monitoring]

----- admin_guide/operations/monitoring/metrics.txt -----
:page-version: 6.1
:page-title: Metrics to monitor
:page-permalink: 6b0667
:page-order: 10

== Monitoring and alerting best practices
Monitoring Domino should happen from the top down, starting at the application layer and working down to the infrastructure layer. This is because, whilst knowing a particular node is about to run out of disk space, or that a certain service is using more CPU than expected is certainly useful, it is often harder to know what constitutes excessive memory usage or pathological network bandwidth consumption, for example, in isolation. By starting at the application layer and working down, you can build a picture of what normal looks like for your Domino deployment and set alerts accordingly.

The following tables start from the application layer and work down through the Kubernetes cluster layer to the underlying infrastructure layer. Each table also includes descriptions with key considerations.

However, everyone's cluster is different so you must monitor and adjust for your environment. For example, consider how long it might take you to respond when storage size is increasing. You may want to set this value to 50% and escalate at 80%.

Or consider the sort of latency that your users are willing to accept when performing actions in the UI and alert on requests that take significantly longer than that.

Also, remember that Kubernetes manages itself so momentary bursts of activity can trigger alerts that might not be a concern on their own without other key indicators being triggered also. For example, high MongoDB CPU usage might be normal during a backup operation, but if it is sustained for a long period of time alongside a lot of collstats queries for a collection, it might indicate an issue with the indexes on that collection.

== Application
[cols="3a,3a,4a",options="header"]
|===
|Metric |Suggested threshold |Description

|Latency to `/health` |1000ms |Measures the time to receive a response
to a request to the Domino API server.
If the response time is too high, this suggests that the system is unhealthy and that user experience might be impacted.
This can be measured by calls to the Domino application at a path of `/health`.

|Dispatcher pod availability from https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/#metrics-server[metrics server^]
|`nucleus-dispatcher` pods available = 0 for >10 minutes
|If the number of pods in the `nucleus-dispatcher` deployment is 0 for
greater than 10 minutes, it's an indication of critical issues that
Domino will not automatically recover from, and functionality will be
degraded.

|frontend pod availability from
https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/#metrics-server[metrics server^]
|`nucleus-front-end` pods available < 2 for >10 minutes
|If the number of pods in the `nucleus-front-end` deployment is less than two for greater than 10 minutes, it's an indication of critical issues that Domino will not automatically recover from, and functionality will be degraded.

|===

== Workload executions

[cols="3a,3a,4a",options="header"]
|===
|Metric |Suggested threshold |Description

|Model pods scheduled
|>0 for 15 minutes
|If model pods are scheduled for a significant amount of time, it might indicate that they will fail to start and must be investigated.
|Zombie Runs
|>0 for 15 minutes (Warn)
|When a Run completes, the pod must shut itself down.
If they continue to run as a zombie pod, this can lead to excess workload on your cluster.
Investigate this to identify why the run did not terminate upon completion.
|Failed workload runs
|rate > 5 for 15 minutes
|If a significant number of runs are failing, this might indicate an issue with the underlying infrastructure or the workload itself. Care should be taken to differentiate between failures due to bad user code and failures due to infrastructure issues.

|===

== Domino services

Many of the metrics and suggested alert thresholds that follow are duplicates of the overall Kubernetes metrics.
However, to ensure that we can identify the issue to a Domino core service and ensure the health of Domino itself, it’s worth monitoring specific events for some of the core services.

To learn more about what each service is responsible for, see link:a0b173[Domino architecture].

=== Nucleus frontend

[cols="3a,3a,4a",options="header"]
|===
|Metric |Suggested threshold |Description

|Evicted pods
|>0 count for 5 minutes (Warn)

>5 count for 5 minutes (Critical)
|See notes under <<Kubernetes>>

|Frontend Pods not ready
|>0 for 5 minutes
|See notes under <<Kubernetes>>

|High GC CPU usage
|>15% for 15 minutes
|The nucleus frontend is a Java application so it’s important to monitor the standard container metrics that we also monitor JVM health.
To do this, use the metric high garbage collection CPU usage.

|===

=== Nucleus dispatcher

[cols="3a,3a,4a",options="header"]
|===
|Metric |Suggested threshold |Description

|Evicted pods
|>0 count for 5 minutes (Warn)

>5 count for 5 minutes (Critical)
|See notes under <<Kubernetes>>

|Pods not ready
|>0 for 5 minutes
|See notes under <<Kubernetes>>

|High GC CPU usage
|>15% for 15 minutes
|Dispatcher, much like the frontend, is a Java-based application, so you must use the Garbage Collection metric to observe the Java application health.
|===

=== MongoDB

[cols="3a,3a,4a",options="header"]
|===
|Metric |Suggested threshold |Description

|Evicted pods
|>0 count for 5 minutes (Warn)

>5 count for 5 minutes (Critical)
|See notes under <<Kubernetes>>
|Pods not ready
|>0 for 5 minutes
|See notes under <<Kubernetes>>
|Replica Set degraded
|>80% for 5 minutes
|See notes under <<Kubernetes>>
|MongoDB high CPU usage
|>85% count for 10 minutes (Warn)

>100% count for 10 minutes (Critical)
|High CPU usage of Mongo might indicate that it is not behaving as expected
|High PVC usage
|>75% count for 15 minutes (Warn)

>80% count for 15 minutes (Critical)
|Mongo uses persistent storage and, as the database grows, this will fill the storage.
This might have to be increased over time.
|High PVC inode usage
|>80% count for 15 minutes (Warn)

>90% count for 15 minutes (Critical)
|As well as filling space, it will continually read and write to disk.
High inode usage can lead to a degradation of performance.

|mongo.mongod.queryexecutor.scannedPerSecond / mongo.mongod.document.returnedPerSecond
|<1
|A value >1 indicates there's an issue with indexing on the collection.

|===

=== Git

[cols="3a,3a,4a",options="header"]
|===
|Metric |Suggested threshold |Description

|Evicted pods
|>0 count for 5 minutes (Warn)

>5 count for 5 minutes (Critical)
|See notes under <<Kubernetes>>
|Pods not ready
|>0 for 5 minutes
|See notes under <<Kubernetes>>
|Replica Set degraded
|>80% for 5 minutes
|See notes under <<Kubernetes>>
|Git high CPU usage
|>85% count for 10 minutes (Warn)

>100% count for 10 minutes (Critical)
|High CPU usage of Git can be an indicator that it is not behaving as expected
|High PVC usage
|>75% count for 15 minutes (Warn)

>80% count for 15 minutes (Critical)
|Git uses persistent storage and, as the number of commits grows, this will fill the storage.
This might have to be increased over time.
|High PVC inode usage
|>80% count for 15 minutes (Warn)

>90% count for 15 minutes (Critical)
|As well as filling space, it will continually read and write to disk.
High inode usage can lead to a degradation of performance.
|Git Error rates
|>1 count for 5 minutes
|Git performs functions such as init, download, and upload as part of its service.
Monitoring for errors from these events is an indicator that users are experiencing issues with version control.
|===

=== Docker registry

[cols="3a,3a,4a",options="header"]
|===
|Metric |Suggested threshold |Description

|Evicted pods
|>0 count for 5 minutes (Warn)

>5 count for 5 minutes (Critical)
|See notes under <<Kubernetes>>
|Not ready
|>0 for 5 minutes
|See notes under <<Kubernetes>>
|Replica Set degraded
|>80% for 5 minutes
|See notes under <<Kubernetes>>
|High CPU usage
|>85% count for 10 minutes (Warn)

>100% count for 10 minutes (Critical)
|If using the deployed Docker registry, you must monitor its CPU usage because significant high usage for prolonged times can be an indicator that it is not behaving as expected.
|Docker registry error rates
|>1 unit for 15 minutes
|The Docker registry is exposed as an https/http service.
Connection failures to the service indicate there might be an issue with images being stored or pulled.
|Docker registry high latency
|>80% count for 15 minutes (Warn)

>90% count for 15 minutes (Critical)
|High latency to the service will impact pull and push times for images and lead to a degradation of 0
service.
|Evicted pods
|>1 count for 5 minutes
|See notes under <<Kubernetes>>

|===


=== RabbitMQ


[cols="3a,3a,4a",options="header"]
|===
|Metric |Suggested threshold |Description

|Evicted pods
|>0 count for 5 minutes (Warn)

>5 count for 5 minutes (Critical)
|See notes under <<Kubernetes>>
|Pods not ready
|>0 for 5 minutes
|See notes under <<Kubernetes>>
|Replica Set degraded
|>80% for 5 minutes
|See notes under <<Kubernetes>>
|High pod memory usage
|>75% count for 15 minutes (Warn)

>90% count for 15 minutes (Critical)
|See notes under <<Kubernetes>>
|High queue rate
|>1000 count for 10 minutes
|Rabbit must be continuously sending messages.
An increased queue count indicates it cannot send messages and a service is not behaving as expected.
|RabbitMQ low memory
|>90 for 10 minutes
|Rabbit is a high-memory consuming application.
It’s memory usage will be constantly high.
A drop in this might indicate it’s not functioning as expected.
|Available TCP sockets
|>90% for 10 minutes
|Rabbit is the message distributor for all services in Domino.
It must be connected to all the services to be able to communicate.
If the TCP socket amount free is significantly low it might struggle to create those connections.
|High PVC usage
|>75% count for 15 minutes (Warn)

>85% count for 15 minutes (Critical)
|Rabbit uses persistent storage.
This might have to be increased over time.
|High PVC inode usage
|>80% count for 15 minutes (Warn)

>90% count for 15 minutes (Critical)
|As well as filling space, it will continually read and write to disk.
High inode usage can lead to a degradation of performance.
|===

== Kubernetes

Observe the following settings across the entire Kubernetes platform.
If the thresholds are hit, it might be an early warning sign of an issue on the platform.
This can lead to an issue with Domino for users.

[cols="3a,3a,4a",options="header"]
|===
|Metric |Suggested threshold |Description

|Failed pods count
|Dependent on cluster (some failed pods in a development environment might be expected).
|Observe the number of pods in a failed state.
Depending on the type of environment you are in and what else runs on your platform it might be normal to have a few failed pods.
Configure the threshold accordingly.
|Containers running out of disk space
|FS >75% for 5 minutes (Warn)

FS >90% for 5 minutes (Critical)
|//SME please clarify as this is not clear the way it is written.
As well as the underlying operating system disk filling the containers running on your platform, use disk, both ephemeral and persistent, and significant increases in this or running for high intervals with high usage may impact service.
|Container memory usage
|>90% for 5 minutes (Warn)

>95% for 5 minutes (Critical)
|Containers consume memory from the underlying operating system.
They are typically configured with requests and limits to prevent one container from consuming all the memory from the system.
As workloads grow, the limits might be reached and need to be adjusted.
If they aren't set, you want to ensure that containers are not consuming too much node memory.
|Container CPU usage
|>90% for 5 minutes (Warn)

>95% for 5 minutes (Critical)
|Container CPU works under the same basis as container memory previously described.
|Pods unschedulable
|>0 for 7 minutes
|If a pod can't be scheduled, there might be issues on the cluster.
You might not have enough nodes so there isn't enough capacity.
There might also be an issue with a specific type of node or a constraint not being met for the pod deployment, such as storage availability.
Check these because it can be an early warning sign of an issue.
|Pods not ready
|>0 for 10 minutes
|Pods must be ready and available in a reasonable time frame.
If they are taking significant time to become ready, this can be a sign that something is not running as expected.
|OOM Killed events
|>0 for 15 minutes
|If a pod consumes too much memory and surpasses its quotas and limits, or significantly impacts the node, an out of memory error can kill it.
If this happens, review the application to see if the memory must be adjusted.
Also, adjust quotas and limits accordingly.
It also might be an underlying issue with the application.
|Pod evictions
|>0 for 10 minutes
|These occur when a node is resource starved.
It might be Kubernetes rebalancing itself and scaling up nodes or shifting workload to another node that is not at capacity.
However, it might be an indication that you must manually scale your cluster.
|Replicaset count
|>0 pods missing from replicaset
|Replicasets are a deployment type that specify a set number of pods that must be running.
If the number of replica pods is less than the count according to the replicaset, something is likely wrong.
|ImagePullBackOff
|>10 count for 5 minutes
|All pods run an image that comes from a registry, either directly from an upstream Domino registry or from some form of proxied internal registry.
If you are getting ImagePullBackOff failures this might indicate an issue with the network issue connecting to it, the registry, or an authentication problem to the registry.
|===

== Infrastructure

Observe and monitor the following for each node in your Kubernetes cluster.

[cols="3a,3a,4a",options="header"]
|===
|Metric |Suggested threshold |Description

|Average CPU usage
|>80% for 15 minutes
|Average node CPU usage must not be significantly high for long periods of time
|Average memory usage
|>90% for 15 minutes
|Average node memory usage must not be significantly high for long periods of time
|Disk usage
|FS >85% for 15 minutes
|Local disk can be used for both the underlying operating system functionality as well as Kubernetes and the containers running on it.
It might spike during high runs of containers and dip.
This is normal behavior, but it should not be consistently high.
|Node not ready status
|>0 for 30 minutes
|If a node is in a not ready state, it cannot accept containers, so your Kubernetes platform will not be run at full capacity.
|Shared file system sizes
|FS>75% (Warn)

FS >90% (Critical)
|Domino uses shared file systems for backing a number of its persistent volumes.
These must be monitored and increased as workloads and volumes grow.
|===

----- admin_guide/operations/monitoring/new-relic.txt -----
:page-version: 6.1
:page-title: Monitor using New Relic
:page-permalink: 61791d
:page-order: 40

Domino uses New Relic to enable developers, operations, and tech teams to measure and monitor the health and
performance of their applications and infrastructure to achieve the following:

* Help Domino understand system load for link:a3ed46[right-sizing] and upgrade planning.
* Send proactive and reactive alerts, which trigger Domino's paging systems.
* Provide general visibility of real-time and historical system health.
* Store aggregated application logs for reference and analysis.

== Data storage

Telemetry from Domino-deployed New Relic agents is sent via HTTPS to newrelic.com domains.
These agents will be initialized with a New Relic API key provided by Domino that will
authenticate them to New Relic. The New Relic agents take a standard proxy configuration, which
can be provided at install time to comply with requirements for proxied internet egress.

Official information about networks, IP addresses, domains, ports, and endpoints used by New
Relic is available from New Relic.

Data is stored in Domino's New Relic account in a dedicated project with the customer's unique ID,
and data is retained for 30 days.

== New Relic monitoring

Domino deploys five types of New Relic agent resources to collect different types of data.

image::/images/diagrams/newrelic.png[alt="New Relic", width=1000, role=noshadow]

=== Logging

New Relic Logging agents are deployed as a DaemonSet and collect Domino application
logs, which are exposed via the standard Kubernetes logging architecture and available
securely from the Kubernetes API.

=== Events

A New Relic Events agent is deployed as a Deployment and collects Kubernetes events
related to Domino components. This data is available securely from the Kubernetes API.

=== Infrastructure

New Relic Infrastructure agents are deployed as a DaemonSet and they collect common
operational metrics from the worker infrastructure, such as CPU, memory, disk utilization
and performance, and active workloads. This data is used to build a picture of cluster
health and provide visibility into system load.

=== Metrics

A single New Relic Open Metrics pod is deployed to collect time series data about
application component performance via metrics endpoints available from those
component services. This targets component endpoints via labels, similar to Prometheus.

Only whitelisted metrics data is transmitted to the upstream Domino New Relic account,
including:

* Cluster Autoscaler performance and configuration.
* Custom Domino metrics.

=== New Relic Application Performance Monitoring (APM)

Domino application components include integrated New Relic APM agents. When enabled,
these send application-specific performance metrics related to the operation and monitoring of the Domino application.

== Security

Domino employees use an SSO provider, who enforces two-factor authentication, to access New Relic. New Relic is a widely used, industry-standard tool.

Read about New Relic's security practices, including SOC2 compliance.

== Privacy and compliance

All New Relic agents deployed and used by Domino only send:

* Application performance metrics.
* Application logs.
* Kubernetes performance, configuration, and resource information.

New Relic agents deployed by Domino never send:

* Data uploaded or created by users.
* Credentials, passwords, or secrets.
* Personally identifiable information.

----- admin_guide/operations/monitoring/remote-data-plane-monitoring.txt -----
:page-title: Remote Data Plane monitoring
:page-permalink: c736dc
:page-version: 6.1
:page-order: 60

For link:c65074[Domino Nexus] deployments, remote data plane monitoring provides the capability to have metrics collected from Domino application components, the Kubernetes cluster, and host infrastructure running in the remote data plane, just as they are collected from the local cluster. This provides a similar level of visibility for the Domino administrator into remote workload resource usage than if the workload was running locally, and helps to reduce the time it takes to resolve remote workload issues.

== Architecture

In addition to the Prometheus server running in a non-Nexus Domino cluster, remote data plane monitoring will deploy and run a pod in each data plane at install time running https://opentelemetry.io/docs/collector/[OpenTelemetry Collector^], also called Otel Collector.

In a remote data plane, the OpenTelemetry Collector pod is configured to scrape metrics from Domino components, Kubernetes, and the underlying infrastructure and then forward the collected metrics to the OpenTelemetry Collector running in the control plane. Metrics sent from each remote data plane have a `data_plane_id` label containing the ID of the data plane they were collected from.

The OpenTelemetry Collector running in the control plane (or local data plane) is responsible for receiving connections from OpenTelemetry Collectors running in each of the remote data planes and forwarding it to the Prometheus server. Once ingested into Prometheus, the remote data plane metrics are available for use in link:53f847[alerting] and link:169f0f[monitoring] just like any other metric collected from the local data plane.

Communication between OpenTelemetry Collectors is secured via TLS and basic Auth authentication. The secret used for the authentication is stored in the secret named `domino-otel-collector-http-auth`.
----- admin_guide/operations/monitoring/view-active-executions.txt -----
:page-version: 6.1
:page-title:  View active executions
:page-permalink: eb97ae
:page-order: 50


You can view a list of the active executions.

In the Admin application, click *Reports > Executions*.
A list of active Domino execution pods opens with the type of workload, the hardware tier used, the originating user and project, and the status for each pod.

* Click *Resources*, *Pod*, or *Node* to view a full `kubectl describe` output.
* Click *Logs* (or *CSV*) to download the deployment lifecycle log for the pod generated by Kubernetes and the Domino application.
* Click *Support Bundle* to download logs and reports about the execution. See link:12e54b[Support Bundles].
+
If you have on-demand Spark clusters, each Spark node, including master and worker nodes, is listed as a separate row with complete information available on the originating project and user, as well as the hardware tier.


----- admin_guide/platform-management/ai-gateway.txt -----
:page-version: 6.1
:page-permalink: cce362
:page-title: AI Gateway
:page-order: 60


Admins can set up AI Gateway to provide Domino users a safe and streamlined way to access external Large Language Models (LLMs) hosted by LLM service providers like OpenAI or AWS Bedrock. This lets users enjoy the benefits of provider-hosted models while ensuring that they follow security best practices.

AI Gateway provides the following benefits:

- Securely manage API keys to prevent leaks.
- Control user access to LLMs.
- Log LLM activity for auditing.
- Provide data scientists with a consistent and streamlined interface to multiple LLM providers.
- Built on top of link:https://mlflow.org/docs/latest/llms/deployments/index.html#supported-provider-models[MLflow Deployments Server^] for easy integration with existing MLflow projects.

== AI Gateway endpoints

AI Gateway link:https://mlflow.org/docs/latest/llms/deployments/index.html#endpoints[endpoints^] are central to AI Gateway. Each endpoint acts as a proxy endpoint for the user, forwarding requests to a specific model defined by the endpoint. Endpoints are a managed way of securely connecting to model providers.

image::/images/5.10/ai-gateway/ai-gateway-diagram.jpg[alt="AI Gateway is a hub for LLM providers, key vault, logging, and Domino executions", width=600]

To create and manage AI Gateway endpoints in Domino, you can go to *Endpoints* > *Gateway LLMs*, or you can use the link:8c929e[Domino Platform API].
The UI provides simple modals to configure endpoint details and permissions. You can also update or delete endpoints at any time.

--
IMPORTANT: The `endpointName` must be unique.
--

See MLflow's Deployment Server documentation for more information on the list of supported LLM providers and link:https://mlflow.org/docs/latest/llms/deployments/index.html#provider-specific-configuration-parameters[provider-specific configuration parameters^].

Once an endpoint is created, authorized users can query the endpoint in any Workspace or Run using the standard link:https://mlflow.org/docs/latest/llms/deployments/index.html#client-api[MLflow Deployment Client API]. For more information, see the documentation to link:c9ac47[Use Gateway LLMs].

=== Endpoint permission management

AI Gateway endpoints can be configured to be accessible to everyone, or to a specific set of users and/or organizations.
These permissions can be configured in the second step of the creation modal and can be changed at any point after.

=== Secure credential storage

When creating an endpoint, you will most likely need to pass a model-specific API key (such as OpenAI's `+openai_api_key+`) or secret access key (such as AWS Bedrock's `+aws_secret_access_key+`). When you create an endpoint, all of these keys are automatically stored securely in Domino's central vault service and are never exposed to users when they interact with AI Gateway endpoints.

The secure credential store helps prevent API key leaks and provides a way to centrally manage API keys, rather than simply giving plain text key to users.


== AI Gateway audit trail

Domino logs all AI Gateway endpoint activity to link:785186[Domino's central audit system].
To see AI Gateway endpoint activity, go to *Endpoints* > *Gateway LLMs* and click on the *Download logs* button. This will download a `txt` or `json` file with all the AI Gateway endpoint activity in the past six (6) months.
You can further customize the audit events fetched by using the link:8c929e[Domino Platform API].

== Next steps

Learn how to link:c9ac47[use AI Gateway endpoints as a Domino user].

----- admin_guide/platform-management/configure-projects/change-the-default-qs-project.txt -----
:page-version: 6.1
:page-title: Change the default Project for new users
:page-sidebar: Default Project for new users
:page-permalink: bb077c
:page-order: 20

By default, every new user is the owner of a quick-start project.
When the user signs up, Domino creates this project.
It has example files that show how to take advantage of Domino features and a detailed `README`.
You can replace the default quick-start with customized projects.

[[custom]]
.Create a custom default project for new users

. Create the quick-start projects that you want all new users to have access to upon signup.
These projects must have names, descriptions, and `READMEs` that describe what's in the project.
Any user can own these projects, however, the project must be *Private*.

. Record the username and project name paths for these projects.
For example:
+
`admin-user/getting-started-project`
+
`admin-user/sample-app-project`
+
Domino reproduces the name of the project for new users.
If you set the previous example projects as default projects, all new users will own copies at:
+
`new-username/getting-started-project`
+
`new-username/sample-app-project`

. After your projects are ready for use by new users, set the `com.cerebro.domino.frontend.overrideDefaultProject` link:71d6ad#projects[configuration records].
+
For the examples shown previously, the value of this setting would be:
+
`admin-user/getting-started-project, admin-user/sample-app-project`

//sysadmins only; cloud admins will need to reach out to support to make the config change

----- admin_guide/platform-management/configure-projects/configure-jira-integration.txt -----
:page-version: 6.1
:page-title: Configure Jira integration
:page-permalink: 0792cf
:page-order: 30

Domino can integrate with Atlassian Jira so users can interact with Jira from inside a Domino project.
For Jira integration to work, you must configure an application link between Domino and Jira.
After you configure this, users with a Domino account and a Jira account can link them with OAuth.

== Requirements
* System administrator access to Domino.
* Jira account with admin permissions.
* Jira Cloud and Jira Server version 7.1.6+.

== Configure Jira integration

. In the Domino Admin application, go to *Platform settings* > *Feature flags*.
. Set `ShortLived.JiraIntegrationEnabled` to `true`.
. In the Domino Admin application, go to *Platform settings* > *JIRA integration*.
. In *Jira URL*, enter the URL for your Jira service, then click *Add Configuration*.
+
WARNING: The `Incoming Consumer Key`, `Incoming Consumer Name`, and `Public Key` won't be visible after you move away from this page. You will need the details from this page in subsequent steps so take note of this information.

. Click the link to restart the services.

. Log in to your Atlassian Jira account.
You must have admin privileges on this account.

. Click the gear icon to open *Settings*.
. Click *Products*.
+
image::/images/4.x/Jira_Settings_Product.png[alt="Jira product settings", width=600]
. Under the *Integrations* section, click *Application Link*.
. Enter your Domino URL and click *Create New Link*
A window opens with your URL pre-filled.
. Ignore the warning and click *Continue*.
. Enter the application name that was generated previously.
. Leave the rest of the fields empty. If your deployment requires that they be filled, enter placeholder values.
. Required: Scroll down and select the *Create incoming link* checkbox, then click *Continue*.
+
image::/images/4.x/Jira_Domino_Form.png[alt="Jira Domino form", width=600]
. Click *Continue*.
. Enter the *Consumer Name*, *Consumer Key* and *Public Key* that you saved previously.

== Remove linked Jira projects

You can remove Jira projects that are linked to Domino.

. In the Domino Admin application, go to *Platform settings* > *JIRA Integration*.
All projects that have a Jira ticket linked to them are listed.
. For the linked project, click *Unlink*.
. Click *Unlink* to confirm.

== Remove Jira integration

You can edit or remove the Jira integration. 

. In the Domino Admin application, go to *Platform settings* > *JIRA Integration*.
. Unlink all Jira-linked projects.
. Click *Delete Configuration*.
. If you want to make changes to the configuration, you must configure a new integration.

//feature flags are not available for cloud admins, but they ARE able to setup the JIRA integration after the feature flag is turned on

----- admin_guide/platform-management/configure-projects/configure-project-stages.txt -----
:page-version: 6.1
:page-title: Configure Project stages
:page-permalink: 6cde3a
:page-order: 10


You can define a set of custom project stages so your users can label their projects.
 Users can use these stages to mark a project's progress throughout the workflow.
This lets them communicate their progress in a project to their colleagues and to leadership.

This also lets them create useful views in the link:13a1a4[projects portfolio].

After you determine and set the stages, you must communicate how they must be used to your team.

To learn more about how users interact with and set project stages, see link:1f1064[Set Project Stages].

.Set up the stages

. In the Admin console, go to *Platform settings* > *Project stages*.

. Click *Add Record* to create a label for a stage that will be available for Domino users to set on their projects.
The record at the top of the list is the default stage all new projects.
. Create *Create*.


Domino recommends setting up a link:bb077c#custom[custom default project] with information in the `README`
about your teams practices, available environments, and how users must use project stages.

//cloud admins & sys admins both have access to this


----- admin_guide/platform-management/configure-projects/index.txt -----
:page-version: 6.1
:page-title: Configure Projects
:page-permalink: 08d4b4
:page-order: 10

You must configure basic project settings for users.

link:6cde3a[Configure Project stages]::
Define custom project stages so users can track a project's progress.
This also informs their colleagues and leadership about the project's progress.

link:bb077c[Change the default Project for new users]::
Change the default quick-start project for new users with a custom project.

link:0792cf[Configure Jira integration]::
Configure an application link between Domino and Jira so that users can link their Domino and Jira accounts with OAuth.

----- admin_guide/platform-management/index.txt -----
:page-permalink: 3164f4
:page-version: 6.1
:page-title: Platform management
:page-order: 60

link:08d4b4[Configure Projects]::
Configure basic project settings for users.

link:bf8b6c[Manage Compute Environments]::
Curate the environments used by your organization.

link:352624[Manage data]::
Manage Project files, Datasets, and external data.

link:91b540[Manage models]::
Configure Model Monitoring notifications and manage Model endpoint resources.

link:5781ea[Manage Data Planes]::
Manage Data Planes in a Domino Nexus deployment.

link:cce362[AI Gateway]::
Set up AI Gateway to provide Domino users a safe and streamlined way to access external Large Language Models (LLMs) hosted by LLM service providers like OpenAI or AWS Bedrock.

link:ba9786[Manage notifications]::
Use notifications to send in-app messages to users about Domino.
----- admin_guide/platform-management/manage-compute-environments/cache-images-in-eks/ami-operations.txt -----
:page-version: 6.1
:page-permalink: 07075d
:page-title: AMI operations
:page-order: 10


The following sections describe how to perform important operations on an EC2 instance to set it up as the template for a new AMI suitable for Domino.

== Install Docker

See the official instructions about how to https://docs.docker.com/engine/install/[install Docker^].

== Pull Environment images

To pre-cache environment images, you must run `docker pull` for the base images that those environments are built on, or the built environments from the internal registry itself.

To pull the link:0d73c6[Domino Standard Environment] base images, your command would look like this (substituting the version string for the image you want to cache):

[source,shell]
----
docker pull quay.io/domino/base:<desired version>
----

To pull a built image from the Domino internal registry, you must find its URI on the *Revisions* tab in the Environment details page.

For example, to cache revision #9 of the environment shown in the previous screenshot, you would run:

[source,shell]
----
docker pull 100.97.56.113:5000/domino-5d7abf2715f3690007f23081:9
----

[[install-nvidia-docker-2-0-gpu-amis-only]]
== Install NVIDIA Docker 2.0 (GPU AMIs only)

See the official instructions for https://github.com/NVIDIA/nvidia-docker/wiki/Installation-(version-2.0)[ installing the nvidia-docker 2.0 runtime^].

== Install GPU drivers (GPU AMIs only)

To use the GPU on a GPU node, install the appropriate driver on the machine image.
Domino does not have a requirement for any specific driver version, however, if you want to use a Domino Standard Environment, it must be a version that is compatible with the current version of Cuda shown in standard environments.

View a https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility__table-toolkit-driver[compatibility matrix^].

If you'd like to install the GPU drivers manually, you can follow these https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/install-nvidia-driver.html#obtain-nvidia-driver-linux[instructions^].

To validate that your GPU machine is configured properly, reboot the machine and run the following:

[source,shell]
----
docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi
----

This shows the driver number and GPU devices if installed successfully.

== Change the default Docker runtime (GPU AMIs only)

See the https://docs.nvidia.com/dgx/nvidia-container-runtime-upgrade/index.html#using-nv-container-runtime[official instructions from NVIDIA^] about using the container runtime.

You must restart Docker before this will work.

----- admin_guide/platform-management/manage-compute-environments/cache-images-in-eks/complete-ami-caching-procedure.txt -----
:page-version: 6.1
:page-permalink: 7057f6
:page-title: AMI caching
:page-order: 20


. Determine which AMI you want to use as the base for the new AMI.
If you're performing this operation on an operational Domino node pool, you must use the AMI that's currently used in the active launch configuration.
+
image::/images/4.x/admin_guide/launch_config_name.png[alt="The launch configuration name", width=1000]
+
After you've identified the name of the active launch configuration, view its details to see the AMI ID it uses.
+
image::/images/4.x/admin_guide/ami_id.png[alt="The AMI ID", width=1000]
. Launch a new EC2 instance from the base AMI.
. Connect to the instance through SSH and perform any of the operations listed previously that you want to apply to your new AMI, including pulling any environment images you want to cache.
. Snap a new AMI from the EC2 instance.
. Create a copy of the launch configuration currently used by any ASGs you want to switch to using the new AMI.
. Edit the AMI for the copied launch configuration to be the ID of the new AMI you snapped.
. For any ASGs that you want to start using the new AMI, switch them over to the new launch configuration.

After you complete the final step, any ASGs you switched to using the new launch configuration will start using the new AMI whenever they create new nodes.
These new nodes will have any environment images you pulled onto the AMI template cached, and will be fast to start new Domino Runs.

----- admin_guide/platform-management/manage-compute-environments/cache-images-in-eks/index.txt -----
:page-version: 6.1
:page-title: Cache Environment images in EKS
:page-permalink: e79fe6
:page-order: 30

[[tr1]]
// As a Domino user, I can execute a run using a node with a cached environment and it will start up faster than a run using a node with an uncached image/environment.
When a user launches a Domino execution, part of the start-up process is loading the user's environment onto the node that will host the execution.
For large images, the process of transferring the image to a new node can take several minutes.
After an image has been loaded onto a node once, it gets cached, and future executions that use the same environment will start up faster.

When running Domino on EKS, you can pre-cache popular environments and base images on the Amazon Machine Image (AMI) used for new nodes.
This can significantly speed up the start time of executions on new nodes.
This topic describes how to create a new AMI with cached environments and configure EKS to use it for new nodes.

In addition to any dependencies required by Kubernetes, your AMI must contain the following:

* Docker
* Cache of Domino's compute environments
* NVIDIA-Docker 2 (GPU nodes only)
* NVIDIA GPU driver 410+ (GPU nodes only)
* Change the default Docker runtime (GPU nodes only)

For simplicity, Domino recommends that you use the official EKS default AMIs, which come pre-configured with Docker and the GPU tools.

* See the https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html[official EKS AMI^] Domino recommends for default compute nodes.
* See the https://docs.aws.amazon.com/eks/latest/userguide/gpu-ami.html[official EKS AMI^]  Domino recommends for GPU nodes.

You can also use https://aws.amazon.com/about-aws/whats-new/2018/07/amazon-eks-ami-build-scripts-available-on-github/[Amazon's build scripts^] to create your own AMI for use with EKS.

== Next steps

* link:07075d[AMI operations]
* link:7057f6[AMI caching]

----- admin_guide/platform-management/manage-compute-environments/clean-environment-catalog.txt -----
:page-version: 6.1
:page-permalink: f2d0f3
:page-title: Clean your Environment catalog
:page-order: 20

It's inevitable that the number of environments will grow.
Therefore, you must occasionally review your environment catalog to remove unused environments, update active ones, and consolidate wherever possible.
Depending on the size of your organization and your use of Domino, this might be a yearly or quarterly task.

== Review current Environment usage

.Review all Environments in your deployment

. In the Admin application, go to *Manage resources > Environments*.
. Sort the table by the *# of Projects* column to get a quick understanding of which environments are in common use.
//. Enter `global` in the search to filter for global environments. - this is not working in new UI https://dominodatalab.atlassian.net/browse/DOM-63023

+
. Click the name of an environment to see its Dockerfile details.
.. Click the *Projects* and *Models* tabs to see the list of projects and models using the environment.
The list also includes the date of the last run for each project and model.
. Watch for environments that make duplicate changes to global base environments, as well as unused or poorly-maintained environments.

== Plan changes to global Environments

After you link:f2d0f3[review existing environments], you must plan an updated set of global environments that include the tools and features frequently added by users.
It might be as easy as adding a few packages to an existing global environment.
You can also create a new global environment when necessary, but Domino recommends erring on the side of larger, more consolidated environments.
Doing so makes it easier for your users to choose an environment.
It will also be easier for you to manage and maintain the collection of environments in your deployment.

Before you execute your plan and change the available global environments, inform your users of the impending changes and solicit their feedback.
Explain the changes to existing environments, announce when you create new ones, and provide recommendations for which environment to use for various projects.

== Sunset Environments

When necessary, you must sunset Environments that are deprecated or no longer used.
You can archive these Environments as long as they are not currently set as the link:5ecbff[Project Default Environment] for any Projects.

TIP: An alternative to archiving Environments is to rename them to indicate that they are End-of-Life.

Archiving an Environment does the following:

* Removes it from the Environments menu so users can no longer select it.
* Encourages adoption of new, up-to-date, and consolidated Environments.

It is important to note the following about archived Environments:

* Historical runs still reference archived Environments, so archiving does not break reproducibility.
* Environments can be unarchived if necessary.


//applies to cloud admins as well

----- admin_guide/platform-management/manage-compute-environments/create-envs-from-external-custom-images.txt -----
:page-version: 6.1
:page-permalink: ed1c08
:page-title: Create Environments from external custom images
:page-sidebar: Create custom Environments
:page-order: 40

This topic describes how to configure Domino so that you can create Environments based on custom images hosted in external, authenticated image registries.

This topic is useful if you want to:

* Enable your users to access an internal registry of images to use in Domino.
* Enable your users to access an authenticated third-party registry containing images related to your data science workflows.
For example, the NVIDIA Container Registry (NGC).

== Pre-requisites

* Access to the Kubernetes cluster where Domino is running and permissions to read and edit secrets in the Domino `compute` namespace.
* Access to and familiarity with using `kubectl`.
You must be comfortable editing Kubernetes objects.
* Access to Basic Authentication (username and password) credentials for the authenticated image registry you want to configure.
* The registry must support HTTPS.
If it doesn't, contact support or your field engineer.

[NOTE]
====
The supplied registry credentials will be configured Domino-wide.
Therefore, they must be safe for use by all Domino platform users, not just specific users.
Domino recommends creating a service account in the image registry specifically designated for use with the Domino platform.
====

== Add authentication details

[[tr1]]

Use this procedure to add authentication details to your registry secret.
Domino's image builder uses this secret to authenticate with registries during environment builds.

. *Determine the registry authentication secret name and namespace*
.. Edit the secret.
The default location for the secret is in the Domino `compute` namespace, under the secret name, `domino-quay-repos`.
However, another user might have changed the location or secret name.
.. Because the secret name and namespace values can be changed, go to *Platform Settings > Configuration records* to confirm that you do not have `com.cerebro.domino.builder.remoteRegistryCredentials.secretName` or `com.cerebro.domino.builder.remoteRegistryCredentials.secretNamespace` set.
If either of these are set use those values instead of the default values listed.
. *Fetch the current secret value*
.. Using the secret name and namespace from the previous steps, run the following command:
+
----
kubectl get -n <secret namespace> secret <secret name> -o jsonpath='{.data}'
----
Your output should look similar to the following: `{".dockerconfigjson":"..."}`
+
The value of `.dockerconfigjson` is the value in the double quotes ("") after `.dockerconfigjson`.
+
Retain this value as it will be used in subsequent steps.
.. Using the value you retained in the previous step, run the following command:
+
----
echo '<the copied value>' | base64 --decode
----
.. Copy the output of this command into a text editor of your choice.
Ensure you do not copy the final `%` symbol.
The value you copy should look similar to this: `{"auths": {"[quay.io]": {"username": "...", "password": "...", "email": "."}}}`
. *Add the new registry authentication*
.. Use your text editor to add your new registry to the previous content.
You'll want to add it at the same level as the existing repositories within the `auths` dictionary.
.. The `key` will be the repository host.
For example `nvcr.io` for NGC.
The value of this key is a dictionary with a username and password entry with your username and password.
.. You can use this template, replacing the values with the pre-existing values and your new values:
+
----
{"auths": {"quay.io": {"username": "...", "password": "...", "email": "."}, "<new registry>": {"username": "...", "password": "..."}}}
----
+
[NOTE]
====
If you are using an NGC container, you can use this template with the required username:

----
{"auths": {"quay.io": {"username": "...", "password": "...", "email": "."}, "nvcr.io": {"username": "$oauthtoken", "password": "..."}}}
----
====
You might want to keep your the text editor open until all the steps are complete.
This will save time if you must debug an error later.
. *Encode the new secret content*
.. Copy the new text from your editor.
Ensure it is all on one line and does NOT have a newline at the end.
.. Using the text copied from your text editor, run this command:
+
----
echo -n '<your copied content>' | base64 -w 0`.
----
You MUST use single quotes to properly escape the double quotes in the copied content.
.. Copy the output of that command.
Ensure you do not to copy the trailing `%` character.
. *Edit the secret*
. If you want to edit the secret interactively, run:
+
----
kubectl edit -n <secret namespace> secret <secret name>
----
.. In the system editor, remove the content after `.dockerconfigjson` replacing it with the previous encoded text.
.. If you want to edit the secret non-interactively, run:
+
----
kubectl get -n <secret namespace> secret <secret name> -o json | jq '.data[".dockerconfigjson"]="<your copied content>"' | kubectl apply -f -
----
. *Validate your edits*
+
.. Repeat the step where you've fetched the current secret value before and ensure the decoded content has your updates.
Carefully validate the dictionary structure.
. *Verify your changes*
.. Go to Domino and create a new Domino Environment in the Environments tab.
..  Use an image URI from the new authenticated registry as the custom base image for the Environment.
..  Watch the build progress in the Environment revisions tab.
Ensure the build succeeds.
..  If the build succeeds, delete all credentials you saved on your system in the text editor.
+
If the build fails, use the revision build logs to determine if the error is related to authentication.
Double check that your credentials are correct and validate that you edited the secret properly, repeating the previous steps as necessary.
+
If your edits do not work and you can not find any errors, see the previous steps to edit the secret but use the original encoded content that you retained in the step where you fetched the current secret value.

----- admin_guide/platform-management/manage-compute-environments/index.txt -----
:page-version: 6.1
:page-title: Manage Compute Environments
:page-permalink: bf8b6c
:page-order: 20

This topic covers best practices for managing compute environments.
As a Domino administrator, you must curate the environments used by your organization.
A proactive approach to environment management can prevent sprawl, avoid duplicate environments, and equip users with the tools they have to succeed in Domino.
You must find the balance between giving users the freedom to be agile in development, while also maintaining enough control that you don't end up with duplicate or unnecessary environments.
Because there is no limit to the number of environments that administrators and users can create, you must manage this to ensure that your users can find what they need.

Domino recommends these best practices:

Limit global environments::
The benefits of focusing on a small number of global environments with broad applications outweighs the benefits of creating many niche global environments.
When a user requests an environment with new capabilities, consider whether you can add their requested features to an existing global environment instead of creating a new one.

Keep global images up-to-date and comprehensive::
Strive to have global images that cover the majority of users' needs.
Users should only have to make minor additions to global environments when they create their own environments, such as installing a specific version of a package.
You don't want users to have to re-install Python or make other major changes, as this results in a bloated and poorly performing environment.

Use clear, descriptive names for your environments::
A common reason why environments get duplicated is that a user cannot tell whether an existing environment meets their needs.
Clear, descriptive names on all environments makes the entire catalog comprehensible to new users and administrators, and makes Domino easier to work with and maintain.

Add comments to the Dockerfile for each environment::
You can add comments to a Dockerfile like this:
+
[source,shell]
----
# This is a comment that can provide a helpful description of the code to follow
RUN echo 'This is an executed Dockerfile instruction'

# Here's a block that installs Ruby
RUN
  apt-get update &&
  apt-get install -y ruby
----
+
Each section must have a clear heading and comments to explain its purpose and implementation.

Share responsibility for environment management::
If you have multiple teams or departments doing separate work in Domino, they must maintain their team-specific environments.
Find an advanced user in each team and make them a deputy for environment management.
This person must be responsible for planning and understanding the environments their team needs, and must work with you on implementation.
This reduces the  administrators' workload and ensures that environments are designed by someone with context about what users need.

Cache global environments on the executor machine image::
Cache your global environments in your executor template machine image.
This ensures that each new executor starts up with the base Docker image for any environment already cached.
If users are setting up environments that have base images that are very different from what is cached on the machine image, it can lead to long pull times when they launch executors.
Contact mailto:support@dominodatalab.com[Domino Support] for help modifying your machine image.

Clean up old or poorly maintained environments::
Create a culture of tidiness around environment creation and management.
Enforce a standard of quality in naming and Dockerfile commenting, and be assertive about pruning unnecessary environments.
See link:f2d0f3[Clean Your Environment Catalog] for a walk-through.

//applies to cloud admins as well

For more information, have a look at these topics:

* link:0093e8[Use Restricted Environments]: Enable `Restricted Environments` in your Domino instance, mark your Project as `Restricted`, and prevent package installation at runtime.
* link:f2d0f3[Clean your Environment catalog]: Review current Environment usage, plan changes to global Environments, and sunset Environments.
* link:e79fe6[Cache Environment images in EKS]: Pre-cache popular Environments and base images on the Amazon Machine Image (AMI) used for new nodes.
* link:ed1c08[Create custom Environments]: Configure Domino so that you can create Environments based on custom images hosted in external, authenticated image registries.

----- admin_guide/platform-management/manage-compute-environments/restricted-environments.txt -----
:page-version: 6.1
:page-title: Use Restricted Environments
:page-permalink: 0093e8
:page-order: 10

You may want to ensure that a Compute Environment remains frozen and unmodifiable to guarantee that only certain packages are used for modeling or analysis. This can be important to ensure compliance with internal policies or external regulations. 

Domino lets you designate Compute Environments as “Restricted”. Environment Restriction ensures that users can’t use an Environment that has been modified after it has been certified as restricted.

Restricted Environments provide the following benefits:

* *Improve security and reduce compliance risk* by enforcing sensitive workloads to use only approved environments.
* *Reduce costs and management overhead* by providing flexibility to run sensitive and non-sensitive workloads on the same Domino deployment, without compromising on compliance. By marking certain environments as "Restricted", admins can ensure that they can enforce compliance for sensitive workloads while allowing data scientists the flexibility to use the latest innovations (new tools and libraries) on the same Domino deployment. 

== Use Restricted Environments

To enable Restricted Environments in your Domino instance, set the link:71d6ad[Configuration records] key `com.cerebro.domino.workbench.restrictedAssets.enabled` to `true`.

Only an admin can mark a revision of an Environment as Restricted.

To designate a Compute Environment as Restricted, go to *Environments*, select the environment and mark a specific Revision of the Environment as Restricted. 

You can also quickly set the latest environment revision to Restricted from the environment's *Overview* tab by clicking the *Restricted* checkbox.

Only a single Environment revision can be Restricted. If you mark another revision as Restricted, the previous revision will become unrestricted.

Once you mark a revision as Restricted, that Environment can’t be unrestricted. You can, however, choose which revision is the Restricted (i.e., frozen) revision. This limitation ensures that anyone using the environment will always be using a revision that was explicitly designated as Restricted.

Even when an Environment is Restricted and changes to its Dockerfile aren't permitted, data scientists can still install packages at runtime. To prevent these types of modifications, see link:#prevent_dynamic_installation[prevent dynamic installation].

[[restricted_projects]]
== Restricted Projects

To enforce that a Project only uses Restricted Environments, mark the Project as `Restricted` when you create the Project.

IMPORTANT: A Restricted project can only be classified as *Restricted* at the time of creation. Existing Projects cannot be classified as *Restricted*. Once a Restricted project is classified as *Restricted*, it cannot be reverted to unrestricted. This ensures that every action in a Restricted project uses a Restricted Environment at all times.

When running workloads from a Restricted Project, Domino will limit your choice of Compute Environment to Restricted Environments for all actions including but not limited to:

- Start a manual job run.
- Start a scheduled job run.
- Start a Workspace.
- Rerun a Job.
- Start an App.
- Start a Launcher.


=== Copy or fork a Restricted Project

When you fork a Restricted Project, the forked Restricted is also classified as Restricted. This ensures that any work performed on the forked Restricted is guaranteed to be done in a Restricted Environment in case you merge the forked Restricted back into the main Restricted.

When a Restricted Project is copied, the copied Restricted is also classified as Restricted. This is especially useful when the source Restricted is a link:a96d2b[Restricted template], since it ensures that Projects created from a template are Restricted if the template is Restricted.


[[prevent_dynamic_installation]]
== Prevent package installation at runtime

Even when an Environment is frozen and changes to its Dockerfile aren't permitted, data scientists may still install packages at runtime through various ways (e.g., running a ``pip`` command or installing an R package from CRAN). To prevent these types of modifications, we recommend adding the below commands in your Environment’s Dockerfile. These commands prevent common ways of installing packages at runtime.

WARNING: These examples are not meant to be exhaustive. We highly recommend that you include additional instructions depending on your organization's policies and technologies.

=== Set read-only libpath and block well known R package repositories

Add the following lines to the compute environment's Dockerfile to set the `R_LIBS_USER directory` to a folder the Domino user can’t write to.

[source,shell]
----
RUN chmod 755 /usr/local/lib/R/site-library && 
    sed -i 's/^R_LIBS_USER/#&/' /usr/local/lib/R/etc/Renviron
----

=== Overwrite package install functions

Add the following lines to the compute environment's Dockerfile to modify the package install functions to empty functions that produce a message that the environment is frozen.

[source,shell]
----
RUN echo 'assign("utils::install.packages", function ()  cat("
 Frozen R environment, no package installation allowed  

"), envir = globalenv())' >> /home/domino/.Rprofile && 
    echo 'assign("install.packages", function (pkg)  cat("
 Frozen R environment, no package installation allowed  

"), envir = globalenv())' >> /home/domino/.Rprofile && 
    echo 'assign("devtools::install_github",  function () cat("
 Frozen R environment, no package installation allowed  

"), envir = globalenv())' >> /home/domino/.Rprofile && 
    echo 'assign("install_github",  function (pkg) cat("
 Frozen R environment, no package installation allowed  

"), envir = globalenv())' >> /home/domino/.Rprofile && 
    echo 'assign("devtools::install_gitlab",  function () cat("
 Frozen R environment, no package installation allowed  

"), envir = globalenv())' >> /home/domino/.Rprofile && 
    echo 'assign("install_gitlab",  function (pkg) cat("
 Frozen R environment, no package installation allowed  

"), envir = globalenv())' >> /home/domino/.Rprofile && 
    chown domino: /home/domino/.Rprofile
----

=== Block package repos by routing package repo requests to loopback address

The following pre-setup script modifies the `/etc/hosts` file to re-route requests to a popular package repo back to localhost.

[source,shell]
----
echo "127.0.0.1 packagemanager.rstudio.com" >> /etc/hosts
echo "127.0.0.1 cran.r-project.org" >> /etc/hosts
echo "127.0.0.1 cran.microsoft.com" >> /etc/hosts
echo "127.0.0.1 bioconductor.org" >> /etc/hosts
----

=== Block pip installs by routing pip to a non-existent URL

One way to disable dynamic installation of packages from within a Workspace or a Job is to set the global index URL in the pip configuration to a non-existing URL:

[source,shell]
----
pip config set global.index-url [https://path/to/a/non-existing/URL]
----

[[limitations]]
== Limitations & caveats    

- If a Job is rerun and the previous Environment revision is no longer Restricted, the rerun will use the current Restricted revision of the Environment. If there is no Restricted revision available, the rerun will fail.

- Scheduled runs will use the Restricted Environment revision at the time of scheduling. If the Environment revision changes before the scheduled run executes, it will continue to use the revision from the time of scheduling.

//available for cloud admins
----- admin_guide/platform-management/manage-data-planes/archive-data-plane.txt -----
:page-version: 6.1
:page-permalink: 2dde11
:page-title: Archive a Data Plane
:page-order: 90

Archiving a data plane effectively deletes it.

WARNING: Once archived, a data plane cannot be un-archived.

.To archive a data plane

. link:eb97ae[Stop all executions] in the data plane.
. link:a6aafd[Delete all workspaces] in the data plane.
. link:1b34b6[Archive all hardware tiers] in the data plane.
. Go to *Manage resources* > *Data Planes*.
. Next to the data plane you want to archive, click the three-dot menu and select *Archive*.
. Click *Archive Data Plane*.
. Use Helm to delete the `data-plane` release in the data plane namespace in the data plane Kubernetes cluster.

----- admin_guide/platform-management/manage-data-planes/disable-file-sync.txt -----
:page-version: 6.1
:page-permalink: c7f87f
:page-title: Disable File Sync on a Data Plane
:page-order: 60

In some cases, data should not leave a specific data plan.
To prevent accidentally pushing data from a data plane to the control plane or any other data plane, you can disable file sync on that data plane.

On a data plane where file sync is disabled:

* Users can pull project files.
* Users can pull and push any mounted Git repositories.
* Users cannot push changes to the Domino File System (DFS).
+
Domino displays a warning before starting executions that cannot push to DFS, and the job history indicates that no artifacts or output files were captured due to running in a restricted data plane.

.Disable file sync on a data plane

. In the admin portal, go to *Manage resources* > *Data Planes*.
. Click the three dots to the right of the desired data plane.
. Click *Edit*.
. Click *Advanced*.
. Select *Disable File Sync*.
. Click *Save*.

----- admin_guide/platform-management/manage-data-planes/edit-data-plane.txt -----
:page-version: 6.1
:page-permalink: f2877a
:page-title: Edit a Data Plane
:page-order: 30

. Go to *Manage resources* > *Data Planes*.

. Next to the data plane you want to edit, click the three-dot menu and select *Edit*.
+
There are some limitations on what can be edited:
+
--
Name::

The data plane name can be freely changed (remember to let your users know).

Namespace::

Migrating a data plane to a new namespace is technically possible, but not generally supported.
Please reach out to Domino for support.

Storage Class::

Changing the storage class does not affect any running executions, but it does apply to new executions.
This may disrupt existing workspaces.

Hostname::

Changing the data plane hostname affects running executions requires some steps to update the configuration in the data plane:

* link:ce0927[Re-generate and run the Helm install command] for the data plane.
* Restart the `auth-proxy` pod in the data plane namespace.

Disable File Sync::

This can be changed at any time and will apply to newly-launched workloads.

Enable Istio::

Enabling or disabling Istio support for an existing data plane is possible, but not trivial.
Please reach out to Domino for support.
--

----- admin_guide/platform-management/manage-data-planes/enable-data-plane.txt -----
:page-version: 6.1
:page-permalink: 491fe8
:page-title: Enable Data Planes for Workspaces and Datasets
:page-sidebar: Enable Data Planes
:page-order: 50

To support workspaces, the data plane must be configured so that users can connect directly to the data plane to access interactive workloads, as described below.

== Configure the hostname
The data plane must be served from a _subdomain_ of the domain used for the control plane.
In other words, if users connect to Domino at `example.com`, then data planes must be served from `data-plane.example.com.`

== Configure load balancing
The hostname above should resolve to a load balancer which routes traffic to port 8080 on Pods with the following label selector:

[source,yaml]
----
app.kubernetes.io/component: auth-proxy
app.kubernetes.io/instance: auth-proxy
app.kubernetes.io/name: auth-proxy
----
You can do this with a combination of a `NodePort` service and load balancer if on-premises, or using `LoadBalancer` service types in major cloud providers.

== Configure TLS
Users must connect to data planes using TLS (HTTPS).
If you are using a load balancer to route traffic to the data plane, it might be easiest to configure the load balancer to serve valid TLS certificates for the domain.

.To configure the data plane to serve TLS certificates that you provide:

. Create a Kubernetes secret containing the certificates:
+
[source,shell]
----
kubectl create secret tls custom-certs -n <data plane namespace> --cert=<cert file> --key=<key file>
----
. Set the following value when deploying the data plane Helm chart:
+
[source,shell]
----
--set auth-proxy.config.nginx.tlsSecretName=custom-certs
----

== Configure Datasets
If you are setting up Datasets for the first time, you'll need to add a link:ce0927[custom value to Helm] first.

----- admin_guide/platform-management/manage-data-planes/hardware-tiers.txt -----
:page-version: 6.1
:page-permalink: 37fdfa
:page-title: Hardware tiers for Data Planes
:page-order: 40

Hardware tiers are the primary way in which users interact with data planes.
Each hardware tier is assigned to a data plane, and access granted to the hardware tier implies access granted to run workloads on that data plane, using the resources defined in the hardware tier.

== Create a hardware tier for a Data Plane

The easiest way to create a hardware tier for a new data plane is to clone an existing hardware tier and change its data plane, as explained below.
// Why is this?  Is this also the easiest way to create a new hardware tier in general?

. In the admin portal, go to *Manage resources* > *Hardware tiers*.
. Next to an existing hardware tier, click *Clone*.
. In the new hardware tier configuration window, change the data plane in the new tier using the dropdown in the 4th form element.
//What's the label for "the 4th form element?"
+
Make sure that the resources requested by the hardware tier are available on the nodes in the data plane it is using.
+
TIP: Cloning a “small” hardware tier is an easy way to test.

. Make sure that:
+
* The nodes you wish to use for compute in your data plane are labeled with `dominodatalab.com/node-pool`.
* The node pool value matches the node pool specified in the hardware tier.

== Restrict access to a Data Plane's hardware tier

In order to restrict access to a hardware tier (and therefore a data plane):

. In the admin portal, go to *Manage resources* > *Hardware tiers*.
. Next to an existing hardware tier, click *Edit*.
. Uncheck *Is Globally Available* at the bottom of the form.
. Navigate to the *Hardware tiers* tab on the organization's page and check the box next to the restricted hardware tier.
----- admin_guide/platform-management/manage-data-planes/index.txt -----
:page-version: 6.1
:page-permalink: 5781ea
:page-title: Manage Data Planes
:page-order: 50

In a link:c65074[Domino Nexus] deployment, you can manage data planes in the admin portal at *Manage resources* > *Data Planes*.

The topics in this section explain how to manage data planes:

link:ce0927[Register a Data Plane]::
Use Helm to register a data plane with Domino.

link:f2877a[Edit a Data Plane]::
You can edit the parameters of a registered data plane.

link:37fdfa[Hardware tiers for Data Planes]::
Each hardware tier is assigned to a data plane.

link:491fe8[Enable a Data Plane for Workspaces]::
To support Workspaces, the data plane must be configured so that users can connect directly to the data plane.

link:c7f87f[Disable File Sync on a Data Plane]::
In cases where data should not leave a specific data plane, you can disable file sync.

link:130dd3[Upgrade a Data Plane]::
Keep your data planes up to date with your control planes.

link:8557f9[Monitor a Data Plane]::
You can view a data plane's health and see which Kubernetes nodes, user executions, and workspaces are in it.

link:2dde11[Archive a Data Plane]::
Archiving a data plane effectively deletes it.

----- admin_guide/platform-management/manage-data-planes/monitor-data-plane.txt -----
:page-version: 6.1
:page-permalink: 8557f9
:page-title: Monitor a Data Plane
:page-order: 80

// This topic should probably appear under "Logs and Monitoring".
You can view a data plane's health and see which Kubernetes nodes, user executions, and workspaces are in it.

== View Data Plane health
// available for cloud admins
To view the health of a data plane, navigate to *Manage resources* > *Data Planes*.

Data planes are marked *Healthy* if they are available for use.

If a data plane indicates `Error`, `Disconnected`, or `Degraded`, there may be an issue with communication between the data plane services and the control plane.
Please reach out to Domino for assistance in troubleshooting.

The data plane health is a simple check to ensure the data plane is contactable. For more advanced monitoring and alerting, please refer to link:c736dc[remote data plane monitoring].

== View Data Planes per Kubernetes node
// not available for cloud admins
You can view Kubernetes nodes across all of your clusters by going to the admin portal and clicking *Reports* > *Infrastructure*.

The column on the right indicates the data plane that each node is in.

NOTE: Some of the node detail views are not yet implemented for nodes in remote data planes.

== View Data Planes per user execution
// avail for cloud admins
You can view all running user executions across all data planes by going to *Reports* > *Executions*.

The data plane for the execution is indicated in the rightmost column.

== View Data Planes per user Workspace

You can view all durable workspaces (running and stopped) by going to *Reports* > *Workspaces*.

The data plane for the workspace is indicated in the rightmost column.

----- admin_guide/platform-management/manage-data-planes/register-data-plane.txt -----
:page-version: 6.1
:page-permalink: ce0927
:page-title: Register a Data Plane
:page-order: 20

Before registering a Data Plane in Domino, you need to link:e6ea06#data-plane-deployments[install a Data Plane].

[NOTE]
====
A new method for registering Data Planes is available from 6.1. This documentation describes how to use the new method, which is available if the Feature Flag `ShortLived.HybridSuppressHelm` is set to `true`. If this Feature Flag is set to `false`, please reference the link:https://docs.dominodatalab.com/en/6.0/admin_guide/ce0927/register-a-data-plane/[6.0 docs for the steps to Register a Data Plane]. Reach out to the Domino Support team with any questions or for assistance in migrating to this new process.
====

Follow these steps to register a Data Plane:

. link:#step1[Configuration].
. link:#step2[Register a Data Plane in Domino UI].
. link:#step3[Generate a Data Plane Token].
. link:#step4[Run the bootstrap script].

[[step1]]
== 1. Configuration

Validate that the required link:71d6ad#data-planes[Data Plane configuration records] are set.

The required configuration records are auto-populated for hybrid-enabled control planes deployed.

[[step2]]
== 2. Register Data Plane in Domino UI

. In the admin portal, go to *Manage resources* > *Data Planes* > *Register Data Plane*.
. Enter the details for the new data plane:

* *Name* - An arbitrary, human-readable name for the data plane.
* *Namespace* - The Kubernetes namespace where you want to install the data plane in the data plane cluster.
This namespace will be created if it does not already exist.
* *Storage Class* - The Kubernetes StorageClass name to use for execution volumes in this data plane.
If blank, it will default to the control plane's storage class specified by `com.cerebro.domino.computegrid.kubernetes.volume.storageClass` in the link:71d6ad#compute-grid[Configuration records] (default: `dominodisk`).
* *Data Plane Hostname* - The subdomain component of the DNS name used for the data plane.
For example, if the control plane is `example.com` and the data plane is `dp.example.com`, then you would enter `dp` here.
You can also append `:` and a custom port number if your data plane load balancer runs on a port other than 443.
See link:491fe8[Enable a data plane for Workspaces].
* Advanced:
** *Disable File Sync* - Disable DFS file sync in this data plane.
// see #heading=h.q6mqw577ujfc[Data Plane Restricted File Sync]; currently that section contains no steps, it just explains that it can be done.
** *Enable Istio* - Indicate that Istio is running in this data plane's cluster. This setting _must_ be set for data planes with Istio; it is not sufficient to set the Helm value.
** *Override S3 Endpoint URL* - Override the default S3 endpoint URL for this data plane.

. Click *Done*.

[[step3]]
== 3. Generate a Data Plane token

When data plane registration is complete, you will need to create a token:

. In the Admin Portal, go to *Manage Resources > Data Planes*.
. In the *Actions* column, click *Setup* for the Data Plane you'd like to register.
. In the window, click *Load bootstrap token*.
. Once that loads, click *Copy token*.

[NOTE]
====
The token generated above is only valid for _10 minutes_.

If you exceed the 10 minute window, you'll need to load a new token through the same steps.
====

[[step4]]
== 4. Run the bootstrap script

. Make sure that your default Kubernetes context is the cluster in which you want to deploy the data plane.

. Obtain Kubernetes context of the Data Plane:
+
[source,yaml]
----
tsh login && tsh kube login <dp-deploy-id>
----
. Download the `bootsrap-data-plane.sh` script from link:https://github.com/dominodatalab/nexus/blob/main/bootstrap-data-plane.sh[GitHub^] and save it as an executable file.
+
[source,yaml]
----
curl -Sso bootstrap-data-plane.sh \
  https://raw.githubusercontent.com/dominodatalab/nexus/refs/heads/main/bootstrap-data-plane.sh && \
  chmod +x bootstrap-data-plane.sh
----
. Execute the bootstrap script with the following command. Replace `<token>` with the token you copied earlier:
+
[source,yaml]
----
./bootstrap-data-plane.sh <token>
----
The script will report creation of a secret and exit. In the Domino Admin UI, the data plane should become healthy within a couple of minutes.

----- admin_guide/platform-management/manage-data-planes/upgrade-data-plane.txt -----
:page-version: 6.1
:page-permalink: 130dd3
:page-title: Upgrade a Data Plane
:page-order: 70

Domino Nexus only allows control planes and data planes to be out of sync by patch version.
For instance, 6.1.0 and 6.1.1 are compatible, but 6.1.0 and 6.2.0 are not.

If your control plane has been upgraded, but the data planes have not, they are marked as degraded in the Admin UI, and new executions are not permitted.

Data plane upgrades can be performed using two methods: UI-based upgrades or manual upgrades.

== Upgrade data planes from the UI

You can use the Domino UI to upgrade data planes:

. To get started, go to the admin portal, then click *Manage resources* > *Data Planes*.
. Identify the data plane that needs an upgrade by looking for a status message indicating that the "Data Plane is running an older version" or "Data Plane is running a different version".
. Click the three vertical dots next to your selected data plane.
. Select *Upgrade* from the dropdown menu. If the upgrade option is greyed out, you'll need to upgrade the Data Plane manually instead.

== Upgrade data planes manually

If UI-based upgrades are not available, you'll need to manually upgrade the Data Plane. See instructions on how to manually link:5522a2#existing-custom-resource[upgrade a Data Plane].

----- admin_guide/platform-management/manage-data/datasets/associate-dataplanes-with-datasets.txt -----
:page-permalink: 9c75e2
:page-version: 6.1
:page-title: Associate Data Planes with Datasets
:page-order: 15

Nexus now includes the Domino Datasets feature, allowing users to create and manage Datasets from remote data planes and using the local data plane. Remote Datasets work just like local ones and support file operations such as snap-shotting, uploading, and downloading. They also have the same permissions that users expect. Users must first obtain data plane permission and the relevant dataset permissions to access remote datasets.

We are introducing the concept of Dataset Storage, which refers to the underlying storage that supports a collection of Datasets. Administrators can register, configure, and unregister these Dataset Storages, enabling users to select the Dataset Storage that will support their Datasets. 

When upgrading to a new release that includes Dataset Storage, the Domino shared storage—and any other storage set up for link:4f5b2a[multi-storage accounts]—will be linked to the corresponding Dataset Storage.

image::/images/6.0/admin-guide/DatasetArchitecture.png[alt="Datasets Architecture across Data Planes"]

== Data Planes
Local data planes—found within the compute namespace in the same cluster as the control plane—are responsible for handling compute operations, such as running a Workspace in the cluster. Remote data planes are the compute planes for other clusters and can connect to the central control plane.

Users receive visual feedback on which data plane a Dataset is located in. They can also filter by data plane when creating a dataset or mounting one for an app or launcher.

== Dataset Storage
Dataset Storage refers to a method of managing storage in Kubernetes. It works with link:https://kubernetes.io/docs/concepts/storage/persistent-volumes/[PersistentVolumeClaim and PersistentVolume] (PVC/PV) to provide the storage needed for Datasets. Here are some essential things to know about how dataset storage functions in Domino:

* Each dataset storage is mapped 1:1 to a corresponding PVC/PV pair. 
* A particular dataset storage can be associated with anywhere from zero to many Datasets. 
* A data plane can have anywhere from zero to many dataset storages. 
* Dataset storage can be in the local data plane or in any remote data plane that supports datasets in Nexus.

== Next Steps

* link:c512e7[Create and register Dataset Storage]
* link:6be870[Delete Datasets or snapshots]
* link:bc084f[Set limits on Dataset usage] 
----- admin_guide/platform-management/manage-data/datasets/create-register-dataset-storages.txt -----
:page-permalink: c512e7
:page-version: 6.1
:page-title: Create and Register Dataset Storage
:page-order: 10

Here are some things to keep in mind when creating Dataset Storage:

* Admins cannot create a Dataset Storage on the local data plane because there can only be one, which is associated with the Domino shared store.
* Dataset Storages can only be edited if no Datasets are currently using them, to avoid disrupting ongoing dataset work. 
* Admins can edit the name, underlying volume, and the is default field.

== Create Dataset Storage
*Prerequisite*: If your Kubernetes cluster is not configured to automatically create a Persistent Volume (VC) when a Persistent Volume Claim (PVC) is used, you must set up a link:4cdae9[Kubernetes PV and PVC] in the cluster first. 

You'll need to add a few things to the PVS specification:

. Add the label *dominodatalab.com/dataset-storage: <driver>*
+
[source,yaml]
----
labels: “dominodatalab.com/dataset-storage:EFS"
----

. Add the namespace used in your data plane (usually *compute*):
+
[source,yaml]
----
 namespace: YOUR-DOMINO-COMPUTE-NAMESPACE
----

. Next, click the Add Dataset Storage button and enter this information:
.. *Name*: specify a name for the Dataset Storage.
.. *Data Plane*: select the data plane you want to use.
.. *Volume*: choose the PVC name from the dropdown.

The volume may not be discoverable immediately. If you don't see the volume you want to use, close the modal and reopen it after a few minutes.

== Register Dataset storage
You'll need to register your newly created Dataset Storages before users can access it. To register a Dataset Storage:

. Go to the Admin UI and select *Datasets*.
. Click on *Dataset Storage*.

image::/images/6.0/admin-guide/RegisterDatasetStorage.png[Register or Add Dataset Storage]

You can only unregister a Dataset Storage if no Datasets are using it. Local Dataset Storage cannot be deleted, but remote Dataset Storages can be. Admins must delete the Persistent Volume Claim (PVC) or Persistent Volume (PV) from the cluster if needed.

Admins can view all Dataset Storages, including names, driver types (like EFS, NFS, SMB), and their associated data planes. The “Is Default” section indicates if a Dataset Storage is the default. This means that when creating a Dataset in that data plane, the default will be used unless changed.

== Configurations

The configurations outlined below are set in the config map of a service on a data plane basis and impact the datasets located there. These configurations serve as a redundancy measure to ensure that temporary files or files marked for deletion are eventually cleaned up, even if the original process that created them fails to do so.

*Note*: Making the grace period settings too short could result in files being deleted while they are still in use. 

* *cleanDownloadDirsPeriod*: Temporary files generated during the download process are regularly cleared.
* *cleanDeleteDirsGracePeriod*: Frequency files that are scheduled for deletion have been cleared.
* *cleanDownloadDirsGracePeriod*: Grace period before deleting any temporary files related to the download process.
* *cleanDeleteDirsGracePeriod*: Grace period before deleting any temporary files related to the delete process.

For more detail on setting these configurations, see link:ce0927#enable-datasets-in-nexus[Enable Datasets in Nexus].

== Next Steps

* link:6be870[Delete Datasets or snapshots]
* link:bc084f[Set limits on Dataset usage] 
* link:4f5b2a[Set up multiple storage accounts]
----- admin_guide/platform-management/manage-data/datasets/datasets-interface.txt -----
:page-permalink: 78b347
:page-version: 6.1
:page-title: Datasets Workflow Updates
:page-order: 70

Here are the general updates for the user interface that apply to all Datasets in version 6.0.0 and the cloud, following the August release.

image::/images/6.0/admin-guide/DatasetInterface.png[alt="Datasets Interface"]

(1) *New Icons*: The Datasets page now shows new icons to display information from left to right:

* *Dataset size*: total of both read-only and read-write snapshots
* *Data plane*: origin of that Dataset
* *Storage amount*: used by the Dataset
* *Directory*: location of the Dataset

(2) *Description* and *Files*: there are a few updates to the functionality of these fields:

* *Description*: Field is now in a separate white panel to make it more transparent.
* *Files*: The dropdown to switch between snapshots has been renamed to refer to files from the latest snapshot data.
* *Dropdown bar*: the width has been reduced to take up less space.

== Other UI Updates
In addition to new icons and improvements, there is a new dropdown to select a data plane along with the corresponding Dataset Storage for the following:

* Jobs
* Scheduled Jobs
* Workspaces

You can now also use the new Data Plane dropdown to filter for hardware tiers for:

* Apps
* Launchers

----- admin_guide/platform-management/manage-data/datasets/delete-datasets-or-snapshots.txt -----
:page-version: 6.1
:page-permalink: 6be870
:page-title: Delete Datasets and snapshots
:page-order: 40

You (an administrator) can delete entire Datasets or individual Snapshots.
Domino recommends the following process for Dataset and Snapshot deletion.

* The user who owns the Dataset marks it for deletion, excluding it from any new executions that start.
Non-administrator users can never permanently delete a Dataset or Snapshots.
* You (the administrator) then delete the Dataset or Snapshot, if appropriate.

.Delete a Dataset or Snapshot

. Go to *Admin* > *Manage resources* > *Datasets*.
. Click the *Datasets* tab or the *Snapshots* tab.
. At the end of the row for the item to delete, click the three vertical dots and then click *Delete*.
+
If you confirm the action, the system permanently deletes the item.
If the action was initiated by mistake, you can still recover a dataset or snapshot before the delete grace period (`com.cerebro.domino.dataset.graceTimeForDeletion`) expires.

[NOTE]
====
The delete confirmation identifies the projects using a given Dataset or Snapshot.
If you delete Datasets that are used by several projects, it can be disruptive because users will no longer have access to this data.
Consider notifying users when you think the impact of the deletion might be significant.
====

When a Dataset or a Snapshot is deleted, it will no longer be available for future executions.
Executions that are in progress will also be affected if they attempt to read or write to the dataset that is deleted.


TIP: You can *Delete all marked datasets* or *Delete all marked snapshots* and perform bulk delete confirmations.
You can also sort the tables by status to easily find all Datasets of Snapshots marked for deletion.

----- admin_guide/platform-management/manage-data/datasets/index.txt -----
:page-version: 6.1
:page-title: Manage Domino Datasets
:page-permalink: ae1654
:page-order: 20

When users have large quantities of data, including collections of many files and large individual files, Domino recommends that users import the data using a link:0a8d11[Domino Dataset].

Datasets are collections of Snapshots, where each Snapshot is an immutable image of a filesystem directory from the time when the Snapshot was created.
These directories are stored in a network filesystem managed by Kubernetes as a shared Persistent Volume.

To view all the Datasets in your Domino deployment, go to *Admin* > *Manage resources* > *Datasets*. From here you can manage permissions, and rename and delete Datasets.

Domino natively supports Domino Datasets with the following cloud storage services:

* Amazon EFS
* Azure File Storage
* Google Cloud Filestore

The Domino File Store can also be backed with a shared Kubernetes Persistent Volume from a link:25b6dc#storage-classes[compatible storage class].
You can provide an NFS storage service, and Domino installation utilities can deploy the https://github.com/kubernetes-retired/external-storage/tree/master/nfs-client[nfs-client-provisioner^] and configure a compatible storage class backed by the provided NFS system.

Each Snapshot of a Domino Dataset is an independent state, and its membership in a Dataset is an organizational convenience for working on, sharing, and permissioning related data.
//What does organizational convenience mean
Domino supports running link:5dce1f[scheduled Jobs] that create Snapshots, so users can write or import data into a Dataset as part of an
link:0a8d11#automatically-pipe-data-from-external-sources-into-domino[ongoing pipeline].

You can permanently delete Dataset Snapshots.
This is a two-step process to avoid data loss.
Users must mark Snapshots to be deleted, then link:6be870[you must confirm the deletion, if appropriate].
This capability makes Datasets the right choice for storing data in Domino that has regulatory requirements for expiration.

Find more detailed information on how to manage Datasets in these topics:

* link:c512e7[Create and register dataset storage.]
* link:9c75e2[Associate Data Planes with Datasets.]
* link:5e5a34[Monitor Dataset usage.]
* link:bc084f[Set limits on Dataset usage.]
* link:6be870[Delete Datasets and snapshots.]
* link:4f5b2a[Set up multiple storage accounts.]
* link:78b347[Datasets Workflow Updates.]

----- admin_guide/platform-management/manage-data/datasets/monitor-dataset-usage.txt -----
:page-version: 6.1
:page-permalink: 5e5a34
:page-title: Monitor Dataset usage
// d4b465
:page-order: 20

From the Admin application, go to *Manage resources* > *Datasets*.

The Datasets page shows information for *Datasets* and *Snapshot* such as the following:

*  Total storage size used by all stored Datasets/Snapshots.
*  The size of all storage used by Datasets/Snapshots marked for deletion.
* A table of all Datasets/Snapshots from the history of the
deployment.
You can sort the table by status, size, and the name of the containing Project/Dataset.

//cloud admin can access

----- admin_guide/platform-management/manage-data/datasets/multi-storage-support.txt -----
:page-version: 6.1
:page-permalink: 4f5b2a
:page-title: Set up multiple storage accounts
:page-order: 60

When your storage account is filling up, you can configure an additional storage location where Domino can store all future datasets and snapshots.
Updates to existing datasets and snapshots are made in their original storage location, while new datasets and snapshots are stored in the secondary location.
You can do this again if the secondary storage location also fills up.

NOTE: We recommend reaching out to your Domino field team to understand whether this solution is appropriate for your scenario.

This topic explains how to set up additional storage accounts for datasets.
For the steps below, we recommend working with the Domino field team to ensure that your configuration is accurate.

== Set up new storage account & Kubernetes resources

1. Create a new storage account.
+
This can be in the cloud or anywhere, as long as it can be registered as a Kubernetes Persistent Volume resource, and made accessible to Domino deployments (namely `nucleus-*` and `run-*`).
2. Create the appropriate PV/PVC pairs for the new storage account in the Domino cluster.
+
Two pairs of PV/PVC should be created in this step: one in the platform and one in the compute namespace. The content of the PV/PVC varies according to your choice of storage.
3. Make the PV/PVC accessible to other resources within the cluster.
+
The newly created PV/PVC must be accessible by pods in the platform and compute namespace.
Depending on your choice of storage, this can mean creating appropriate Kubernetes secrets for the new storage, editing the storages network policies to authorize write-access, and so on.
4. Edit the Kubernetes `nucleus-*` deployments to mount the newly-registered PV at a specific `mountPoint`.
5. Once the pods have restarted, test that the PV can be accessed correctly:
.. Execute inside one of the `nucleus-frontend` pods.
.. Navigate to the specified `mountPoint`.
.. Try to write a test file.

== Enable multi storage in the Domino UI
//not available for cloud admins
Start this procedure only after successfully completing step 5 above.
At this point, you should have a new storage account that is accessible to Domino via its own PV/PVC pairs.
In order to instruct Domino to write every future dataset and snapshot in the new storage account, follow the below steps:

1. Go to *Admin* > *Platform settings* > *Configuration records*, and add the following key/value pair in the `common` namespace:
+
--
- `com.cerebro.domino.datacache.pvc.names` - This is a comma-separated list of all compute-PVC names used for dataset/snapshot storage at Domino. If you only added one additional storage account, this list should contain two PVCs: the original one and the newly-created one.
- `com.cerebro.domino.datacache.pvc.originalName` - The compute-PVC name corresponding to the original Domino storage (this defaults to the first PVC in the comma-separated list of step 1).
- `com.cerebro.domino.datacache.pvc.primaryName` - The compute-PVC name of the volume in which you would like to store every _next_ dataset and snapshot.
- `com.cerebro.domino.datacache.pvc.<COMPUTE-PVC-NAME>.mountPoint` - Configure this for each PVC name specified in step 1; it is the mount point at which the PVC is mounted in the `nucleus-*` deployments.
--
+
NOTE: All the PersistentVolumeClaims (PVC) specified in the configuration records settings above must correspond to the claim names in the `*-compute` namespace.

2. Restart the necessary services as specified at the top of the *Configuration records* page.
3. Go to *Admin* > *Platform settings* > *Feature flags* and set the `ShortLived.EnableDatasetMultiStorageSupport` to `true` if it is not already.
4. Test that all dataset-related features work properly in the new storage account:
+
--
* Create a dataset and uploading data to it.
* Take a snapshot.
* Download data from a dataset/snapshot.
* Delete a dataset/snapshot.
* Mount datasets/snapshots to executions.
--
+
Verify that all these actions took place in the newly-configured storage account as expected.

== Preserve changes to Kubernetes deployments throughout upgrades
To preserve the changes made to the `nucleus-*` deployments and ensure that multi-storage support continues to work properly after a Domino upgrade, you must add the following specs to the custom `agent.yaml`, inside the `nucleus` > `chart_values` section:
[source,yaml]
----
config:
  multiStorageSupportEnabled: true
  additionalStorages:
    - name: "<name of the additional volume(s) as per the modified nucleus-* deployment>"
      mountPath: "<mount path at which volume was mounted>"
      platformPvc: "<platform-PVC name corresponding to the new volume>"
      computePvc: "<compute-PVC name corresponding to the new volume>"
----

----- admin_guide/platform-management/manage-data/datasets/set-limits-on-usage.txt -----
:page-version: 6.1
:page-permalink: bc084f
:page-title: Set limits on Dataset usage
// d4b465
:page-order: 30

If you want to limit the storage consumed in Datasets, you can use the following Configuration keys. See the link:71d6ad#tr300[Read-write datasets configuration keys] for details.


* `com.cerebro.domino.dataset.quota.maxActiveSnapshotsPerDataset`


* `com.cerebro.domino.dataset.graceTimeForDeletion`


If a Dataset reaches one of these limits and a user starts an execution with a Dataset configuration that can output a new Snapshot, they will receive an error message.
Before additional Snapshots can be written, you must delete old snapshots or increase the limit.

You can authorize individual projects to ignore these limits.

. Go to the project whose limits you want to ignore.
. Click *Settings*.
In the *Hardware & Environment* tab, select the *Ignore Dataset Limits* checkbox.

----- admin_guide/platform-management/manage-data/external-data/configure-ds-auth/index.txt -----
:page-version: 6.1
:page-permalink: 6a8639
:page-title: Configure Data Source authentication
:page-order: 30

By default, only basic authentication is supported for Data Source authentication.
For some Data Source types, you can enable additional authentication methods.

== Enable authentication methods

Add an authentication method for a Data Source type:

. In the Admin application, go to *Platform settings* > *Configuration records*.
. Click *Add Record*.
. Add a key such as: `com.cerebro.domino.datasource.SnowflakeConfig.enabledAuthTypes`.
. Enter the value as a comma-separated text value where each word is an authentication type.
See link:71d6ad#ds-auth[Configuration records] for the valid keys and values for each Data Source type.
+
NOTE: The order of the values in the configuration matters: the first value will be set as default authentication method for all users.

. Click *Create*.

[NOTE]
====
See link:e3a1a9[OAuth] and link:9a4aea[IAM] for instructions as these authentication methods require additional steps.

OAuth-authenticated connections can be used for any execution type _except_ Domino endpoints.
====
// not for cloud admins

== Next steps

* link:497692[Remove an authentication method]
* link:e3a1a9[Set up OAuth for Snowflake]
* link:9a4aea[Set IAM Authentication]
----- admin_guide/platform-management/manage-data/external-data/configure-ds-auth/remove-an-auth-method.txt -----
:page-version: 6.1
:page-permalink: 497692
:page-title: Remove an authentication method
:page-order: 10

. In the Admin application, go to *Platform settings* > *Configuration records*.
. Click the Edit icon next to the relevant link:71d6ad#_data_source_authentication[configuration record].
. Remove the authentication method from the list of values.
. Click *Save*.
//not for cloud admins
----- admin_guide/platform-management/manage-data/external-data/configure-ds-auth/set-up-iam-auth.txt -----
:page-version: 6.1
:page-permalink: 9a4aea
:page-title: Set IAM Authentication
:page-order: 30


// To do: Add instructions for AWS IAM configuration.

You can enable users to create Data Sources using IAM authentication for these AWS-based Data Source types:

* link:69b09a[MySQL]
* link:f095e7[PostgreSQL]
* link:444abc[Redshift]
* link:947ddd[S3]

NOTE: IAM-authenticated connections can be used only to execute jobs and workspaces.
Other execution types, such as scheduled jobs and Domino endpoints, require basic authentication.

.Set up IAM authentication
. link:0e334d[Configure single sign-on (SSO)].
. link:eb6a88[Configure AWS credential propagation].
. Add IAM to the available authentication options for one or more of the supported Data Source types.
. In the Admin application, go to *Platform settings* > *Configuration records*.
. Click *Add Record*.
. Enter one of the following keys with the value `AWSIAMRole` or `Basic, AWSIAMRole`.
+
* `com.cerebro.domino.datasource.MySQLConfig.enabledAuthTypes`
* `com.cerebro.domino.datasource.PostgreSQLConfig.enabledAuthTypes`
* `com.cerebro.domino.datasource.RedshiftConfig.enabledAuthTypes`
* `com.cerebro.domino.datasource.S3Config.enabledAuthTypes`
+
See the link:71d6ad#ds-auth[Configuration Reference] for details about the supported configurations.
. Click *Create*.
// not for cloud admins
----- admin_guide/platform-management/manage-data/external-data/configure-ds-auth/set-up-oauth-for-snowflake.txt -----
:page-version: 6.1
:page-permalink: e3a1a9
:page-title: Set up OAuth for Snowflake
:page-order: 20

Use this topic to enable users to create OAuth-backed link:d4ef2b[Snowflake Data Sources], and authenticate and query Snowflake with
a JWT token.
After this is set up, users can select *OAuth* when they create a Snowflake Data Source.

When an OAuth-configured Snowflake is queried in a Domino execution, the `dominodatalab-data` library uses the Domino JWT token to authenticate against the underlying Snowflake database.
If this is set up properly, no extra user action is needed.

To set up Snowflake OAuth, you must:

. Set up an External OAuth Security Integration in your Snowflake account to link:#configure-snowflake[configure Snowflake].
This dictates what values your Snowflake account will expect to be propagated inside the token for successful authentication.
. link:#keycloak-setup[Set up Keycloak] to ensure correct propagation of the necessary values in the Domino JWT access token.

NOTE: OAuth is not supported for service accounts.
OAuth-authenticated connections can be used for any execution type _except_ Domino endpoints.

NOTE: Because authentication depends on the Domino JWT token propagating the correct set-up values in the context of an execution, you cannot verify successful authentication when you create a Data Source.
Therefore, the green checkmark that shows when you create a Data Source does not guarantee that authentication will succeed.
To confirm successful value propagation in a Domino JWT token, use the `dominodatalab-data` library to query data from an OAuth Data Source in a Domino execution, such as a Workspace.

[[configure-snowflake]]
== 1. Configure Snowflake

To set up an External OAuth Security Integration in Snowflake, see the https://docs.snowflake.com/en/user-guide/oauth-ext-custom.html#configure-custom-clients-for-external-oauth[Snowflake documentation^].
Get familiar with the Snowflake terminology and the values that Snowflake needs in the token; correct spelling of the values is critical.

The following is a sample command to create a base security integration that allows all account roles to successfully authenticate.
You can use this as a starting point to build an authentication configuration that fits your security needs.

----
create security integration <external_oauth_name>
    type = external_oauth
    enabled = true
    external_oauth_type = custom
    external_oauth_issuer = 'https://<domino-domain>/auth/realms/DominoRealm'
    external_oauth_rsa_public_key = <keycloak-realm-settings-public-key>
    external_oauth_audience_list=('account')
    external_oauth_scope_mapping_attribute = 'scp'
    external_oauth_token_user_mapping_claim='preferred_username_snowflake'
    external_oauth_snowflake_user_mapping_attribute='login_name';

alter security integration <external_oauth_name> set EXTERNAL_OAUTH_ANY_ROLE_MODE = ENABLE;
----

Find the value for these keys:

* `external_oauth_issuer` in Keycloak's *Realm Settings* > *OpenID Endpoint Configuration* > *Issuer*
* `external_oauth_rsa_public_key` in Keycloak's *Realm Settings* > *Keys* > *Public Key from RS256*

The configured security integration (`desc security integration <external_oauth_name>`) should look similar to the following:

image::/images/5.2/datasources/security_integration.png[alt="Snowflake Security Integration", width=900]

[[keycloak-setup]]
== 2. Set up Keycloak integration

The following steps describe how to instruct Keycloak to pass the values that Snowflake's custom security integration expects in the Domino JWT access token.
To do this, create mappers in the `domino-play` client and specify the option to `add to access token` in each mapper.
This creates a key-value entry in the Domino JWT token with key being what is specified under `Token Claim Name`.

The value depends on what type of mapper is created.
For example, a hardcoded-claim mapper requests a value, which ultimately propagates equally to all users.
By contrast, a user-attribute mapper requests a `User Attribute` name, then looks into each Keycloak user attribute for that attribute name and propagates its value.
This lets you customize how the Domino JWT token is augmented by each user.

The following is a base solution that matches the base Snowflake custom security integration described previously.
This solution is a starting point to build upon for a customized authentication configuration that fits your security needs.

// The screenshots in this section could use recapturing with better window proportions

.Set up the Keycloak integration:
. Go to the keycloak admin console at `\https://<domino-domain>/auth/`.
. Click *Domino Realm* > *Clients*.
+
image::/images/6.1/datasources/dashboard.png[alt="Keycloak Client Dashboard", width=900]

. From the lookup list, click *domino-play*, then click *Client scopes* > *domino-play-dedicated*.
+
image::/images/6.1/admin-guide/client-scopes.png[alt="Client Scopes", width=900]
+
image::/images/6.1/admin-guide/mappers.png[alt="Mappers", width=900]

. To add mappers, click *Mappers* > *Add mapper* > *By configuration*.
+
image::/images/6.1/admin-guide/add-mappers.png[alt="Add mappers", width=900]

. Choose a mapper from the listed mappings and configure the new mapper.
+
image::/images/6.1/admin-guide/configure-mappers.png[alt="Configure mappers", width=900]
+
The mappers are simple examples.
For the `scp` claim specifically, it is highly recommended that you make use of the prepopulated "Scope Claim Aggregator" mapper.
Otherwise you can write your mappers differently, as long as they conform to the Snowflake Security Integration's requirements.
+
[cols="1a,2a",options="header"]
|===
2+|SCP
|Token Claim Name |`scp`
|User Attribute |The name of the user attribute containing the desired Snowflake scope name.
|Static Values (comma-separated) |`all`
|Add to access token  |`ON`
|===
+
image::/images/6.1/datasources/scp.png[alt="SCP Mapper", width=900]
+
[cols="1a,2a",options="header"]
|===
2+|Issuer
|Token Claim Name |`external_oauth_issuer`
|Claim JSON Type |`String`
|Add to access token |`ON`
|Claim value |The name of your OpenID Endpoint Configuration issuer. You can find the value in *Realm Settings* > *OpenID Endpoint Configuration* > *Issuer*.
|===
+
image::/images/6.1/datasources/issuer.png[alt="Issuer", width=900]
+
[cols="1a,2a",options="header"]
|===
2+|Audience
|Token Claim Name |`external_oauth_audience_list`
|Claim JSON Type |`String`
|Add to access token |`ON`
|Multivalued |`ON`
|Claim value |This should be a list of audiences supported by the security integration.
|===
+
image::/images/6.1/datasources/aud.png[alt="Audience Mapper", width=900]
+
[cols="1a,2a",options="header"]
|===
2+|User Mapping
|Token Claim Name |`external_oauth_user_mapping_claim`
|Claim JSON Type |`String`
|Add to access token  |`ON`
|Claim value |This should represent the mapping between keycloak user and Snowflake user.
|===
+
image::/images/6.1/datasources/external_user_mapping_claim.png[alt="External User Mapper", width=900]
+
[cols="1a,2a",options="header"]
|===
2+|External User Mapping Attribute
|Token Claim Name |`preferred_username_snowflake`
|Claim JSON Type |`String`
|Add to access token  |`ON`
|Claim value |This is a mapper that goes with your custom security integration.  Its value is the Snowflake username of the authenticating user. Note how in the Snowflake custom security integration, we specified `external_oauth_token_user_mapping_claim='preferred_username_snowflake'`, instructing Snowflake to use the value of the key `preferred_username_snowflake`.
|===
+
image::/images/6.1/datasources/snowflake-preferred-username.png[alt="Preferred Username Mapper", width=900]
+
[cols="1a,2a",options="header"]
|===
2+|RSA Public Key Value
|Token Claim Name |`external_oauth_rsa_public_key_value`
|Claim JSON Type |`String`
|Add to access token  |`ON`
|Claim value |This should be the name of your OpenID Endpoint Configuration issuer. You can find the value in *Realm Settings* > *Keys* > *Public Key from RS256*
|===
+
image::/images/6.1/datasources/rsa_pk.png[alt="PK Mapper", width=900]
+
[cols="1a,2a",options="header"]
|===
2+|External User Mapping Attribute
|Token Claim Name |`external_oauth_snowflake_user_mapping_attribute`
|Claim JSON Type |`String`
|Add to access token  |`ON`
|Claim value |This is the attribute that is used to map the access token to a Snowflake user record.
|===
+
image::/images/6.1/datasources/user_mapping_attr.png[alt="User Mapping Mapper", width=900]
+
Keycloak propagates the values obtained by the mappers into the JWT token that is used to authenticate against the External OAuth Security Integration you created in Snowflake.

. If you added a mapper that required a user attribute, add the proper user attribute at *Users* > *View All Users* > *<user ID>* > *Attributes*.
+
You can also automate this to create further roles and mappers, as in this example user attribute:
+
image::/images/6.1/datasources/user_attr.png[alt="User Attribute", width=900]

. Add OAuth to the available authentication types for Snowflake:
.. Go to *Platform settings* > *Configuration records*.
.. Click *Add Record*.
.. Enter the `com.cerebro.domino.datasource.SnowflakeConfig.enabledAuthTypes` key with the value `OAuth` or `Basic, OAuth`.
The order of the values matters, and the first value will be set as default authentication method for all users.
+
image::/images/5.2/datasources/cc_setting.png[alt="CC-Sample", width=900]

. Click *Create*.

== 3. Test the configuration

There are two ways to inject user attributes for testing (the user attributes `SCP`, `external_oauth_audience_list`, and `preferred_username_snowflake` must be present in the Domino user JWT token):

* Locally in Keycloak: +
For quick testing.
User attributes can be manually added under the Keycloak user.

* Inject via SAML attributes: +
For enterprise-level implementations.
The recommended way to pass the user attributes is from the Identity Provider as SAML attributes.
When SAML attributes include the user attributes, additional mappers are needed to extract them and make them available for the Domino play client.

Three IDP user attribute mappers must be configured:

. Go to *Domino realm* > *Identity providers* > *Identity provider* > *Mappers*.
. For the *Mapper type*, select `Attribute importer`.
. In the *Attribute Name* and *User Attribute Name* fields, add the user attributes to be passed to the Domino play client, for example, `SCP`, `external_oauth_audience_list`, and `preferred_username_snowflake`.

image::/images/6.1/datasources/snowflake-oauth.png[alt="Edit the Identity Provider Mapper", width=1000, role=noshadow]

To test that OAuth is working with your Snowflake Data Source, follow these steps:

. Register a Snowflake Data Source.
. Select `OAuth` as the authentication type.
. Open an execution.
. Query data from the registered Data Source.

TIP: To get the token itself, make a GET request to the link:40b91f#_use_the_api_proxy_for_domino_api_authentication[Domino API Proxy] by running `curl $DOMINO_API_PROXY/access-token` in an execution like in a terminal of a Workspace or a script of a Job.
You can use https://jwt.io/[https://jwt.io/^] to copy/paste the token and see what is and is not being propagated correctly into the Domino JWT token from Keycloak.

----- admin_guide/platform-management/manage-data/external-data/data-sources/connect-to-mongodb.txt -----
:page-version: 6.1
:page-permalink: ed290d
:page-title: Create a MongoDB Data Source
:page-sidebar: Connect to MongoDB
:page-order: 10

Create a Data Source for https://www.mongodb.com/[MongoDB^] that your Domino users can access. Only administrators can create this type of Data Source.

== Prerequisites

- Starburst needs to be enabled in the deployment. If not enabled, please follow the instructions to link:ae2e6b#starburst[enable Starburst-powered Data Sources].
- You must have network connectivity between MongoDB and your Domino deployment.
- Domino must enable this feature. If you do not see this Data Source type in your deployment, contact your Customer Success Manager.

== Create a MongoDB Data Source
//available for cloud admins
. Go to *Manage Resources* > *Data Sources*.
. Click *Connect to External Data*.
. In the New Data Source window, select *MongoDB*.
. Enter your MongoDB hostname or IP address.
. Optional: Enter the *Port*.
. Enter a *Name* and *Description* for this Data Source.
. Click *Next* twice.
. Enter your MongoDB credentials.
. Click *Next*.
. Select whether *Everyone* can access this Data Source or just *Specific users or organizations*.
. Click *Finish Setup*.
+
Domino alerts you to restart the cluster.
+
TIP: You must restart your Starburst cluster after any configuration changes. Perform the restart when no executions are running that interact with Starburst-powered Data Sources.

. Click *Click here to trigger a restart*.
. When prompted to confirm the cluster restart, click *Restart Cluster*.
The restart takes approximately 5-10 minutes.
When the restart is complete, Domino users can use the MongoDB Data Source.

----- admin_guide/platform-management/manage-data/external-data/data-sources/connect-to-palantir.txt -----
:page-version: 6.1
:page-permalink: a932d7
:page-title: Create a Palantir Foundry Data Source
:page-sidebar: Connect to Palantir Foundry
:page-order: 20

Create a Data Source for https://www.palantir.com/platforms/foundry/[Palantir Foundry^] that your Domino users can access.

NOTE: Only administrators can create this type of Data Source.

== Prerequisites

- link:ae2e6b#starburst[Starburst needs to be enabled in the deployment].
- You must have network connectivity between Palantir and your Domino deployment.
- Domino must enable this feature. If you do not see this Data Source type in your deployment, contact your Customer Success Manager.

== Create a Palantir Data Source
//available for cloud admins

To create a Palantir Data Source:

. Go to *Manage Resources* > *Data Sources*.
. Click *Connect to External Data*.
. In the New Data Source window, select *Palantir*.
. Enter a *Name*, *Description*, Palantir hostname or IP address. Optionally: Enter the *Port*.
. Select between Individual or Service Account Credential type.
+
* *Individual* - each user, when they are granted access to the Data Source, must provide their own credentials before using a Data Source.
* *Service Account* - Domino administrators provide a set of static credentials that will be automatically applied on behalf of all users with permissions to a given Data Source. End users cannot access or extract the credentials, nor can they manage user-level permissions on such Data Sources.
+
. Select between *Client Credentials* and *OAuth Token* for authenticating to Palantir.
+
NOTE: Individual Credentials can only use *OAuth Tokens* to authenticate.
+
+
. Enter your Palantir credentials. See the following section to learn how to retrieve your *Client Credentials*.
. Select whether *Everyone* can access this Data Source or just *Specific users or organizations*.
. Domino alerts you to restart the cluster. Click *Click here to trigger a restart* > *Restart Cluster*. You must restart your Starburst cluster after any configuration changes. Perform the restart when no executions are running that interact with Starburst-powered Data Sources.

The restart takes approximately 5-10 minutes. When the restart is complete, Domino users can link:ccd264[use the Palantir Data Source].

== Get Palantir Client Credentials

To connect to Palantir using a Service Account with Client Credentials you will need to retrieve your Palantir client ID and client secret:

. Go to your Palantir instance.
. Navigate to *Control Panel* > *Third-party applications*.
+
If a Domino integration already exists you will see it here.
+
. Next to the Domino integration, select *Actions* > *Manage application* to find your client ID.
+
If a Domino integration does not exist, link:https://www.palantir.com/docs/foundry/platform-security-third-party/register-3pa/index.html[register Domino as a third-party application^]. Make sure you choose *Confidential client* as the client type and save the client secret. You will not be able to view your secret again, so store your secret securely.

If your Domino-Palantir integration has already been created and you cannot see your client secret, contact your Palantir administrator.

== Generate Palantir OAuth token

To connect to Palantir using a OAuth token, generate a new link:https://www.palantir.com/docs/foundry/platform-security-third-party/user-generated-tokens/#generation[Palantir token^].

== Next steps

After your Domino administrator creates the Data Source, learn how to link:ccd264[connect to Palantir as a user].

----- admin_guide/platform-management/manage-data/external-data/data-sources/connect-to-s3-with-aws-glue.txt -----
:page-version: 6.1
:page-permalink: 72362d
:page-title: Connect to Tabular S3 with AWS Glue
:page-sidebar: Connect to Tabular S3
:page-order: 30

You can create a Data Source for https://docs.aws.amazon.com/athena/latest/ug/data-sources-glue.html[tabular S3 with AWS Glue^] that your Domino users can access. Only administrators can create this type of Data Source, and only service accounts are supported.

== Prerequisites

- Starburst needs to be enabled in the deployment. If not enabled, please follow the instructions to link:ae2e6b#starburst[enable Starburst-powered Data Sources].
- You must have network connectivity between S3 and your Domino deployment.
- Domino must enable this feature. If you do not see this Data Source type in your deployment, contact your Customer Success Manager.

== Create a Data Source for S3 with AWS Glue
//available for cloud admins

. Go to *Manage Resources* > *Data Sources*.
. Click *Connect to External Data*.
. In the New Data Source window, select *Tabular S3 with AWS Glue*.
. Enter your S3 account ID, database, and region.
. Enter a name for this Data Source.
. Click *Next*.
+
In the Credential Type screen, *Service Account* is pre-selected.  Individual accounts are not supported for this type of Data Source.
. Click *Next*.
. Enter your S3 service account access key ID and secret access key.
. Click *Next*.
. Select whether *Everyone* can access this Data Source or just *Specific users or organizations*.
. Click *Finish Setup*.
Domino alerts you that a cluster restart is needed.
+
TIP: You must restart your Starburst cluster after any configuration changes. Perform the restart when no executions are running that interact with Starburst-powered Data Sources.

. Click *Click here to trigger a restart*.
Domino prompts you to confirm that you want to restart the cluster.
. Click *Restart Cluster*.
The restart takes approximately 5-10 minutes.
When the restart is complete, Domino users can use the new Data Source.

----- admin_guide/platform-management/manage-data/external-data/data-sources/connect-to-starburst-jdbc.txt -----
:page-version: 6.1
:page-permalink: f32f69
:page-sidebar: Connect to Starburst JDBC
:page-title: Connect to Starburst JDBC data entities
:page-order: 40

Domino Data Sources can leverage the vast capabilities of Starburst's JDBC connection to connect to any Starburst-JDBC data entity. Enjoy all the benefits of link:fbb41f[Domino Data Sources] with expanded connectivity through Starburst JDBC.

The following Starburst JDBC-based data entities are enabled by default:

* IBM DB2
* IBM Netezza
* SAP HANA

== Prerequisites

Starburst needs to be enabled in the deployment. If not enabled, please follow the instructions to link:ae2e6b#starburst[enable Starburst-powered Data Sources].

== Create a Starburst JDBC-powered Data Source
 
To create a Starburst JDBC-powered Data Source, you must have network connectivity between the data platform and your Domino deployment.

//available for cloud admins

. Go to *Manage Resources* > *Data Sources*.
. Click *Connect to External Data*.
. Select the data entity you want to connect to under the *Powered by Starburst JDBC* section.
+
. Enter the appropriate connection parameters.
+
. Enter the *Data Source Name* and optionally provide a *Description* to explain the purpose of the Data Source to others.
. Select the credential type.
. Optionally, enter your credentials.
You can choose to enter these later.
. Click *Next*.
. Select whether *Everyone* can access this data source or just *Specific users or organizations*.
. Click *Finish Setup*.
+
To finish setup, you must restart the cluster. Domino will alert you that a cluster restart is needed and provide a link to restart the cluster. 
+
TIP: The restart takes approximately 5-10 minutes and disrupts any interactions with Starburst-powered Data Source. Plan on performing the restart when no executions are running that interact with Starburst-powered data sources.

== Add Starburst JDBC data entities

Not all JDBC-powered data entities are available by default. You can enable additional JDBC data entities such as the following:

* ClickHouse 
* Druid 
* Greenplum 
* MariaDB 
* Ignite
* SingleStore (MemSQL) 
* Synapse 
* Vertica 
* Generic JDBC capability provided by Starburst

NOTE: You may need to contact your Customer Success engineer for assistance to add some additional JDBC data entities.

. Go to *Admin* > *Platform settings* > *Configuration records*.
. Add or edit the following config option `com.cerebro.domino.datasource.starburstJdbc.enabledConnectors`.
. Add a comma-separated list of data-entity names as the value for the config option: 
clickhouse,druid,db2,synapse,greenplum,ignite,mariadb,singlestore,vertica,generic_jdbc,sap_hana,netezza
. Restart the Domino services to apply the new setting, following the prompt on the admin page.

You should now be able to see the newly added data entity in the drop-down list.

----- admin_guide/platform-management/manage-data/external-data/data-sources/connect-to-teradata.txt -----
:page-version: 6.1
:page-permalink: 0fc615
:page-title: Create a Teradata Data Source
:page-sidebar: Connect to Teradata
:page-order: 50

You can create a Data Source for https://www.teradata.com/[Teradata^] that your Domino users can access.  Only administrators can create this type of Data Source.

== Prerequisites

- Starburst needs to be enabled in the deployment. If not enabled, please follow the instructions to link:ae2e6b#starburst[enable Starburst-powered Data Sources].
- You must have network connectivity between Teradata and your Domino deployment.
- Domino must enable this feature. If you do not see this Data Source type in your deployment, contact your Customer Success Manager.

== Create a Teradata Data Source
//available for cloud admins

. Go to *Manage Resources* > *Data Sources*.
. Click *Connect to External Data*.
. In the New Data Source window, select *Teradata*.
. Enter your Teradata hostname or IP address.
. Enter a name for this Data Source.
. Click *Next*.
. Select the credential type.
. Optionally, enter your Teradata credentials.
You can choose to enter these later.
. Click *Next*.
. Select whether *Everyone* can access this Data Source or just *Specific users or organizations*.
. Click *Finish Setup*.
Domino alerts you that a cluster restart is needed.
+
image::/images/5.3/starburst-cluster-restart-prompt.png[Starburst cluster restart prompt]
+
TIP: You must restart your Starburst cluster after any configuration changes. Perform the restart when no executions are running that interact with Starburst-powered Data Sources.

. Click *Click here to trigger a restart*.
Domino prompts you to confirm that you want to restart the cluster.
. Click *Restart Cluster*.
The restart takes approximately 5-10 minutes.
When the restart is complete, Domino users can use the new Data Source.

[NOTE]
====
Users must specify the database name in their queries, as in this example where `td_demo` is the database name and `yellow_cab` is the table name:

----
example res = ds.query("select * from td_demo.yellow_cab")
----
====

----- admin_guide/platform-management/manage-data/external-data/data-sources/index.txt -----
:page-version: 6.1
:page-title: Create a Data Source
:page-permalink: ae2e6b
:page-order: 10

[[tr1]]
// Admin can register Service account Data Source
Admins can create, configure, and manage Data Sources.

See link:6a8639[Configure Data Source Authentication] to learn how to enable methods of authentication other than basic authentication for Data Sources that support them.

== Create a Data Source as an admin

Unlike regular users, Admins can expose their Data Sources to all or a subset of users. If you link:fa5f3a[create a Data Source as a user], you must specify the users or organizations that can access the Data Source.

. Go to *Manage Resources* > *Data Sources*.
. Click *Connect to External Data*.
. Select your Data Source.
. Enter the configuration details for accessing your Data Source.
+
The details vary depending on your Data Source type.
. Select the credential type:
+
[[tr5]]
// User with valid Data Source credentials should be able to authenticate and use Data Source with Individual permissions
* *Individual* - each user, when they are granted access to the Data Source, must provide their own credentials before using a Data Source.
[[tr6]]
// Any user with service Data Source permissions should be able to use Data Source without providing personal credentials
* *Service Account* - Domino administrators provide a set of static credentials that will be automatically applied on behalf of all users with permissions to a given Data Source. End users cannot access or extract the credentials, nor can they manage user-level permissions on such Data Sources.
+
Data source credentials are stored securely in the Domino secret store which is backed by https://www.vaultproject.io/[HashiCorp Vault^].
+
NOTE: OAuth authentication is not supported for service accounts. End users are not distinguished by the Data Source itself upon usage, as the same credentials are used to authenticate all queries.
+
. Enter your credentials for Data Source authentication.
+
. Click *Test Credentials* to verify that authentication works, or click *Skip for Now* to continue.
. Select whether *Everyone* can use this Data Source or just *Specific users or organizations*.
+
. Click *Finish Setup*.
// To do: add that if you have enabled advanced authentication then you will see an OAuth option in the Authenticate step.

//ifeval for Starburst
[[starburst]]
== Starburst-powered Data Sources

Unlike other Data Sources, Starburst-powered Data Sources require an administrator to configure since they require an admin to restart the Starburst cluster in Domino.
To enable Starburst-powered Data Sources in your Domino instance, add the following block to `domino.yaml` and `agent.yaml`.

[source,yaml]
----
release_overrides:
starburst-enterprise:
  installed: true
----

See the following guides to configure Starburst-powered Data Sources as an admin:

* link:ed290d[MongoDB]
* link:a932d7[Palantir Foundry]
* link:72362d[Tabular S3 with AWS Glue]
* link:f32f69[Starburst JDBC]
* link:0fc615[Teradata]

For additional support in working with Starburst, please contact your Domino representative.

=== Starburst cluster restarts

You must restart the Starburst cluster in Domino after performing the actions including, but not limited to:

- Creating the Data Source
- Updating the Data Source configuration
- Updating credentials

Domino notifies you to restart your Starburst cluster and provides a convenient link to trigger the restart. Restarting the Starburst cluster disrupts executions that interact with Starburst-powered Data Sources, so plan accordingly.

image::/images/5.3/starburst-cluster-restart-prompt.png[Starburst cluster restart prompt]

=== Starburst Data Source limitations

- Your Domino representative must enable this feature. If you do not see these Data Source types in your deployment, contact your Customer Success engineer.
- In a hybrid deployment, Starburst-powered Data Sources are only available from the local data plane.


== Nexus hybrid Data Plane accessibility

Nexus hybrid deployments have a few caveats regarding data plane accessibility for Data Sources.


Starburst-powered Data Sources can only be used in local data planes.


The following table shows accessibility configuration settings for different Data Source types:

// tag::hybrid-data-table-head[]
[cols="2a,2a,2a",options="header"]
|===
|Data Source Type
|Remote Data Plane(s)
|Local Data Plane
// end::hybrid-data-table-head[]

|Domino native Data Sources using individual credentials
|Automatically accessible and unconfigurable.
|Automatically accessible and unconfigurable.

|Domino native Data Sources using service account credentials
|Accessible with the ability to disable access from any or all remote data planes.
|Accessible with the ability to disable access.

|Starburst-powered Data Sources
|Unsupported.
|Supported.
|===


// Clarify if Starburst-powered Data Sources are available in hybrid
You can disable adding a Data Source in remote workspaces by setting the `EnableAddingDataSourcesInRemoteWorkspaces` link:6469bf[feature flag] to `false`.


//Cloud admins have access to this
----- admin_guide/platform-management/manage-data/external-data/external-data-volumes/add-a-data-plane.txt -----
:page-version: 6.1
:page-permalink: 368a08
:page-title: Associate Data Planes with EDVs
:page-order: 30

In a hybrid deployment, EDVs are associated with specific link:5781ea[data planes].
For security reasons, the set of data planes which can be used with an EDV does not change automatically.
However, you can edit it by modifying the PVCs present in the relevant data plane, then editing the EDV as explained below.

== Add a Data Plane to an EDV

. Create the PV and PVC in the new data plane, with names matching the PVC already used for the EDV.
. Click the three-dot menu on the right side of the EDV table for the EDV that you wish to modify, then click *Edit*.
+
You will see the new data plane at the bottom of the modal, highlighted in green to indicate that it is being added to the EDV.
+
If you do not see the new data plane, confirm the name of the PV and the name and label of the PVC.

. Click *Update* to update the EDV and add the new data plane.


== Remove access to an EDV from a Data Plane

. Remove the PVC and PV from that data plane.
. Edit the EDV as described above.
+
You will see the data plane highlighted in red to indicate that it will be removed.

. Click *Update* to save these changes and remove the data plane from the EDV.

----- admin_guide/platform-management/manage-data/external-data/external-data-volumes/configure-censorship.txt -----
:page-version: 6.1
:page-title:  Configure censorship
:page-permalink: eb09fa
:page-order: 70

Multiple users collaborating on the same project might not all have the same level of access to External Data Volumes (EDVs).
EDVs added to the project must not be accessible to users without volume access, and under no circumstance will a user without volume access to an EDV be able to mount that EDV in a supported execution.
However, you can manage the visibility of the EDV in the application.
To do this, use the levels of censorship to choose between security and discoverability needs.

[[tr13]]
// EDVs are not discoverable when full censorship is set to TRUE
* *Full censorship*.
Only the existence of any inaccessible EDV is made known to the user; the quantity and any metadata (such as name or description) is not made known to the user.
This is the level for those that want the highest level of security.
[[tr12]]
//  Censorship banner is displayed and EDVs are discoverable when censorship is inactive
* *Inactive censorship*.
Inaccessible EDVs are made known to the user; the EDV metadata (such as name and description) is made known to the user.
This is the level that promotes discoverability.
With discoverability, users can escalate to you to gain volume access.
This is the default level of censorship.
+
See link:ee8d01[Mount an External Volume] to see what your users see.

To set the level of censorship, use the `ShortLived.ExternalDataVolumesFullCensor` link:6469bf#ExternalDataVolumesFullCensor[feature flag].


[[tr14]]
// External Data Volume Support Jobs

[[tr15]]
// Domino User can access mounted external data volumes in Scheduled Jobs

[[tr16]]
// External Data Volumes in Workspaces [DFS Project]

[[tr17]]
// External Data Volume Support Git-Based Projects

[[tr18]]
// External Data Volume Support Launchers

[[tr19]]
// External Data Volume Support Apps

----- admin_guide/platform-management/manage-data/external-data/external-data-volumes/edit-registered-edv.txt -----
:page-version: 6.1
:page-title: Edit registered External Data Volumes
:page-sidebar: Edit registered EDVs
:page-permalink: 901698
:page-order: 50


[[tr10]]
// External Data Volume Admin Edit Workflow

. From the Admin application, go to
*Manage resources* > *External Data Volumes*.
. Go to the end of the row for the EDV entry and click the vertical three dots.
. Click *Edit*.
The Edit an External Volume page opens.
----- admin_guide/platform-management/manage-data/external-data/external-data-volumes/index.txt -----
:page-version: 6.1
:page-title: Manage External Data Volumes
:page-sidebar: Manage EDVs
:page-permalink: 053e1f
:page-order: 20

You can use External Data Volumes (EDVs) to mount and access data that might exist on external remote file systems.

See link:f12554[External Data Volumes] in the User guide for information.

[[tr1]]
// Kubernetes Persistent Volume (PV) and Persistent Volume Claim (PVC) setup
You must register external data volumes with Domino before they can be used.
Prior to registering an EDV, you must create a Kubernetes
Persistent Volume (PV) and Persistent Volume Claim (PVC) that represent the remote file system to be accessed through the EDV.

[[tr2]]
// External Data Volume Admin Page Table
All registered External Data Volumes are listed in a standard table, which display the EDV name, type, description, and volume access. In addition, for each registered EDV, the Projects column indicates which projects have added the EDV.

//available for cloud admins

Find more detailed information on how to manage EDVs in these topics:

* link:4cdae9[Set up Kubernetes PV and PVC.]
* link:9c3564[Register EDVs.]
* link:368a08[Associate Data Planes with EDVs.]
* link:190ca9[View registered EDV details.]
* link:901698[Edit registered EDVs.]
* link:d84809[Unregister EDVs.]
* link:eb09fa[Configure censorship.]

----- admin_guide/platform-management/manage-data/external-data/external-data-volumes/register-edvs.txt -----
:page-version: 6.1
:page-title: Register External Data Volumes
:page-sidebar: Register EDVs
:page-permalink: 9c3564
:page-order: 20

Follow the instructions in this topic to register an external data volume (EDV) with Domino.

In a hybrid deployment, if you have a shared volume that can be accessed from multiple link:5781ea[data planes], you can manage access to it using a single EDV across multiple data planes.
Domino matches EDVs to PersistentVolumeClaims (PVCs) using the name of both the PVC and the underlying persistent volume (PV).
Multiple data planes are automatically detected when registering the EDV.

If you see a PVC showing up in a data plane that you do not intend to use with an EDV, rename that PVC or PV so as not to match the PVC name of your EDV.


[[tr3]]
// External Data Volumes Testable Requirements Register EDV
.To register an EDV
//available for cloud admins BUT requires nexus enabled (verify)
. From the Admin application, go to
*Manage resources* > *External Data Volumes*.
. Click *Register External Volume*.
The Register an External Volume wizard opens.
. On the *Volume* page:
.. Select the volume type.
The supported volume types are NFS, Windows Share(SMB),
EFS, and Generic.

.. From *Available Volumes*, select a volume.
The volume names are those of the backing Kubernetes https://kubernetes.io/docs/concepts/storage/persistent-volumes/[persistent volume claims^] (PVCs).
+
When you select a volume in a hybrid deployment, Domino displays a list of link:5781ea[data planes] where that EDV can be used, such as "local" in the example below:
+
image::/images/5.5/hybrid/register-edv.png[alt="Register an External Data Volume", width=600, role=noshadow]

. Click *Next*
. On the *Configuration* step:
.. Enter a *Name*. This field defaults to the selected PVC name but can be changed.
Domino recommends that you name the EDV so that it is recognized by users based on the supporting use case or some organization-defined convention.
[[tr4]]
// Relative Mount Path should be unique
.. Enter a unique *Relative Mount Path* for the EDV for supported executions. This field defaults to the selected PVC name but can be changed.
This field must be unique to all registered EDVs.
There are a few reserved words.
.. Select *Mount as read-only* to set the EDV as read-only or clear the checkbox to make the EDV read-write.
This is enforced at the Domino layer.
More restrictive access controls at the Kubernetes or NFS layer overrule this setting.
For example, if the PVC access mode is set to read-only, it does not matter if this field allows for read-write; the underlying permission of read-only will be enforced.
[[tr5]]
// Read-only access
.. Enter a Description.
+
image::/images/4.x/edv-admin-register-configuration.png[alt="Configure an External Data Volume", width=600, role=noshadow]
. Click *Next*.
. On the *Access* step:
.. Select the *Volume Access*:
[[tr6]]
// EDVs access set to Everyone
* *Everyone*.
Allow EDV access to all logged-in users.
[[tr7]]
// EDVs access set to Specific users or organizations
* *Specific users or organizations*.
Limit EDV access to specific users and organizations.
+
image::/images/4.x/edv-admin-register-visibility.png[alt="Register visibility of an External Data Volume", width=600, role=noshadow]
+
[NOTE]
====
[[tr8]]
// EDV Admin Access
Regardless of this setting, you (SysAdmin) can always access any external data volume.
====
. Click *Register*.

----- admin_guide/platform-management/manage-data/external-data/external-data-volumes/set-up-kubernetes.txt -----
:page-version: 6.1
:page-title: Set up Kubernetes PV and PVC
:page-permalink: 4cdae9
:page-order: 10

[NOTE]
====
A Kubernetes administrator must set up Kubernetes
https://kubernetes.io/docs/concepts/storage/persistent-volumes/[persistent volumes^] (PVs) and https://kubernetes.io/docs/concepts/storage/persistent-volumes/[Persistent Volume Claims^] (PVCs).
====

Domino runs on a Kubernetes cluster and EDVs must be backed by an underlying Kubernetes https://kubernetes.io/docs/concepts/storage/persistent-volumes/[Persistent Volume^] (PV).
That persistent volume must be bound to a
https://kubernetes.io/docs/concepts/storage/persistent-volumes/[Persistent Volume Claim^] (PVC) that must be labeled with a key `dominodatalab.com/external-data-volume`.
The value of that key represents the type of external data volume.
The supported types are `NFS`, `SMB`,
`EFS`, and `Generic`.
The `Generic` type lets you attach your own volumes to Domino that you must deploy and maintain in your environment.
Finally, the PVC must be created in the Domino compute namespace.

The following are example `yaml` files for creating PVs and PVCs for each of the supported EDV types.

[NOTE]
====
Adjust the Domino compute namespace and PVC names before using the examples.
====

== NFS PV/PVC example

The following is a simple static provisioning example that you can use to create the required PV and PVC.
You can provision the PV that refers to the NFS share to expose in Domino using any mechanism appropriate for their environment.

[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
   name: pvc-nfs
   namespace: YOUR-DOMINO-COMPUTE-NAMESPACE    #change for your deployment
   labels:
      "dominodatalab.com/external-data-volume": "NFS"
spec:
   storageClassName: ""
   accessModes:
   - ReadWriteMany
   resources:
      requests:
         storage: 30Gi
   volumeName: pv-nfs
---
apiVersion: v1
kind: PersistentVolume
metadata:
   name: pv-nfs
spec:
   storageClassName: ""
   accessModes:
   - ReadWriteMany
   capacity:
      storage: 30Gi
   # Optionally explicitly specify a reference to the PVC that will be using this PV
   # Name and namespace must match the PVC created
   claimRef:
      apiVersion: v1
      kind: PersistentVolumeClaim
      name: pvc-nfs
      namespace: YOUR-DOMINO-COMPUTE-NAMESPACE   #change for your deployment
   persistentVolumeReclaimPolicy: Retain
   nfs:
      path: /mnt/export
      server: 10.0.0.26
----

== EFS PV/PVC example

The following is a simple static provisioning example that you can use to create the required PV and PVC.
For more detailed information, see the
https://github.com/kubernetes-sigs/aws-efs-csi-driver#example-links[EFS CSI documentation^].
You can only use the configuration options supported by the EFS CSI driver.

[source,yaml,subs="attributes"]
----
apiVersion: v1
kind: PersistentVolume
metadata:
   name: pv-efs
spec:
   storageClassName: ""
   accessModes:
   - ReadWriteMany
   capacity:
      storage: 30Gi
   # Optionally explicitly specify a reference to the PVC that will be using this PV
   # Name and namespace must match the PVC created
   claimRef:
      apiVersion: v1
      kind: PersistentVolumeClaim
      name: pvc-efs
      namespace: YOUR-DOMINO-COMPUTE-NAMESPACE   #change for your deployment
   persistentVolumeReclaimPolicy: Retain
   csi:
      driver: efs.csi.aws.com
      volumeHandle: <EFS file system id>:/<path>    #the path within the file system is optional
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
   name: pvc-efs
   namespace: YOUR-DOMINO-COMPUTE-NAMESPACE
   labels:
      "dominodatalab.com/external-data-volume": "EFS"
spec:
   storageClassName: ""
   accessModes:
   - ReadWriteMany
   resources:
      requests:
         storage: 30Gi
   volumeName: pv-efs
----

== SMB PV/PVC example

NOTE: The Server Message Block protocol (SMB) driver is disabled by default.
If you want to use an external data volume that is backed by an SMB persistent volume, add the following to your installer configuration and then run the installer:

[source,yaml]
----
release_overrides:
  csi-driver-smb:
    installed: true
----

The following is a simple static provisioning example that you can use to create the required PV and PVC.
For more detailed information, see the
https://github.com/kubernetes-csi/csi-driver-smb/blob/master/docs/driver-parameters.md[SMB CSI documentation^].
You can only use the configuration options supported by the SMB CSI driver.

[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
   name: pvc-smb
   namespace: YOUR-DOMINO-COMPUTE-NAMESPACE
   labels:
      "dominodatalab.com/external-data-volume": "SMB"
spec:
   storageClassName: ""
   accessModes:
   - ReadWriteMany
   resources:
      requests:
         storage: 30Gi
---
apiVersion: v1
kind: PersistentVolume
metadata:
   name: pv-smb
spec:
   storageClassName: ""
   accessModes:
   - ReadWriteMany
   capacity:
      storage: 100Gi
   claimRef:
      apiVersion: v1
      kind: PersistentVolumeClaim
      name: pvc-smb
      namespace: YOUR-DOMINO-COMPUTE-NAMESPACE    #change for your deployment
   persistentVolumeReclaimPolicy: Retain
   csi:
      driver: smb.csi.k8s.io
      nodeStageSecretRef:
         name: SMB-SECRET-NAME
         namespace: NAMESPACE-OF-SMB-SECRET
      volumeAttributes:
         source: //10.0.0.11/Share        #SMB server address and path
      volumeHandle: volume-handle-unique  # must be unique id for the cluster
   mountOptions:
   - dir_mode=0777
   - file_mode=0777
   - vers=3.0
----


== Generic PV/PVC example

The following is a simple static provisioning example that you can use to create a required generic PV and PVC with the EDV type as Generic.
Registered EDVs are available to add to projects and will be mounted to executions.

You must configure the PV based on the documentation available for the system that you are using.

[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
   name: generic-EDV
   namespace: YOUR-DOMINO-COMPUTE-NAMESPACE    #change for your deployment
   labels:
      "dominodatalab.com/external-data-volume": "Generic"
spec:
   storageClassName: ""
   accessModes:
   - ReadWriteMany
   resources:
      requests:
         storage: 30Gi
   volumeName: generic-EDV
---
apiVersion: v1
kind: PersistentVolume
metadata:
   name: generic-EDV
spec:
   storageClassName: ""
   accessModes:
   - ReadWriteMany
   capacity:
      storage: 30Gi
   # Optionally explicitly specify a reference to the generic EDV that will be using this generic EDV
   # Name and namespace must match the generic EDV created
   claimRef:
      apiVersion: v1
      kind: PersistentVolumeClaim
      name: generic-EDV
      namespace: YOUR-DOMINO-COMPUTE-NAMESPACE   #change for your deployment
   persistentVolumeReclaimPolicy: Retain
   nfs:
      path: /mnt/export
      server: 10.0.0.26
----


All properly labeled PVCs will be available candidates to register in the Domino External Data Volumes page.

== Next steps

* After link:9c3564[registering External Data Volumes (EDVs)], you can link:368a08[associate Data Planes with EDVs].
* Learn how to link:190ca9[view], link:901698[edit] EDVs, or link:d84809[unregister EDVs].
* Manage the visibility of the EDV by link:eb09fa[configuring censorship].
----- admin_guide/platform-management/manage-data/external-data/external-data-volumes/unregister-edvs.txt -----
:page-version: 6.1
:page-title:  Unregister External Data Volumes
:page-sidebar:  Unregister EDVs
:page-permalink: d84809
:page-order: 60

[[tr11]]
// External Data Volume Admin Unregister Workflow

. From the Admin application, go to
*Manage resources* > *External Data Volumes*.
. Go to the end of the row for the EDV entry and click the vertical three dots.
. Click *Unregister*.

. Click *Yes* to confirm that you want to unregister the volume.

----- admin_guide/platform-management/manage-data/external-data/external-data-volumes/view-registered-edv-details.txt -----
:page-version: 6.1
:page-title: View registered external data volume details
:page-sidebar: View registered EDV details
:page-permalink: 190ca9
:page-order: 40

[[tr9]]
// View EDV

. From the Admin application, go to
*Manage resources* > *External Data Volumes*.
+
In a hybrid deployment, the list of EDVs includes the link:368a08[data planes associated with each one].

. Click the *Name* of the EDV to see more details.


----- admin_guide/platform-management/manage-data/external-data/index.txt -----
:page-version: 6.1
:page-title: Manage external data connections
:page-permalink: 3653e8
:page-order: 10


link:ae2e6b[Data Sources]::
You can configure Domino to connect to external data sources such as databases and data services.
You can also write data from the external data source to a Domino Dataset.

link:053e1f[External Data Volumes]::
How to access data from remote file systems.

link:6a8639[Configure Data Source authentication]::
Enable additional authentication methods such as OAuth.

See also link:ba5bad[Work with Domino Datasets] in the User Guide to learn how to solve problems, improve collaboration, and open new workflow possibilities in Domino.

----- admin_guide/platform-management/manage-data/index.txt -----
:page-version: 6.1
:page-title: Manage data
:page-permalink: 352624
:page-order: 30

Data moves in and out of Domino executions in the following ways:

link:a7d4f1[Domino Volumes for NetApp ONTAP]:: 
Domino Volumes for NetApp ONTAP (NetApp Volumes) let Domino practitioners share data more easily across projects by storing files on external NetApp-backed storage. They enhance performance, eliminate duplication, and provide snapshot-based recovery.

link:3653e8[External data]::
User code that runs in Domino can use link:3653e8[Domino data sources] or third-party drivers to interact with external databases, APIs, and file systems to which the Domino-hosting cluster can connect.
Users can read and write from these external systems, and they can import data into Domino from such systems by saving files to their project link:305721[writing files to an output Dataset].

link:ae1654[Domino Datasets]::
You can configure Domino executions to mount link:0a8d11[Domino datasets] for input and output.
Datasets are network volumes mounted in the execution environment.
Mounting an input Dataset lets executions start quickly.
It also gives them access to large quantities of data because the data is not transferred to the local execution volume until user code performs read operations from the mounted volume.
Any data written to an output Dataset is saved by Domino as a new snapshot.

link:373d37[Domino Project file storage]::
Every execution occurs in a link:a8e081[Project].
Based on the link:de4abb[Domino File System], the files for the active revision of the project are automatically loaded into the local execution volume for a Job or Workspace.
These files are retrieved from the link:373d37[Domino File Store], and any changes to these files are written back to the Domino File Store as a new revision of the project's files.
//confirmed with Murat that Domino File Store and Domino Blob Store are the same.

The following diagram shows the series of operations that happens when a user starts an execution (Job or Workspace) in Domino, and illustrates when and how various data systems can be used.

image::/images/4.x/admin_guide/data-flow.png[alt="The data flow", role=noshadow]

For quick tips to troubleshoot issues with data in Domino, see link:e7055b[Troubleshoot data issues].

----- admin_guide/platform-management/manage-data/netapp-volumes/configure-netapp-volumes.txt -----
:page-permalink: 1cb009
:page-version: 6.1
:page-title: Configure NetApp Volumes
:page-sidebar: Configure NetApp Volumes
:page-order: 30

The Domino admin interface provides tools to configure and manage NetApp Volumes through a guided workflow. From the UI, administrators can register ONTAP Filesystems, define storage mappings, and monitor volume usage across projects. This includes setting dataplane associations, linking Kubernetes PVCs, and specifying storage classes backed by Trident.

== Register a Filesystem

Once a Filesystem is registered, the UI also enables viewing, auditing, and managing individual NetApp Volumes, streamlining volume lifecycle operations from creation to snapshot tracking.

image::/images/6.1/admin-guide/register-netapp-filesystem.png[alt="Register NetApp Filesystem", role=noshadow, width=400]

Admins can create ONTAP Filesystems from the *ONTAP* tab in the admin console:

. In the admin panel, go to *Manage Resources* > *NetApp Volumes*.
. Click *Add Filesystem* to create a new Filesystem.
. Provide the following information:
* *Name*: 1–250 characters; can include letters, digits, underscores, hyphens, and spaces
* *Data Plane*: Select from existing data planes; defines where the data will reside
* *Kubernetes Root PVC*: Name of the Root PVC (must exist in the selected dataplane)
* *Storage Class*: Kubernetes storage class name that uses Trident as the provisioner
. Click *Save*.

After registering a Filesystem, the next step is to create NetApp Volumes within that Filesystem. Here's how the flow continues from an admin and user perspective:

== View all NetApp Volumes

To view all existing NetApp Volumes in Domino, navigate to *Admin Panel* > *Manage Resources* > *NetApp Volumes*, then select the *Volumes* tab. This gives you a comprehensive list of all configured volumes along with key details for each one:

[cols="1,3", options="header"]
|===
| Column
| Description

| *Name*
| The name of the NetApp Volume as registered in Domino.

| *Owner Name*
| The user or project that owns the volume.

| *ONTAP Filesystem*
| The ONTAP Filesystem in which this volume resides.

| *Status*
| The current lifecycle state of the volume (e.g., Active, Deleted, Error).

| *Permissions*
| Access level settings for the volume (e.g., read/write access by owner or project).

| *Size*
| The allocated capacity of the volume.

| *Number of Snapshots*
| Total count of saved snapshots for this volume.
|===

== Next steps

* link:2b2aa2[Setup NetApp Volumes] to get *Domino Volumes for NetApp ONTAP* ready for use.
* link:e6887f[Create and configure NetApp Volumes] from Domino or within a specific project.
----- admin_guide/platform-management/manage-data/netapp-volumes/index.txt -----
:page-permalink: a7d4f1
:page-version: 6.1
:page-title: Setup NetApp Volumes
:page-order: 5

link:cf04be[Domino Volumes for NetApp ONTAP overview]::
Learn how Domino and NetApp fit together to provide a centralized data storage solution.

link:2b2aa2[Setup NetApp Volumes]::
Has the information you need to get Domino Volumes for NetApp ONTAP (NetApp Volumes) ready for use.

link:1cb009[Configure NetApp Volumes]::
Guides you with instructions on how to configure NetApp Volumes with the UI.

----- admin_guide/platform-management/manage-data/netapp-volumes/netapp-volumes-overview.txt -----
:page-permalink: cf04be
:page-version: 6.1
:page-title: Domino Volumes for NetApp ONTAP overview
:page-sidebar: NetApp Volumes overview
:page-order: 10

Domino Volumes for NetApp ONTAP (NetApp Volumes) let Domino practitioners share data more easily across projects by storing files on external NetApp-backed storage. They enhance performance, eliminate duplication, and provide snapshot-based recovery.

image::/images/6.1/admin-guide/netapp-domino-diagram.png[alt="Domino Volumes for NetApp ONTAP", width=800]

This feature replaces scattered, project-specific data solutions with a centralized system. NetApp Volumes live inside ONTAP Filesystems, which define where the data is stored and how it's managed.

== How NetApp ONTAP Filesystems work in Domino

A link:https://docs.netapp.com/us-en/ontap/concepts/snapshot-copies-concept.html[NetApp ONTAP Filesystem] defines where NetApp Volumes live by linking a dataplane and storage class to a named storage container. It helps with:

* Clear visibility into where volumes are stored.
* Support for multiple accounts and dataplanes, which is essential due to per-account volume limits.
* Improved deletion flow by allowing volumes to be moved to cold storage instead of being permanently deleted. This makes it easier to enforce retention policies while reducing the risk of accidental data loss.

This section provides a detailed walkthrough for manually setting up the NetApp Volumes feature in a Domino deployment.

It is intended for administrators who need complete control over the infrastructure and who are deploying into AWS environments.

== Next Steps 
* link:2b2aa2[Setup NetApp Volumes]: has what you need to do to get *Domino Volumes for NetApp ONTAP* ready for use.
* link:1cb009[Configure NetApp Volumes]: has information and instructions on how to configure NetApp Volumes with the UI.
* link:e6887f[Create NetApp Volumes from Domino or a project]: Create and configure new NetApp Volumes from Domino or within a specific project.
----- admin_guide/platform-management/manage-data/netapp-volumes/setup-domino-for-netapp-volumes.txt -----
:page-permalink: 2b2aa2
:page-version: 6.1
:page-title: Setup NetApp Volumes
:page-sidebar: Setup NetApp Volumes
:page-order: 20

Domino Volumes for NetApp ONTAP provides a centralized data storage solution that integrates Domino with external NetApp-backed infrastructure. It enables cross-project data sharing, improves I/O performance, eliminates redundancy, and supports snapshot-based recovery.

Volumes reside within ONTAP filesystems, which bind a Domino dataplane and storage class to a named storage container. This architecture supports multi-account scaling, provides precise visibility into storage allocation, and enables retention-aware deletion through cold storage offloading.

This guide provides step-by-step instructions for manually configuring NetApp Volumes in a Domino deployment, intended for administrators managing infrastructure in AWS environments.

== Prerequisites

Before you can begin working with NetApp Volumes, you’ll need to verify that you have these prerequisites in place:

[cols="1,3", options="header"]
|===
| Component
| Description

| *Domino*
| Have a Domino deployed and configured.

| *ONTAP filesystem*
| Have a supported ONTAP filesystem. This is where the volumes will be made and how they will interact with your underlying infrastructure.

| *Trident*
| Installed and working with your file system.

| *Communication*
| Verified that Domino and ONTAP can talk to each other.

| *Storage Configuration*
| *Root system file name*: This is the base of the filesystem where all other volumes are mounted.

| *Feature Flag*
| Verify that the feature flag `EnableDominoNetAppVolumes` is set to `true`.
|===

== Step 1: Kubernetes configuration for Domino

There are a couple of steps you’ll need to take to configure Kubernetes for Domino.

* *Role and RoleBinding*: A Kubernetes `Role` and `RoleBinding` are required to enable the microservice to authenticate and communicate with Trident.
* *Network policy*: You’ll also need a network policy to allow the right Kubernetes resources to communicate with Trident.

=== Create Role and RoleBinding

You can use these examples as-is to create and apply your Role and RoleBinding:

==== Create Role

[source,yaml]
----
# role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: data-plane-agent
  namespace: trident
rules:
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["get"]
----

*Apply the Role*:

[source,yaml]
----
vim <my_role>.yaml  # copy/paste the contents above
kubectl apply -f <my_role>.yaml
----


==== Create RoleBinding

[source,yaml]
----
# rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: data-plane-agent
  namespace: trident
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: data-plane-agent
subjects:
  - kind: ServiceAccount
    name: data-plane-agent
    namespace: domino-compute
----

*Apply the RoleBinding*:

[source,yaml]
----
vim <my_role_binding>.yaml  # copy/paste the contents above
kubectl apply -f <my_role_binding>.yaml
----

=== Create a Network Policy

Use this example as a template for your `NetworkPolicy` resource:

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: trident-operator
  namespace: trident
spec:
  podSelector:
    matchLabels:
      app: controller.csi.trident.netapp.io
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from: []
      ports:
        - protocol: TCP
          port: 8443
        - protocol: TCP
          port: 8001
  egress:
    - to: []
----

*Apply the Policy*:

[source,yaml]
----
vim <my_network_policy>.yaml  # copy/paste the contents above
kubectl apply -f <my_network_policy>.yaml
----

== Step 3: Create Root PVC

When you created a filesystem in Amazon FSx or NetApp Cloud ONTAP, it included a special root volume at the path `/`. This is the base of the filesystem where all other volumes are mounted.

To make it usable in Domino, you’ll need to create a Kubernetes `PersistentVolumeClaim` (PVC) that maps to this root volume. The example below shows how to define that PVC using an import:

[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: fsx-imported-volume                                                     # Arbitrary name
  namespace: domino-compute                                                     # Should always be the compute namespace
  labels:
    dominodatalab.com/netapp-storage: "true"                                    # IMPORTANT. This is needed so the Root PVC can be found during Filesystem creation (in the next step)
  annotations:
    trident.netapp.io/importBackendUUID: "c9c51153-a02d-4f67-b3ca-6e11c8e80ee2" # This can be found by inspecting the tridentbackendconfig custom resource in the cluster, under the status.backendInfo.backendUUID field
    trident.netapp.io/importOriginalName: demo_root                             # Name of the root volume in your Filesystem. Can be found in AWS FSx UI (The name is usually <deployment_id>_svm_root)
    trident.netapp.io/notManaged: "false"
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 953Mi
  storageClassName: democlass                                                   # Use the storage class of the trident storage class in the cluster
  volumeMode: Filesystem
----

*Apply Root PVC*:

[source,yaml]
----
vim <root_pvc>.yaml <paste template and change values> kubectl apply -f <root_pvc>.yaml
----

*Verify status*:

[source,yaml]
----
kubectl get pvc <root_pvc> -n domino-compute
----

Make sure the returned message indicates `STATUS - Success`.

Your Domino deployment is now fully configured to use NetApp Volumes for scalable, managed storage.

== Next Steps

* link:1cb009[Configure NetApp Volumes]: has information and instructions on how to configure NetApp Volumes with the UI.
* link:e6887f[Create NetApp Volumes from Domino or a project]: Create and configure new NetApp Volumes from Domino or within a specific project.
----- admin_guide/platform-management/manage-data/project-file-storage/delete-file-instances.txt -----
:page-version: 6.1
:page-permalink: 26c944
:page-title: Delete file instances
:page-order: 10

//DOCS-1658 moved to project file storage as Nitin said this applies to project files and not datasets 09/2022

You can use the full delete operation to completely remove all instances of a file from the Domino File System.
A full delete finds all instances of the file's contents across all revisions of all projects, erases the contents, and replaces them with a message indicating that the file was subject to a full delete.
This affects all files that have contents that are identical to the target file, even if they have different filenames.
It does not affect files with the same filename if they have different contents.

== Perform a full delete

. In the Project that the file belongs to, click
*Code*.
. Click the three dots at the end of the row and click *Full Delete*.
+
OR
+
Click the file to delete, and then click *Full Delete*.
+
NOTE: You can perform a full delete on one file at a time.

. When prompted, enter a commit message and then confirm the full delete.
The full delete removes all instances of the file contents from the Domino File System.
+
[NOTE]
====
* This operation does not alter the revision history of the affected projects.
All commits that contained the file contents continue to exist, but when viewing the file contents in the project, you only see the full delete message.
* A limitation of full delete is if there are published Domino endpoints from project revisions containing files now fully deleted. Those Domino endpoint images still contain that file.
You cannot permanently purge that image from Domino.
Contact support@dominodatalab.com if you have questions about such files.
====

== Browse to a file and click delete

In rare cases, you may need to purge a file completely from the Blob Store. For example, if you discover that a file contains data that must be deleted for compliance reasons.

To remove a file so that it cannot be restored, browse to the file and click *Full Delete*.

You'll be prompted for a commit message and warned about the implications of this.

IMPORTANT: A full delete will irrevocably remove all files that match the exact _contents_ of the file you are deleting. For example, if multiple projects all contain a `data.txt` file with the exact same contents, full-deleting it in one Project will full-delete it everywhere.

After a full-delete operation, someone who tries to access the deleted file in the future will instead find a file with the message you entered in the “commit message” prompt.

----- admin_guide/platform-management/manage-data/project-file-storage/index.txt -----
:page-version: 6.1
:page-title: Manage Project file storage
:page-permalink: 373d37
:page-order: 30

While at rest, link:a8e081[project files] are stored in a durable object storage system, referred to as the Domino Blob Store.

Domino natively supports the Domino File Store with the following cloud storage services:

* Amazon S3
* Azure File Storage
* Google Cloud Storage

The Domino File Store can also be backed with a shared Kubernetes Persistent Volume from a link:25b6dc#storage-classes[compatible storage class].
You can provide an NFS storage service and the Domino installation utilities can deploy the https://github.com/kubernetes-retired/external-storage/tree/master/nfs-client[nfs-client-provisioner^] and configure a compatible storage class backed by the NFS system.

== Encrypted Project files

Domino supports server-side encryption with customer-provided keys (https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html[SSE-C^]) for Amazon S3.

Domino supports https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html[EBS file system encryption^] using the industry-standard AES-256 algorithm on Elastic Block Store.

Domino also supports default encryption keys for:

* Amazon S3
* Azure File Storage
* Google Cloud Filestore

Domino does not provide pre-write encryption for `nfs-client-provisioner` volumes.

== How data is stored in Project files

When a user starts an link:942549[execution] in Domino, the files from their project are fetched from the Domino File Store and loaded into the execution in the working directory of the link:de4abb[Domino File System].
When the execution finishes, or the user initiates a manual link:262fef[sync] in a link:e6e601[Workspace session], any changes to the contents of the working directory are written to Domino as a new revision of the project files.
Domino's versioning system tracks file-level changes and can provide link:dae129[rich file difference information] between revisions.

Domino also has several features that provide users with paths to quickly initiate a file sync.
The following events in Domino can trigger a file sync, and the subsequent creation of a new revision of a project's files.

* User uploads files from the Domino application upload interface.
//define this interface
* User authors or edits a file in the Domino web application file editor.
//define this interface
* User syncs their local files to Domino from the link:e21e55[Domino Command Line Interface^].
* User uploads files to Domino through the link:f35c19[Domino API^].
* User executes code in a link:942549[Domino Job] that writes files to the working directory.
* User writes files to the working directory during a
link:867b72[Workspace session], and then initiates a manual sync or commits those files when the session finishes.

By default, all revisions of project files that Domino creates are kept indefinitely, since project files are a component in the Domino Reproducibility Engine.
//somewhere - architecture - the Domino Reproducibility Engine should be described.
Users can always return to and work with past revisions of project files, except for files that have been subjected to a link:26c944[full delete] by a system administrator.

=== Access to data in Project files

Users can read and write files to the projects they create, on which they automatically are granted an Owner role.
Owners can add collaborators to their projects with roles and associated files permissions.
See link:7876f1[Collaborator Permissions] for details.

Users can also inherit roles from membership in Domino link:526a62[Organizations].
Domino users with some administrative system roles are granted additional access to project files across the Domino deployment they administer.
See link:2611b7[Roles] for more information.

----- admin_guide/platform-management/manage-data/troubleshoot-data.txt -----
:page-version: 6.1
:page-permalink: e7055b
:page-title: Troubleshoot data issues
:page-order: 50

These are some quick tips to troubleshoot issues with data in Domino.

== Check the network connection

Ensure that there is a clear route between Domino and the external data store.
Confirm that firewall access rules allow traffic between the Domino deployment and the data store.

In the case of a link:c65074[Domino Nexus hybrid deployment], check that there is a clear route between the Domino control plane and the link:5781ea[data plane].

== Check which Data Planes have access to the data

In a link:c65074[Domino Nexus hybrid deployment], different data stores are available on different link:5781ea[data planes].

Some features are only support data available in the `Local` data plane (hosted in the control plane):

* Domino endpoints
* Apps
* Starburst-powered data sources

Other types of data sources can be accessed on both the `Local` and remote data planes, but functionality on remote data planes is in Preview.

EDVs are the primary method for accessing large data sets in Domino Nexus; they have a first-class notion of data locality.

----- admin_guide/platform-management/manage-model-deployments/deployment-targets.txt -----
:page-permalink: 57edf2
:page-version: 6.1
:page-title: External deployment targets
:page-order: 40

External deployment targets are connections to external resources and can be configured to enable the deployment of models outside of Domino. Typically, a deployment target is a unique account and region to which models can be deployed. This documentation explains how to setup an external deployment target.

For information on how to deploy an external endpoint, see the link:02ec6d[Deploy Models to SageMaker] documentation.

NOTE: SageMaker is the only type of external deployment target that is currently supported. Deploying models to SageMaker is only supported when the Domino control plane is running on an link:https://aws.amazon.com/eks[AWS EKS cluster^].

== Create an external deployment target

To create an external deployment target:

. Navigate to the *External deployment targets* page, located under *Resources* in the *Admin panel*, then click on *Create deployment target*.
+
image::/images/6.0/sagemaker/deployment-targets.png[alt="Deployment Targets Landing", width=1000, role=noshadow]

. Complete all the required details for the external deployment target:

- *Name*: The name of the external deployment target. Give this a user-friendly name (e.g. Production West) to help data scientists choose the appropriate target during deployment.

- *Account ID*: The AWS account ID to use with this deployment target.

- *Use IRSA role credentials*: Whether to assume the execution role in the target AWS account using the Domino external deployments IAM role.

- *Credentials file contents*: The contents of an AWS credentials file containing credentials for a user that can assume the execution role in the target account. See the link:https://docs.aws.amazon.com/sdkref/latest/guide/file-format.html[AWS documentation^] for the format of this file. This field will be ignored if IRSA role credentials are enabled.

- *Execution role name*: The name of the execution role to assume when accessing AWS.  This execution role must exist in the target account.

- *ECR repository name*: The name of the ECR repository where you want the deployment images to be stored.  This repository will be created if it does not exist in the target account.

- *Artifacts S3 bucket name*: The S3 bucket name where you want the deployment artifacts to be stored.  This bucket will be created if it does not exist in the target account.

- *Region*: The AWS region code in which deployments using this target will create resources. For example, `us-east-1` or `eu-west-2`.

- *Resource configurations*: The list of compute resources that are available in this deployment target. When practitioners deploy their endpoints, they will choose between the list of available resources. Make sure the selected instance types are available in the selected region and that you have quota available in your AWS account.

- *Permissions*: Select between making the deployment target available to everyone or only to specific users and organizations.

. Create the deployment target.

== Default deployment target

When Domino is running on an AWS EKS cluster, a default deployment target for the Domino AWS Account is automatically created.  This allows users to deploy external endpoints within the AWS account that runs the Domino platform.  This option can be disabled when Domino is deployed.  Domino admins can also change or remove this default target.
image::/images/6.0/sagemaker/default-deployment-target.png[alt="Default deployment target", width=1000, role=noshadow]

== Configure an AWS account for use as a deployment target

For information on how to configure an AWS account for use as a Domino deployment target, please reference the terraform module in the link:https://github.com/dominodatalab/terraform-external-sagemaker[dominodatalab/terraform-external-sagemaker^] repository.
----- admin_guide/platform-management/manage-model-deployments/dmm-access-control.txt -----
:page-version: 6.1
:page-permalink: 98361a
:page-title: Model Monitoring access control
:page-order: 20

This page describes how Domino administrators can grant or revoke access to Domino Model Monitor. This page includes a walkthrough of how to use the Keycloak UI and a script that you can use to grant or remove user access to Domino Model Monitor.


== Use the Keycloak UI

. Open the admin UI at `https://<deployment-url>/auth/admin` and log in to Keycloak. 

. Navigate to the *DominoRealm* realm from the dropdown:
+
image::/images/5.5/dmm_access_control_ui/dmm_access_1.png[alt="Navigate to the realm", width=400]

. Navigate to the *Users* tab. 

. Find the desired user (you may need to click *View all users*) and click *edit*.
+
image::/images/5.5/dmm_access_control_ui/dmm_access_2.png[alt="Find and edit the user"]

. Navigate to the *Groups* tab.
+
--
.To grant access:
.. Click *Available Groups* > *dmm-users*.
.. Click *Join*.

.To revoke access:
.. Click *Group Membership* > *dmm-users*.
.. Click *Leave*.
--
+
image::/images/5.5/dmm_access_control_ui/dmm_access_3.png[alt="Manage access for groups"]

. Repeat for all desired users.


=== Using the script

[IMPORTANT]
====
If there is a high volume of users, adjust the Keycloak *AccessToken* settings in the UI so the script runs uninterrupted:

. Log in to Keycloak.
. Navigate to *Realm Settings* in the *Master* realm.
. Click the *Tokens* Tab. 
. Increase the *Access Token Lifespan* to something sufficiently large.
. Save the changes, then follow the steps below to run the script. 

Make sure to undo the changes after the script is done running.

image::/images/5.5/dmm_access_control_ui/dmm_access_4.png[alt="Manage access token lifespan"]
====


. To use the link:/attachments/5.5/keycloak_dmm_access_control.sh[Domino Model Monitor Access Control Script], you must set environment variables based on your deployment:
+
--
* Domino host URL (`DOMINO_HOST`)
* Keycloak admin username (`KEYCLOAK_ADMIN_USERNAME`)
* Keycloak admin password (`KEYCLOAK_ADMIN_PASSWORD`)
* The Keycloak realm (`KEYCLOAK_REALM`) environment variable defaults to `DominoRealm` and can be overridden if needed.
--
+
[source, shell]
----
#   Domino host: DOMINO_HOST
#   Keycloak admin username: KEYCLOAK_ADMIN_USERNAME
#   Keycloak admin password: KEYCLOAK_ADMIN_PASSWORD
KEYCLOAK_REALM="${KEYCLOAK_REALM:-"DominoRealm"}"
----
+
To set these environment variables, replace the placeholders and run the following commands in the terminal:
+
[source, shell]
----
export DOMINO_HOST=https://placeholder.domino.tech
export KEYCLOAK_ADMIN_USERNAME=placeholder_username
export KEYCLOAK_ADMIN_PASSWORD=placeholder_password
----
+
Make sure that the trailing "/" is not included in the `DOMINO_HOST` variable.

. Run the script with a list of usernames and the command to execute.
+
The example below shows how to format the list of usernames:
+
[source, shell]
----
'("user_1", "user_2", "user_3", ... )'
----
+
Ensure that the usernames are correctly spelled out. If a username is misspelled or doesn't exist, the script silently skips that user and continues with the remaining users.
+
The command can either be `GRANT` or `REMOVE` to grant or remove Model Monitor access for the specified usernames.
+
For example, to grant Model Monitor access to two users (usernames `foo` and `bar`), run the following command:
+
[source, shell]
----
./keycloak_dmm_access_control.sh '("foo", "bar")' GRANT
----

----- admin_guide/platform-management/manage-model-deployments/index.txt -----
:page-version: 6.1
:page-title: Manage models
:page-permalink: 91b540
:page-order: 40

== Model Monitoring

link:16cd89[Model Monitoring Notifications]::
Configure Notifications for Model Monitoring.
Users can send notifications to recipients about thresholds that are breached during scheduled checks.

link:98361a[Model Monitoring Access Control]::
Configure access for Model Monitoring.

== Domino endpoint resources

link:9dece2[Scale Python Domino endpoints]::
Scale all Python Domino endpoints by setting the degree of parallelism.

== External deployment targets

link:57edf2[External deployment targets]::
Setup deployment targets for enabling external hosting of models.

----- admin_guide/platform-management/manage-model-deployments/model-monitoring-notifications.txt -----
:page-version: 6.1
:page-permalink: 16cd89
:page-title: Model Monitoring notifications
:page-order: 10

//This is in a folder in case there are future admin topics for Model Monitoring. Then, this can become a topic in the folder and Model Monitoring can become the first-level header.
// Configure Notifications for Model Monitoring

Users can send notifications to recipients about thresholds that are breached during scheduled checks.

See the following for more information:

* See link:386451[Notification Channels].
* See link:287438[Set Up Notifications] for a specific Domino endpoint.
* See link:9ebe39[Set Up Notifications] for a Model Monitor.

== Configure notifications

Before notifications can be sent, you must set up the email server.

. In the admin portal, go to *Platform settings > Configuration records*.
. Set the keys specified in link:71d6ad#tr240[Notifications for monitoring].
. Click *here* in the message at the top of the page to restart services.

----- admin_guide/platform-management/manage-model-deployments/scale-domino-endpoint.txt -----
:page-version: 6.1
:page-title: Scale deployed Domino endpoints
:page-permalink: 9dece2
:page-order: 30

To scale the performance of link:8dbc91[Domino endpoints] in Domino, you can scale hardware using Domino endpoint hardware tiers, or increase the degree of parallelism.


== Domino endpoint hardware tiers

Use *Domino endpoint hardware tiers* to scale your models deployed as link:8dbc91[Domino endpoints].

Since Domino endpoints often have different requirements than Workspaces and Jobs, Domino lets you classify specific hardware tiers for Domino endpoints, allowing you to tailor your hardware to meet the unique demands of machine learning model deployment.

NOTE: Domino endpoint tiers and regular hardware tiers are non-interchangeable.

== Create a Domino endpoint hardware tier

To create a new Domino endpoint hardware tier:

. From the admin home page, go to *Advanced* > *Hardware Tiers*.
. Click *New* to create a hardware tier, or click *Edit* to modify an existing hardware tier or set a default hardware tier.
. Select the desired hardware tier values, see link:c3aaf3[Create hardware tier].
. Select *Is Domino endpoint Tier*.
. You can also specify if you'd like this Domino endpoint tier to be the default for all Domino endpoints.

image::/images/model-apis/create-model-api-hardware-tier.png[alt="Create Domino endpoint hardware tier", width=1200, role=noshadow]




== Scale Domino endpoints

[[tr4]]
// Scale all python models

To scale all Python Domino endpoints, set the degree of parallelism.

NOTE: Only synchronous models support this.

. Go to *Admin* > *Platform settings* > *Configuration records*.
. Set `com.cerebro.domino.modelmanager.uWsgi.workerCount` to a value greater than its default value of `1` to increase the uWSGI worker count.
See the https://uwsgi-docs.readthedocs.io/en/latest/[uWSGI documentation^] for more information.
. The system shows the following message: `Changes here do not take effect until services are restarted.
Click here to restart services.`
. Click *here* to restart the services.

== Next steps

Learn more about link:9d16e5[Domino hardware tiers].

----- admin_guide/platform-management/notifications.txt -----
:page-version: 6.1
:page-permalink: ba9786
:page-title: Manage notifications
:page-order: 70

// available for cloud admins
Use notifications to send in-app messages to users about Domino.
For example, you might want to inform users about planned maintenance, remind them to save their work, or send onboarding messages to a small group of new hires.

The navigation bar alerts users if they have notifications.
See link:b889e6[Event notifications] for information about how your users experience these notifications.

[[tr1]]
// As a user, notifications do not require refresh to appear in my notifications list
Domino polls for notifications every 30 seconds.

[[send-custom-notifications]]
== Send custom notifications

[[tr2]]
// As an administrator, I cannot create more than 100 total notifications on the system
Administrators can create up to 100 notifications.


[[tr3]]
// As an administrator, I can create a notification from the Admin UI and give it a Priority level

. From the Admin application, click *Manage resources > Notifications*.
. Click *Create Notification*.
. In the new notification window:
.. Set the *Priority* to indicate the criticality of the message.
Messages set to Critical are prioritized to the top of the user's list.
[[tr4]]
// As an administrator, I can target notifications to specific users
[[tr5]]
// As an administrator, I can target notifications to specific organizations
[[tr6]]
// As an administrator, I can target notifications to specific global roles
.. Select one or more recipients.
The default is to send to all users and organizations.
Remove this option to limit the recipients to those that you select.
.. Type the title and text for the notification.
[[tr7]]
// As an administrator, I can set a notification to start at a later time
.. If necessary, change the *Timezone* to set the start and (optional) end time for the notification.
The timezone defaults to your system timezone which is used to timestamp when you created the notification.
[[tr8]]
// As an administrator, I can set a notification to automatically expire at a given time
.. Specify when you want the notification to be sent.
If you set an end time, the message will expire at this time. When the message expires, it is moved to the bottom of the user's list. If you do not set an end time, the notification will expire after the default expiration period that you configure.
.. Click *Create notification*.The new notification is listed in the Notifications table.

[[view-notifications]]
== View notifications

// As an administrator, I can view all notifications on the system

View the Domino home page to see a list of recent notifications. Click *View all* to see all notifications.

* Use the *Type* list to sort by Active or Expired notifications.
* Use the *Priority* list to see *All notifications* or limit the table to *Critical* or *Default* notifications only.

[[expire-notifications]]
== Expire notifications

// As an administrator, I can manually expire a notificiation effective immediately

If an issue is resolved, and you want to terminate a notification before its expiration date:

. From the Admin application, click *Manage resources > Notifications*.
. At the end of the row for the notification to terminate, click the three vertical dots.
. Click *Expire*.

[[delete-notifications]]
== Delete notifications

// As an administrator, I can manually delete a notification completely

You can manage your list of notifications, delete outdated or incorrect notifications.
When you delete a notification, no one can access it.

. From the Admin application, click *Manage resources > Notifications*.
. At the end of the row for the notification to delete, click the three vertical dots.
. Click *Delete*.

----- admin_guide/security-and-compliance/ca-certificates/index.txt -----
:page-version: 6.1
:page-title: Work with CA certificates
:page-permalink: dafa95
:page-order: 20

Intra-cluster encryption in transit is implemented (if enabled) through a deployed service mesh called https://istio.io[Istio^].
At installation, Domino can deploy Istio for Domino use only, or you can configure Domino to leverage an existing deployed Istio mesh on the Kubernetes cluster (potentially shared with other applications).
See link:7f4331#istio[Istio^] in Configuration Reference for details.

//Had to move these out of sentences as they were causing odd breaks
[[tr1]]
[[tr2]]

The following topics explain how to work with Certificate Authority (CA) certificates:

* link:9c5914[Set up custom CA certificates.]
* link:29ebd4[Update custom CA certificates.]

----- admin_guide/security-and-compliance/ca-certificates/set-up-custom-ca-certs.txt -----
:page-version: 6.1
:page-title: Set up custom Certificate Authority (CA) certificates
:page-sidebar: Set up custom CA certificates
:page-permalink: 9c5914
:page-order: 10

[IMPORTANT]
====
When Domino deploys Istio mesh, out of the box, Istio provides scalable https://istio.io/latest/docs/concepts/security/#pki[identity and X.509 certificate management^] for use with mTLS encryption, including periodic certificate and key rotation.

Because all encrypted communication is internal, [[tr3]] these certificates are not exposed or required for communication to any external services, such as web browsers and clients.

Domino acknowledges that enterprise policies might mandate the use of corporate public key infrastructure (PKI) and necessitate the use of certificate authority (CA) certificates.

====


NOTE: [[tr4]] All certificates must be X.509 PEM format and [[tr5]] keys must be passwordless.

. Obtain the certificate files, noting the file names for use in future commands.
+
[cols="2a,4a",options="header"]
|===
^|Filename ^|Description
|`root-cert.pem` |Root CA certificate for PKI.

|`ca-cert.pem` |Intermediate CA certificate from root CA. This is the
Istio CA certificate.

|`ca-key.pem` |Private key for Istio CA certificate.

|`cert-chain.pem` |Full chain from `ca-cert.pem` to `root-cert.pem`
[[tr6]](including both certificates).
|===
+
. Use the code in the following sample.
+
[source,shell]
----
# Concatenate all certificates into a certificate chain file
# Assuming `N` intermediate certificates denoted as `int-ca-<i>.pem`, with `i = {1,...,N}`
cat ca-cert.pem int-ca-1.pem ... int-ca-N.pem root-cert.pem > cert-chain.pem

# Create new kubernetes secret with CA certificate files
kubectl -n istio-system create secret generic cacerts 
    --from-file=./ca-cert.pem 
    --from-file=./ca-key.pem 
    --from-file=./root-cert.pem 
    --from-file=./cert-chain.pem
----
+
In a new standard Domino install, if you follow the install process, the `fleetcommand-agent` (Domino installer) automatically picks up the https://kubernetes.io/docs/concepts/configuration/secret/[secret^] that you created and Istio uses the configured certificates.
[[tr7]]
+
In an existing Domino installation, you must restart all the pods.
[[tr8]]

----- admin_guide/security-and-compliance/ca-certificates/update-custom-ca-certs.txt -----
:page-version: 6.1
:page-title: Update custom Certificate Authority (CA) certificates
:page-sidebar: Update custom CA certificates
:page-permalink: 29ebd4
:page-order: 20

This section describes how to update the custom CA certificates that Istio uses for intra-cluster encryption in transit in some scenarios.
You must always create a new full chain certificate file (`cert-chain.pem`).

[TIP]
====
Domino recommends backing up existing certificates and keys before updating new ones.
====

.Scenario 1: No changes were made to the private key and common name
[[tr9]]
This assumes only `ca-cert.pem` is updated.

Create a secret with the new files and restart the Istio daemon (`istiod`).

[source,shell]
----
# Delete existing secret with CA cert files
kubectl -n istio-system delete secret cacerts

# Create new secret with CA cert files
kubectl -n istio-system create secret generic cacerts 
    --from-file=./ca-cert.pem 
    --from-file=./ca-key.pem 
    --from-file=./root-cert.pem 
    --from-file=./cert-chain.pem

# Restarting all istiod pods
kubectl -n istio-system delete po -l app=istiod
----

.Scenario 2: Updates were made to the private key, common name, or upstream certificates
[[tr10]]

Changes made to the private key, common name (CN) or upstream certificates require that you recreate the `cacerts` secret and restart the Istio daemon.

[source,shell]
----
# Delete existing secret with CA cert files
kubectl -n istio-system delete secret cacerts

# Create new secret with CA cert files
kubectl -n istio-system create secret generic cacerts 
    --from-file=./ca-cert.pem 
    --from-file=./ca-key.pem 
    --from-file=./root-cert.pem 
    --from-file=./cert-chain.pem

# Full restart for all Istio pods
for NS in istio-system domino-platform domino-compute; do
    kubectl -n $NS get po --no-headers -o custom-columns=name:metadata.name | xargs kubectl -n $NS delete po
done
----

----- admin_guide/security-and-compliance/control-plane-security.txt -----
:page-version: 6.1
:page-permalink: 98ad4f
:page-title: Control plane security guidance
:page-order: 30

This guide outlines important security considerations that must be taken when configuring ingress for control plane services in link:c65074[Domino Nexus]. Depending on the customer's requirements and particular Domino configuration, the exact configuration of this ingress may vary significantly. This guide will therefore discuss these security principles in the abstract, without describing solutions for a particular infrastructure vendor.

== Architecture

Domino Nexus control planes host several important services which must be accessible from data plane clusters:

*Domino API* - This is the API which is used by the Domino CLI, UI, and now data planes. Ingress is already configured on the main endpoint by which users access Domino. This endpoint should be routable from data plane clusters. Because this is already configured, this guide will not discuss it further.

*Vault* - Data plane clusters use Hashicorp's Vault to authenticate with the control plane, encrypt secrets in transit, and authenticate with RabbitMQ. Communication with Vault happens over HTTPS on port 8200.

*RabbitMQ* - The primary communication channel between the Domino control plane and the data plane is based on RPC over RabbitMQ. Additionally, data planes publish execution state back to the control plane over RabbitMQ. This communication occurs using AMQP and RabbitMQ Stream protocols  on ports 5672 and 5552.

*Docker Registry* - If you are using Domino's internal Docker Registry then data plane Kubelets must be able to access the registry over HTTPS on port 443. This is used to pull environment images for user executions.

Vault and RabbitMQ in particular are sensitive services. Authentication and authorization for these services is tightly controlled. These services should not be generally available to the public Internet, to reduce the risk of compromised credentials or vulnerabilities in the services themselves.

image::/images/5.5/hybrid/security.png[alt="Control plane security", role=noshadow]

== Load balancers

Load balancers are typically used for Kubernetes ingress to route traffic from a stable DNS host to the variable set of cluster nodes hosting the service. Most infrastructure providers are able to configure Layer 4 (Network) and / or Layer 7 (Application) load balancers. Application load balancers can be used for Vault and Docker Registry because they use HTTPS, however a network load balancer is required for RabbitMQ because it is using AMQP.

=== TLS termination

Data planes _must_ communicate with the control plane services using TLS. These services do not support serving TLS connections themselves (unless Istio is in use, see below), and so load balancers must perform TLS termination. If your company issues non-public certificates, these can be used here, however see Domino's administrator documentation for guidance on creating data planes when non-public certificates are in use.

=== Network access

Load balancers should not be accessible from the public Internet. This can be achieved in two ways:

* The recommended way is to configure _internal_ load balancers, which sit in a private subnet, do not have a public IP address, and are only accessible using private networking such as VPN or peering.
Even with internal load balancers, you should limit source IP ranges to subnets which you expect to host data plane services.

* If your company does not have private networking between Nexus clusters, you _must_ restrict the network CIDRs which can access your load balancers. This involves identifying the IP addresses for NAT gateways used by data plane services, and limiting load balancer access to these source ranges, either through load balancer configuration or network security groups.

=== DNS

Data plane services must be able to reach load balancers using a stable DNS host.

Most cloud providers will publish _public_ DNS records for both _public_ and _private_ load balancers. This allows you to use public DNS to resolve private load balancers.

If you are hosting your own load balancers (such as on-premises), you must configure a DNS record (either public or private) to resolve to the load balancer's IP.
If the IP changes over time, you must keep the DNS record updated.
If you are using private DNS, ensure that pods in data plane clusters are able to resolve such hosts.

== Network policies

In traditional Domino deployments, access to RabbitMQ and Vault by services inside the cluster is tightly controlled.
Allowing ingress to these services from outside the cluster while limiting access from services inside the cluster is nontrivial because many types of access control operate at the network level and are not aware of cluster application level semantics.

In the architecture pictured above, it is recommended that load balancers are placed in a _separate_ subnet from cluster nodes and pods. This allows network policies to be defined which only allow traffic from this subnet, and not other pods in the cluster. Some load balancer types forward client IP addresses (used for evaluating network policies). In this case you can allow only address space known to be used by data plane sources.

In addition to network policies, you configure security groups between load balancers and cluster nodes in accordance with best practice.

== Istio

Communication between load balancers and backend services is often not encrypted in standard Kubernetes configurations. If you are not using Istio in your cluster, such traffic will already not be encrypted between local cluster pods and these services. To encrypt traffic between load balancers and Pods, you must have Istio enabled in your deployment.

If your company requires encryption in transit to be enabled, and you are running Istio in your Domino deployment, then you must take steps to allow the load balancer to connect to these services without using mTLS, while continuing to use TLS.

One way this can be accomplished is by using Istio's Gateway. In this architecture, load balancers will instead route traffic to Istio's Gateway service, which originates mTLS and allows traffic to enter the service mesh. If you go this route, ensure that the load balancer configured by Istio conforms with the guidance above, and uses TLS to communicate with the Istio Gateway service backend.

Another option is to configure https://istio.io/latest/docs/tasks/traffic-management/ingress/ingress-sidecar-tls-termination/[Sidecar TLS termination^]. With this method, _mutual_ TLS is disabled for the services and ports that we wish to expose. The sidecar is configured to serve custom TLS certificates to the load balancer to provide encryption in transit between load balancer and service. Domino only relies on Istio for encryption in transit, not service identity, so _mutual_ TLS is not necessarily required.

In either case, you can chose to have the Istio Gateway or Sidecar serve the external certificates for the control plane, or to leverage TLS termination in the load balancer.

----- admin_guide/security-and-compliance/custom-certificates/add-pem-bundle.txt -----
:page-version: 6.1
:page-permalink: 5349d1
:page-title: Add the PEM bundle to the ConfigMap
:page-sidebar: Add the PEM bundle
:page-order: 20

During Domino installation, you can add the contents of PEM bundle to the `domino.yml` configuration file with the top-level key `custom_certificates`.

The following is an example:

[source,shell]
----
...
custom_certificates: |
  -----BEGIN CERTIFICATE-----
  MIICKzCCAbGgAwIBAgIKe3G2gla4EnycqDAKBggqhkjOPQQDAzBaMQswCQYDVQQG
  EwJVUzETMBEGA1UECxMKZW1TaWduIFBLSTEUMBIGA1UEChMLZU11ZGhyYSBJbmMx
  IDAeBgNVBAMTF2VtU2lnbiBFQ0MgUm9vdCBDQSAtIEMzMB4XDTE4MDIxODE4MzAw
  MFoXDTQzMDIxODE4MzAwMFowWjELMAkGA1UEBhMCVVMxEzARBgNVBAsTCmVtU2ln
  biBQS0kxFDASBgNVBAoTC2VNdWRocmEgSW5jMSAwHgYDVQQDExdlbVNpZ24gRUND
  IFJvb3QgQ0EgLSBDMzB2MBAGByqGSM49AgEGBSuBBAAiA2IABP2lYa57JhAd6bci
  MK4G9IGzsUJxlTm801Ljr6/58pc1kjZGDoeVjbk5Wum739D+yAdBPLtVb4Ojavti
  sIGJAnB9SMVK4+kiVCJNk7tCDK93nCOmfddhEc5lx/h//vXyqaNCMEAwHQYDVR0O
  BBYEFPtaSNCAIEDyqOkAB2kZd6fmw/TPMA4GA1UdDwEB/wQEAwIBBjAPBgNVHRMB
  Af8EBTADAQH/MAoGCCqGSM49BAMDA2gAMGUCMQC02C8Cif22TGK6Q04ThHK1rt0c
  3ta13FaPWEBaLd4gTCKDypOofu4SQMfWh0/434UCMBwUZOR8loMRnLDRWmFLpg9J
  0wD8ofzkpf9/rdcw0Md3f76BB1UwUCAU9Vc4CqgxUQ==
  -----END CERTIFICATE-----
  -----BEGIN CERTIFICATE-----
  MIICTjCCAdOgAwIBAgIKPPYHqWhwDtqLhDAKBggqhkjOPQQDAzBrMQswCQYDVQQG
  EwJJTjETMBEGA1UECxMKZW1TaWduIFBLSTElMCMGA1UEChMcZU11ZGhyYSBUZWNo
  bm9sb2dpZXMgTGltaXRlZDEgMB4GA1UEAxMXZW1TaWduIEVDQyBSb290IENBIC0g
  RzMwHhcNMTgwMjE4MTgzMDAwWhcNNDMwMjE4MTgzMDAwWjBrMQswCQYDVQQGEwJJ
  TjETMBEGA1UECxMKZW1TaWduIFBLSTElMCMGA1UEChMcZU11ZGhyYSBUZWNobm9s
  b2dpZXMgTGltaXRlZDEgMB4GA1UEAxMXZW1TaWduIEVDQyBSb290IENBIC0gRzMw
  djAQBgcqhkjOPQIBBgUrgQQAIgNiAAQjpQy4LRL1KPOxst3iAhKAnjlfSU2fySU0
  WXTsuwYc58Byr+iuL+FBVIcUqEqy6HyC5ltqtdyzdc6LBtCGI79G1Y4PPwT01xyS
  fvalY8L1X44uT6EYGQIrMgqCZH0Wk9GjQjBAMB0GA1UdDgQWBBR8XQKEE9TMipuB
  zhccLikenEhjQjAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0TAQH/BAUwAwEB/zAKBggq
  hkjOPQQDAwNpADBmAjEAvvNhzwIQHWSVB7gYboiFBS+DCBeQyh+KTOgNG3qxrdWB
  CUfvO6wIBHxcmbHtRwfSAjEAnbpV/KlK6O3t5nYBQnvI+GDZjVGLVTv7jHvrZQnD
  +JbNR6iC8hZVdyR+EhCVBCyj
  -----END CERTIFICATE-----
----

The installer copies the contents of the custom certificates into the `domino-custom-certificates` ConfigMap.

----- admin_guide/security-and-compliance/custom-certificates/apply-custom-certs.txt -----
:page-version: 6.1
:page-permalink: 0c1d63
:page-title: Apply custom certificates
:page-order: 40



// [[tr6]] As an admin, I can restart certificate-dependent services to pick up cert changes

To apply the new certificate bundle, restart the Domino services that use custom certificates.

[CAUTION]
====
You might interrupt user workflows by restarting pods or deleting services. If downtime is acceptable in your environment, you can use the following command to delete all pods. This replaces the other commands in this section. Update the `namespace` to match the one used in your Domino installation.

[source,shell]
----
kubectl get pods -n <namespace> -ojson | jq -r '.items[] | select(.spec.volumes[]? | select(.configMap.name == "domino-generated-certificates")) | .metadata.name' | xargs kubectl delete pod -n <namespace> --wait=false
----

====

. Delete a pod to restart a service:
+
[source,shell]
----
kubectl delete pod -n <namespace> <pod name>
----
. Find the pods and services that use custom certificates. Update the `namespace` in each command to match the one used in your Domino installation.
+
[source,shell]
----
kubectl get pods -n <namespace> -ojson | jq -r '.items[] | select(.spec.volumes[]? | select(.configMap.name == "domino-generated-certificates")) | .metadata.name'
----

// [[tr7]] As an admin, when upgrading, if I do not specify a new ConfigMap the legacy map is copied and upgraded to the new format

----- admin_guide/security-and-compliance/custom-certificates/domino-custom-cert-configmap.txt -----
:page-version: 6.1
:page-permalink: 77bb0e
:page-title: Domino-custom-certificates ConfigMap
:page-sidebar: Domino-custom-certificates
:page-order: 10

Domino checks for `domino-custom-certificates` ConfigMap in the Kubernetes cluster's `default` namespace.
This ConfigMap must have a key named `bundle` with plaintext data of a certificate bundle in PEM format.
If the bundle exists, then Domino uses certificates from this bundle to connect to the external services.

Domino supports the following certificate types:

Custom Certificate Authority (CA):: Certificates that certify the other certificates issued under this authority.
Self-signed certificates:: Certificates that do not have a reference to the authority signing them.

The bundle is formatted as a series of concatenated certificates in PEM format.
You must have the line breaks around the lines:

// [[tr1]] As an admin, I can provide required certificates in the domino-custom-certificates ConfigMap
// [[tr2]] bundle header and footer must have surrounding line breaks

[source,shell]
----
-----BEGIN CERTIFICATE—--
----

and

[source,shell]
----
-----END CERTIFICATE—--
----

// [[tr3]] As an admin, I should provide intermediate and root certs for my external service certs
// [[tr4]] As an admin, I can duplicate certificates in my bundle without causing any issue
// [[tr5]] As an admin, I can provide my PEM bundle during the install process in the domino.yml file

The bundle must contain all the certificates that you would typically use to connect to the private services, including intermediate and root certificates.

Domino includes public certificates, such as DigiCert root certificates, by default so you do not have to include them.
Duplicate certificates will not cause any issues.

For user executions, all Domino certificates, including public and customer-provided, will be stored in `/etc/ssl/certs/domino-custom`.

== Use custom certificates

If you must use custom certificates in a user session, such as to connect to internal Git servers from a Workspace command line, manually reference the certificates in
`/etc/ssl/certs/domino-custom`.
The following are some ways that you can do this:

* Copy the certs from `/etc/ssl/certs/domino-custom` to `/etc/ssl/certs` in a compute environment pre-run script.
* Add a Java truststore option such as: `-Djavax.net.ssl.trustStore=/etc/ssl/certs/domino-custom/cacerts.p12 -Djavax.net.ssl.trustStoreType=PKCS12 -Djavax.net.ssl.trustStorePassword=changeme`
* Add application-specific configurations such as setting the `GIT_SSL_CAINFO` variable for Git or passing the `--cacert` option for Curl.
+
You can also set these options at runtime or modify the relevant configuration files in the compute environment.


----- admin_guide/security-and-compliance/custom-certificates/index.txt -----
:page-version: 6.1
:page-permalink: 4caa42
:page-title: Work with custom certificates
:page-order: 10

You can configure Domino to connect to services that use custom certificates that are external to the Domino cluster.
In addition to public services like AWS S3, you might want to use private services in your security domain that are secured with custom certificates or a custom certificate authority.

The following are examples of private services:

* Docker registry
* Git server
* S3 service
* LDAPs
* OIDC
* Data sources

Domino recommends that you add certificates for private services to the installation configuration file (`domino.yml`) as described in this topic. This ensures that Domino propagates the certificates throughout the system and maintains them, even when you upgrade Domino.

NOTE: If you add custom certificates to `etc/ssl/certs` in a link:f51038[compute environment], the system overwrites them at runtime. If you cannot use Domino's recommended method or must configure the certificates for a specific compute environment, see https://tickets.dominodatalab.com/hc/en-us/articles/6665122897428-Configure-Certificates-for-A-Specific-Compute-Environment[Configure Certificates for A Specific Compute Environment^].

The topics in this section explain how to work with custom certificates:

* link:77bb0e[Domino-custom-certificates ConfigMap.]
* link:5349d1[Add the PEM bundle to the ConfigMap.]
* link:743717[Update the ConfigMap.]
* link:0c1d63[Apply custom certificates.]
* link:7d1286[Upgrade Domino.]

----- admin_guide/security-and-compliance/custom-certificates/update-configmap.txt -----
:page-version: 6.1
:page-permalink: 743717
:page-title: Update the ConfigMap
:page-order: 30


After Domino is installed and running, you can recreate the `domino-custom-certificates` ConfigMap to update it.
To do this, run the following commands:

[source,shell]
----
kubectl delete configmap domino-custom-certificates
kubectl create configmap domino-custom-certificates --from-file=bundle
----

where `bundle` is the new certificate bundle in concatenated PEM format.

----- admin_guide/security-and-compliance/custom-certificates/upgrade-domino.txt -----
:page-version: 6.1
:page-permalink: 7d1286
:page-title: Upgrade Domino
:page-order: 50

When upgrading a Domino deployment to a new version, you can use the `custom_certificates` key in `domino.yml` to provide a new certificate bundle.

If the key is not yet specified, the installer will do the following:

* Any existing `domino-custom-certificates` bundle will continue to be used, and can still be updated independently of the installer.
* If there is no existing `domino-custom-certificates` ConfigMap, `custom_certificates` is not set, and the legacy `domino-executor-certificates` ConfigMap exists, it will be copied and upgraded to the new format/name.
  If you are using the `custom_certificates` field to configure certificate bundles and the `domino-executor-certificates` ConfigMap exists, make sure to copy the relevant contents of the `domino-executor-certificates` ConfigMap to the `domino.yml` as the ConfigMap will not be used.

----- admin_guide/security-and-compliance/index.txt -----
:page-version: 6.1
:page-title: Security and compliance
:page-permalink: c91e77
:page-order: 70

link:4caa42[Custom certificates]::
Configure Domino to connect to services that use custom certificates that are external to the Domino cluster.

link:dafa95[CA certificates]::
If enabled, Domino uses the Istio service mesh for intra-cluster encryption in transit.

link:98ad4f[Control plane security guidance]::
Important security considerations must be taken when configuring ingress for control plane services in Domino Nexus.

link:cb1049[Kubernetes pod security]::
To provide a secure platform, Domino leverages Kubernetes pod security contexts.

link:900572[Submit General Data Protection Regulation (GDPR) requests]::
Provide Domino with the necessary information so that we can assist you with your obligation as a data controller under GDPR.

----- admin_guide/security-and-compliance/pod-security-policies.txt -----
:page-version: 6.1
:page-title: Kubernetes pod security
:page-permalink: cb1049
:page-order: 40

Domino leverages Kubernetes https://kubernetes.io/docs/tasks/configure-pod-container/security-context/[security contexts^] to deploy pods with the least privileges necessary for them to function.

By default, pods created by Domino run with the following settings:

* Run as a non-root user with `runAsRoot: false`, `runAsNonRoot: true`, and `privileged: false` set.
* Acquisition of additional system-level `capabilities` must be prevented with `drop: ["ALL"]`.

Domino does not support a policy where `readOnlyRootFilesystem` is set.

In addition, due to inconsistency between SELinux support and volume relabeling, all Domino pods run with an SELinux type option `spc_t`.
This setting purposefully bypasses volume relabeling due to the performance impact of volume relabeling while creating Domino user workloads.

Pods that do not adhere to the policies above, such as requiring root privileges or additional capabilities, are documented below with `privileged` indicating pods that need additional user or root privileges and `capabilities` indicated pods that may require additional system-level capabilities.

[%header,format=csv,cols="1a,1a,2a",options="header"]
|===
include::https://domino-artifacts.s3.amazonaws.com/pod-security/pod-security-audit-5.11.csv[]
|===

All user-generated pods from workspaces, jobs, apps, or on-demand compute clusters run with a user and capabilities according to the Configuration record entry for `com.cerebro.domino.computegrid.kubernetes.nonRootExecutions.enabled`.
See link:71d6ad#compute-grid[Configuration records] and link:/release_notes/5-8-0#_non_root_executions[non-root executions release notes] for details.

All user-generated pods from models run as a non-root user but with privilege-escalation capability.

Please see link:cb1049[Pod Security Policies] for an overview of how policies are enforced and specific information about the removal of Pod Security Policies in Kubernetes v1.25.

== Pod Security Policies

A link:https://kubernetes.io/docs/concepts/security/pod-security-policy/[Pod Security Policy (PSP)^] is a cluster-level resource that controls security-sensitive aspects of the pod specification. `PodSecurityPolicy` objects define a set of conditions that a pod must run with in order to be accepted into the system, as well as defaults for the related fields.

Pod security policies serve two purposes in helping secure a Domino installation:

* They prevent the admission (creation) of pods in the cluster if their security stance does not meet the defined policy.
* They mutate (modify) pods that have been allowed by the policy to set defaults matching the PSP (where no values are set).

=== Pod Security Policy deprecation and removal

As of https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/[Kubernetes 1.21^], PSPs are a deprecated feature. In Kubernetes 1.25, support for PSPs has been completely removed.

In order to support the removal of Pod Security Policies, Domino 5.4 and above do not create per-service PSPs as part of the installation. Instead, all pods define an explicit security context. This removes the need for duplicate modification of security contexts during pod creation by PSPs.

Domino will not support a replacement for runtime policy enforcement.  Because Domino controls the definition of all pods created in its namespaces, PSP enforcement of security boundaries after installation (during “runtime”) is not necessary. Domino can control and report (during “build time”) the security context of every pod to ensure all services meet our security standards. See link:cb1049[Kubernetes Pod Security] for an overview with specifics and exemptions.

=== OpenShift

OpenShift maintains its own version of pod security policies named "Security Context Constraints" and Red Hat has laid out a different evolution for that feature going forward from upstream Kubernetes.

Domino does not deploy any custom security contexts but makes use of OpenShift-provided SCCs (`anyuid` and `privileged`) bound to all Domino namespaces. Any additional SCCs will cause the installation of Domino to fail.

In addition, UID remapping is not supported in Domino and all services run as the user ID defined in their container images.

=== Kubernetes < 1.25 support

Domino binds a blanket unrestricted policy for clusters where PSPs are still enabled. Many cluster providers, such as EKS, already do this.
If Domino is the only application deployed in a cluster running Kubernetes < 1.25, it is advisable to turn off pod security policy support entirely.

----- admin_guide/security-and-compliance/submit-gdpr-requests.txt -----
:page-version: 6.1
:page-title: Submit General Data Protection Regulation (GDPR) requests
:page-sidebar: Submit GDPR requests
:page-permalink: 900572
:page-order: 50

//This topic tells what info to provide but not HOW to submit the request.

Domino Data Lab can assist you in your obligations as data controllers under GDPR.
This topic describes how to submit requests to Domino, including the information required to process the request.
Because Domino does not systematically access the contents of files, uploaded requests must either reference specific users or specific files.

* *User Deletion* - Domino can purge personal data about the name, email address and IP address of Domino users, if required.
To process a request, provide:
** The user account name.
** A substitute user to inherit any owned projects and files.

* *File versions request* - Domino can provide the hash of all files in a version chain, as well as access to those files.
To process a request, provide a text file with the username, project name, and file path.
The file must be in this format: `username/file_path_1/file_name.csv`.
* *List of projects referencing a file* - Domino can provide the list of all projects referencing a specific file version in Domino.
This helps identify the potential impact of changing a source file or version.
To process a request, provide a text file with the username, project name, and file path.
The file must be in this format: `username/file_path_1/file_name.csv`.
* *File deletion or substitution request* - To process a request, you must provide:
** A text file with the username, project name, and file path that must be deleted or substituted.
The file must be in this format: `username/file_path_1/file_name.csv`.
** A text file with the username, project name, and file path to substitute, if applicable.
The file must be in this format:
`username/file_path_1/file_name.csv`.

After you make a GDPR request, Domino provides evidence of the completed actions, but can no longer access the data.

Most customers do not have to have data deleted or returned through the course of doing business.
If you must, for GDPR or other reasons, this can impact your history for reproducibility or auditability.

Domino does not accept responsibility for identifying derived data from files, nor ensuring the stability of projects or other work referencing a user or file altered in a request.

----- admin_guide/troubleshooting/basic-domino-health.txt -----
:page-version: 6.1
:page-permalink: db0568
:page-title: Basic Domino health
:page-order: 10

The following sections describe the basic troubleshooting steps and useful tools that cover the overall Domino platform health.

== Admin toolkit
Deploy the Domino link:9c6da7[Admin toolkit] if you have not already.

Use the Admin toolkit to run a health check and observe the overall system status.

image::/images/admin-troubleshooting/admtk1.png[alt="Admin toolkit dashboard"]

Expand each failed section to get more information.

image::/images/admin-troubleshooting/admtk2.png[alt="Admin toolkit entries"]

== Grafana dashboards
Domino Grafana dashboards provide various metric visualizations around Domino services.
For production deployments, a Grafana instance is deployed and externally accessible at <domino url>/grafana. The default username is `grafana` and the password is stored in the cluster.

[source, shell]
----
<user-id>$ kubectl get secret -n domino-platform grafana -ojsonpath=
'{.data.admin-password}'| base64 -d; echo
----

There are preconfigured dashboards that are readily available for review during troubleshooting.
Below is an example of the Nucleus dispatcher's health.

image::/images/admin-troubleshooting/grafana1.png[alt="Grafana Dashboard"]

== Kubernetes pod health
Check if there are any pods that are not in Active or Completed status, as this is the starting place to look at potential issues.

[source, shell]
----
<user-id>$ ~ % kubectl get pods -A | egrep -v "(Running|Completed)"
NAMESPACE         NAME                                                         READY   STATUS             RESTARTS           AGE
domino-compute    model-64484be8a7d82e39bb554a67-65fb5f9c48-7hlgd              3/4     CrashLoopBackOff   5547 (3m43s ago)   19d
tigera-operator   tigera-operator-657cc89589-k7tp5                             0/1     CrashLoopBackOff   5241 (2m1s ago)    19d

----
Keep an eye on the RESTARTS column. If there are many restarts of a certain pod, that pod is having issues.
Describing the pods should give you more information as to why they are not in the running state or frequently restart.

== Kubernetes events
Kubernetes events usually provide underlying issues behind the user experience. Look for related events in domino-platform and domino-compute namespaces sorted by time stamp.

[source, shell]
----
<user-id>$ ~ % kubectl get events --sort-by='.metadata.creationTimestamp' -n domino-platform
LAST SEEN   TYPE      REASON             OBJECT                                          MESSAGE
53m         Warning   Unhealthy          pod/zookeeper-2                                 Liveness probe failed:
52m         Normal    Scheduled          pod/kuberhealthy-frontend-liveness-1686765147   Successfully assigned domino-platform/kuberhealthy-frontend-liveness-1686765147 to ip-10-0-124-141.us-west-2.compute.internal
52m         Normal    Created            pod/kuberhealthy-frontend-liveness-1686765147   Created container kuberhealthy-frontend-liveness
52m         Normal    Pulled             pod/kuberhealthy-frontend-liveness-1686765147   Container image "quay.io/domino/kuberhealthy.http-check:latest-358998" already present on machine
52m         Normal    Started            pod/kuberhealthy-frontend-liveness-1686765147   Started container kuberhealthy-frontend-liveness
45m         Normal    SuccessfulCreate   job/dmm-parquet-conversion-job-28112760         Created pod: dmm-parquet-conversion-job-28112760-k4ptr
45m         Normal    SuccessfulCreate   cronjob/dmm-parquet-conversion-job              Created job dmm-parquet-conversion-job-28112760
45m         Normal    Scheduled          pod/dmm-parquet-conversion-job-28112760-k4ptr   Successfully assigned domino-platform/dmm-parquet-conversion-job-28112760-k4ptr to ip-10-0-124-141.us-west-2.compute.internal

----

== Resource availability
Sometimes pods can exceed the CPU and memory limits imposed by the default Kubernetes configuration.

[source, shell]
----
<user-id>$ kubectl top pods -n domino-platform --sort-by=cpu
NAME                                                         CPU(cores)   MEMORY(bytes)
docker-registry-0                                            259m         37Mi
fleetcommand-reporter-64fbb8cb6b-bcdxs                       184m         12Mi
rabbitmq-ha-311-2                                            163m         216Mi
prometheus-server-0                                          105m         1763Mi
rabbitmq-ha-311-1                                            96m          193Mi
rabbitmq-ha-311-0                                            93m          179Mi
mongodb-replicaset-1                                         80m          744Mi
mongodb-replicaset-0                                         71m          910Mi
mongodb-replicaset-2                                         71m          783Mi
newrelic-infrastructure-monitor-services-68c5c66c-fqqnm      55m          65Mi
nucleus-frontend-7749b69687-d6sh4                            48m          7929Mi
nucleus-frontend-7749b69687-8kwhz                            45m          7853Mi
newrelic-infrastructure-d4g8f                                40m          29Mi
dmm-redis-ha-server-0                                        35m          16Mi
nucleus-dispatcher-56cb5884b7-b5sgp                          34m          6185Mi
redis-ha-server-0                                            29m          1123Mi
mongodb-primary-0                                            28m          262Mi
mongodb-secondary-0                                          26m          259Mi

----

The following shows the CPU and memory usage of the current nodes.

[source, shell]
----
<user-id>$ prod-field % kubectl top node
NAME                                            CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
ip-10-0-101-39.us-west-2.compute.internal       103m         1%     1600Mi          5%
ip-10-0-118-253.us-west-2.compute.internal      291m         3%     9306Mi          30%
ip-10-0-124-141.us-west-2.compute.internal      421m         5%     12506Mi         40%
ip-10-0-33-46.us-west-2.compute.internal        125m         1%     5670Mi          18%
ip-100-164-53-223.us-west-2.compute.internal    1212m        30%    2935Mi          19%
$ prod-field % kubectl top node -l dominodatalab.com/node-pool=platform
NAME                                         CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
ip-10-0-118-253.us-west-2.compute.internal   374m         4%     9308Mi          30%
ip-10-0-124-141.us-west-2.compute.internal   380m         4%     12484Mi         40%
ip-10-0-34-218.us-west-2.compute.internal    338m         4%     6127Mi          20%

----

== Next steps

The following sections provide useful steps to troubleshoot:

- link:44c8d9[Connectivity and latency]
- link:4482de[Workspaces and Jobs issues]
- link:16d1e2[Compute environment build issues]
- link:d0b4a6[Domino endpoint issues]
- link:b4c15f[Distributed model monitoring issues]
- link:8b645f[Data sources issues]

----- admin_guide/troubleshooting/compute-environment-build-issues.txt -----
:page-version: 6.1
:page-permalink: 16d1e2
:page-title: Compute environment build troubleshooting
:page-sidebar: Compute environment build
:page-order: 40

New compute environment or compute environment updates are carried out by the Hephaestus build pods inside the domino-compute namespace. Inspecting the build pod status and describing the pods for any issues are the first steps of troubleshooting.

[source, shell]
----
<user-id>$ kubectl get pods -n domino-compute | grep -i build
hephaestus-buildkit-0                             1/1     Running   0             2m59s

----

If a compute environment revision fails, the build logs will give you the reason for the failure.

image::/images/admin-troubleshooting/build-error.png["Build Log"]

== Next steps

The following sections provide useful steps to troubleshoot:

- link:db0568[Basic Domino health]
- link:44c8d9[Connectivity and latency]
- link:4482de[Workspaces and Jobs issues]
- link:d0b4a6[Domino endpoint issues]
- link:b4c15f[Distributed model monitoring issues]
- link:8b645f[Data sources issues]

----- admin_guide/troubleshooting/connectivity-and-slowness-issues.txt -----
:page-version: 6.1
:page-permalink: 44c8d9
:page-title: Connectivity and latency troubleshooting
:page-sidebar: Connectivity and latency
:page-order: 20

Nucleus frontend and dispatcher pod logs are usually helpful starting points to troubleshoot various connections, latency, and miscellaneous issues.

The following example shows how to find the Domino Nucleus pod name.

[source, shell]
----
<user-id>$ prod-field % kubectl get pods -n domino-platform | grep -i nucleus
nucleus-create-db-2cwf9                                      0/1     Completed   0          20d
nucleus-dispatcher-cbc564d96-r9whr                           2/2     Running     0          20h
nucleus-frontend-7dd78879f7-bsm6b                            2/2     Running     0          20h
nucleus-frontend-7dd78879f7-vnqps                            2/2     Running     0          20h
nucleus-keycloak-realm-migrations-lrdbp                      0/1     Completed   0          20d
nucleus-mongodb-migrations-d55ch                             0/1     Completed   0          20d
nucleus-secret-migrations-mkk9j                              0/1     Completed   0          20d
nucleus-seed-central-config-defaults-rwbbr                   0/1     Completed   0          20d
nucleus-train-588c78cfb5-xnq8c                               2/2     Running     0          20h
nucleus-workspace-volume-snapshot-cleaner-65f567bd8b-nfvlz   2/2     Running     0          20h

----

The following shows how to view the Nucleus logs.

[source, shell]
----
<user-id>$ prod-field % kubectl logs nucleus-frontend-7dd78879f7-bsm6b -n domino-platform -c nucleus-nginx | grep '"status": "500"'
<snip>
{ "time": "2023-06-15T17:39:25+00:00", "remote_addr": "10.0.84.117", "real_addr": "202.164.132.175", "remote_user": "-", "body_bytes_sent": "57", "request_time": "0.006", "status": "500", "request": "GET /v4/users/notifications/unreadStatus HTTP/1.1", "request_method": "GET", "http_referrer": "https://prod-field.cs.domino.tech/loggedout", "http_user_agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" }

----

== Next steps

The following sections provide useful steps to troubleshoot:

- link:db0568[Basic Domino health]
- link:4482de[Workspaces and Jobs issues]
- link:16d1e2[Compute environment build issues]
- link:d0b4a6[Domino endpoint issues]
- link:b4c15f[Distributed model monitoring issues]
- link:8b645f[Data sources issues]

----- admin_guide/troubleshooting/data-sources-issues.txt -----
:page-version: 6.1
:page-permalink: 8b645f
:page-title: Data sources troubleshooting
:page-sidebar: Data sources
:page-order: 70

Connectivity to data sources such as Snowflake is handled by the Domino data source proxy pods in the domino-platform namespace.

[source, shell]
----
<user-id>$ prod-field % kubectl get pods -n domino-platform | grep -i proxy
datasource-proxy-5c4496685d-2dpkg                            1/1     Running     0             21d
datasource-proxy-5c4496685d-mrjsk                            1/1     Running     0             21d

----

Logs from those pods can help to troubleshoot connectivity to data sources.


[source, shell]
----
<user-id>$ prod-field % kubectl logs datasource-proxy-5c4496685d-2dpkg -n domino-platform | grep -i snow
time="2023-06-16T12:18:30Z" level=error msg="Authentication FAILED" func="gosnowflake.(*defaultLogger).Errorln" file="log.go:242"
{"level":"error","ts":1686917910.709162,"caller":"server/auth.go:61","msg":"call error","error":"rpc error: code = InvalidArgument desc = 390913 (08004): Your free trial has ended and all of your virtual warehouses have been suspended. Add billing information in the Snowflake web UI to continue using the full set of Snowflake features.","stacktrace":"github.com/cerebrotech/datasource-proxy/internal/server.(*AuthnMiddleware).CallCompleted
	/datasource-proxy/internal/server/auth.go:61
github.com/apache/arrow/go/arrow/flight.CreateServerMiddleware.func2
	/go/pkg/mod/github.com/apache/arrow/go/arrow@v0.0.0-20211112161151-bc219186db40/flight/server.go:80
google.golang.org/grpc.(*Server).processStreamingRPC
	/go/pkg/mod/google.golang.org/grpc@v1.45.0/server.go:1548
google.golang.org/grpc.(*Server).handleStream
	/go/pkg/mod/google.golang.org/grpc@v1.45.0/server.go:1623
google.golang.org/grpc.(*Server).serveStreams.func1.2
	/go/pkg/mod/google.golang.org/grpc@v1.45.0/server.go:921"}

<user-id>$ prod-field %

----

== Next steps

The following sections provide useful steps to troubleshoot:

- link:db0568[Basic Domino health]
- link:44c8d9[Connectivity and latency]
- link:4482de[Workspaces and Jobs issues]
- link:16d1e2[Compute environment build issues]
- link:d0b4a6[Domino endpoint issues]
- link:b4c15f[Distributed model monitoring issues]

----- admin_guide/troubleshooting/dmm-issues.txt -----
:page-version: 6.1
:page-permalink: b4c15f
:page-title: Distributed model monitoring troubleshooting
:page-sidebar: Distributed model monitoring
:page-order: 60

Issues related to distributed model monitoring (DMM) require looking at the Kubernetes pods, since the UI provides only minimal logs. The pod status and logs below should be inspected for any issues.
Domino DMM architecture consists of many Kubernetes pods. Any pod in a failed state may indicate an issue with model monitoring.

[source, shell]
----
<user-id>$ prod-field % kubectl get pods -n domino-platform| grep -i DMM
dmm-compute-59d655fb6d-zl6z7                                 1/1     Running     0             21d
dmm-config-backup-28111680-wpdb6                             0/1     Completed   0             2d11h
dmm-config-backup-28113120-5xjbz                             0/1     Completed   0             35h
dmm-config-backup-28114560-f5m48                             0/1     Completed   0             11h
dmm-frontend-78c575d4dc-2wwbz                                1/1     Running     0             21d
dmm-parquet-cleanup-job-28114560-2q7vh                       0/1     Completed   0             11h
dmm-parquet-conversion-job-28115220-h7x9x                    0/1     Completed   0             59m
dmm-plier-59d9b68c9c-gn2tq                                   1/1     Running     0             21d
dmm-plier-59d9b68c9c-w6zbj                                   1/1     Running     0             21d
dmm-plier-migrations-qmpzh                                   0/1     Completed   0             21d
dmm-redis-ha-server-0                                        4/4     Running     0             21d
dmm-scheduler-674946f659-7wskh                               1/1     Running     0             21d

<user-id>$ prod-field % kubectl get pods -n domino-platform| grep -i spark
spark3-master-0                                              1/1     Running     0             21d
spark3-master-1                                              1/1     Running     0             21d

<user-id>$ prod-field % kubectl get pods -n domino-compute| grep -i spark
spark3-worker-0                                   1/1     Running            0                21d

<user-id>$ prod-field %

----

The following is a simple script to gather logs for the support team and the engineering team for further analysis. Update `PLATFORM_NS` and `COMPUTE_NS` to match your Domino deployment.


[source, shell]
----
PLATFORM_NS=domino-platform
COMPUTE_NS=domino-compute
mkdir logs/
kubectl logs -n $PLATFORM_NS statefulset/spark3-master -c spark-master --timestamps=true --prefix=true > logs/spark3-master.log
kubectl logs -n $COMPUTE_NS statefulset/spark3-worker -c spark-worker --timestamps=true --prefix=true > logs/spark3-worker.log
kubectl logs -n $PLATFORM_NS deployment/dmm-compute -c compute --timestamps=true --prefix=true > logs/dmm-compute.log
kubectl logs -n $PLATFORM_NS deployment/dmm-plier -c plier --timestamps=true --prefix=true > logs/dmm-plier.log
kubectl logs -n $PLATFORM_NS deployment/dmm-scheduler -c scheduler --timestamps=true --prefix=true > logs/dmm-scheduler.log
kubectl logs -n $PLATFORM_NS -l app.kubernetes.io/component=dmm-parquet-conversion-job --timestamps=true --prefix=true > logs/dmm-parquet-conversion-job.log
kubectl describe cronjob -A > logs/describe-cronjobs.log
kubectl describe pods -A > logs/describe-pods.log
kubectl describe deployments -A > logs/describe-deployments.log
kubectl describe statefulsets -A > logs/describe-statefulsets.log
tar czvf logs.tar.gz logs/
rm -rf logs/

----

== Next steps

The following sections provide useful steps to troubleshoot:

- link:db0568[Basic Domino health]
- link:44c8d9[Connectivity and latency]
- link:4482de[Workspaces and Jobs issues]
- link:16d1e2[Compute environment build issues]
- link:d0b4a6[Domino endpoint issues]
- link:8b645f[Data sources issues]

----- admin_guide/troubleshooting/index.txt -----
:page-version: 6.1
:page-permalink: 741fc7
:page-title: Troubleshooting guidelines for administrators
:page-sidebar: Troubleshooting
:page-order: 120
:page-separator: true
:page-section: Support

This section provides basic troubleshooting guidelines related to common issues you may encounter with your Domino deployments at the infrastructure level.

This information is provided in addition to the existing https://tickets.dominodatalab.com/[knowledge base^] provided by the support team.

In the following sections, we provide steps and useful tools to troubleshoot some of the common issues you may encounter. Based on this initial troubleshooting, you can contact the link:80328c[Domino support team^].

The Domino administrators should be familiar with the topics covered in the Architecture section of the admin guide to thoroughly understand the various Domino components and how they are deployed onto the cloud provider infrastructure. In addition, the Domino administrators following this section should have access to the cloud provider infrastructure, as well as the Kubernetes clusters.

link:db0568[Basic Domino health]::
The following sections describe the basic troubleshooting steps and useful tools that cover the overall Domino platform health.

link:44c8d9[Connectivity and latency]::
This section provides useful steps to troubleshoot connectivity and latency.

link:4482de[Workspaces and Jobs issues]::
This section provides useful steps to troubleshoot Workspace and Jobs related issues.

link:16d1e2[Compute environment build issues]::
This section provides useful steps to troubleshoot compute environment build issues.

link:d0b4a6[Domino endpoint issues]::
This section provides useful steps to troubleshoot Domino endpoint publishing issues.

link:b4c15f[Distributed Model Monitoring issues]::
This section provides useful steps to troubleshoot distributed model monitoring (DMM) issues.

link:8b645f[Data sources issues]::
This section provides useful steps to troubleshoot Domino data source issues.

----- admin_guide/troubleshooting/model-api-issues.txt -----
:page-version: 6.1
:page-permalink: d0b4a6
:page-title: Domino endpoint troubleshooting
:page-sidebar: Domino endpoint
:page-order: 50

Domino endpoint publishing issues can be investigated from the UI to check the logs.

The build logs contain the messages from the Domino endpoint image build stage. The instance logs contain the messages from the Domino endpoint execution.

You can get more information from the Kubernetes logs.

[source, shell]
----
<user-id>$ prod-field % kubectl get pods -n domino-compute | egrep -v "Running"
NAME                                              READY   STATUS              RESTARTS           AGE
model-64484be8a7d82e39bb554a67-65fb5f9c48-7hlgd   3/4     CrashLoopBackOff    5767 (3m41s ago)   20d
model-648b14732d311553013beec2-5469b84d68-9vdzz   3/4     CrashLoopBackOff    1 (6s ago)         14s
model-648b14732d311553013beec2-5469b84d68-nhv6w   3/4     CrashLoopBackOff    1 (7s ago)         14s
run-648b14bb73f8e83c8a394443-nmp6q                0/4     ContainerCreating   0                  49s

----

In this case, model pods are in CrashLoopBackOff state. To get more details on the root cause, describe the Domino endpoint pods.


[source, shell]
----
<user-id>$ prod-field % kubectl describe pod model-648b14732d311553013beec2-5469b84d68-9vdzz -n domino-compute
Name:         model-648b14732d311553013beec2-5469b84d68-9vdzz
Namespace:    domino-compute
Priority:     0
Node:         ip-10-0-96-113.us-west-2.compute.internal/10.0.96.113
Start Time:   Thu, 15 Jun 2023 09:40:49 -0400
Labels:       datasource-proxy-client=true

<snip>
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  33s                default-scheduler  Successfully assigned domino-compute/model-648b14732d311553013beec2-5469b84d68-9vdzz to ip-10-0-96-113.us-west-2.compute.internal
  Normal   Pulling    32s                kubelet            Pulling image "172.20.87.71:5000/dominodatalab/model:63e5651859d2635c5c8c7247-v2-202361513390_WsB7eIW7"
  Normal   Pulled     29s                kubelet            Container image "quay.io/domino/fluent.fluent-bit:1.9.4-358991" already present on machine
  Normal   Started    29s                kubelet            Started container fluent-bit
  Normal   Created    29s                kubelet            Created container fluent-bit
  Normal   Pulled     29s                kubelet            Container image "quay.io/domino/logrotate:3.16-351989-357437" already present on machine
  Normal   Created    29s                kubelet            Created container logrotate
  Normal   Started    29s                kubelet            Started container logrotate
  Normal   Pulled     29s                kubelet            Successfully pulled image "172.20.87.71:5000/dominodatalab/model:63e5651859d2635c5c8c7247-v2-202361513390_WsB7eIW7" in 2.472524739s
  Normal   Created    29s                kubelet            Created container harness-proxy
  Normal   Started    29s                kubelet            Started container harness-proxy
  Normal   Pulled     29s                kubelet            Container image "quay.io/domino/harness-proxy:5.6.0.2969129" already present on machine
  Warning  Unhealthy  28s                kubelet            Readiness probe failed: dial tcp 10.0.112.55:8080: i/o timeout
  Normal   Started    12s (x3 over 29s)  kubelet            Started container model-648b14732d311553013beec2
  Normal   Created    12s (x3 over 29s)  kubelet            Created container model-648b14732d311553013beec2
  Normal   Pulled     12s (x2 over 27s)  kubelet            Container image "172.20.87.71:5000/dominodatalab/model:63e5651859d2635c5c8c7247-v2-202361513390_WsB7eIW7" already present on machine
  Warning  BackOff    10s (x3 over 25s)  kubelet            Back-off restarting failed container
----

Looking at pod logs can also provide useful information about the errors.

[source, shell]
----
<user-id>$ prod-field % kubectl logs -f model-648b14732d311553013beec2-5469b84d68-9vdzz -n domino-compute -c model-648b14732d311553013beec2
*** Starting uWSGI 2.0.19.1 (64bit) on [Thu Jun 15 13:41:38 2023] ***
compiled with version: 9.3.0 on 02 March 2021 12:42:27
os: Linux-5.10.178-162.673.amzn2.x86_64 #1 SMP Mon Apr 24 23:34:06 UTC 2023
nodename: model-648b14732d311553013beec2-5469b84d68-9vdzz
machine: x86_64
clock source: unix
<snip>
/domino/model-manager/app.cfg
working_directory: /mnt/wasantha_gamage/quick-start script_path: model.py endpoint_function_name: predict2
Traceback (most recent call last):
  File "/domino/model-manager/model_harness.py", line 5, in <module>
    application = make_model_app(load_model_config())
  File "/domino/model-manager/model_harness_utils.py", line 51, in load_model_config
    return make_model_config(CONFIG_FILE_PATH)
  File "/domino/model-manager/model_harness_utils.py", line 26, in make_model_config
    USER_FUNCTION = harness_lib.get_endpoint_function(
  File "/domino/model-manager/harness_lib.py", line 49, in get_endpoint_function
    return getattr(module, function_name)
AttributeError: module '_domino_model' has no attribute 'predict2'
unable to load app 0 (mountpoint='') (callable not found or import error)
*** no app loaded. GAME OVER ***
SIGINT/SIGQUIT received...killing workers...
Exception ignored in: <module 'threading' from '/opt/conda/lib/python3.9/threading.py'>
Traceback (most recent call last):
  File "/opt/conda/lib/python3.9/threading.py", line 1428, in _shutdown
    def _shutdown():
KeyboardInterrupt:
OOPS ! failed loading app in worker 1 (pid 8)
Thu Jun 15 13:41:39 2023 - need-app requested, destroying the instance...
goodbye to uWSGI.

----

== Next steps

The following sections provide useful steps to troubleshoot:

- link:db0568[Basic Domino health]
- link:44c8d9[Connectivity and latency]
- link:4482de[Workspaces and Jobs issues]
- link:16d1e2[Compute environment build issues]
- link:b4c15f[Distributed model monitoring issues]
- link:8b645f[Data sources issues]

----- admin_guide/troubleshooting/workspaces-and-jobs-issues.txt -----
:page-version: 6.1
:page-permalink: 4482de
:page-title: Workspaces and Jobs troubleshooting
:page-sidebar: Workspaces and Jobs
:page-order: 30

The following section covers the basic troubleshooting steps for common Workspace and Job issue scenarios.

== Autoscaling issues

When node pools have used all compute nodes specific with node pool's max-nodes, Workspaces, Jobs, Apps, and Domino endpoints may not start at all. The following is an example error from a user's logs:

image::/images/admin-troubleshooting/auto-scale.png[alt="Auto Scaler Errors"]

[source, shell]
----
"description" : "pod didn't trigger scale-up: 8 node(s) didn't match Pod's node affinity/selector, 2 node(s) had volume node affinity conflict, 4 max node group size reached"
----

The small-k8s hardware tier is mapped to the node pool with the nodeSelector: dominodatalab.com/node-pool: default.
The message "8 node(s) didn't match Pod's node affinity/selector" means there are 8 nodes with room in terms of CPU/RAM, but they have different labels/selectors (for example they might be in a different node-pool than the one specified in the hardware tier chosen).

The message "2 node(s) had volume node affinity conflict" means the Autoscaling groups need to be set to one AZ per group.  If they are not, this volume node affinity conflict will appear because the node (EC2 instance for example) and volume are in different availability zones.

The message "4 max node group size reached" means the scale-up cannot occur in four suitable (label-wise) node groups because they're maxed out. Therefore the AWS ASG max-node size should be increased in this case.

== Misc Workspaces and Jobs issues
Inspecting the Workspace and Job pod status from Kubernetes combined with Kubernetes events usually gives a good starting point for troubleshooting.

[source, shell]
----
<user-id>$ prod-field % kubectl get pods -n domino-compute | egrep -v "Running"
NAME                                              READY   STATUS             RESTARTS         AGE
model-64484be8a7d82e39bb554a67-65fb5f9c48-7hlgd   3/4     CrashLoopBackOff   5755 (65s ago)   20d

<user-id>$ prod-field %

----

Use the described pod commands to troubleshoot further root causes.

[source, shell]
----
<user-id>$ prod-field % kubectl describe pod run-648b134573f8e83c8a3942ea-8m6bg -n domino-compute
Name:         run-648b134573f8e83c8a3942ea-8m6bg
Namespace:    domino-compute
Priority:     0
Node:         ip-100-164-3-12.us-west-2.compute.internal/100.164.3.12
Start Time:   Thu, 15 Jun 2023 09:33:59 -0400

<snip>


                        smarter-devices/fuse:NoSchedule op=Exists
Events:
  Type     Reason                  Age                From                     Message
  ----     ------                  ----               ----                     -------
  Normal   Scheduled               64s                default-scheduler        Successfully assigned domino-compute/run-648b134573f8e83c8a3942ea-8m6bg to ip-100-164-3-12.us-west-2.compute.internal
  Normal   SuccessfulAttachVolume  61s                attachdetach-controller  AttachVolume.Attach succeeded for volume "pvc-40785798-8fc2-48a2-98b0-9ea64388dc82"
  Normal   Pulled                  59s                kubelet                  Container image "quay.io/domino/executor:5.6.0.2969129" already present on machine
  Normal   Pulled                  58s                kubelet                  Container image "172.20.87.71:5000/dominodatalab/environment:62bf7f682f6f27046aa8611d-11" already present on machine
  Normal   Started                 58s                kubelet                  Started container executor
  Normal   Pulled                  58s                kubelet                  Container image "quay.io/domino/openresty.openresty:1.21.4.1-6-buster-359055" already present on machine
  Normal   Created                 58s                kubelet                  Created container nginx
  Normal   Started                 58s                kubelet                  Started container nginx
  Normal   Created                 58s                kubelet                  Created container executor
  Normal   Created                 58s                kubelet                  Created container run
  Normal   Started                 58s                kubelet                  Started container run
  Normal   Pulled                  58s                kubelet                  Container image "quay.io/domino/credentials-propagator:v1.0.4-1277" already present on machine
  Normal   Created                 58s                kubelet                  Created container tooling-jwt
  Normal   Started                 58s                kubelet                  Started container tooling-jwt
  Warning  Unhealthy               56s                kubelet                  Readiness probe failed: dial tcp 100.164.45.62:8888: i/o timeout
  Warning  Unhealthy               41s (x7 over 56s)  kubelet                  Readiness probe failed: dial tcp 100.164.45.62:8888: connect: connection refused


----

== Use the web UI for troubleshooting
Most of the troubleshooting requires Kubernetes and AWS dashboard access. However, there are cases where users or administrators might be able to gather information using the Domino UI.

Workspace logs are an example of logs accessible to the users. There are two types of workspace logs:

. Setup logs: Messages related to setting up the underlying compute infrastructure and pulling the compute environment images into Workspace Kubernetes pods.
. User logs: Messages related to pre-run scripts, package install with a `requirements.txt` file, and other messages during the Workspace execution.

image::/images/admin-troubleshooting/workspace-error.png[alt="Workspace Logs"]

*Support bundle*

As an admin, there are additional logs available from the admin UI. The support bundle can also be downloaded from this section. The support bundle includes Workspace logs as well as logs from the Kubernetes resources.


== Next steps

The following sections provide useful steps to troubleshoot:

- link:db0568[Basic Domino health]
- link:44c8d9[Connectivity and latency]
- link:16d1e2[Compute environment build issues]
- link:d0b4a6[Domino endpoint issues]
- link:b4c15f[Distributed model monitoring issues]
- link:8b645f[Data sources issues]

----- admin_guide/upgrade-domino.txt -----
:page-permalink: 5522a2
:page-title: Upgrade Domino
:page-version: 6.1
:page-order: 110

IMPORTANT: The following guide assumes the link:88f534[Platform Operator] and link:c3298b[ddlctl command line] have already been installed.

== Before you upgrade

=== Verify cluster health

Before starting a Domino upgrade, run the link:9c6da7[Domino Admin Toolkit] to verify that the Domino instance is healthy.

The Admin Toolkit deploys inside the Domino cluster and performs a series of checks to ensure that Domino services are operational. It also checks the cluster for known infrastructure bugs.

=== Back up Domino

Prior to the upgrade, we recommend that you back up your Domino to ensure that no data is lost during the upgrade process. Data loss only occurs in rare, catastrophic update failures.

For information on how to run ad hoc backups of Domino, see link:2722ce[run a manual Domino backup].

=== Check for drifted Helm releases

If you are upgrading from Domino 6.0.0 or later, check whether any Helm releases have drifted from their desired state prior to an upgrade.

[source,shell]
----
ddlctl get helmrelease -A --status stalled=true
----

These out of band changes are not natively supported via the Domino configuration. If for some reason they are required for your upgrade to succeed, make sure to account for them in your upgrade plan so those resources can be appropriately patched post upgrade.

=== Put Domino in maintenance mode

When only upgrading Domino versions, user executions can typically continue to run while the platform upgrades. However, user executions should be stopped to avoid losing work in the following cases:

* Upgrading Kubernetes.
* Upgrading from a Domino version earlier than 5.3.0.

Put Domino into https://github.com/dominodatalab/domino-maintenance-mode[maintenance mode^] to pause all user executions and resume them after upgrading.

When upgrading, Domino recommends that you notify your users in advance about a maintenance window when the system will be unavailable.

=== Upgrade your infrastructure

If applicable, first upgrade your infrastructure to match the version of Domino you are upgrading to.

This may involve:

- Upgrading Kubernetes versions (refer to the link:7b2cbe[Kubernetes compatibility] table for supported Kubernetes versions).
- Upgrading the Domino infrastructure.

Consult the documentation for your target cloud provider for instructions on how to upgrade your infrastructure:

* **AWS**: Follow the instructions in the link:e3bf0a[AWS documentation].
* **GCP**: Follow the instructions in the link:e96f4c[GCP documentation].
* **Azure**: Follow the instructions in the link:bf888c[Azure documentation].
* **OpenShift**: Follow the instructions in the link:3fb861[OpenShift documentation].

== Upgrade Domino while migrating to the platform operator

For those users who already have a Domino cluster but are upgrading to a version of the platform that uses the Platform Operator, the process is straightforward.

=== Upgrade your existing configuration

Previously this file was known as the `domino.yml` file, but as part of the transition to an operator-orchestrated deployment, this standalone file has been incorporated into the Domino custom resource definition in the `.spec.config` field.

To prepare your existing configuration for the upgrade, you must feed the existing configuration file into the `ddlctl` command line so that it can migrate the configuration to the latest schema, populate any defaults, and potentially prompt you for any missing, required values.

NOTE: Get the `$FLEETCOMMAND_AGENT_TAG` for your target release from the link:5246aa[releases] page.

When supplying a path to `--from-file`, make sure that it is relative to the current working directory.

[source,shell]
----
ddlctl create config --from-file domino.yml --agent-version $FLEETCOMMAND_AGENT_TAG
----

The upgraded configuration file will be created in the current directory with the name:

[source,shell]
----
config-$FLEETCOMMAND_AGENT_TAG.{timestamp}.yaml
----

=== Create the Domino custom resource

Once you are satisfied with the upgraded configuration file, you can create the Domino custom resource. This will get picked up by the link:88f534[Platform Operator] which will then handle installation of all Domino components.

[source,shell]
----
ddlctl create domino --config config-$FLEETCOMMAND_AGENT_TAG.{timestamp}.yaml --agent-version $FLEETCOMMAND_AGENT_TAG
----

If you would prefer to just generate the `Domino` custom resource YAML, you can supply the `--export` flag and pipe the result to a file.

When the upgrade completes successfully, you should see a message that says:

[source,console]
----
2019-11-26 21:20:20,214 - INFO - fleetcommand_agent.Application - Deployment complete.
Domino is accessible at $YOUR_FQDN
----

[[existing-custom-resource]]
== Upgrade Domino when a Domino custom resource already exists

If you are comfortable modifying the upgraded configuration file as part of a single workflow, you can use the dedicated `upgrade` subcommand of `ddlctl` to perform an upgrade of Domino that sources the configuration to migrate from an existing Domino custom resource.

[source,shell]
----
ddlctl upgrade domino --agent-version $FLEETCOMMAND_AGENT_TAG
----

This will fetch the existing `.spec.config` from the Domino custom resource in your cluster, upgrade it to the latest schema, and open a text editor for you to review and/or modify the configuration file. Once saved, it will patch the Domino custom resource with the new configuration, agent data, and overall version information.

This will transition into a live tail of the logs of the Domino agent as your cluster is upgraded.

== Upgrade the configuration file by sourcing an existing Domino custom resource

You can upgrade the configuration file as a standalone operation, sourcing an existing Domino custom resource.

[source,shell]
----
ddlctl create config --from-domino --agent-version $FLEETCOMMAND_AGENT_TAG
----

== Dry run an upgrade of Domino

Follow the same steps as above to upgrade your configuration.

With an upgraded configuration file in hand, you can generate a `Domino` custom resource with the `--export` flag and pipe the result to a file.

[source,shell]
----
ddlctl create domino --config config-$FLEETCOMMAND_AGENT_TAG.{timestamp}.yaml --agent-version $FLEETCOMMAND_AGENT_TAG --export > domino-cluster.yaml
----

In the `domino-cluster.yaml` file, modify the `spec.agent.spec` to put the operator in a dry run mode.

[source,yaml]
----
spec:
  agent:
    spec:
      dryRunMode: "true"
----

You can also edit the live `domino` resource to put it in this mode:

[source,shell]
----
kubectl edit domino -n domino-operator
----

Dry running an upgrade of Domino will report on the changes that will be made to the Domino cluster according to what is already represented in the Helm manifests in Helm storage.

----- api_guide/data_api/authentication.txt -----
:page-version: 6.1
:page-title: Authentication
:page-permalink: 490a1c
:page-order: 30

== Methods

These are the authentication methods available when using the Domino Data API:

* *Credential propagation*: A token is periodically refreshed and stored in a file. The file location is defined in the *DOMINO_TOKEN_FILE* env variable

* *API Key*: If enabled, the user API key is exported in the *DOMINO_USER_API_KEY* env variable

== Availability

NOTE: Those methods are not available in all run types. The following table lists the availability for these methods.

[cols="2a,1a,^1a,^1a",options="header"]
|===
|Run type |Who? |Token |API Key
|Workspace |Creator |Yes |Yes
|Job |Creator |Yes |Yes
|Scheduled Job |Creator |Yes |Yes
|Launcher |User |Yes |Yes
|App |Creator |Yes |Yes
|Domino endpoint |N/A |No |No
|===

NOTE: Anonymous users don't have a token or API key, and are therefore not allowed to access data sources through public launchers automatically. If you still want to support anonymous execution, the launcher author would need to configure the launcher following the instructions in the link:9edb62[custom authentication] section or pass the credentials as a parameter.

IMPORTANT: As described in the matrix, Domino endpoint does not support automatic authentication. You can set the *DOMINO_USER_API_KEY* environment variable for Domino endpoint or follow the instructions in the link:9edb62[custom authentication] section to use the _DataSourceClient_ or the _TrainingSetClient_ in this type of run.

----- api_guide/data_api/datasource_usecases/access-datasets-via-proxy.txt -----
:page-permalink: fc01e5
:page-version: 6.1
:page-title: Access Datasets via Datasource proxy
:page-order: 10

Domino provides secure, programmatic access to datasets through the Datasource Proxy. This allows your application to treat Domino datasets as standard datasources.

When a user interacts with your app, Domino automatically includes an authorization token in the request header. Your app uses this token to access datasets on behalf of the user, ensuring that data access adheres to their permissions.

Although you can call the Datasource Proxy API directly, we recommend using the Domino client library for Python. This library simplifies integration and results in cleaner, more maintainable code.

Note: Example code snippets are available on each dataset’s detail page in the Domino UI.

[source,python]
----
from domino_data.datasets import DatasetClient, DatasetConfig

# instantiate a client
dataset = DatasetClient(token = request.headers.get('Authorization', '')[7:]).get_dataset("dataset-quick-start-67bca8d95df4c35e79db40f5")

# select a specific snapshot, if a specific snapshot is not set the read/write snapshot is used
dataset.update(config=DatasetConfig(snapshot_id="[your own snapshotID]"))

# list files in the dataset
dataset.list_files()

# get files in the dataset
dataset.get("/path/to/file")
----

----- api_guide/data_api/datasource_usecases/config_override.txt -----
:page-version: 6.1
:page-title: Configuration override
:page-permalink: e5761b
:page-order: 60

Data Source configurations are set in the Data / Data Sources panel of the Domino UI. You can update it there for any permanent change.

Some configuration attributes can be overridden locally in the API. A sample of Data Source type configuration is described in the link:#classes[Classes] section below.

== Usage
You can create a configuration override with any of the config classes and update your Data Source entity:

[source,python]
----
from domino.data_sources import DataSourceClient, SnowflakeConfig

snowflake = DataSourceClient().get_datasource("snowflake-prod")

# Build a override config with a different warehouse than configured in Domino
config_xxl = SnowflakeConfig(warehouse="compute-xxl")

# Local update with no permanent change
snowflake.update(config=config_xxl)
res = snowflake.query("SELECT COUNT(*) FROM very_large_table")

# Override can also be used for temporary credentials
snowflake.update(config=SnowflakeConfig(username="admin", password="<password>"))
res = snowflake.query("SELECT secret_data FROM secret_table LIMIT 10")

----
[source,r]
----
library(DominoDataR)
client <- DominoDataR::datasource_client()

# Query with a different warehouse than configured in Domino
table <- DominoDataR::query(
  client,
  "snowflake-prod",
  "SELECT COUNT(*) FROM very_large_table",
  DominoDataR::add_override(warehouse = "compute-xxl")
)

# Query with temporary credentials
table <- DominoDataR::query(
  client,
  "snowflake-prod",
  "SELECT secret_data FROM secret_table LIMIT 10",
  DominoDataR::add_override(username = "admin", password="<password>")
)
----

To remove the configuration override, simply reset it:

[source,python]
----
from domino.data_sources import DataSourceClient, SnowflakeConfig

snowflake = DataSourceClient().get_datasource("snowflake-prod")
# Update to dev database
snowflake.update(SnowflakeConfig(database="dev"))

# Reset to default values
snowflake.reset_config()
res = snowflake.query("SELECT * FROM prod_table")
----

Starting from Domino 6.1.1, relational Data Sources support an optional datetime precision field, 
which is overridable in the Data API for version 6.3.0 and above. Adjusting the precision prevents extreme datetimes from overflowing.
The precision can be set to "seconds", "milliseconds", "microseconds", or "nanoseconds", with "nanoseconds" as the default.

[source,python]
----
from domino.data_sources import DataSourceClient, SnowflakeConfig

snowflake = DataSourceClient().get_datasource("snowflake-prod")

# Update to datetime precision to milliseconds to support extremely large datetime values
snowflake.update(SnowflakeConfig(datetime_precision="milliseconds"))

res = snowflake.query("SELECT * FROM prod_table")

----
[source,r]
----
library(DominoDataR)
client <- DominoDataR::datasource_client()

# Query with a different datetime precsision than the default
table <- DominoDataR::query(
  client,
  "snowflake-prod",
  "SELECT * FROM prod_table",
  DominoDataR::add_override(datetime_precision = "milliseconds")
)
----

[[classes]]
== Classes
Here are examples of Data Sources and their configurable override options.

=== ADLS

[source,python]
----
class domino_data.data_sources.ADLSConfig(*, container=None, access_key=None)[source]
----

ADLS Data Source configuration.

* `access_key`: Optional[str]

* `container`: Optional[str]

=== BigQuery

[source,python]
----
class domino_data.data_sources.BigQueryConfig(*, gcp_project_id=None)[source]
----

BigQuery Data Source configuration.

* `datetime_precision`: Optional[str] +
For `dominodatalab-data>=6.3.0`, valid values are: `"seconds"`, `"milliseconds"`, `"microseconds"`, or `"nanoseconds"` (default: `"nanoseconds"`).

* `gcp_project_id`: Optional[str]

* `private_key_json`: Optional[str]

=== GCS

[source,python]
----
class domino_data.data_sources.GCSConfig(*, bucket=None, private_key_json=None)[source]
----

GCS Data Source configuration.

* `bucket`: Optional[str]

* `private_key_json`: Optional[str]

=== GenericS3

[source,python]
----
class domino_data.data_sources.GenericS3Config(*, bucket=None, host=None, region=None, aws_access_key_id=None, aws_secret_access_key=None)[source]
----

Generic S3 Data Source configuration.

* `aws_access_key_id`: Optional[str]

* `aws_secret_access_key`: Optional[str]

* `bucket`: Optional[str]

* `subfolder`: Optional[str]

* `host`: Optional[str]

* `region`: Optional[str]

=== MySQL

[source,python]
----
class domino_data.data_sources.MySQLConfig(*, database=None, password=None, username=None, aws_access_key_id=None, aws_secret_access_key=None, aws_session_token=None)[source]
----

MySQL Data Source configuration.

* `aws_access_key_id`: Optional[str]

* `aws_secret_access_key`: Optional[str]

* `aws_session_token`: Optional[str]

* `database`: Optional[str]

* `datetime_precision`: Optional[str] +
For `dominodatalab-data>=6.3.0`, valid values are: `"seconds"`, `"milliseconds"`, `"microseconds"`, or `"nanoseconds"` (default: `"nanoseconds"`).

* `password`: Optional[str]

* `region`: Optional[str]

* `username`: Optional[str]

=== Oracle

[source,python]
----
class domino_data.data_sources.OracleConfig(*, database=None, password=None, username=None)[source]
----

Oracle Data Source configuration.

* `database`: Optional[str]

* `datetime_precision`: Optional[str] +
For `dominodatalab-data>=6.3.0`, valid values are: `"seconds"`, `"milliseconds"`, `"microseconds"`, or `"nanoseconds"` (default: `"nanoseconds"`).

* `password`: Optional[str]

* `username`: Optional[str]

=== PostgreSQL

[source,python]
----
class domino_data.data_sources.PostgreSQLConfig(*, database=None, password=None, username=None, aws_access_key_id=None, aws_secret_access_key=None, aws_session_token=None)[source]
----

PostgreSQL Data Source configuration.

* `aws_access_key_id`: Optional[str]

* `aws_secret_access_key`: Optional[str]

* `aws_session_token`: Optional[str]

* `database`: Optional[str]

* `datetime_precision`: Optional[str] +
For `dominodatalab-data>=6.3.0`, valid values are: `"seconds"`, `"milliseconds"`, `"microseconds"`, or `"nanoseconds"` (default: `"nanoseconds"`).

* `password`: Optional[str]

* `region`: Optional[str]

* `username`: Optional[str]

=== Redshift

[source,python]
----
class domino_data.data_sources.RedshiftConfig(*, database=None, password=None, username=None, aws_access_key_id=None, aws_secret_access_key=None, aws_session_token=None)[source]
----

Redshift Data Source configuration.

* `aws_access_key_id`: Optional[str]

* `aws_secret_access_key`: Optional[str]

* `aws_session_token`: Optional[str]

* `database`: Optional[str]

* `datetime_precision`: Optional[str] +
For `dominodatalab-data>=6.3.0`, valid values are: `"seconds"`, `"milliseconds"`, `"microseconds"`, or `"nanoseconds"` (default: `"nanoseconds"`).

* `password`: Optional[str]

* `region`: Optional[str]

* `username`: Optional[str]

=== S3

[source,python]
----
class domino_data.data_sources.S3Config(profile=None, *, bucket=None, region=None, aws_access_key_id=None, aws_secret_access_key=None, aws_session_token=None)
----

S3 Data Source configuration.

* `aws_access_key_id`: Optional[str]

* `aws_secret_access_key`: Optional[str]

* `aws_session_token`: Optional[str]

* `bucket`: Optional[str]

* `subfolder`: optional[str]

* `region`: Optional[str]

=== SQLServer

[source,python]
----
class domino_data.data_sources.SQLServerConfig(*, database=None, password=None, username=None)
----

SQL Server datasource configuration.

* `database`: Optional[str]

* `datetime_precision`: Optional[str] +
For `dominodatalab-data>=6.3.0`, valid values are: `"seconds"`, `"milliseconds"`, `"microseconds"`, or `"nanoseconds"` (default: `"nanoseconds"`).

* `password`: Optional[str]

* `username`: Optional[str]

=== Snowflake

[source,python]
----
class domino_data.data_sources.SnowflakeConfig(*, database=None, schema=None, warehouse=None, role=None, password=None, username=None, token=None)
----

Snowflake datasource configuration.

* `database`: Optional[str]

* `datetime_precision`: Optional[str] +
For `dominodatalab-data>=6.3.0`, valid values are: `"seconds"`, `"milliseconds"`, `"microseconds"`, or `"nanoseconds"` (default: `"nanoseconds"`).

* `password`: Optional[str]

* `role`: Optional[str]

* `schema`: Optional[str]

* `token`: Optional[str]

* `username`: Optional[str]

* `warehouse`: Optional[str]

----- api_guide/data_api/datasource_usecases/custom_auth.txt -----
:page-version: 6.1
:page-title: Custom authentication
:page-permalink: 9edb62
:page-order: 50

See link:490a1c[Authentication] for the default behavior of the DataSourceClient.

== Override the API key

[source,python]
----
from domino.data_sources import DataSourceClient

custom_api_key = "VALID_API_KEY"

client = DataSourceClient(api_key=custom_api_key)
db = client.get_datasource("my-db")
----
[source,r]
----
library(DominoDataR)
client <- DominoDataR::datasource_client(api_key = custom_api_key)
----

== Override the location of the token file

[source,python]
----
from domino.data_sources import DataSourceClient

custom_token_file = "/valid/token/file/location"

client = DataSourceClient(token_file=custom_token_file)
db = client.get_datasource("my-db")
----
[source,r]
----
library(DominoDataR)
client <- DominoDataR::datasource_client(token_file = "/valid/token/file/location")
----

----- api_guide/data_api/datasource_usecases/index.txt -----
:page-version: 6.1
:page-title: Data Source use cases
:page-permalink: d49ab1
:page-order: 30

Domino data sources offer a mechanism to create and manage connection properties to a supported external data service. Both administrators and users create data sources, and share them among collaborators. Connection properties are stored securely and there is no need to install data source specific drivers or libraries. A tightly coupled library provides a consistent access pattern for both tabular and file based data. To learn more about data sources, see link:fbb41f[Domino data sources].

Here is a list of Data Source operations:

* link:fc01e5[Access Datasets via Datasource Proxy]
* link:40ee54[Tabular Store]
* link:20d551[Object Store]
* link:50ffa7[Write to a Local File]
* link:9edb62[Custom Authentication]
* link:e5761b[Configuration Override]

----- api_guide/data_api/datasource_usecases/object_store.txt -----
:page-version: 6.1
:page-title: Read/Write to an object store
:page-permalink: 20d551
:page-order: 30


The API supports object store type datasources (such as S3) and allows for easy retrieval and upload of objects.

NOTE: The following APIs are only available when using this type of datasource.

// tag::usage[]
== List
Get the datasource from the client:

[source,python]
----
from domino.data_sources import DataSourceClient

s3_dev = DataSourceClient().get_datasource("s3-dev")
----
[source,r]
----
library(DominoDataR)
client <- DominoDataR::datasource_client()
----

You can list objects available in the datasource. You can also specify a prefix:

[source,python]
----
objects = s3_dev.list_objects()

objects_under_path = s3_dev.list_objects("path_prefix")
----
[source,r]
----
keys <- DominoDataR::list_keys(client, "s3-dev")

keys_under_path <- DominoDataR::list_keys(client, "s3-dev", "path_prefix")
----

By default the number of returned objects is limited by the underlying datasource. You can specify how many keys you want as an optional parameter:

[source,python]
----
objects = s3_dev.list_objects(page_size = 1500)
----

[source,r]
----
keys = DominoDataR::list_keys(client, "s3-dev", page_size = 1500)
----

== Read

You can get object content, without having to create object entities, by using the datasource API and specifying the `Object` key name:

[source,python]
----
# Get content as binary
content = s3_dev.get("key")

# Download content to file
s3_dev.download_file("key", "./path/to/local/file")

# Download content to file-like object
f = io.BytesIO()
s3_dev.download_fileobj("key", f)
----
[source,r]
----
# Get content as raw vector
content <- DominoDataR::get_object(client, "s3-dev", "key")

# Get content as character vector
content <- DominoDataR::get_object(client, "s3-dev", "key", as = "text")

# Download content to file
DominoDataR::save_object(client, "s3-dev", "key", "./path/to/local/file")
----

You can also get the datasource entity content from an object entity (Python only):

[source,python]
----
# Key object
my_key = s3_dev.Object("key")

# Get content as binary
content = my_key.get()

# Download content to file
my_key.download_file("./path/to/local/file")

# Download content to file-like object
f = io.BytesIO()
my_key.download_fileobj(f)
----

== Write

Similar to the read/get APIs, you can also write data to a specific object key. From the datasource:

[source,python]
----
# Put binary content to given object key
s3_dev.put("key", b"content")

# Upload file content to specified object key
s3_dev.upload_file("key", "./path/to/local/file")

# Upload file-like content to specified object key
f = io.BytesIO(b"content")
s3_dev.upload_fileobj("key", f)
----
[source,r]
----
# Put raw or character vector to object key
DominoDataR::put_object(client, "s3-dev", "key", what = "content")

# Upload file content to specified object key
DominoDataR::upload_object(client, "s3-dev", "key", file = "./path/to/local/file")
----

You can also write from the object entity (Python only).

[source,python]
----
# Key object
my_key = s3_dev.Object("key")

# Put content as binary
my_key.put(b"content")

# Upload content from file
my_key.upload_file("./path/to/local/file")

# Upload content from file-like object
f = io.BytesIO()
my_key.upload_fileobj(f)
----

// end::usage[]

----- api_guide/data_api/datasource_usecases/simple_query.txt -----
:page-version: 6.1
:page-title: Query a tabular store
:page-permalink: 40ee54
:page-order: 20


NOTE: Ensure that the API is available in your workspace environment; see link:5267b0[Install the Data API] for setup information.

== Authentication
The Datasource client uses environment variables available in the workspace to automatically authenticate your identity.

To override this behavior, see link:9edb62[Custom Authentication].

== Usage

// tag::usage[]
Assuming a data source named `redshift-test` is configured with valid credentials for the current user:

[source,python]
----
from domino.data_sources import DataSourceClient

# instantiate a client and fetch the datasource instance
redshift = DataSourceClient().get_datasource("redshift-test")

query = """
     SELECT
         firstname,
         lastname,
         age
     FROM
         employees
     LIMIT 1000
 """

 # res is a simple wrapper of the query result
 res = redshift.query(query)
 # to_pandas() loads the result into a pandas dataframe
 df = res.to_pandas()
 # check the first 10 rows
 df.head(10)
----

[source,r]
----
library(DominoDataR)
client <- DominoDataR::datasource_client()

query <- "
  SELECT
    firstname,
    lastname,
    age
  FROM
    employees
  LIMIT 1000"

# table is a https://arrow.apache.org/docs/r/reference/Table-class.html
table <- DominoDataR::query(client, "redshift-test", query)
#> Table
#> 1000 rows x 3 columns
#> $firstname <string not null>
#> $lastname <string not null>
#> $age <int32 not null>
----
// end::usage[]

----- api_guide/data_api/datasource_usecases/write_to_file.txt -----
:page-version: 6.1
:page-title: Write to a local file
:page-permalink: 50ffa7
:page-order: 40

// tag::usage[]
== Parquet

Because Domino uses https://arrow.apache.org/docs/python/[PyArrow^] to serialize and transport data, the query result is easily written to a local parquet file. You can also use pandas as shown in the CSV example.

[source,python]
----
redshift = DataSourceClient().get_datasource("redshift-test")

res = redshift.query("SELECT * FROM wines LIMIT 1000")

# to_parquet() accepts a path or file-like object
# the whole result is loaded and written once
res.to_parquet("./wines_1000.parquet")
----
[source,r]
----
client <- DominoDataR::datasource_client()
table <- DominoDataR::query(client, "redshift-test", "SELECT* FROM wines LIMIT 1000")

# We can use https://arrow.apache.org/docs/r/reference/write_parquet.html since we leverage the arrow library
arrow::write_parquet(table, "./wines_1000.parquet")
----

== CSV

Because serializing to a CSV is lossy, Domino recommends using the https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html#pandas-dataframe-to-csv[Pandas.to_csv] API so you can leverage the multiple options that it provides.

[source,python]
----
redshift = DataSourceClient().get_datasource("redshift-test")

res = redshift.query("SELECT * FROM wines LIMIT 1000")

# See Pandas.to_csv documentation for all options
csv_options = {header: True, quotechar: "'"}

res.to_pandas().to_csv("./wines_1000.csv", **csv_options)
----
[source,r]
----
client <- DominoDataR::datasource_client()
table <- DominoDataR::query(client, "redshift-test", "SELECT* FROM wines LIMIT 1000")

# We can use https://arrow.apache.org/docs/r/reference/write_csv_arrow.html since we leverage the arrow library
arrow::write_csv_arrow(table, "./wines_1000.csv")
----
// end::usage[]

----- api_guide/data_api/index.txt -----
:page-version: 6.1
:page-permalink: 140b48
:page-title: Domino Data API
:page-order: 30

The https://pypi.org/project/dominodatalab-data/[Python library^] and https://github.com/dominodatalab/r-domino-data[R library^] provides access to file-based data and SQL-based tabular data.
The results are available as dataframe abstractions for popular libraries. You can query connectors interactively, and you don't have to restart a workload to install drivers.

== Compatibility

This Domino release is compatible with these versions of the Domino Data API:

* Python:
For Domino 6.1.0: `dominodatalab-data>=6.2.0`
For Domino 6.1.1 and above: `dominodatalab-data>=6.3.0`

* R:
For all Domino 6.1.x versions: `dominodatalab/DominoDataR>=v0.2.3`

== Supported data access methods

* Snowflake
* AWS Redshift
* S3
* Generic-S3
* MySQL
* PostgreSQL
* MSSQL
* Oracle DB
* GCS
* ADLS
* GCP BigQuery
* Databricks
* Azure Blob Storage

For the full list of supported data access methods, see the link:user_guide/fbb41f[Data Source Connectors] page.

== Additional documentation

* Find out how to link:5267b0[install the Data API] so that you can use it with the correct Domino environment.
* Learn about the link:490a1c[authentication methods] available when using the Domino Data API.
* Learn how to create and manage connection properties to a supported external data service with Domino's link:d49ab1[Data Source operations].
* Learn how you can link:440de9[use Domino TrainingSets] to persist dataframes for model training and other analysis.

----- api_guide/data_api/install_the_data_api.txt -----
:page-version: 6.1
:page-title: Install the Data API
:page-permalink: 5267b0
:page-order: 20

Use the Data API with the right Domino environment. If you want to install it manually, with `pip`, Domino recommends you install the Data API as an extra dependency of the main Python Domino package.

== Domino Standard Environment

The Data API comes pre-packaged in the Domino Standard Environment (DSE) starting with
version 5.0 for Python and version 5.3 for R.

If you want to use your own environment, you can install the API by adding the following to the Dockerfile Instructions section:

Please refer to the link:140b48[Index^] page to find which version of the packages you need to install on your version of Domino.

.To install the Python library
[source,dockerfile]
----
USER root

## Install Domino and Data API Python packages
RUN python -m pip install dominodatalab[data]

## Alternatively if you need to specify the version of the Data library
RUN python -m pip install dominodatalab dominodatalab-data=={x.y.z}

USER ubuntu
----

.To install the R library
[source,dockerfile]
----
USER root

## Install Domino Data API as dependency of R library
RUN python -m pip install dominodatalab-data=={x.y.z}

## Install R library via github (using https://remotes.r-lib.org/)
RUN R --no-save -e "install.packages('remotes')"
RUN R --no-save -e "remotes::install_github('dominodatalab/DominoDataR@{a.b.c}')"

USER ubuntu
----

== Python Package Index (PyPI)

NOTE: If you use the API as a standalone library, the package name will be domino_data. We recommend installing it as part of the main domino package for easier and consistent import see link:40ee54[Tabular store] for an example.

[source,python]
----
python -m pip install dominodatalab[data]

----

NOTE: The Data API is published in https://pypi.org/[PyPI^]:

[source,python]
----
python -m pip install dominodatalab-data=={x.y.z}

----

// === Releases

// Release history is available link:[here].

----- api_guide/data_api/trainingsets_usecases/create_trainingset.txt -----
:page-version: 6.1
:page-title: Create TrainingSets
:page-permalink: 9c4dec
:page-order: 10

A `TrainingSet` is a versioned set of data, column information, and other metadata. `TrainingSets` are created implicitly when the first `TrainingSetVersion` with a particular `training_set_name` are added using the `create_training_set_version` function.

A `TrainingSet` can include versions from the same project. Attempting to add a version from a different project can result in an error.

`TrainingSet` names are strings containing only alphanumeric characters in the basic Latin alphabet including dash and underscore: `[-A-Za-z_-]`

[source,python]
----
from domino.training_sets import TrainingSetClient, model

training_set_version = TrainingSetClient.create_training_set_version(
    training_set_name=training_set_name,
    df=my_pandas_dataframe,
    key_columns=["user_id", "transaction_id"],
    target_columns=["is_fraud"],
    exclude_columns=["extra_column1", "extra_column2"],
    monitoring_meta=model.MonitoringMeta(
        timestamp_columns=["ts"],
        categorical_columns=["categorical_column1", "categorical_column2"],
        ordinal_columns=["ordinal_column1"],
    ),
    meta={"year": "2021"}
)

----

NOTE: To use a `TrainingSet` for model monitoring the `monitoring_meta` keyword argument must have a value for classification models. You can create a `TrainingSet` without this argument, but it can not be used until the argument has a value. Trying to register a Domino endpoint for monitoring, from the *Grafana Monitoring* tab for that Domino endpoint, with a null `monitoring_meta` value displays the following error: `The selected Feature Set Version cannot currently be used for monitoring because it does not contain a schema definition.`

----- api_guide/data_api/trainingsets_usecases/custom-auth.txt -----
:page-version: 6.1
:page-permalink: 4915a4
:page-title: Custom authentication
:page-order: 99

The TrainingSetClient is a Python module wrapping a set of API methods.
See link:490a1c[Authentication] for the default behavior of the TrainingSetClient.

To override authentication, you need to set the right environment variable with your own user API key:

[source,python]
----
import os

os.environ["DOMINO_USER_API_KEY"] = "<your-own-api-key>"

# In Domino endpoint if the client version is <0.1.8
os.environ["DOMINO_API_HOST"] = os.getenv("DOMINO_USER_HOST")
----

[source,shell]
----
export DOMINO_USER_API_KEY=<your-own-api-key>

# In Domino endpoint if the client version is <0.1.8
export DOMINO_API_HOST=$DOMINO_USER_HOST

----

----- api_guide/data_api/trainingsets_usecases/delete_trainingset.txt -----
:page-version: 6.1
:page-title: Delete TrainingSets
:page-permalink: 6def59
:page-order: 40

You can delete `TrainingSets` and `TrainingSetVersions`. The following examples show how to delete `TrainingSets` and `TrainingSetVersions`.

== Delete TrainingSetVersion

[source,python]
----
TrainingSetClient.delete_training_set_version("my-training-set", 2)

----

== Delete TrainingSet

NOTE: `TrainingSets` may only be deleted if they have no versions.

[source,python]
----
TrainingSetClient.delete_training_set("my-training-set")

----

The following example shows how to delete the `TrainingSet` and all versions:

[source,python]
----
found_tsvs = TrainingSetClient.list_training_set_versions(
    training_set_name="my-training-set",
)

for tsv in found_tsvs:
    TrainingSetClient.delete_training_set_version(tsv.training_set_name, tsv.number)

TrainingSetClient.delete_training_set("my-training-set")

----

----- api_guide/data_api/trainingsets_usecases/get_trainingset.txt -----
:page-version: 6.1
:page-title: Retrieve TrainingSets
:page-permalink: 5cef05
:page-order: 20

You can get `TrainingSets` using their version number or using an explicit search.

== Get TrainingSetVersion

`TrainingSetVersions` can be retrieved by name and number.

[source,python]
----
tsv_by_num = TrainingSetClient.get_training_set_version(
    training_set_name="my-training-set",
    number=2,
)

----

You can also access the raw data from the `TrainingSetVersion`.

[source,python]
----
raw_df = tsv_by_num.load_raw_pandas()

----

Training data can also be accessed.

[source,python]
----
training_df = tsv_by_num.load_training_pandas()

----

== Find TrainingSetVersions

You can search for `TrainingSetVersions` for the current project using the training set name, `TrainingSet` metadata, and `TrainingSetVersion` metadata. Results matching all search fields are returned.

[source,python]
----
versions = TrainingSetClient.list_training_set_versions(
    training_set_name="my-training-set",
    meta={"year": "2021"},
    training_set_meta={"category": "widgets"}
)

----

== Get TrainingSet

You can retrieve `TrainingSets` by name.

[source,python]
----
ts = TrainingSetClient.get_training_set("my-training-set")

----

== Find TrainingSets

You can search `TrainingSets` by metadata. Results matching all metadata fields are returned.

[source,python]
----
ts = TrainingSetClient.list_training_sets(
    meta={"category": "widgets"},
)

----

----- api_guide/data_api/trainingsets_usecases/index.txt -----
:page-version: 6.1
:page-title: TrainingSets use cases
:page-permalink: 440de9
:page-order: 40

[NOTE]
====
This feature is accessible only through the link:140b48/[Domino Data API].
====

Domino training sets contain the initial data used for training your machine learning models. A `TrainingSet` is a versioned set of data, column information, and other metadata.

[[tr20]]
Use Domino Training Sets to persist dataframes for model training and other analysis.
You can store and load multiple versions of a given dataframe from a training set so you can connect a model to the specific version of a dataframe that was used to train it.

[[tr21]]
The dataframe used as the basis for a Training Set can be constructed with the result of a Domino Data Source query (as described previously) or through any other construction method.

[[tr22]]
In addition to storing the underlying dataframe, training sets can be used to capture additional monitoring metadata.
When this additional data is present, Training Set versions will be available as sources of baseline training data when publishing Domino endpoints in Domino.

[[tr23]]
Training sets are only scoped to the projects in which they are created.
Users with project contributor permissions can create, load, and delete Training Set versions in that project.

Training sets are available as an API-only feature.

The following is a list of `TrainingSet` operations:

* link:9c4dec[Create TrainingSets]
* link:5cef05[Retrieve TrainingSets]
* link:d72d8a[Update TrainingSets]
* link:6def59[Delete TrainingSets]

You can also override authentication. For more information, see link:4915a4[custom authentication].

----- api_guide/data_api/trainingsets_usecases/update_trainingset.txt -----
:page-version: 6.1
:page-title: Update TrainingSet
:page-permalink: d72d8a
:page-order: 30

The following example shows how to update your `TrainingSet`:

[source,python]
----
ts = TrainingSetClient.get_training_set("my-training-set")

ts.description = "widget transactions"
ts.metadata.update({"region": "west"})

updated = TrainingSetClient.update_training_set(ts)
----

== Update TrainingSetVersion

The following example shows how to update your TrainingSetVersion:

[source,python]
----
tsv = TrainingSetClient.get_training_set_version("my-training-set", 2)

tsv.description = "2020 transactions"
tsv.metadata.update({"status": "final"})

updated = TrainingSetClient.update_training_set_version(tsv)

----

----- api_guide/domino-open-api.txt -----
:page-version: 6.1
:page-permalink: 8c929e
:page-title: Domino Platform API reference
// :toc: left
// :numbered:
// :toclevels: 3
:source-highlighter: highlightjs
:page-api-schema: public-api.yaml
:page-order: 20

This topic provides complete reference information about all of Domino's public REST API endpoints.

//OpenAPI content gets appended here at build time.

//static content for 5.2 below

----- api_guide/domino-r-package.txt -----
:page-version: 6.1
:page-title: Domino R package
:page-permalink: ad1224
:page-order: 30

// tag::intro[]
The Domino R package gives you access to the full power of Domino's cloud compute and version control without leaving your R environment.
// end::intro[]

NOTE: This information may be outdated. For additional functionality, consider writing your own R wrapper around the platform API.

== Install

The package is http://cran.r-project.org/web/packages/domino/[hosted on CRAN^], like any standard R package.

Install the Domino link:e21e55[command line client]. You won't use it directly, but it does have to be installed on your computer.

From R, run the following:

[source,r]
----
install.packages("domino", repos='http://cran.r-project.org', type="source")
----

After it's installed you can import the package as you would any other R package:

[source,r]
----
library(domino)
----

== Log in

You must login again from R to verify your identity to Domino. To do that type:

[source,r]
----
domino.login("yourUsername", host="https://app.dominodatalab.com")
----

You should get a success message. If you're running in a private deployment, your host will be the address of your internal Domino server.

== Get a Project

NOTE: If you already have a copy of your project on your computer, skip this step. Set your working directory to the root of that project folder and continue to the next section.

To download an existing project, run this command:

[source,r]
----
domino.get("quick-start")
----

You should see something like this:

[source,r]
----
> domino.get("quick-start")
Getting project chris/quick-start...
Project get complete. Make sure to navigate to your new project folder by running:
    cd quick-start
To run a file on Domino, type:
    domino run [command-to-run]
[1] "Changed working directory to new project's director"
----

This command also changes your working directory to the root of the project folder. In general, these Domino commands will only work if your working directory is set to be inside your project folder.

== Run code

To start the execution of a script on Domino, type:

[source,r]
----
domino.run("myScript.R")
----

You should see something like this:

[source,r]
----
> domino.run("myScript.R")
Determining which files are out of date...

No changes to upload to server, because you are up to date.
Run for project chris/quick-start started. You can view progress here:
  https://app.dominodatalab.com/chris/quick-start/run/53051188e4b0b5aeedd6a73b
----

You can also pass in arguments into the scripts:

[source,r]
----
domino.run("myScript.R", 1, "someString")
----

You can read in those parameters in your script:

[source,r]
----
> firstParameter  1
> secondParameter  "someString"
----

You can launch several jobs:

[source,r]
----
for (alpha <- 1:3) {
    for (beta <- 1:3) {
        domino.run("myScript.R", alpha, beta)
    }
}
----

This will queue 9 jobs, each with a unique setting of the values alpha and beta.

== Download results

After your runs have completed, you get download the results back to your computer by running:

[source,r]
----
domino.download()
----

This will download all the new and modified files in your project to your local computer. Once this command completes, these files will be available immediately to be loaded into your R workspace:

[source,r]
----
> domino.download()
Downloading latest changes to project...

Determining which files are out of date...

Changes from the server:
------------------------

  x Modifying file results/stdout.txt
  x Modifying file someData.RData

Starting download of new files:
-------------------------------
    Downloading "someData.RData" ...
      complete!
    Downloading "results/stdout.txt" ...
      complete!
Download complete.

> load("someData.RData") # -> load the R objects
----

== Two-way sync

You can perform a two-way sync with the command:

[source,r]
----
domino.sync()
----

This runs a download followed by an upload. If there is a conflict between the local and server versions of a file, you will get both versions so that you can fix the conflict and then re-sync.

== Initialize a Project

To initialize a new project inside an existing directory, go to the directory and then run:

[source,r]
----
domino.init("projectName")
----

This will create a Domino project named `projectName`, with its root at the current working directory. You can then sync this project to the server:

[source,r]
----
domino.sync()
----

== Other commands

You can also run other commands from the link:9355a5[CLI] with methods named according to their CLI counterparts. For additional details, see the documentation https://cran.r-project.org/web/packages/domino/domino.pdf[hosted on CRAN^].

----- api_guide/domino-volumes-api.txt -----
:page-permalink: b3b2a1
:page-version: 6.1
:page-iversion: 610
:page-title: Domino Volumes API
:page-order: 65

Microservice responsible for managing Domino Volumes for NetApp ONTAP feature
v1.0

== API Information

=== General Info

*Swagger Version:* 2.0 +
*Title:* Domino Volumes for NetApp ONTAP Microservice API +
*Description:* Microservice responsible for managing Domino Volumes for NetApp ONTAP feature +
*Version:* 1.0

=== Contact Information

*Name:* Domino Data Lab +
*URL:* https://tickets.dominodatalab.com/hc/en-us[https://tickets.dominodatalab.com/hc/en-us^] +
*Email:* mailto:support@dominodatalab.com[support@dominodatalab.com^]

=== License

*Name:* Apache 2.0 +
*URL:* http://www.apache.org/licenses/LICENSE-2.0.html[http://www.apache.org/licenses/LICENSE-2.0.html^]

=== Server Details

*Host:* (none specified) +
*BasePath:* /domino-netapp-volumes/v1

== Endpoints

=== /filesystems

==== GET /filesystems

Description: List all filesystems with optional filters and pagination +
Consumes: application/json +
Produces: application/json +
Tags: filesystems +
Summary: List filesystems

Parameters::

[cols="1,1,1,3", options="header"]
|===
|Name |Location |Type |Description

|`limit`
|query
|integer
|Maximum number of results to return

|`offset`
|query
|integer
|Starting index of the returned results

|`order`
|query
|string
|Sort direction (asc or desc)

|`search`
|query
|string
|Search term

|`sort_by`
|query
|string
|Field name to sort by

|`data_plane_id`
|query
|array (string, multi)
|Filter by one or more Data Plane IDs
|===

Responses::

200 - OK (Schema: #/definitions/server.PaginatedFilesystems)

400 - Bad Request (Schema: #/definitions/server.HTTPError)

403 - Forbidden (Schema: #/definitions/server.HTTPError)

404 - Not Found (Schema: #/definitions/server.HTTPError)

500 - Internal Server Error (Schema: #/definitions/server.HTTPError)

==== POST /filesystems

Description: Create a new filesystem +
Consumes: application/json +
Produces: application/json +
Tags: filesystems +
Summary: Create a new filesystem

Parameters::

[cols="1,1,1,3", options="header"]
|===
|Name |Location |Type |Description

|`request`
|body (required)
|object
|Filesystem to create (Schema: #/definitions/server.Filesystem)
|===

Responses::

200 - OK (Schema: #/definitions/remotefs.Filesystem)

400 - Bad Request (Schema: #/definitions/server.HTTPError)

403 - Forbidden (Schema: #/definitions/server.HTTPError)

404 - Not Found (Schema: #/definitions/server.HTTPError)

500 - Internal Server Error (Schema: #/definitions/server.HTTPError)

==== GET /filesystems/{id}

Description: Retrieve a filesystem by its ID +
Consumes: application/json +
Produces: application/json +
Tags: filesystems +
Summary: Get a filesystem by ID

Parameters::

[cols="1,1,1,3", options="header"]
|===
|Name |Location |Type |Description

|`id`
|path (required)
|string
|ID of the filesystem to retrieve

|`include_hostname`
|query
|boolean
|Whether to include the data plane host name in the response
|===

Responses::

200 - OK (Schema: #/definitions/remotefs.Filesystem)

400 - Bad Request (Schema: #/definitions/server.HTTPError)

403 - Forbidden (Schema: #/definitions/server.HTTPError)

404 - Not Found (Schema: #/definitions/server.HTTPError)

500 - Internal Server Error (Schema: #/definitions/server.HTTPError)

==== PUT /filesystems/{id}

Description: Update an existing filesystem +
Consumes: application/json +
Produces: application/json +
Tags: filesystems +
Summary: Update filesystem

Parameters::

[cols="1,1,1,3", options="header"]
|===
|Name |Location |Type |Description

|`id`
|path (required)
|string
|ID of the filesystem to update

|`request`
|body (required)
|object
|Updated filesystem data (Schema: #/definitions/server.Filesystem)
|===

Responses::

200 - OK (Schema: #/definitions/remotefs.Filesystem)

400 - Bad Request (Schema: #/definitions/server.HTTPError)

403 - Forbidden (Schema: #/definitions/server.HTTPError)

404 - Not Found (Schema: #/definitions/server.HTTPError)

500 - Internal Server Error (Schema: #/definitions/server.HTTPError)

==== DELETE /filesystems/{id}

Description: Delete a filesystem by ID +
Consumes: application/json +
Produces: application/json +
Tags: filesystems +
Summary: Delete a filesystem

Parameters::

[cols="1,1,1,3", options="header"]
|===
|Name |Location |Type |Description

|`id`
|path (required)
|string
|ID of the filesystem to delete
|===

Responses::

204 - No Content

400 - Bad Request (Schema: #/definitions/server.HTTPError)

403 - Forbidden (Schema: #/definitions/server.HTTPError)

404 - Not Found (Schema: #/definitions/server.HTTPError)

500 - Internal Server Error (Schema: #/definitions/server.HTTPError)

=== /volumes

==== GET /volumes

Description: List all volumes with optional filters and pagination +
Consumes: application/json +
Produces: application/json +
Tags: volumes +
Summary: List volumes

Parameters::

[cols="1,1,1,3", options="header"]
|===
|Name |Location |Type |Description
|`limit` |query |integer |Maximum number of items to return
|`offset` |query |integer |Offset for pagination
|`order` |query |string |Sort direction (asc or desc)
|`sort_by` |query |string |Field to sort by
|`search` |query |string |Search term for volume name or ID
|`data_plane_id` |query |array (string, multi) |Filter by Data Plane ID(s)
|`filesystem_id` |query |array (string, multi) |Filter by Filesystem ID(s)
|`project_id` |query |array (string, multi) |Filter by Project ID(s)
|`show_deleted` |query |boolean |Whether to include volumes marked as deleted
|===

Responses::

200 - OK (Schema: #/definitions/server.PaginatedVolumes)

400, 403, 404, 500 - Various errors (Schema: #/definitions/server.HTTPError)

==== POST /volumes

Description: Create a new volume +
Consumes: application/json +
Produces: application/json +
Tags: volumes +
Summary: Create a volume

Parameters::

[cols="1,1,1,3", options="header"]
|===
|Name |Location |Type |Description
|`request` |body (required) |object |Volume creation request (Schema: #/definitions/server.CreateVolumeRequest)
|===

Responses::

200 - OK (Schema: #/definitions/remotefs.Volume)

400, 403, 404, 500 - Various errors (Schema: #/definitions/server.HTTPError)

=== /volumes/{id}

==== GET /volumes/{id}

Description: Retrieve volume details by ID +
Consumes: application/json +
Produces: application/json +
Tags: volumes +
Summary: Get volume by ID

Parameters::

[cols="1,1,1,3", options="header"]
|===
|Name |Location |Type |Description
|`id` |path (required) |string |ID of the volume
|===

Responses::

200 - OK (Schema: #/definitions/remotefs.Volume)

400, 403, 404, 500 - Various errors (Schema: #/definitions/server.HTTPError)

==== PATCH /volumes/{id}

Description: Update specific fields of a volume +
Consumes: application/json +
Produces: application/json +
Tags: volumes +
Summary: Patch volume

Parameters::

[cols="1,1,1,3", options="header"]
|===
|Name |Location |Type |Description
|`id` |path (required) |string |ID of the volume
|`request` |body (required) |object |Patch request (Schema: #/definitions/server.PatchVolumeRequest)
|===

Responses::

200 - OK (Schema: #/definitions/remotefs.Volume)

400, 403, 404, 500 - Various errors (Schema: #/definitions/server.HTTPError)

=== /volumes/{id}/grants

==== GET /volumes/{id}/grants

Description: Get user or project grants for a volume +
Consumes: application/json +
Produces: application/json +
Tags: volumes +
Summary: List grants for a volume

Parameters::

[cols="1,1,1,3", options="header"]
|===
|Name |Location |Type |Description
|`id` |path (required) |string |ID of the volume
|===

Responses::

200 - OK (Schema: #/definitions/server.Grants)

400, 403, 404, 500 - Various errors (Schema: #/definitions/server.HTTPError)

==== PUT /volumes/{id}/grants

Description: Replace grants for a volume +
Consumes: application/json +
Produces: application/json +
Tags: volumes +
Summary: Set grants for a volume

Parameters::

[cols="1,1,1,3", options="header"]
|===
|Name |Location |Type |Description
|`id` |path (required) |string |ID of the volume
|`request` |body (required) |object |Grant request (Schema: #/definitions/server.SetGrantsRequest)
|===

Responses::

200 - OK (Schema: #/definitions/server.Grants)

400, 403, 404, 500 - Various errors (Schema: #/definitions/server.HTTPError)

=== /rpc/attach-volume-to-project

==== POST /rpc/attach-volume-to-project

Description: Attach a volume to a project +
Consumes: application/json +
Produces: application/json +
Tags: volumes +
Summary: Attach volume to project

Parameters::

[cols="1,1,1,3", options="header"]
|===
|Name |Location |Type |Description
|`request` |body (required) |object |Attach request (Schema: #/definitions/server.UpdateVolumeProject)
|===

Responses::

204 - No Content (Schema: #/definitions/remotefs.Volume)

400, 403, 404, 500 - Various errors (Schema: #/definitions/server.HTTPError)

=== /rpc/delete-volume

==== POST /rpc/delete-volume

Description: Delete a volume by ID +
Consumes: application/json +
Produces: application/json +
Tags: volumes +
Summary: Delete volume

Parameters::

[cols="1,1,1,3", options="header"]
|===
|Name |Location |Type |Description
|`request` |body (required) |object |Volume ID (Schema: #/definitions/server.VolumeIDRequest)
|===

Responses::

204 - No Content

400, 403, 404, 500 - Various errors (Schema: #/definitions/server.HTTPError)

=== /rpc/get-volume-mounts

==== POST /rpc/get-volume-mounts

Description: Retrieve mounts for volumes and snapshots by project +
Consumes: application/json +
Produces: application/json +
Tags: volumes +
Summary: Get volume mounts

Parameters::

[cols="1,1,1,3", options="header"]
|===
|Name |Location |Type |Description
|`request` |body (required) |object |Volume mount query (Schema: #/definitions/server.ListVolumeMountsRequest)
|===

Responses::

200 - OK (Schema: array of #/definitions/remotefs.VolumeMount)

400, 403, 404, 500 - Various errors (Schema: #/definitions/server.HTTPError)

=== /rpc/mark-volume-for-deletion

==== POST /rpc/mark-volume-for-deletion

Description: Mark a volume for deletion +
Consumes: application/json +
Produces: application/json +
Tags: volumes +
Summary: Mark volume for deletion

Parameters::

[cols="1,1,1,3", options="header"]
|===
|Name |Location |Type |Description
|`request` |body (required) |object |Volume ID (Schema: #/definitions/server.VolumeIDRequest)
|===

Responses::

204 - No Content (Schema: #/definitions/remotefs.Volume)

400, 403, 404, 500 - Various errors (Schema: #/definitions/server.HTTPError)

=== /rpc/detach-volume-from-project

==== POST /rpc/detach-volume-from-project

Description: Detach a volume from a project +
Consumes: application/json +
Produces: application/json +
Tags: volumes +
Summary: Detach volume from project

Parameters::

[cols="1,1,1,3", options="header"]
|===
|Name |Location |Type |Description
|`request` |body (required) |object |Detach request (Schema: #/definitions/server.UpdateVolumeProject)
|===

Responses::

204 - No Content (Schema: #/definitions/remotefs.Volume)

400, 403, 404, 500 - Various errors (Schema: #/definitions/server.HTTPError)

=== /rpc/restore-volume

==== POST /rpc/restore-volume

Description: Restore a volume to active state +
Consumes: application/json +
Produces: application/json +
Tags: volumes +
Summary: Restore volume

Parameters::

[cols="1,1,1,3", options="header"]
|===
|Name |Location |Type |Description
|`request` |body (required) |object |Volume ID (Schema: #/definitions/server.VolumeIDRequest)
|===

Responses::

204 - No Content (Schema: #/definitions/remotefs.Volume)

400, 403, 404, 500 - Various errors (Schema: #/definitions/server.HTTPError)

=== /rpc/check-is-over-limit

==== POST /rpc/check-is-over-limit

Description: Check whether requesting user is over their volume limit +
Consumes: application/json +
Produces: application/json +
Tags: volumes +
Summary: Check volume limit

Responses::

200 - OK (Schema: boolean)

400, 403, 404, 500 - Various errors (Schema: #/definitions/server.HTTPError)

=== /snapshots

==== GET /snapshots

Description: List all snapshots with optional filters and pagination +
Consumes: application/json +
Produces: application/json +
Tags: snapshots +
Summary: List snapshots

Parameters::

[cols="1,1,1,3", options="header"]
|===
|Name |Location |Type |Description
|`limit` |query |integer |Maximum number of items to return
|`offset` |query |integer |Offset for pagination
|`order` |query |string |Sort direction (asc or desc)
|`sort_by` |query |string |Field to sort by
|`search` |query |string |Search term for snapshot name or ID
|`volume_id` |query |array (string, multi) |Filter by Volume ID(s)
|`project_id` |query |array (string, multi) |Filter by Project ID(s)
|`show_deleted` |query |boolean |Whether to include snapshots marked as deleted
|===

Responses::

200 - OK (Schema: #/definitions/server.PaginatedSnapshots)

400, 403, 404, 500 - Various errors (Schema: #/definitions/server.HTTPError)

==== POST /snapshots

Description: Create a snapshot +
Consumes: application/json +
Produces: application/json +
Tags: snapshots +
Summary: Create snapshot

Parameters::

[cols="1,1,1,3", options="header"]
|===
|Name |Location |Type |Description
|`request` |body (required) |object |Snapshot creation request (Schema: #/definitions/server.CreateSnapshotRequest)
|===

Responses::

200 - OK (Schema: #/definitions/remotefs.Snapshot)

400, 403, 404, 500 - Various errors (Schema: #/definitions/server.HTTPError)

=== /snapshots/{id}

==== GET /snapshots/{id}

Description: Retrieve snapshot details by ID +
Consumes: application/json +
Produces: application/json +
Tags: snapshots +
Summary: Get snapshot by ID

Parameters::

[cols="1,1,1,3", options="header"]
|===
|Name |Location |Type |Description
|`id` |path (required) |string |Snapshot ID
|===

Responses::

200 - OK (Schema: #/definitions/remotefs.Snapshot)

400, 403, 404, 500 - Various errors (Schema: #/definitions/server.HTTPError)

==== PUT /snapshots/{id}

Description: Update a snapshot's metadata +
Consumes: application/json +
Produces: application/json +
Tags: snapshots +
Summary: Update snapshot

Parameters::

[cols="1,1,1,3", options="header"]
|===
|Name |Location |Type |Description
|id |path (required) |string |Snapshot ID
|`request` |body (required) |object |Patch request (Schema: #/definitions/server.PatchSnapshotRequest)
|===

Responses::

200 - OK (Schema: #/definitions/remotefs.Snapshot)

400, 403, 404, 500 - Various errors (Schema: #/definitions/server.HTTPError)

=== /rpc/delete-snapshot

==== POST /rpc/delete-snapshot

Description: Delete a snapshot by ID +
Consumes: application/json +
Produces: application/json +
Tags: snapshots +
Summary: Delete snapshot

Parameters::

[cols="1,1,1,3", options="header"]
|===
|Name |Location |Type |Description
|`request` |body (required) |object |Snapshot ID (Schema: #/definitions/server.SnapshotIDRequest)
|===

Responses::

204 - No Content

400, 403, 404, 500 - Various errors (Schema: #/definitions/server.HTTPError)

=== /rpc/restore-snapshot

==== POST /rpc/restore-snapshot

Description: Restore a snapshot to active state +
Consumes: application/json +
Produces: application/json +
Tags: snapshots +
Summary: Restore snapshot

Parameters::

[cols="1,1,1,3", options="header"]
|===
|Name |Location |Type |Description
|`request` |body (required) |object |Snapshot ID (Schema: #/definitions/server.SnapshotIDRequest)
|===

Responses::

204 - No Content (Schema: #/definitions/remotefs.Snapshot)

400, 403, 404, 500 - Various errors (Schema: #/definitions/server.HTTPError)

=== /account/authz/permissions/authorizedactions

==== POST /account/authz/permissions/authorizedactions

Description: Get all permissions for the requesting user with optional volume or project context +
Consumes: application/json +
Produces: application/json +
Tags: authz +
Summary: Get all authorized actions for the user

Parameters::

[cols="1,1,1,3", options="header"]
|===
|Name |Location |Type |Description
|`request` |body (required) |object |Permission check input (Schema: #/definitions/server.PermissionRequest)
|===

Responses::

200 - OK (Schema: #/definitions/server.PermissionResponse)

400, 403, 404, 500 - #/definitions/server.HTTPError

=== /account/authz/permissions/volume/{id}

==== POST /account/authz/permissions/volume/{id}

Description: Check if user has all provided permissions on a volume +
Consumes: application/json +
Produces: application/json +
Tags: authz +
Summary: Check volume permissions

Parameters::

[cols="1,1,1,3", options="header"]
|===
|Name |Location |Type |Description
|`id` |path (required) |string |Volume ID
|`request` |body (required) |object |Permissions to check (Schema: #/definitions/server.VolumePermissionsRequest)
|===

Responses::

200 - OK (Schema: boolean)

400, 403, 404, 500 - #/definitions/server.HTTPError

=== /downloads

==== POST /downloads

Description: Create a download task that generates an archive from given volume paths +
Consumes: application/json +
Produces: application/json +
Tags: downloads +
Summary: Create download archive

Parameters::

[cols="1,1,1,3", options="header"]
|===
|Name |Location |Type |Description
|`request` |body (required) |object |Archive request (Schema: #/definitions/server.CreateDownloadArchiveRequest)
|===

Responses::

200 - OK (Schema: #/definitions/remotefs.FiletaskTask)

400, 403, 404, 500 - #/definitions/server.HTTPError

=== /downloads/{id}

==== GET /downloads/{id}

Description: Get status of a download archive task +
Consumes: application/json +
Produces: application/json +
Tags: downloads +
Summary: Check archive download status

Parameters::

[cols="1,1,1,3", options="header"]
|===
|Name |Location |Type |Description
|`id` |path (required) |string |Download task ID
|`volume_id` |query (required) |string |Volume ID of the download
|===

Responses::

200 - OK (Schema: #/definitions/remotefs.FiletaskTask)

400, 403, 404, 500 - #/definitions/server.HTTPError

== Definitions

=== remotefs.Volume

Type: object

_Properties:_ +
id: string +
name: string +
capacity: integer +
dataPlaneId: string +
filesystemId: string +
projectId: string +
status: remotefs.ResourceStatus +
createdBy: remotefs.UserSummary +
updatedBy: remotefs.UserSummary +
createdAt: string +
updatedAt: string +
deletedAt: string (nullable) +
grants: array of server.Grant

=== remotefs.Snapshot

Type: object

_Properties:_ +
id: string +
name: string +
volumeId: string +
projectId: string +
createdBy: remotefs.UserSummary +
updatedBy: remotefs.UserSummary +
createdAt: string +
updatedAt: string +
deletedAt: string (nullable)

=== remotefs.Filesystem

Type: object

_Properties:_ +
id: string +
name: string +
dataPlaneId: string +
storageClass: string +
rootPvc: string +
hostname: string +
isDataPlaneDefault: boolean +
createdBy: remotefs.UserSummary +
updatedBy: remotefs.UserSummary +
createdAt: string +
updatedAt: string

=== remotefs.FiletaskTask

Type: object

_Properties:_ +
id: string +
status: string +
archivePath: string +
createdAt: string +
volumeId: string +
requestedBy: remotefs.UserSummary

=== remotefs.VolumeMount

Type: object

_Properties:_ +
projectId: string +
volumeId: string +
mountPath: string +
readOnly: boolean

=== remotefs.UserSummary

Type: object

_Properties:_ +
id: string +
username: string +
fullName: string

=== remotefs.ResourceStatus

Type: string

Enum: Active, Deleted, Pending, Failed

=== server.HTTPError

Type: object

_Properties:_ +
message: string +
status: integer +
errorCode: string +
details: object (optional)

=== server.PermissionRequest

Type: object

_Properties:_ +
actions: array of string +
context: object +
resource: string

=== server.PermissionResponse

Type: object

_Properties:_ +
granted: boolean +
missingPermissions: array of string

=== server.CreateDownloadArchiveRequest

Type: object

_Properties:_ +
volumeId: string +
paths: array of string

=== server.CreateVolumeRequest

Type: object

_Properties:_ +
name: string +
capacity: integer +
filesystemId: string +
projectId: string

=== server.CreateSnapshotRequest

Type: object

_Properties:_ +
volumeId: string +
name: string

=== server.PatchVolumeRequest

Type: object

_Properties:_ +
name: string (optional) +
projectId: string (optional)

=== server.PatchSnapshotRequest

Type: object

_Properties:_ +
name: string (optional)

=== server.VolumeIDRequest

Type: object

_Properties:_ +
volumeId: string

=== server.SnapshotIDRequest

Type: object

_Properties:_ +
snapshotId: string

=== server.UpdateVolumeProject

Type: object

_Properties:_ +
volumeId: string +
projectId: string

=== server.SetGrantsRequest

Type: object

_Properties:_ +
volumeId: string +
grants: array of server.Grant

=== server.Grant

Type: object

_Properties:_ +
entityId: string +
entityType: string (e.g. user, project) +
accessLevel: string (e.g. read, write, admin)

=== server.ListVolumeMountsRequest

Type: object

_Properties:_ +
projectId: string

=== server.VolumePermissionsRequest

Type: object

_Properties:_ +
volumeId: string +
permissions: array of string

=== server.PaginatedVolumes

Type: object

_Properties:_ +
items: array of remotefs.Volume +
totalCount: integer

=== server.PaginatedSnapshots

Type: object

_Properties:_ +
items: array of remotefs.Snapshot +
totalCount: integer

=== server.PaginatedFilesystems

Type: object

_Properties:_ +
items: array of remotefs.Filesystem +
totalCount: integer

== Security Definitions

=== BearerAuth

*Type:* token +
*Name:* Authorization +
*In:* header

You can find the required token by logging into Domino and making a request to the endpoint `/account/auth/service/authenticate?tokenFromIdp=true`.
The token should be passed in the format `"Authorization": "Bearer <token>"`.
----- api_guide/get-api-key.txt -----
:page-version: 6.1
:page-permalink: d982cc
:page-title: Get API key
:page-order: 10

To interact with secured endpoints of the link:f31cde[Model Monitoring API] or link:8c929e[Domino Platform API], you must send an API authentication token or key along with your request.
This parameter identifies you as a specific Domino user so Domino can check for authorization.

== Find your Domino Rest API key

CAUTION: This feature will be deprecated in future versions of Domino.
Domino recommends using link:6921e5[Service Accounts] as they provide stable, token-based authentication for API calls, independent access management, and stronger security.

To authenticate your requests, include your API key with the header `X-Domino-Api-Key`.

.Get your API key

. Go to *Account* > *Account Settings*.
+
image::/images/6.0/get-api-key.png[alt="Get API key", width=250]

. Click *API Key* in the left navigation to get your key.
+
image::/images/6.0/view-api-key.png[alt="View API key", width=1000]

CAUTION: Anyone with this key can authenticate to the Domino application as the user it represents and take any actions that the user is authorized to perform.
Treat it like a sensitive password.


----- api_guide/index.txt -----
:page-version: 6.1
:page-title: API Guide
:page-permalink: f35c19

Much of Domino's functionality is available programmatically through the link:8c929e[public Platform API endpoints].
Domino also provides a link:c5ef26[Python wrapper].

The Domino API is different from our link:8dbc91[Domino endpoint] functionality:

[cols="1a,1a",options="header"]
|===
| Domino API        | Domino endpoints

| The Domino API lets you control Domino's core functionality programmatically.

See the topics in this guide to learn about the methods to access this functionality.

| Domino endpoints are REST API endpoints that provide programmatic access to your R and Python data science code.

See link:8dbc91[Domino endpoints] to learn how to publish and access your models as APIs.
|===

See also the link:9355a5[Domino CLI].

== The Domino API

The Domino API is a collection of public REST API endpoints that give you access to Domino's core functionality.  You can use the API to manage projects, datasets, environments, jobs, and more.

See link:8c929e[Domino Platform API reference] for complete reference information about all of the public API endpoints.

Some endpoints require an link:d982cc[API key] to check for authorization to access specific resources.


== Domino Data API

This library for Python 
and R 
provides access to your data, in files or SQL-based tables, using consistent access patterns. The results are available as dataframe abstractions for popular libraries. You can query connectors interactively and you don't have to restart a workload to install drivers.

See link:140b48[Domino Data API^] for details.


== The python-domino library

The `python-domino` library is a wrapper for many of the Domino API endpoints.  You can install it in Domino and use it in your project code to manage your files, executions, datasets, the properties of the project, and more.

See link:c5ef26[The python-domino Library] for details.  The public repository is available at https://github.com/dominodatalab/python-domino[https://github.com/dominodatalab/python-domino^].

== Domino R package

The Domino R package gives you access to the full power of Domino's cloud compute and version control without leaving your R environment.
To use this package, you must install link:9355a5[the Domino CLI].
See link:ad1224[Domino R package] for details.

== Domino Volumes for NetApp ONTAP API

The Domino Volumes API gives you programmatic access to the link:cf04be[Domino Volumes for NetApp ONTAP] feature.
See the link:adc186[Domino Volumes API reference] for more detail.

== Model Monitoring API

The Model Monitoring API gives you programmatic access to the link:715969[Model Monitor].
See the link:f31cde[Model Monitoring API reference] for more detail.

You can also use Domino’s link:a5bd0c[Model Monitoring custom metrics Python SDK] to define custom metrics and use them alongside drift and model quality monitored metrics.

----- api_guide/model_monitoring/custom-metrics.txt -----
:page-version: 6.1
:page-permalink: a5bd0c
:page-title: Model Monitoring custom metrics Python SDK
:page-sidebar: Model Monitoring custom metrics SDK
:page-order: 20

// tag::body[]

Use Domino's Model Monitoring custom metrics Python SDK to define custom metrics and use them alongside out-of-the-box drift and model quality metrics that are monitored in Domino Model Monitor.
With this SDK, you can register new metrics and define the logic to compute them.
You can author this logic and evaluate it from within a Domino project.

For every model that you register for monitoring, you can select a registered metric, associate the data sources from which the metric is computed, and set up the execution environment to compute this metric on a periodic basis.
//Domino stores these metrics and presents them in dashboards.
You are notified by email when a metric behaves abnormally based on threshold definitions.

For end-to-end working code with a description of the workflow, see the `/custom metrics example` folder in the quick-start project.

== Use custom metrics

Use the following library functions from the custom metrics Python SDK to build and deploy your own monitoring metrics.

. Instantiate the client.
Enter your `DMM model ID`:
+
[source, python]
----
import domino
dmm_model_id = "XXXXXXXXXXXXX"
d = domino.Domino("integration-test/quick-start")
metrics_client = d.custom_metrics_client()
----

. Log the custom metrics:
+
[source, python]
----
metrics_client.log_metric(dmm_model_id, "accuracy", 7.1234, "2022-10-08T00:00:00Z", { "example_tag1" : "value1", "example_tag2" : "value2" })

metrics_client.log_metrics([
{ "modelMonitoringId" : dmm_model_id, "metric" : "accuracy", "value" : 7.1234,
"timestamp" : "2022-10-08T00:00:00Z",
"tags" : { "example_tag1" : "value1", "example_tag2" : "value2" }
]
},
{ "modelMonitoringId" : dmm_model_id, "metric" : "accuracy", "value" : 8.4567,
"timestamp" : "2022-10-09T00:00:00Z" }
])
----
+
* `modelMonitoringId`: ID of the monitored model to send metric alerts for
* `metric`: Name of the metric to send alert for
* `value`: Value of the metric
* `timestamp`: Timezone is in UTC in ISO 8601 format.
* `tags`: Custom metadata for metric represented as key-value string pairs

. Send a custom metrics alert:
+
[source, python]
----
metrics_client.trigger_alert(dmm_model_id, "metric_name", 3.14,
condition = metrics_client.BETWEEN,
lower_limit=2.0, upper_limit=3.0,
description = "Breached 2.0-3.0 range." )
----
+
* `modelMonitoringId`: ID of the monitored model for which to send metric alerts.
* `metric`: Name of the metric for which to send the alert.
* `value`: Value of the metric.
* `condition`: Target range for the metric defined by lower and upper limit bounds. +
The following are potential values for the `condition` argument:
** `metrics_client.LESS_THAN = "lessThan"`
** `metrics_client.LESS_THAN_EQUAL = "lessThanEqual"`
** `metrics_client.GREATER_THAN = "greaterThan"`
** `metrics_client.GREATER_THAN_EQUAL = "greaterThanEqual"`
** `metrics_client.BETWEEN = "between"`
* `lower_limit`: The lower limit for the `condition`.
* `upper_limit`: The upper limit for the `condition`.
* `description`: Optional message included in the alert.

. Retrieve the custom metrics:
+
[source, python]
----
res = metrics_client.read_metrics(dmm_model_id, "accuracy",
"2022-10-08T00:00:00Z", "2022-10-10T00:00:00Z" )
----
+
* `modelMonitoringId`: ID of the monitored model for which to retrieve the metric values.
* `metric`: Name of the metric for which to retrieve the metric values.
* `start_timestamp`: The start timestamp of the range when the metrics were logged. The timezone is in UTC in ISO 8601 format.
* `end_timestamp`: The end timestamp of the range when the metrics were logged. The timezone is in UTC in ISO 8601 format.

== Sample output

----
{
  'metricValues': [
      {'timestamp': '2022-10-08T00:00:00Z', 'value': 3.123, 'tags': []},
      {'timestamp': '2022-10-08T00:00:00Z', 'value': 3.123, 'tags': []},
      {'timestamp': '2022-10-08T00:00:00Z', 'value': 3.123, 'tags': []},
      {'timestamp': '2022-10-08T00:00:00Z', 'value': 3.123, 'tags': []},
{'timestamp': '2022-10-08T00:00:00Z', 'value': 7.1234, 'tags': {'example_tag1': 'value1', 'example_tag2': 'value2'}},
      {'timestamp': '2022-10-09T00:00:00Z', 'value': 8.4567, 'tags': []}
   ],
   'metadata': {'requestId': '97e61f63-3be5-4766-a07a-06c931d91268', 'notices': []}
}
----

.Results:

* `timestamp`: The time this metric was logged.
* `value`: The value of the metric.
* `tags`: Custom metadata for metric represented as key-value string pairs.



// end::body[]

----- api_guide/model_monitoring/dmm-api.txt -----
:page-version: 6.1
:page-permalink: f31cde
:page-title: Model Monitoring API reference
:page-order: 10
:page-api-schema: dmm.yaml

// tag::intro[]
The Model Monitoring API gives you programmatic access to the link:715969[Model Monitor].

To invoke these APIs, you must pass the API token in the header. See link:bd3a62[API Token] for more information.

Domino Model Monitoring can only ingest prediction data for Domino endpoints in the form of link:ae1654[Domino Datasets] captured using link:93e5c0[Domino's prediction capture library]. To ingest different data types, create a new model in Domino Model Monitor and use a link:a94c1c#_datasource[Model Monitor Data Source].

// end::intro[]

----- api_guide/model_monitoring/index.txt -----
:page-permalink: c6b52a
:page-version: 6.1
:page-title: Model Monitoring API
:page-order: 60

link:f31cde[Model Monitoring API reference]::
A reference guide with detailed information about the Model Monitoring API endpoints and models.

link:a5bd0c[Model Monitoring custom metrics Python SDK]::
A guide on how to define custom metrics and use them alongside out-of-the-box drift and model quality metrics that are monitored in Domino.

----- api_guide/python-domino.txt -----
:page-version: 6.1
:page-permalink: c5ef26
:page-title: The python-domino library
:page-order: 50

// How to update this page:
// 1. Clone https://github.com/dominodatalab/python-domino.
// 2. Create a new branch.
// 3. Update the README file (which one?  .md or .adoc?) as needed, commit it, and push your branch.
// 4. To preview your remote branch here in this topic, change `master` below to the name of your branch and then build the preview site.
// 5. Before merging your branch into `master`, change the path below back to `master`.


// The content from the remote repository's README gets included here:
include::https://raw.githubusercontent.com/dominodatalab/python-domino/Release-1.4.8/README.adoc[lines=2..-1]

// Missing versions here; default to the only in-between version that has a branch in the remote repo.



== Visit the python-domino repository

You can find the complete library, with documentation and example code, in the public repository at https://github.com/dominodatalab/python-domino[https://github.com/dominodatalab/python-domino^].

// https://dominodatalab.slack.com/archives/C17AZ88MU/p1667494544965619?thread_ts=1667489714.471159&cid=C17AZ88MU
// Do we have a matrix anywhere of which py-dom version shipped/was qualified with each Domino/DSE release?
// Unfortunately not, but if we have DSE container names, we can just interrogate the containers:
// docker run --rm quay.io/domino/compute-environment-images:ubuntu18-py3.8-r4.1-domino5.1-standard bash -c 'pip show python-domino || pip show dominodatalab'
// Comments below indicate the names of the images that I queried to get their `dominodatalab` library versions; all of them said that `python-domino` was not found.

----- api_guide/python-wrapper-for-domino-api.txt -----
:page-version: 6.1
:page-title: Python Wrapper for Domino API
:page-permalink: d9cc25
:page-order: 40

The Python binding for the Domino API is available in PyPI:

----
pip install dominodatalab
----

If you want to use the Python binding in a Domino workbook session include `dominodatalab` in your project's link:/user_guide/9c4f82#python[requirements.txt file].
This makes the Python binding available for each new workbook session (or batch run) started within the project.


[TIP]
====
Full documentation for this API is forthcoming. For more information now, see https://github.com/dominodatalab/python-domino[Domino's public Python bindings project^].
====



.In existing deployments

After installation, you can instantiate a library either with your API Key or Auth Token File.

*Token File:* To use Token File to instantiate Python Domino, pass the path to this token file, either through class constructor (`domino_token_file=<Path to Token file>`) or through an environment variable.

[source,python]
----
export DOMINO_TOKEN_FILE=PATH_TO_DOMINO_TOKEN_FILE
----

//This will be phased out when the legacy info is phased out.
If you are using Python package in code that is already running in Domino, the `DOMINO_TOKEN_FILE` will be set automatically to be the token file for the user who started the run.

*API Key:* You must get your API key from your account page. To get your API Key, log into Domino and click your name in the menu. Click *Account Settings* and select *API Key*. Copy the API key to your clipboard.

The Python library will read this key from environment variables, so set it as follows in your shell:

[source,python]
----
export DOMINO_USER_API_KEY=YOUR_API_KEY
----

If you are using the Python package in code that is already running in Domino, the `DOMINO_API_USER_KEY` variable will be set automatically to be the key for the user who started the run.

[NOTE]
====
* If both API Key and Token file are present, default preference will be given to token file. To use API Key instead, clear the `DOMINO_TOKEN_FILE` environment variable.

* See the link:8c929e[Domino Platform API reference] for more detail.
====

The following is an example of usage:

[source,python]
----
from domino import Domino

# By and large your commands will run against a single project,
# so you must specify the full project name
domino = Domino("chris/canon")

# List all runs in the project, most-recently queued first
all_runs = domino.runs_list()['data']

latest_100_runs = all_runs[0:100]

print(latest_100_runs)

# all runs have a commitId (the snapshot of the project when the
# run starts) and, if the run completed, an "outputCommitId"
# (the snapshot of the project after the run completed)
most_recent_run = all_runs[0]

commitId = most_recent_run['outputCommitId']

# list all the files in the output commit ID -- only showing the
# entries under the results directory.  If not provided, will
# list all files in the project.  Or you can say path=“/“ to
# list all files
files = domino.files_list(commitId, path='results/')['data']

for file in files:
print file['path'], '->', file['url']

print(files)

# Get the content (i.e. blob) for the file you're interested in.
# blobs_get returns a connection rather than the content, because
# the content can get quite large and it's up to you how you want
# to handle it
print(domino.blobs_get(files[0]['key']).read())

# Start a run of file main.py using the latest copy of that file
domino.runs_start(["main.py", "arg1", "arg2"])

# Start a "direct" command
domino.runs_start(["echo 'Hello, World!'"], isDirect=True)

# Start a run of a specific commit
domino.runs_start(["main.py"], commitId="aabbccddee")
----

//Remove the version-specific info when we remove the legacy content

.In new deployments

Domino updated the Python library to work with the API Proxy inside the execution.
If the library is instantiated without any authentication arguments, it will detect the API Proxy (see the `DOMINO_API_PROXY` environment variable) and route the calls through it automatically.
If you explicitly specify other means of authentication (`domino_token`, `domino_token_file`, `api_key`), it will work exactly like it did previously.

See link:40b91f[Use API Proxy to Authenticate Calls to the Domino API].


----- release_notes/6-1-0.txt -----
:page-permalink: 99ef5c
:page-version: 6.1
:page-title: Domino 6.1.0 (June 2025)
:page-order: 24

== New features

=== Domino Volumes for NetApp ONTAP
// DOM-60058
link:06da1b[NetApp Volumes] allow for users to instantly snapshot training data for auditability and reproducibility with no extra storage required.

=== Local IDE Support via SSH
// DOM-64437
Users are now able to link:9976a5[use SSH to create a connection] from their local machine to a remote Domino workspace, allowing them to browse and edit files (code, datasets, artifacts, etc.) in the remote workspace from their local IDE. Additionally, it allows users to connect their local desktop GUI tools to remote compute in Domino workspaces.

== Improvements

=== Governance enhancements
// DOM-62881
* *Governance Findings*: link:1ce615[Track unresolved risks and actions] linked to approval workflows.
// DOM-62884
* *Automated Checks for Governed Bundles*: Run link:f30aab[scripted or metric-based validations] on model bundles pre-approval.

=== Domino Apps Enhancements
// DOM-64917 / DOM-64916 / DOM-57222
Apps have been reimagined in Domino to include the following improvements:

* The new publish modal (including new vanity URL customization).
* The ability to create multiple apps in a single Domino Project.
* The new global apps list page and viewing running applications that use iFrames vs those without iFrames (which can use deep links with query parameters).
* The new app details page and switching between viewing different versions.
* (Soft-)Deleting an app.
* Managing user access to apps.
* Ability to publish apps from Git branches in Git-based projects.

=== Environment Enhancements
// DOM-64452
Domino Environments now offer the following:

* Package and home directory persistence across workspace sessions.
* link:748f25[Auto-rebuild of subscribed environments] when base environments update.

=== Audit Trail Enhancements
// DOM-62502
* Designate reason for change with electronic signature.
** Users are now able to apply an electronic signature to complete certain tasks. From within the Audit Trail, Admins have full visibility into all e-signature details, including the username, user authentication status, action taken, timestamp of action, and additional metadata entered at the time of e-signature.
// DOM-60932 / DOM-63966 / DOCS-4164
** Domino's new link:208cc3[Download as CSV feature] allows SysAdmin and GovernanceAdmin users to export Audit Trail data directly from the interface. This functionality helps support both internal and external compliance requirements.
+
When the *Download CSV* button is clicked, the generated CSV files will reflect the filters and table columns selected.

=== Domino Flows Enhancements
// DOM-60673 / DOCS-4333
link:3db42c[Publish Domino Flow Outputs as Reusable Datasets]: Automatically convert Flow outputs (e.g., CSVs, files) to versioned Datasets for downstream use.

=== Cost Center Dashboard
// DOM-63886 / DOCS-4844
The Cost Center Dashboard lets administrators easily view infrastructure costs across their Domino deployment. It provides quick insights into resource usage and cost trends over time. Learn more about link:d4b465[the Cost Center].

=== Simplified Data Plane Management (Public Preview)
// DOM-62650 / DOM-62651
Admins are now able to more easily link:ce0927[register] and link:130dd3[upgrade] Data Planes through the UI. Any Data Planes installed before 6.1 will need to be migrated to this new model. Your Domino Account team will work with you to assist with this migration.

=== Configurable Idle Timeout
// DOM-63184 / DOCS-3938
Admins now have the ability to link:71d6ad#idle-timeout[configure an idle timeout period] for automatic session management. When users remain inactive across all Domino tabs for a specified period, the system will now automatically log them out of their entire session for enhanced security. This intelligent timeout mechanism monitors actual user interaction rather than just session duration, providing better security without disrupting active work. Unlike the previous behavior where sessions remained active indefinitely or required manual logout, this new feature ensures automated session termination during extended periods of inactivity, even if the browser remains open or the system is locked.

=== Enhanced monitoring capabilities
// RE-2398 / DOCS-3765
Enhanced monitoring capabilities for Domino Admins with new metrics and dashboards that provide visibility into workload success rates and startup performance. These improvements enable proactive issue detection, faster debugging, and reduced reliance on external monitoring tools, ultimately improving administrator self-sufficiency and reducing resolution time for issues.

== Bug fixes

// DOM-64877 / DOCS-4293
* When `cache=True` is set in a Flow, it now correctly uses cached outputs when the underlying code changes or other flow configuration settings are changed, even when `use_latest=True` or `use_project_defaults_for_omitted=True` is set.

// DOM-66566 / DOCS-4794
* Kerberos keytab is now mounted on the workload service, enabling jobs (batch or schedule) to work as expected.

// DOM-66555 / DOCS-4789
* Fixed an issue that caused authentication problems for Git projects on Bitbucket when tokens contained special characters requiring escaping.

// DOM-62681 / DOCS-3860
* Addressed a Jupyter AI Lab 2.24 issue where using `/learn` with wildcards could fail. A newer version that addresses the issue is included.

// PLAT-9257 / DOCS-4816
* Keycloak chart has been enhanced to allow for idempotent installation after its deletion. Note that if this is done during an upgrade and the Keycloak version is prior to v22, some configuration may be lost. Reach out to mailto:support@dominodatalab.com[Domino Support] if this is the case.

// DOM-67356 / DOCS-4935
* GitHub Enterprise projects with internal visibility can be created.

// DOM-54417 / DOCS-4981
* Enhances reliability of MongoDB restart sequence by explicitly checking for port closure to avoid race conditions or startup conflicts.

// DOM-48583 / DOCS-4246
* The confirmation modal when transferring a Project was improved by removing redundant inputs.

// DOM-67996 / DOCS-4983
* The Environment's "Pluggable Workspace Tools" property is now properly validated: Valid YAMLs with the wrong structure/values will display a validation error toast and prevent the revision from being created.

// DOM-67287 / DOCS-5173
* Data Plane agent initialization issues that caused Workspace runs to halt have been resolved.

== Known issues

// DOM-52802 / DOM-52803 / DOCS-3308
* Flows is only supported in Domino Cloud, AWS, and Azure. A future update will add support for Google Cloud and on-premises installs.

// DOCS-3308 / DOM-55145 / DOM-59070 / DOM-59109 DOM-59110
* Some Flyte features are not yet available in Flows, such as https://docs.flyte.org/en/latest/user_guide/advanced_composition/dynamic_workflows.html[dynamic workflows^], https://docs.flyte.org/en/latest/user_guide/advanced_composition/eager_workflows.html[eager workflows^], and https://docs.flyte.org/en/latest/user_guide/advanced_composition/map_tasks.html[map_task^].

// DOM-62743 / DOCS-3907
// should be fixed in 6.2.0
* When annotating a Domino Flows sub workflow's outputs as a Flows Artifact, the artifacts will not be properly exposed in the Artifacts UI for that Flow.

// DOM-44695 / DOCS-2094
// this is a documented workaround, fix is labeled as won't do in DOM-44695
* In Azure Blob Store deployments, Projects with many files may fail to sync through the Domino CLI. To work around this issue, do not disable file locking when prompted by Domino.

// DOM-40929 / DOCS-1653
// won't do in DOM-40929
* You cannot view the latest raw file if you click *View Latest Raw File*.
In the navigation pane, go to *Files* and click a file to view its details.

// DOCS-4311 
// limitation of feature. will not be addressed
* If idle timeout is configured on a deployment, Domino may not recognize pages opened in spawned tabs (e.g., via `Ctrl+Click`) or outside of an iFrame as being active. Users may be unintentionally logged out despite activity in the spawned pages. 

// DOM-42591 / DOCS-1947
* When uploading a large file to the Azure blob store by link:262fef[syncing a Workspace], you may encounter a Java Out of Memory error from Azure if the file/blob already exists. To work around this issue, use the link:30b067[Domino CLI] to upload the file to the Project.

// DOCS-1783, DOM-41298
// sitting in backlog
* Model Monitoring data sources aren't validated.
If you enter an invalid bucket name and attempt to save, the entry will go through.
However, you won't be able to see metrics for that entry, because the name points to an invalid bucket.

// DOM-67754 / DOCS-4950
* When restarting an RStudio workspace with home directory persistence enabled, the user may see a notice stating that the previous session was abnormally terminated due to a crash. This notice is only due to how the new feature interacts with RStudio and does not result in any data loss.

// DOM-44833 / DOCS-2104
* Domino instances that make use of Azure Blob Storage may experience stalled Jobs within Projects with many large files.

// DOM-48271 / DOCS-2408
// workaround. ticket in backlog. affects 5.5.2 / 5.7.0 / 5.11.1
* If you attach a Git repository to a DFS Project that points to a tagged release, the tag won't be honored when building a Domino endpoint in that Project. The build log will show an error similar to the following, and the model will be built using the default branch of your Git repository instead of the tagged branch:
+
----
Jul 05 2023 14:36:27 -0500 #10 6.481 WARN [d.r.d.GitRepoUpdater] could not parse ref: v1.3.0 checking out default branch correlationId="iA2qWrYSLQ" thread="main"
----
+
To work around this issue, use the branch name when building Domino endpoints instead of the release tag.

// DOM-48718 / DOCS-2450
// marked as won't do - documented workaround
* If an admin resets a user's password, it invalidates all the user's authentication tokens, including tokens used for long-running tasks like Jobs, Workspaces, or Apps. The user must create a new password, log back into Domino, and restart all executions. This also applies to CLI authentication; the user must re-login to their Domino CLI.

// DOM-66386 / DOCS-4795
* Data storage page fails to load when the dataplane agent in the remote data plane is scaled down to zero.

// DOM-54417 / DOCS-4980
* Nucleus Dispatcher restart hangs during Quartz Scheduler startup following a MongoDB replica set scale-down. As a workaround, manually remove the unavailable replica from the replica set using the Mongo CLI command.

// DOM-68423 / DOCS-5012
* SSH is not yet supported for Workspaces running in remote data planes, only the local data plane is supported. The fix is planned for 6.1.1.

// DOM-66159 / DOCS-4850
* One misconfigured endpoint causes the mlflow-deployments pod to crashloop and AI gateway to fail with 502 errors.

// DOM-67945 / DOCS-4999
* During a 48-hour soak test, the `nginx-ingress-controller` pods exhibited a gradual increase in memory usage while handling heavy Domino Endpoint workloads. The cause is under investigation.

// DOM-68895 / DOCS-5065
* Cost Analyzer Diagnostics pod fails to launch if image pulls require quay secrets.

// DOM-69376 / DOCS-5132
* Intermittent authentication errors occur when running with multiple Keycloak replicas due to a known issue in Keycloak 26.1.4.  A workaround, which does not require an upgrade, is to run with a single replica, but an upgrade is recommended.

// DOM-69455 / DOCS-5136
* Flyte scp claim conflicts with Snowflake OAuth scp claim. A workaround, which does not require an upgrade, is to manually add a mapper to join both claims into a single multivalued claim.

// DOM-70031 / DOCS-5300
* Multi-app URL routing fails for projects with multiple apps, whereby only one app becomes reachable.

// DOM-69962 / DOCS-5306
* BitBucket Data Center URIs don't work with context path due to incorrect parsing logic.

// DOM-62680 / DOCS-5302
* Audit trail events are not available for Domino Flows.

// DOM-70100 / DOCS-5304
// solved in 6.1.2
* Windows users are unable to use SSH-enabled workspaces due to the incorrect file lock path that has a leading `.`.

== Validated frameworks

The following versions have been validated with Domino 6.1.0. Other versions might be compatible but are not guaranteed.

* Kubernetes - see the link:7b2cbe[Kubernetes compatibility chart]
* Ray - 2.43.0
* Spark - 3.5.4
* Dask - 2024.12.1
* MPI - 4.1.4

See also the link:5246aa[ddlctl and fleetcommand-agent Release Notes].

== Upgrade notes

// SUP-1041
* link:https://support.domino.ai/support/s/article/Rancher[Rancher deprecation notice]

// DOCS-3963
* OpenShift version compatibility remains at 1.30 (OpenShift 4.17).

// DOCS-3316
* GKE version compatibility remains at 1.32.

// DOCS-3961
* AKS version compatibility has been updated to 1.32.

// DOCS-3960
* EKS version compatibility remains at 1.32.

// DOM-66756
* Discontinued support for connecting to the Domino Pinecone data source through Pinecone Python client versions < 3.0.0, as Pinecone has sunset all legacy environments.

// RE-2658
* The `mixpanel.token` configuration in the `deploy.yaml` file is no longer required. When `mixpanel.enabled` is set to true, the system automatically uses the appropriate token value. Remove `mixpanel.token` from your `deploy.yaml` if present, as it may cause issues during upgrades.

// DOCS-4180
* Upgraded Keycloak to 26.1.4.

----- release_notes/6-1-1.txt -----
:page-permalink: d7bbf6
:page-version: 6.1
:page-iversion: 611
:page-title: Domino 6.1.1 (July 2025)
:page-order: 24

See also the link:5246aa[ddlctl and fleetcommand-agent Release Notes].

== Improvements

// DOM-67824 / DOCS-5059
=== SSH support for seamless local development
* Workspace SSH support for Domino deployments with custom TLS CA certificates.
* Verified Workspace SSH support for Cursor, Positron and PyCharm IDEs.

// DOM-68514
=== Configurable datetime precision for Data Sources
Users can now configure the resolution of datetime columns for relational Data Sources. See the link:e5761b[Data Source overridable configurations page] for more information.

// DOM-68947
=== Support for direct script execution as app entry point
App users are no longer limited to `.sh` files only and will now be able to deploy an app using any file type.

// DOM-66668
=== Flows output registration
Users can now use the link:6a57f5#programmatically-export[Flyte Python SDK] to programmatically transfer Flow artifacts to a Domino Dataset and create a snapshot, automating the registration step as part of their Flow.

// DOM-67096 / DOCS-5056 - commenting out as per comments in DOCS-5056
// === Support for Custom Directory Persistence in Workspace Persistence
// In addition to the `/home` directory, workspace persistence now supports persisting custom directories. Specify additional paths using the `DOMINO_WORKSPACE_PERSISTENCE_DIRS` environment variable (colon-separated). Persisting additional directories may increase startup time the first time they're saved. Logs tagged with `[directory-persistence]` provide more information.

== Bug fixes

// DOM-66386 / DOCS-4796
* Fixed the issue preventing the list of dataset storage from displaying in the Admin UI or the create dataset for users whenever a data plane was decommissioned without unregistering an underlying dataset storage.

// DOM-68895 / DOCS-5066
* Fixed the issue where Cost Analyzer Diagnostics pod failed to launch if image pulls required Quay secrets.

// DOM-68423 / DOCS-5013
* SSH access is now supported for Workspaces running in remote data planes.

// DOM-69376 / DOCS-5132
* Keycloak version 26.2.5 upgrade in Domino 6.1.1 resolves the issue where using multiple Keycloak replicas caused intermittent authentication errors.

// DOM-69455 / DOCS-5137
* Fixed the issue where Flyte and Snowflake OAuth required conflicting values for the `scp` claim in the user access token.

== Known issues

// DOM-68423 / DOCS-5013
* After upgrading to version 6.1.1 from 6.1.0, Workspaces running in the local data plane with SSH enabled must be restarted once to retain SSH access.

// DOM-52802 / DOM-52803 / DOCS-3308
* Flows is only supported in Domino Cloud, AWS, and Azure. A future update will add support for Google Cloud and on-premises installs.

// DOCS-3308 / DOM-55145 / DOM-59070 / DOM-59109 DOM-59110
* Some Flyte features are not yet available in Flows, such as https://docs.flyte.org/en/latest/user_guide/advanced_composition/dynamic_workflows.html[dynamic workflows^], https://docs.flyte.org/en/latest/user_guide/advanced_composition/eager_workflows.html[eager workflows^], and https://docs.flyte.org/en/latest/user_guide/advanced_composition/map_tasks.html[map_task^].

// DOM-62743 / DOCS-3907
// should be fixed in 6.2.0
* When annotating a Domino Flows sub workflow's outputs as a Flows Artifact, the artifacts will not be properly exposed in the Artifacts UI for that Flow.

// DOM-44695 / DOCS-2094
// this is a documented workaround, fix is labeled as won't do in DOM-44695
* In Azure Blob Store deployments, Projects with many files may fail to sync through the Domino CLI. To work around this issue, do not disable file locking when prompted by Domino.

// DOM-40929 / DOCS-1653
// won't do in DOM-40929
* You cannot view the latest raw file if you click *View Latest Raw File*.
In the navigation pane, go to *Files* and click a file to view its details.

// DOCS-4311
// limitation of feature. will not be addressed
* If idle timeout is configured on a deployment, Domino may not recognize pages opened in spawned tabs (e.g., via `Ctrl+Click`) or outside of an iFrame as being active. Users may be unintentionally logged out despite activity in the spawned pages.

// DOM-42591 / DOCS-1947
* When uploading a large file to the Azure blob store by link:262fef[syncing a Workspace], you may encounter a Java Out of Memory error from Azure if the file/blob already exists. To work around this issue, use the link:30b067[Domino CLI] to upload the file to the Project.

// DOCS-1783, DOM-41298
// sitting in backlog
* Model Monitoring data sources aren't validated.
If you enter an invalid bucket name and attempt to save, the entry will go through.
However, you won't be able to see metrics for that entry, because the name points to an invalid bucket.

// DOM-67754 / DOCS-4950
* When restarting an RStudio workspace with home directory persistence enabled, the user may see a notice stating that the previous session was abnormally terminated due to a crash. This notice is only due to how the new feature interacts with RStudio and does not result in any data loss.

// DOM-44833 / DOCS-2104
* Domino instances that make use of Azure Blob Storage may experience stalled Jobs within Projects with many large files.

// DOM-48271 / DOCS-2408
// workaround. ticket in backlog. affects 5.5.2 / 5.7.0 / 5.11.1
* If you attach a Git repository to a DFS Project that points to a tagged release, the tag won't be honored when building a Domino endpoint in that Project. The build log will show an error similar to the following, and the model will be built using the default branch of your Git repository instead of the tagged branch:
+
----
Jul 05 2023 14:36:27 -0500 #10 6.481 WARN [d.r.d.GitRepoUpdater] could not parse ref: v1.3.0 checking out default branch correlationId="iA2qWrYSLQ" thread="main"
----
+
To work around this issue, use the branch name when building Domino endpoints instead of the release tag.

// DOM-48718 / DOCS-2450
// marked as won't do - documented workaround
* If an admin resets a user's password, it invalidates all the user's authentication tokens, including tokens used for long-running tasks like Jobs, Workspaces, or Apps. The user must create a new password, log back into Domino, and restart all executions. This also applies to CLI authentication; the user must re-login to their Domino CLI.

// DOM-66386 / DOCS-4795
* Data storage page fails to load when the dataplane agent in the remote data plane is scaled down to zero.

// DOM-54417 / DOCS-4980
* Nucleus Dispatcher restart hangs during Quartz Scheduler startup following a MongoDB replica set scale-down. As a workaround, manually remove the unavailable replica from the replica set using the Mongo CLI command.

// DOM-66159 / DOCS-4850
* One misconfigured endpoint causes the mlflow-deployments pod to crashloop and AI gateway to fail with 502 errors.

// DOM-67945 / DOCS-4999
* During a 48-hour soak test, the `nginx-ingress-controller` pods exhibited a gradual increase in memory usage while handling heavy Domino Endpoint workloads. The cause is under investigation.

// DOM-70031 / DOCS-5300
* Multi-app URL routing fails for projects with multiple apps, whereby only one app becomes reachable.

// DOM-69962 / DOCS-5306
* BitBucket Data Center URIs don't work with context path due to incorrect parsing logic.

// DOM-70100 / DOCS-5304
* Windows users are unable to use SSH-enabled workspaces due to the incorrect file lock path that has a leading `.`.

// DOM-62680 / DOCS-5302
* Audit trail events are not available for Domino Flows.

// DOM-70885 / DOCS-5495
* App content may fail to load in air-gapped deployments, and users may receive a `500` error stating "Failed to record events: connection reset by peer, client" after a timeout period. This error occurs when apps attempt to connect to Mixpanel analytics service but cannot reach external endpoints due to network restrictions. Workaround: Set the Central Configuration flag `com.cerebro.domino.mixpanel.eventsEndpoint` to a value that will not resolve in DNS. For example, setting this flag to `disabled` will work.

// == Validated frameworks
// 
// The following versions have been validated with Domino 6.1.0. Other versions might be compatible but are not guaranteed.
// 
// * Kubernetes - see the link:7b2cbe[Kubernetes compatibility chart]
// * Ray - 2.43.0
// * Spark - 3.5.4
// * Dask - 2024.12.1
// * MPI - 4.1.4

== Upgrade notes

* OpenShift version compatibility remains at 1.30 (OpenShift 4.17).
* GKE version compatibility remains at 1.32.
* AKS version compatibility remains at 1.32.
* EKS version compatibility remains at 1.32.
* Keycloak version upgraded to 26.2.5.

----- release_notes/6-1-2.txt -----
:page-permalink: 4b59dd
:page-version: 6.1
:page-iversion: 611
:page-title: Domino 6.1.2 (September 2025)
:page-order: 24

See also the link:5246aa[ddlctl and fleetcommand-agent Release Notes].

== Improvements

This patch contains security updates and critical bug fixes.

== Bug fixes

// DOM-70031 / DOCS-5301
* Multi-app URL routing works for projects with multiple apps, which means multiple apps under the same project are accessible at the same time.

// DOM-69962 / DOCS-5307
* BitBucket Data Center URIs with context paths are parsed correctly.

// DOM-70100 / DOCS-5305
* Windows users are able to connect to the SSH-enabled workspaces after successfully installing the Dom client on Windows. 

// DOM-62680 / DOCS-5303
* Audit trail for Domino Flows is now enabled.

== Known issues

// DOM-68423 / DOCS-5013
* After upgrading to version 6.1.1 from 6.1.0, Workspaces running in the local data plane with SSH enabled must be restarted once to retain SSH access.

// DOM-52802 / DOM-52803 / DOCS-3308
* Flows is only supported in Domino Cloud, AWS, and Azure. A future update will add support for Google Cloud and on-premises installs.

// DOCS-3308 / DOM-55145 / DOM-59070 / DOM-59109 DOM-59110
* Some Flyte features are not yet available in Flows, such as https://docs.flyte.org/en/latest/user_guide/advanced_composition/dynamic_workflows.html[dynamic workflows^], https://docs.flyte.org/en/latest/user_guide/advanced_composition/eager_workflows.html[eager workflows^], and https://docs.flyte.org/en/latest/user_guide/advanced_composition/map_tasks.html[map_task^].

// DOM-62743 / DOCS-3907
// should be fixed in 6.2.0
* When annotating a Domino Flows sub workflow's outputs as a Flows Artifact, the artifacts will not be properly exposed in the Artifacts UI for that Flow.

// DOM-44695 / DOCS-2094
// this is a documented workaround, fix is labeled as won't do in DOM-44695
* In Azure Blob Store deployments, Projects with many files may fail to sync through the Domino CLI. To work around this issue, do not disable file locking when prompted by Domino.

// DOM-40929 / DOCS-1653
// won't do in DOM-40929
* You cannot view the latest raw file if you click *View Latest Raw File*.
In the navigation pane, go to *Files* and click a file to view its details.

// DOCS-4311
// limitation of feature. will not be addressed
* If idle timeout is configured on a deployment, Domino may not recognize pages opened in spawned tabs (e.g., via `Ctrl+Click`) or outside of an iFrame as being active. Users may be unintentionally logged out despite activity in the spawned pages.

// DOM-42591 / DOCS-1947
* When uploading a large file to the Azure blob store by link:262fef[syncing a Workspace], you may encounter a Java Out of Memory error from Azure if the file/blob already exists. To work around this issue, use the link:30b067[Domino CLI] to upload the file to the Project.

// DOCS-1783, DOM-41298
// sitting in backlog
* Model Monitoring data sources aren't validated.
If you enter an invalid bucket name and attempt to save, the entry will go through.
However, you won't be able to see metrics for that entry, because the name points to an invalid bucket.

// DOM-67754 / DOCS-4950
* When restarting an RStudio workspace with home directory persistence enabled, the user may see a notice stating that the previous session was abnormally terminated due to a crash. This notice is only due to how the new feature interacts with RStudio and does not result in any data loss.

// DOM-44833 / DOCS-2104
* Domino instances that make use of Azure Blob Storage may experience stalled Jobs within Projects with many large files.

// DOM-48271 / DOCS-2408
// workaround. ticket in backlog. affects 5.5.2 / 5.7.0 / 5.11.1
* If you attach a Git repository to a DFS Project that points to a tagged release, the tag won't be honored when building a Domino endpoint in that Project. The build log will show an error similar to the following, and the model will be built using the default branch of your Git repository instead of the tagged branch:
+
----
Jul 05 2023 14:36:27 -0500 #10 6.481 WARN [d.r.d.GitRepoUpdater] could not parse ref: v1.3.0 checking out default branch correlationId="iA2qWrYSLQ" thread="main"
----
+
To work around this issue, use the branch name when building Domino endpoints instead of the release tag.

// DOM-48718 / DOCS-2450
// marked as won't do - documented workaround
* If an admin resets a user's password, it invalidates all the user's authentication tokens, including tokens used for long-running tasks like Jobs, Workspaces, or Apps. The user must create a new password, log back into Domino, and restart all executions. This also applies to CLI authentication; the user must re-login to their Domino CLI.

// DOM-66386 / DOCS-4795
* Data storage page fails to load when the dataplane agent in the remote data plane is scaled down to zero.

// DOM-54417 / DOCS-4980
* Nucleus Dispatcher restart hangs during Quartz Scheduler startup following a MongoDB replica set scale-down. As a workaround, manually remove the unavailable replica from the replica set using the Mongo CLI command.

// DOM-66159 / DOCS-4850
* One misconfigured endpoint causes the mlflow-deployments pod to crashloop and AI gateway to fail with 502 errors.

// DOM-67945 / DOCS-4999
* During a 48-hour soak test, the `nginx-ingress-controller` pods exhibited a gradual increase in memory usage while handling heavy Domino Endpoint workloads. The cause is under investigation.

// DOM-70993 / DOCS-5503
* In certain cases, when creating multiple Service Accounts, the tokens are generated with mismatched information, rendering them invalid.

// DOM-70885 / DOCS-5495
* App content may fail to load in air-gapped deployments, and users may receive a `500` error stating "Failed to record events: connection reset by peer, client" after a timeout period. This error occurs when apps attempt to connect to Mixpanel analytics service but cannot reach external endpoints due to network restrictions. Workaround: Set the Central Configuration flag `com.cerebro.domino.mixpanel.eventsEndpoint` to a value that will not resolve in DNS. For example, setting this flag to `disabled` will work.

// == Validated frameworks
//
// The following versions have been validated with Domino 6.1.0. Other versions might be compatible but are not guaranteed.
//
// * Kubernetes - see the link:7b2cbe[Kubernetes compatibility chart]
// * Ray - 2.43.0
// * Spark - 3.5.4
// * Dask - 2024.12.1
// * MPI - 4.1.4

== Upgrade notes

* OpenShift version compatibility updated to 1.31 (OpenShift 4.18).
* GKE version compatibility remains at 1.32.
* AKS version compatibility updated to 1.33.
* EKS version compatibility updated to 1.33.
* Keycloak version upgraded to 26.2.5.

----- user_guide/access-external-data-directly.txt -----
:page-version: 6.1
:page-permalink: 3d3103
:page-title: Access external data directly
:page-order: 150

The best way to connect to an external data store from Domino is to use a Domino connector to link:fbb41f[create a Domino data source].
However, you also have the option to connect to an external data store directly from a Domino application.
Because Domino can run any user-produced code, you can always execute code that connects to any data source you wish.
As long as the Domino executor machines have network connectivity to your data, Domino will run the code.

image::/images/5.7/direct-access.png[Direct access to external data, width=400, role=noshadow]

Common use cases for directly accessing external data include pulling data from APIs, accessing a database without going through a Domino connector, or accessing a network-based file store without going through an External Data Volume (EDV).

The general procedure is as follows:

. Install the package for the data store in your environment or in your workspace session.
+
See link:bfa148[Add packages to environments].
// does this mean customers need to create a custom environment for any data store that does not have a connector?

. Set up the credentials for the data store.
+
Be sure to store your credentials securely and use environment variables to access them in your code.
See link:d8dde6[Store Project credentials].

. Fetch data from the data store.
+
The method for fetching data depends on the data store.


The following data sources are not supported by a Domino Data Source connector, but you can still connect directly to these data sources. Domino does not provide support for any issues related to direct connections.

* link:452cff[DataRobot]
* link:f200d6[Impala]

----- user_guide/ai-gateway.txt -----
:page-version: 6.1
:page-permalink: c9ac47
:page-title: AI Gateway
:page-order: 260

Use Domino's AI Gateway to access multiple external Large Language Model (LLM) providers securely within your Workspaces and Runs. The AI Gateway serves as a bridge between Domino and external LLM providers such as OpenAI or AWS Bedrock. By routing requests through the AI Gateway, Domino ensures that all interactions with external LLMs are secure, monitored, and compliant with organizational policies. This guide outlines how to use the AI Gateway to connect to LLM services while adhering to security and auditability best practices.

The AI Gateway provides:

* *Security*: Ensures that all data sent to and received from LLM providers is encrypted and secure.
* *Auditability*: Keeps comprehensive logs of all LLM interactions, which is crucial for compliance and monitoring.
* *Ease of access*: Provides a centralized point of access to multiple LLM providers, simplifying the user experience.
* *Control*: Allows administrators to manage and restrict access to LLM providers based on user roles and project needs.

The AI Gateway is built on top of an link:https://mlflow.org/docs/latest/llms/deployments/index.html#supported-provider-models[MLflow Deployments Server^] for easy integration with existing MLflow projects.

== Get started using the AI Gateway

You can access AI Gateway endpoints from the Domino platform and also from within a Domino Workspace.

=== Access AI Gateway endpoints in Domino

To view AI Gateway endpoints in Domino:

. In the top navigation pane, go to *Develop* > *Gateway LLMs*.
. Search for endpoints using basic information like the name, type, provider, or model.


=== Access AI Gateway endpoints from a Workspace

To see the AI Gateway endpoints available to use in your Workspace:

* Create a new Workspace and open the *Gateway LLMs* side panel.
+
image::/images/6.0/gateway-llms.png[alt="See available endpoints in workspace side panel", width=400, role=noshadow]

=== Query an endpoint

To query an endpoint:

. Click on the *Copy Code* icon.
. Paste the code in your Workspace and adjust the query to fit your needs.

Alternatively, you can use the link:https://mlflow.org/docs/latest/python_api/mlflow.deployments.html#mlflow.deployments.MlflowDeploymentClient[MLflow Deployment Client API^] to create your own query.

NOTE: When using the MLflow Deployment Client, Domino supports `+predict()+`, `+get_endpoint()+`, and `+list_endpoints()+` API endpoints.


== Configure AI Gateway endpoints

AI Gateway link:https://mlflow.org/docs/latest/llms/deployments/index.html#endpoints[endpoints^] are central to AI Gateway. Each endpoint acts as a proxy endpoint for the user, forwarding requests to a specific model defined by the endpoint. Endpoints are a managed way to securely connect to model providers.

image::/images/5.10/ai-gateway/ai-gateway-diagram.jpg[alt='AI Gateway is a hub for LLM providers, key vault, logging, and Domino executions', width=600, role=noshadow]

To create and manage AI Gateway endpoints in Domino, go to *Endpoints* > *Gateway LLMs* and configure the endpoint details and permissions.  Alternatively, you can use the link:8c929e[Domino Platform API]. You can update or delete endpoints at any time.


See MLflow's Deployment Server documentation for more information on the list of supported LLM providers and link:https://mlflow.org/docs/latest/llms/deployments/index.html#provider-specific-configuration-parameters[provider-specific configuration parameters^].

Once an endpoint is created, authorized users can query the endpoint in any Workspace or Run using the standard link:https://mlflow.org/docs/latest/llms/deployments/index.html#client-api[MLflow Deployment Client API^]. For more information, see the documentation to link:c9ac47[Use Gateway Domino endpoints].

=== Endpoint permission management

You can configure AI Gateway endpoints to be accessible to everyone or a specific set of users and/or organizations.
These permissions can be configured in the second step of the creation modal and can be changed at any point after.

=== Secure credential storage

When creating an endpoint, you will most likely need to pass a model-specific API key (such as OpenAI's `+openai_api_key+`) or secret access key (such as AWS Bedrock's `+aws_secret_access_key+`). When you create an endpoint, all of these keys are automatically stored securely in Domino's central vault service and are never exposed to users when they interact with AI Gateway endpoints.

The secure credential store helps prevent API key leaks and provides a way to centrally manage API keys, rather than simply giving plain text keys to users.


== AI Gateway audit trail

Domino logs all AI Gateway endpoint activity to link:785186[Domino's central audit system].
To see AI Gateway endpoint activity, go to *Endpoints* > *Gateway Domino endpoints* and click on the *Download logs* button. This will download a `txt` or `json` file with all the AI Gateway endpoint activity from the past six (6) months.
You can further customize the fetched audit events by using the link:8c929e[Domino Platform API].

== Next steps

Learn how to link:cce362[create AI Gateway endpoints as a Domino admin].

----- user_guide/apps/app-configuration-settings.txt -----
:page-permalink: cfc79e
:page-version: 6.1
:page-title: Configuration settings for Apps
:page-sidebar: Configuration settings for Apps
:page-order: 110

The behavior of Domino Apps can be adjusted by enabling specific feature flags or tuning the settings of configuration records. These controls govern how apps handle identity propagation, dataset access, and execution limits across projects.

To support recent app enhancements, Domino is enabling the `SecureIdentityPropagationToAppsEnabled` feature flag. This change may affect older apps due to updates in the Domino host path, and some users may need to link:https://support.domino.ai/support/s/article/SecureIdentityPropagationToAppsEnabled-feature-flag-impacts-app-behavior[update app configurations] to restore expected behavior.

The table below summarizes key settings that influence app behavior:

[cols="3,2,5", options="header"]
|===
| Setting | Type | Purpose

| `SecureIdentityPropagationToAppsEnabled`
| Feature Flag
| Enables JWT tokens for identifying the requesting user.

| `dataset.mountingEnabledInApps`
| Configuration record
| Controls whether Datasets are mounted or accessed via API only.

| `apps.MaxAppsPerProject`
| Configuration record
| Limits how many apps can exist within a single project.

| `app.MaxActiveAppRunsPerProject`
| Configuration record
| Caps how many app sessions can run concurrently per project.
|===



== Next steps

* link:e3ec27[Apps in Domino] gives an overview of how apps work within the Domino ecosystem.
* link:cd0095[Create and Publish an App] has instructions on creating and publishing your Apps, customizing the App's URL, and sharing Apps with authorized users.
* Learn more about link:5bef19[how Apps in Domino run] and what identity and permissions are used.

----- user_guide/apps/app-execution-context.txt -----
:page-permalink: 5bef19
:page-version: 6.1
:page-title: Execution context and authentication
:page-sidebar: App execution and authentication
:page-order: 50

Each App in Domino runs with the identity and permissions of the user who starts it. This identity determines what the app can access during execution, including:

* *Datasets*: Mounted with read/write access if the starter has permission.
* *Data Sources*: Accessible based on the starter’s credentials.
* *Project Files*: Fully available within the App container.

This model ensures that apps operate within the same security context as the user who launched them. It is particularly effective for apps designed as trusted interfaces to governed resources, such as model exploration dashboards backed by pre-approved data inputs.

== Identify the requesting user

In some cases, you may want to control behavior based on who is using the App, not who started it. For example, you might want to show different data or limit functionality based on the viewer’s identity.

Domino supports this through two identity propagation mechanisms:

=== HTTP header (`domino-username`)

Domino securely injects the username of the current requester into every proxied HTTP request using the `domino-username` header.

* Value is the logged-in Domino username (e.g., `jane.smith`).
* Anonymous users will have the value `Anonymous`.
* Supported by default in *Dash* and *Flask*.
* *Shiny* requires *Shiny Server Pro* to receive these headers.

[TIP]
====
This is the simplest way to get per-user awareness in your app logic.
====

=== JWT token (Optional)

If the `SecureIdentityPropagationToAppsEnabled` feature flag is enabled, Domino also passes a signed JSON Web Token (JWT) in the `Authorization` header.

The token contains:

* Username
* User ID
* Email

Your App can verify and decode this token to enforce fine-grained access controls or integrate with other identity systems.

==== Access the JWT token

You can drop this into most web frameworks (Dash, Flask, etc.) as long as `request.headers` behaves like a dictionary.

[source,python]
----
# Assume 'request' is the incoming HTTP request object

auth_header = request.headers.get("Authorization")

if auth_header:
    token = auth_header.replace("Bearer ", "")
    print("JWT token:", token)
else:
    print("No Authorization header found.")
----

Together, these options let you build Apps that are both secure and flexible, whether access is tied to the App starter or the individual user making the request.

=== Dataset access in Apps

If your App needs to control access to Datasets based on the requesting user’s identity, follow this pattern:

. From the Data menu in the App settings, choose *Don't mount Datasets to App filesystems*.
+
This disables direct filesystem access to Datasets and ensures access is routed through Domino’s permission-aware API.

. In your App code, use:
* The `domino-username` header, or
* The decoded JWT token

. Use the Dataset API to access specific dataset files, enforcing access logic based on the App starter’s identity.

This setup enables you to serve a single App to multiple users while tailoring data access according to Domino’s role-based permissions.

== Next steps

* link:e3ec27[Apps in Domino] gives an overview of how apps work within the Domino ecosystem.
* link:b99d5d[Persist data] using Datasets or external storage to make your Apps more dynamic and interactive.
* link:cd0095[Create and Publish an App] has instructions on creating and publishing your Apps, customizing the App's URL, and sharing Apps with authorized users.

----- user_guide/apps/app-log-version-control.txt -----
:page-version: 6.1
:page-permalink: bb4b24
:page-title: App logs and versioning
:page-order: 80

Domino provides detailed logging and version tracking for apps to support debugging, improve reliability, and provide operational transparency. This guide outlines the available logs and metadata, how to access and interpret them, and how Domino handles versioning for published apps.

== App logs

Domino captures a range of logs during app execution to help you monitor performance, troubleshoot issues, and understand system behavior. These logs offer visibility into both your app code and the underlying platform infrastructure.

=== Available logs

When viewing or troubleshooting an app, the following types of logs may be available:

* *App stdout/stderr*: Captures standard output, error messages, and runtime print statements.
* *Kubernetes events*: Includes pod scheduling, image pulls, and container state transitions.
* *Execution logs*: Generated by Domino's execution engine for each app run.
* *System logs*: Captures platform-level components such as executor, logjam, and frontend services.
* *NGINX & nucleus logs*: Provide visibility into routing, proxy behavior, and request handling.
* *Performance metrics (optional)*: CSV-format data that tracks system-level performance like CPU and memory.

=== Accessing logs

You can access logs directly from the App UI or download them for deeper inspection. Domino also provides a *Support Bundle* that consolidates key logs and metadata for comprehensive troubleshooting.

=== Support bundle contents

The support bundle is a downloadable archive that includes comprehensive logs and metadata:

* `execution.log`, `executor.log` – App subprocess execution and engine output.
* `events.json`, `events.csv` – Timeline of internal execution events.
* `kubernetes-events.json` – Cluster and pod lifecycle messages.
* `logjam-*.log` – Runtime traces and orchestration activity.
* `nginx.log`, `nucleus-*.log` – Proxy and routing behavior.
* `execution.yaml`, `manifest.csv` – Runtime configuration and file manifest.
* `performance-metrics.csv` *(optional)* – System resource utilization metrics.
* `project.json` – Project-level settings and environment metadata.
* `run.json` – Detailed run metadata, diagnostics, and resource info.
* `run-history.json` – Sequence of lifecycle status changes.
* `saga.json` – Infrastructure orchestration details and container startup data.

These files provide full insight into runtime behavior, app infrastructure, and system performance.

== App version control

Domino automatically tracks versions of your app each time it’s published. This version history helps you audit changes, review configuration details, and trace issues across deployments.

=== Automatic versioning

Every time you publish an app, Domino captures a version snapshot. Each version includes metadata such as:

* Linked environment
* File references
* Code paths

=== Usage

Domino Apps maintain a version history that you can view directly from the App UI. This lets you browse and inspect previous versions for auditing, debugging, or review purposes—especially helpful when tracking changes over time or reviewing how a particular configuration behaved in the past.

[NOTE]
====
While version history is available for reference, Domino does not currently support rollback or diff comparisons between versions.
====

=== Best practices

To ensure your Apps run reliably and are easy to maintain, follow these best practices during development, deployment, and monitoring. These tips can help you catch issues early, maintain reproducibility, and support smooth collaboration across teams.

* *Use support bundles* to investigate failed or abnormal app behavior.
* *Validate environments* before publishing to ensure consistency and reproducibility.
* *Audit version metadata* to track deployments and operational history.
* *Review logs regularly*, especially after environment updates or dependency changes.

=== Troubleshooting tips

If your App isn’t behaving as expected, these quick diagnostics can help you identify and resolve common issues:

* *No output in logs?* Check `executor.log` and `logjam-*.log` for errors.
* *Scheduling issues?* Inspect `kubernetes-events.json` for pod failures or capacity problems.
* *Unexpected behavior after publishing?* Compare version metadata and environment differences.

== Next steps

* link:e3ec27[Apps in Domino] gives an overview of how apps work within the Domino ecosystem.
* link:cd0095[Create and Publish an App] has instructions on creating and publishing your Apps, customizing the App's URL, and sharing Apps with authorized users.
* Learn more about link:5bef19[how Apps in Domino run] and what identity and permissions are used.


----- user_guide/apps/app-scalability-performance.txt -----
:page-version: 6.1
:page-title: App scalability and performance
:page-permalink: a29497
:page-order: 120

//TR note - most of the functionality described here is covered by TRs on other pages

== Domino Applications

The following guidelines apply to all Domino Apps, regardless of framework:

* Apps run until you explicitly stop them. If your app isn't used anymore, stop it to minimize your compute spend.
* Unlike a job or a workspace, Domino Apps never commit (or sync) file changes from the local disk to the Domino project's file store.
To persist data, your app's logic must write it to an external data source.
* The performance of your app depends on the design of the application.

Launching an link:user_guide/71635d[app in Domino] works the same as any other Domino run.
Domino assigns hosting of your app to an executor machine in the
link:908bd9[hardware tier]
your App is configured to use.
That executor then retrieves the default link:user_guide/f51038[Domino environment] configured for your project, and creates a container based on that image.
Domino then loads your project files onto the machine and executes the app file that you have selected, at which point your application will be running in its container.

Depending on which hardware tier you select, the container running your application may share a host machine with other containers or run on a dedicated host.
Your selection of hardware tier allows you to specify available memory and CPU.
However, Domino Apps do not automatically scale horizontally to multiple hosts.
Your App can scale vertically to use all available resources on the executor host machine by correctly configuring the underlying application.

The following sections describe configuring web applications to optimize scalability and performance for several popular frameworks.

== Flask and Dash (Python)

By default, Flask and Dash will run single-threaded on a single process.
The authors of Flask do not recommend this configuration if you are going to serve more than 10 users concurrently, or for any externally consumed applications.
The
https://flask.palletsprojects.com/en/3.0.x/deploying/[Flask
documentation] provides many ways to serve the application in a more scalable way.

[[tr1]]
// host Flask App through gunicorn

For example, you can serve a Flask application through
https://gunicorn.org/[gunicorn^].
To do this in Domino, change the project's app file.
See https://docs.gunicorn.org/en/stable/run.html#commands[Gunicorn commands] for details.
You can change:

----
export LC_ALL=C.UTF-8
export LANG=C.UTF-8
export FLASK_APP=app-flask.py
export FLASK_DEBUG=1
python -m flask run --host=0.0.0.0 --port=8888
----

to

----
gunicorn -w 4 -b 0.0.0.0:8888 app-flask:app
----

This will start serving the Flask application on four processes.

The performance and scalability of your App will depend on the compute demands of your application, and the compute resources available on the host machine.
If there is a command in your application that will use 100MB RAM and 20% of a standard VM CPU, then an executor host machine with 1 core and 1 GB RAM could handle 5 concurrent users running that command without suffering reduced performance.
A 6th user attempting to run the command would cause the App's performance to suffer.
There would be RAM available, but not enough CPU cycles.

== Shiny \(R)

Like Python Apps, your Shiny Apps' performance will depend on design of the underlying application.
While multiple users can view Shiny applications in independent sessions, R is a single process language.
This means that multiple users can view and interact with the App in their own isolated session, but only one can do any processing at a time regardless of the memory or CPU of the machine.

Shiny Apps typically cannot scale to more than a handful of concurrent users.

See https://rstudio.com/resources/rstudioconf-2018/make-shiny-fast-by-doing-as-little-work-as-possible/[Make
Shiny fast by doing as little work as possible].

----- user_guide/apps/app-security.txt -----
:page-version: 6.1
:page-permalink: cb9195
:page-title: App security and identity
:page-order: 70

Secure your Domino app with access control and permissions.

== Access controls and permissions

Manage access with the *Permission* tab:

* *Anyone, including anonymous users* - In this mode, anyone with the URL can access your app, even if they don't have a Domino account.
* *Anyone with an account* - Anyone logged in to Domino with an account can access the app.
* *Invited users only* - Only users you explicitly invite can access the app.
* *Invited users (others can request access)* - Only users you explicitly invite can access the app, but users can request access (that you can approve).


[[access-identity]]
== Access the identities of app users

You might want to create apps that need to know who uses them.
For example, this is useful if you want to load specific default values or preferences, or if you want to access different data based on who views your app.

// Confirm this is secure, i.e., not spoofable.

To enable this, Domino passes the username of a user who accesses your Domino app in an HTTP header named `domino-username`.

If your app framework gives you access to the HTTP headers of the active request, retrieve the `domino-username` for use by your app code. If you allow users who are not logged in to Domino to view your apps, the value of the `domino-username` header is `Anonymous`.

Additionally, if the `SecureIdentityPropagationToAppsEnabled` link:6469bf[Feature Flag] is turned on, Domino passes a JWT authorization token that can be used to identify the requesting user in the `Authorization` HTTP header. This JWT token's integrity can be verified and it can then be decoded to obtain the user's Domino username, ID, and email.

NOTE: These identity headers are only available when you use app frameworks that support _proxied HTTP headers_. These headers are supported by Flask and Dash by default, but Shiny requires that you use link:https://docs.rstudio.com/shiny-server/#proxied-headers[Server Pro^].

=== Access username example

Create the files for this http://flask.pocoo.org/[Flask^] example that gets the Domino username of an app viewer in your project:

[source,shell]
----
#!/usr/bin/env bash
export LC_ALL=C.UTF-8
export LANG=C.UTF-8
export FLASK_APP=app.py
export FLASK_DEBUG=1
python -m flask run --host=0.0.0.0 --port=8888
----

Here is a simple `app.py` file that renders a template named `index.html`.
This app imports `request` from `flask`, which gives you access to the headers of the active HTTP request.

[source,python]
----
import flask
from flask import request, redirect, url_for

class ReverseProxied(object):
  def __init__(self, app):
      self.app = app
  def __call__(self, environ, start_response):
      script_name = environ.get('HTTP_X_SCRIPT_NAME', '')
      if script_name:
          environ['SCRIPT_NAME'] = script_name
          path_info = environ['PATH_INFO']
          if path_info.startswith(script_name):
              environ['PATH_INFO'] = path_info[len(script_name):]
      return self.app(environ, start_response)

app = flask.Flask(__name__)
app.wsgi_app = ReverseProxied(app.wsgi_app)

# Homepage which uses a template file
@app.route('/')
def index_page():
  return flask.render_template("index.html")
----

There is a template file at `templates/index.html` that fetches the
`domino-username` header from the `requests` object and renders it.

[source,html]
----
<!DOCTYPE html>
<html>
  <body>
    <h1>Your username is {{ request.headers.get("domino-username") }}</h1>
  </body>
</html>
----

If you host this app in Domino and open it, you'll see something like this where the username shown matches the username of the app user.

== App secure identity propagation

When the `SecureIdentityPropagationToAppsEnabled` link:6469bf[Feature Flag] is turned on, Domino passes a JWT authorization token that can be used to identify the requesting user in the `Authorization` HTTP header. This JWT token's integrity can be verified and it can then be decoded to obtain the user's username, email and Domino user ID.

[NOTE]
====
When `SecureIdentityPropagationToAppsEnabled` is enabled apps are hosted at `https://<your-domino-domain>/apps/<app-id>`. This might require configuring your application server's base path for your application. For convenience, apps can read the base path from the `DOMINO_RUN_HOST_PATH` environment variable.
====

=== Access the user's identity example

First extract the token from the `Authorization` header of the incoming request.

[source,python]
----
def extract_token(self, headers):
  auth_header = headers.get('Authorization', '')
  return auth_header[7:] if auth_header.startswith('Bearer ') else None
----

You can then verify the integrity of the token by retrieving the Domino installation's public certificates and verifying the signature of the token. The token can then be decoded to obtain the user's identity information.
Here is an example of how to retrieve the certificates and use them to verify and decode the token in the app.

[source,python]
----
from cryptography import x509
from cryptography.hazmat.backends import default_backend
from cryptography.hazmat.primitives import serialization
from http.server import HTTPServer, BaseHTTPRequestHandler
from jwt import decode, get_unverified_header
from jwt.exceptions import InvalidTokenError
from pprint import pformat
import base64
import html
import json
import jwt
import os
import requests
import socket
import sys

def verify_and_decode_jwt_token(self, token):
  # Public key URL
  keycloak_domain = "http://keycloak-http.domino-platform" # Use the external domain if running the app on a remote data plane.
  jwks_url = f"{keycloak_domain}/auth/realms/DominoRealm/protocol/openid-connect/certs"

  # Retrieve JWKS (JSON Web Key Set) from URL
  jwks = requests.get(jwks_url).text
  jwks_dict = json.loads(jwks)

  # Get the key ID from the token header
  unverified_header = get_unverified_header(token)
  kid = unverified_header.get('kid')

  if not kid:
    raise ValueError("No 'kid' found in token header")

  # Find the corresponding public key
  public_key = None
  for key in jwks_dict['keys']:
    if key['kid'] == kid:
      x5c = key['x5c'][0]
      cert_bytes = x5c.encode('ascii')
      cert_der = base64.b64decode(cert_bytes)
      cert = x509.load_der_x509_certificate(cert_der, default_backend())
      public_key = cert.public_key()
      break

  if not public_key:
    raise ValueError(f"No public key found for kid: {kid}")

  # Convert the public key to PEM format
  pem = public_key.public_bytes(
    encoding=serialization.Encoding.PEM,
    format=serialization.PublicFormat.SubjectPublicKeyInfo
  )

  try:
    # Verify the token
    payload = jwt.decode(
      token,
      pem,
      algorithms=['RS256'],
      audience="apps",
      options={"verify_signature": True}
    )
    return payload
  except jwt.InvalidSignatureError:
    raise ValueError("Invalid signature")
  except jwt.ExpiredSignatureError:
    raise ValueError("Token has expired")
  except jwt.InvalidTokenError as e:
    raise ValueError(f"Invalid token: {str(e)}")
----

After decoding the JWT token you will obtain a JSON object containing the user's identity information in the following format:

[source,javascript]
----
{
  "exp": 1726076562,
  "iat": 1726076262,
  "auth_time": 1726005915,
  "jti": "8d3febf7-068b-4240-bb3c-c361406119bf",
  "iss": "https://dominoDomain/auth/realms/DominoRealm",
  "aud": [
    "apps",
    "app-user-token-exchange-client"
  ],
  "sub": "66d9fc7a538b60bef7c02c9c",
  "typ": "Bearer",
  "azp": "domino-play",
  "session_state": "2125b0d6-35aa-49b6-a63a-c274bf07a3f9",
  "scope": "openid email profile",
  "sid": "2125b0d6-35aa-49b6-a63a-c274bf07a3f9",
  "email_verified": true,
  "idp_id": "74e837a7-61ec-4777-8862-772037aec72f",
  "name": "givenName familyName",
  "preferred_username": "userName",
  "given_name": "givenName",
  "family_name": "familyName",
  "email": "userName@domain.com"
}
----

=== Configuring application base path

The base path for the application can be read from the `DOMINO_RUN_HOST_PATH` environment variable and used to set the base path for your application:

[source,python]
----
import os

from flask import (
    Flask,
)


class ReverseProxied(object):
    def __init__(
        self,
        app,
    ):
        self.app = app

    def __call__(
        self,
        environ,
        start_response,
    ):
        script_name = os.environ.get('DOMINO_RUN_HOST_PATH', '')
        if script_name:
            environ["SCRIPT_NAME"] = script_name
            path_info = environ["PATH_INFO"]
            if path_info.startswith(script_name):
                environ["PATH_INFO"] = path_info[len(script_name) :]
        # Setting wsgi.url_scheme from Headers set by proxy before app
        scheme = environ.get(
            "HTTP_X_SCHEME",
            "https",
        )
        if scheme:
            environ["wsgi.url_scheme"] = scheme
        return self.app(
            environ,
            start_response,
        )


app = Flask(__name__)

from app import (
    views,
)

app.wsgi_app = ReverseProxied(app.wsgi_app)
----

== iFrame security

If your Domino deployment exercises iFrame security or requires a content security policy for web apps and your app behaves in unexpected ways, see link:aec000[Whitelist resources].

By default, Apps are limited to load only within an iFrame. Attempting to access an App URL directly will result in a `400 Bad Request` error for users. To control this behavior see the `ShortLived.iFrameRequired` link:6469bf[Feature Flag].


----- user_guide/apps/apps-overview.txt -----
:page-permalink: e3ec27
:page-version: 6.1
:page-title: Apps in Domino
:page-sidebar: Apps in Domino
:page-order: 10

Domino Apps let you share interactive web tools built with frameworks like Streamlit, Dash, Shiny, and Flask. Apps run in containerized environments with automatic routing, authentication, and resource management.

Apps in Domino are built for flexibility, letting you run custom web-based tools and expose them to others while Domino manages infrastructure, permissions, and routing. Each App follows a lifecycle, from setup and execution to its interaction with datasets, environments, and user identity.

image::/images/6.1/user-guide/apps-list-snapshot-view.png[alt="App Detail", width=800, role="noshadow"]

This guide covers how Apps are defined, executed, and secured, and includes lifecycle details, access controls, and examples.

== App structure in Domino

Domino Apps are defined at the project level, and each project can contain multiple apps. Every app uses the project’s files but can be configured with its own environment, hardware tier, and code branch. 

This section explains how each app is structured and how its runtime settings are kept separate from other project workflows and jobs.

Key elements of an App’s definition include:

* *Launch File*: The core entry point. It must start a process that runs a web server on `0.0.0.0`, exposed on port `8888`. Shell scripts such as `app.sh` as well as Python or R files are commonly used as launch files.
* *Compute Environment*: Defines the runtime environment, including installed packages, libraries, and system dependencies.
* *Hardware Tier*: Specifies the resources allocated to run the App. Resources include CPU, memory, GPU, etc.
* *Code Branch*: Allows the App to run from a specific branch in the project’s Git-based file repository.

Together, these elements give you full control over how the App runs without affecting the development environment or job execution settings used elsewhere in the project.

== App lifecycle

Domino manages the entire lifecycle of your App, from infrastructure provisioning to request routing, so that you can stay focused on your app logic, not backend operations.

=== Start an App

When you start an App:

* *Container Provisioning*: Domino launches a Kubernetes pod using:
** The Compute Environment you selected (as a Docker image)
** The Hardware Tier you specified
* *Launch File Execution*: Domino runs the launch file you configured. This file must start a process that serves web traffic on port `8888` from `localhost`.
* *Monitoring and Uptime*: Domino monitors the App to ensure it stays available. If the App exits or crashes, Domino may automatically restart it based on platform settings.
* *Secure Proxying*: All incoming web requests are routed through Domino’s proxy layer. This allows Domino to:
** Authenticate users
** Apply access controls
** Log requests
** Inject HTTP headers (like user identity or path prefix)

=== Stop an App

When you stop an App:

* Domino shuts down the container and releases any allocated compute resources.
* Any changes made inside the container, such as new files or edits, are not saved back to your project’s file repository.

NOTE: This is different from Jobs or Workspaces, where output files can be persisted automatically.

== Feature compatibility with Apps

Apps can integrate with many of Domino’s core platform features, but not all features behave the same way in the context of an App. Use the table below to understand what is supported, what is limited, and what to watch out for when building Apps that need access to datasets, data sources, clusters, volumes, and more.

[cols="1,1,3", options="header"]
|===
| Feature | Supported in Apps | Notes

| Datasets
| Yes
| Accessible via file API; use the Dataset API + user identity for per-user control.

| Data Sources
| Yes
| App inherits the starter’s permissions.

| Compute Clusters
| Partially
| Requires manual configuration and uses the App starter’s permissions for access.

| NetApp Volumes
| Yes (Read/Write)
| Mounted volumes are available inside the App container.

| Environment Variables
| Yes
| Project-level and user-level variables from the App starter are injected at runtime.

| Version Control (Git)
| Yes
| App runs code from the selected Git branch and commit.

| App Output Persistence
| No
| File changes inside the App are not saved to the project.
|===

== Next steps

* Use link:6d4f60[Best Practices for Apps] to make your apps easier to find and maintain.
* link:cd0095[Create and Publish an App] has instructions on creating and publishing your Apps, customizing the App's URL, and sharing Apps with authorized users.

----- user_guide/apps/best-practices-for-apps.txt -----
:page-permalink: b04682
:page-version: 6.1
:page-title: Best practices for Domino Apps
:page-sidebar: Best practices for Apps
:page-order: 30

Follow these practices to help your apps stay clear, maintainable, and scalable as adoption grows across your organization. 

== Developer practices
These practices help you build Apps that are stable, reusable, and easy to maintain—especially as your team scales development across multiple environments and contributors.

=== Use clear titles and avoid hardcoded references

Give your App a clear, descriptive title that reflects its purpose, and avoid embedding fixed tokens or URLs that limit reusability.

* Use names like `"Churn Prediction Explorer"` or `"Marketing ROI Dashboard"` rather than `"test-app"` or `"new-dash-demo"`.
* Explain the App’s purpose in the description field—include who it’s for, what data it uses, and any assumptions or limitations.
* Avoid hardcoded tokens, URLs, usernames, or project paths in your App code. Use environment variables or configuration files instead.
* This makes your App easier to share, maintain, and move between environments like dev, staging, and production.

=== Organize code cleanly

Structure your App’s code for clarity and maintainability by separating concerns and avoiding bloated single-file implementations.

* Keep your main entry file (e.g., `app.py`) focused on layout, routing, or callbacks only.
* Move business logic, data loading, and reusable functions into helper modules.
* Use minimal shell scripts (e.g., `app.sh`) for launching the App and handling environment setup if necessary.
* This approach simplifies debugging, encourages reuse, and makes collaboration easier.

=== Support multiple environments

Use branches and isolated environments to separate development, staging, and production workflows.

* Create dedicated Git branches for each environment to test changes without affecting stable versions.
* Pair each branch with a corresponding Domino compute environment, such as `dev-env`, `prod-env`.
* Test new features side-by-side by publishing different App versions from separate branches.
* This keeps experiments safe, reproducible, and easy to roll forward.

=== Version your environment

Avoid unexpected breakages by versioning your compute environments rather than editing them directly.

* Clone your environment when making updates, instead of overwriting an existing one.
* Use semantic versioning or meaningful tags (for example, `v1.2-data-upgrade`) to track changes.
* Document the purpose of each version so you can match it to published Apps later.
* This ensures App behavior stays consistent over time, even as your tooling evolves.

=== Review permissions intentionally

Control access to Apps thoughtfully to balance collaboration and security.

* Set clear access levels: use *Anyone can access* for public tools, and restrict sensitive ones to specific users.
* Review permissions before publishing or republishing—especially when cloning or renaming Apps.
* Remember: App URLs can be shared externally. Use Domino’s access controls to enforce protection.
* This helps prevent unintentional exposure and keeps your platform governance strong.

== User experience and support

These tips ensure your Apps are approachable, well-documented, and easy for others to use, support, and extend—whether you're sharing with a small team or a wider audience.

=== Link to supporting documentation

Help users understand how to use your App—and what it’s doing under the hood.

* Add links to internal wikis, technical guides, or dashboards that explain data sources, outputs, and model behavior.
* Mention key assumptions or limitations directly in the App description.
* Consider adding an in-App link - something like a "Learn more" button - to surface docs during usage.
* This reduces confusion and increases the App’s usefulness across teams.

=== Provide contact info for feedback

Make it easy for users to reach out when they have questions or run into issues.

* Domino automatically includes an *Email Owner* option in the App toolbar—encourage users to use it.
* Optionally, include a contact email or Slack handle in the App UI or footer.
* For shared tools, consider a team alias or support channel instead of a personal address.
* This keeps feedback flowing and helps you spot bugs or usability gaps early.

== Next steps

* link:e3ec27[Apps in Domino] gives an overview of how apps work within the Domino ecosystem.
* link:cd0095[Create and Publish an App] has instructions on creating and publishing your Apps, customizing the App's URL, and sharing Apps with authorized users.
* link:5bef19[Learn more about how Apps in Domino] run and what identity and permissions are used.

----- user_guide/apps/common-app-frameworks/dash-app.txt -----
:page-version: 6.1
:page-title: Publish a Dash App in Domino
:page-sidebar: Dash Apps
:page-permalink: de2589
:page-order: 20

Dash is a popular Python framework for building interactive web applications. This guide shows how to run Dash apps in Domino, without diving into how Dash itself works. If you're new to Dash or want examples of what's possible, visit the link:https://dash.gallery[Dash Gallery^] or the link:https://dash.plotly.com/[Dash documentation^].

[[tr1]]
// Setup and Publish a Python Dash App

[[tr2]]
// Code snippets work as expected in the documentation
== What you'll do

* Set up a custom Domino environment that includes Dash
* Create a project and configure it for app publishing
* Add and run Dash sample code that works with Domino routing
* Publish and share your app using the Domino Launchpad interface

=== Step 1: Set up a compute environment

First, create a Domino compute environment that includes Dash:

. In Domino, go to *Govern* > *Environments* > *Create Environment*.
. Provide a name and description.
. Select the *Domino Standard Environment* as the base image.
. Click *Create Environment*.
. To add Dash, click *Edit Definition* and append the following lines to the Dockerfile:

+
[source,python]
----
USER root
RUN pip install dash==3.0.4
USER ubuntu
----
. Then, click *Build* and wait for the process to complete.

This sets up a custom environment that supports Dash apps.

=== Step 2: Create a Project

Now you’ll create a project in Domino to hold your Dash app:

. From Domino, go to *Develop* > *Projects* > *Create Project*.
. Name your Project and click *Create*.
. In the sidebar, go to *Settings* > *Compute environment* and select the environment you just built.
. Go to *Code* > *Add File*, name the file `app.py`, and click *Save*.

Your project is now configured with the correct environment and is ready for app development.

=== Step 3: Add app code

You’ll now add the Python code for your Dash app. Domino requires a few lines of setup to handle routing correctly, but you can use any Dash app layout or logic you like. This basic app confirms that Dash is working in Domino. It renders a static Plotly chart with no data dependencies or interactivity. 

To make your Dash app work in Domino, you must set routing using Domino environment variables and serve the app from `0.0.0.0` on port `8888`.

. Open the `app.py` file you created earlier.
. Paste in the following code and save the file:
+
[source,python]
----
import os
import plotly.express as px
from dash import Dash, dcc, html

# Domino-specific routing
runurl = os.environ.get("DOMINO_RUN_HOST_PATH")

# Create app with routing config
app = Dash(__name__,
            routes_pathname_prefix='/',
            requests_pathname_prefix=runurl)

# Basic static chart
fig = px.scatter(x=[1, 2, 3], y=[4, 1, 6], title="Sample Scatter Plot")

# App layout
app.layout = html.Div([
    html.H1("Minimal Dash App"),
    dcc.Graph(figure=fig)
])

# Launch on Domino-compatible host and port
if __name__ == '__main__':
    app.run(port=8888, host='0.0.0.0', debug=True)
----

=== Step 4: Add a startup script

Domino runs your app using a startup script. Create it like this:

. In the project sidebar, go to *Code* > *Add File*.
. Name the file `app.sh` and add `python app.py` to the file.
. Click *Save*.

This tells Domino how to launch your app during publishing.

=== Step 5: Publish the App

To make the app available through a browser:

. Go to *Deployments* > *App* in the sidebar.
. Add a title and description for your app.
. Set Permissions to *Anyone can access* to share the app broadly.
. Leave *Make globally discoverable* checked. If needed, you can disable this later.
. Click *Publish*. You may need to refresh your browser to view it.

Once the app is Running, click *View App* to open it in a new tab. This will launch your Dash app with a Domino toolbar and shareable link.

=== Step 6: Share the App

To share your app:

. Go to *Deployments* > *App*, and click *Copy App Link*.
. Open the link in an incognito or private browser window to test how the app appears to external users.
. Share the URL with collaborators.
. To browse other published apps, go to *Deploy* > *Apps* in the top nav.

Your Dash app is now hosted, versioned, and accessible in Domino. To explore more app designs or features, visit the link:https://dash.gallery[Dash Gallery^] or the link:https://dash.plotly.com/[Dash documentation^]. These resources offer a wide range of examples you can experiment with and adapt to your own needs.

== Next Steps

* link:e3ec27[Apps in Domino] gives an overview of how apps work within the Domino ecosystem.
* link:cd0095[Create and publish an app] has instructions on creating and publishing your apps, customizing the app's URL, and sharing apps with authorized users.
* link:5bef19[Learn more about how apps in Domino] run and what identity and permissions are used.

----- user_guide/apps/common-app-frameworks/flask-app.txt -----
:page-version: 6.1
:page-title: Publish a Flask App
:page-sidebar: Flask Apps
:page-permalink: 2039f2
:page-order: 40

The following guide will set up your Domino project more as a proper website powered by Flask.
This isn't difficult; you just need to make sure all the folders and files are in place to ensure the app runs correctly.

[[tr1]]
// Setup and Publish a Python + Flask App

[[tr2]]
// Code snippets work as expected in the documentation

== Create the App structure

First, you'll need to create the app structure.
The following is what we'll end up with and will discuss each area in turn.

Note that `/mnt` is your Files directory on Domino.

Also, the following names matter: `__init__.py`, `/static`, `/templates`.

Besides those, everything can be customized to your liking in this topic.

----
/mnt
|-- ...
|-- run.py
|-- /flask_app
    |-- __init__.py
    |-- views.py
    |-- /static
    |-- /templates
----

=== Create the folders

In the Files section of Domino make a new folder called `flask_app`.
This will be a Python package that will contain the web app.

In the flask_app folder put two other folders, one called `static` the other `templates`.
In the `static` folder you put your static files and in the `templates` folder you put your template files.
The static and templates folders will be recognized by Flask to know from where to fetch the appropriate files.
It's possible to change which folder is noticed as the static folder, but common practice is to just leave it as
`static`.

=== Create the files

Create an `__init__.py` file in the flask_app folder.
This file will make flask_app a Python package.
In that file put the following:

[source,python]
----
from flask import Flask

class ReverseProxied(object):
  def __init__(self, app):
      self.app = app
  def __call__(self, environ, start_response):
      script_name = environ.get('HTTP_X_SCRIPT_NAME', '')
      if script_name:
          environ['SCRIPT_NAME'] = script_name
          path_info = environ['PATH_INFO']
          if path_info.startswith(script_name):
              environ['PATH_INFO'] = path_info[len(script_name):]
      # Setting wsgi.url_scheme from Headers set by proxy before app
      scheme = environ.get('HTTP_X_SCHEME', 'https')
      if scheme:
        environ['wsgi.url_scheme'] = scheme
      # Setting HTTP_HOST from Headers set by proxy before app
      remote_host = environ.get('HTTP_X_FORWARDED_HOST', '')
      remote_port = environ.get('HTTP_X_FORWARDED_PORT', '')
      if remote_host and remote_port:
          environ['HTTP_HOST'] = f'{remote_host}:{remote_port}'
      return self.app(environ, start_response)

app = Flask(__name__)
app.wsgi_app = ReverseProxied(app.wsgi_app)
----

Back in your Files section of Domino create a new file called `run.py`.

In `run.py` add the following:

[source,python]
----
from flask_app import app
from flask_app import views

if __name__ == '__main__':
    app.run()
----

Also in your *Files* section of Domino, create a new file called  `app.sh` with the following content:

[source,shell]
----
#!/usr/bin/env bash
export FLASK_APP=run.py
export FLASK_DEBUG=1
python -m flask run --host=0.0.0.0 --port=8888
----

The purpose of this file is to provide the configuration including the host and port for your flask app when starting the web hosting process. You will select this file as your app file which Domino will then run within the container that contains your project files

NOTE: Domino requires this configuration for app service: `0.0.0.0` on port `8888`.
If your hosting network uses a different default, specify that default in your `app.sh` file or some alternate configuration.

=== Customization

Now we're going to add a `views.py` file.
This file is where you place all your app logic and should be created in the flask_app directory.

We'll make a very simple app that just returns “Hello World!”

Add the following to `views.py`:

[source,python]
----
from flask_app import app

@app.route('/')
def index():
    return "Hello World!"
----

If you have a template in the templates folder, you would add:

[source,python]
----
from flask import render_template
from flask_app import app

@app.route('/')
def index():
    return render_template('hello.html')
----

== Publish the App

. Click *Deployments > App* from the project sidebar.
. Give your app an informative title and description, and set permissions to *Anyone can access*.
This allows anyone with a network connection to your Domino deployment to access the app if they have the URL.
+
TIP: Select *Make globally discoverable* to make your app easily discoverable. You can always change this later.
. Select `app.sh` as your app file.
. Click *Publish Domino App*.
. After the App status says "Running", click *View App* to load your app.

== Share and consume

Now, you can set the permissions on your app to *Anyone can access* to share it with colleagues who have access to your Domino instance.
You can try this out yourself by opening a private or incognito browser, or logging out of Domino, and navigating to the *Deployments > App > Copy App Link*.

To browse more apps, click *Deploy > Apps* in the top navigation menu.
----- user_guide/apps/common-app-frameworks/index.txt -----
:page-permalink: a537c2
:page-version: 6.1
:page-title: Common App Frameworks
:page-sidebar: Common App Frameworks
:page-order: 40

Domino can run apps built in a variety of popular frameworks, as long as they launch a web server on `localhost:8888`. Below are minimal examples that demonstrate how to start each supported type of app. These are intended to help you get a working stub up and running quickly.

== Dash (Python)
Dash is a lightweight Python framework for building interactive analytic apps with Plotly visualizations.

[source,python]
----
import dash

app = dash.Dash(__name__, routes_pathname_prefix='/')
if __name__ == '__main__':
    app.run_server(port=8888, host='0.0.0.0', debug=True)
----

* Make sure `dash` is installed in your environment.
* Use `routes_pathname_prefix='/'` for proper routing inside Domino.

== Streamlit (Python)
Streamlit is a rapid app framework for turning Python scripts into shareable web apps with minimal code.

[source,python]
----
import streamlit as st

st.title("Minimal Streamlit App")
st.write("Hello from Domino!")
----

In your `.sh` file, launch with:

[source,bash]
----
streamlit run app.py --server.port=8888 --server.address=0.0.0.0
----

* Streamlit apps require a separate `.sh` such as `app.sh` startup script.
* You can configure the route prefix using `DOMINO_RUN_HOST_PATH`.

== Flask (Python)
Flask is a lightweight Python web framework that is ideal for creating custom endpoints or fully bespoke applications.

[source,python]
----
from flask import Flask
import os

app = Flask(__name__)

@app.route("/")
def index():
    return "Hello from Flask in Domino!"

if __name__ == '__main__':
    app.run(port=8888, host='0.0.0.0')
----

* Works out of the box with Domino's routing and identity headers.

== Shiny \(R)
Shiny is R’s standard framework for building interactive web applications with reactive components.

=== Option 1: Inline R script

[source,r]
----
shiny::runApp("./", port = 8888, host = "0.0.0.0")
----

=== Option 2: Shell script to launch Shiny

In your `.sh` file:

[source,sh]
----
R -e 'shiny::runApp("./", port=8888, host="0.0.0.0")'
----

* Shiny must run on `localhost:8888` to work in Domino.
* User-level identity headers are only supported by Shiny Server Pro.

These minimal examples are designed to help you quickly stand up a working app in Domino using your preferred framework. Each framework has specific requirements for launch behavior and routing, but all follow the same basic pattern.

== Next steps
* link:e3ec27[Apps in Domino] gives an overview of how apps work within the Domino ecosystem.
* link:cd0095[Create and Publish an App] has instructions on creating and publishing your Apps, customizing the App's URL, and sharing Apps with authorized users.
* link:5bef19[Learn more about how Apps in Domino] run and what identity and permissions are used.

----- user_guide/apps/common-app-frameworks/shiny-app.txt -----
:page-version: 6.1
:page-title: Publish a Shiny App
:page-sidebar: Shiny Apps
:page-permalink: e92082
:page-order: 30


This topic will show you how to publish an R App with
https://shiny.rstudio.com/[Shiny^] in Domino,

In this tutorial you will:

* Create a project and set it up for App publishing
* Publish a simple two file Shiny App to make it globally discoverable
* Observe how other users in Domino can use the App

You'll be working with the
https://shiny.rstudio.com/gallery/telephones-by-region.html[Telephones
by region] example from the https://shiny.rstudio.com/gallery/[Shiny
gallery]. In this example, the application serves an interactive bar
chart of consumer telephones by region from 1951 to 1961.

It will take approximately 10 minutes to get this example running in
Domino.

[[tr1]]
// Setup and Publish a R + Shiny App

[[tr2]]
// Code snippets work as expected in the documentation

== Set up Project

The first step is creating a project with the settings and content you
need to publish your App.

. From the home page, click *Develop > Projects*.
. Click *New Project*.
. Give your project an informative name, choose the *Private* visibility
setting, then click *Create Project*.
. Click *Settings* in the project sidebar, then set the Compute environment to 
*Domino Standard Environment Py3.8 R4.1*. Read
link:0d73c6[Domino standard environments],
to learn more about the contents of this base image.

. Click
*Code*
in the project sidebar, then click *Add File*.
. Name the file `server.R` in the title field above the editor. It's
important that the file be named exactly this, as launching a two file
Shiny application requires a directory to contain files named `server.R`
and `ui.R.`
. In the body of the file, paste the following example Shiny server
code.
+
[source,r]
----
# Rely on the 'WorldPhones' dataset in the datasets
# package (which generally comes preloaded).
library(datasets)

# Define a server for the Shiny app
function(input, output) {

 # Fill in the spot we created for a plot
 output$phonePlot <- renderPlot({

 # Render a barplot
 barplot(WorldPhones[,input$region]*1000,
 main=input$region,
 ylab="Number of Telephones",
 xlab="Year")
 })
}
----
. Click *Save* when finished.
. Click *Add File* again, and name the new file `ui.R`.
. Paste in the following content, then click *Save* when finished.
+
[source,r]
----
# Rely on the 'WorldPhones' dataset in the datasets
# package (which generally comes preloaded).
library(datasets)

# Use a fluid Bootstrap layout
fluidPage(

  # Give the page a title
  titlePanel("Telephones by region"),

  # Generate a row with a sidebar
  sidebarLayout(

    # Define the sidebar with one input
    sidebarPanel(
      selectInput("region", "Region:",
                  choices=colnames(WorldPhones)),
      hr(),
      helpText("Data from AT&T (1961) The World's Telephones.")
    ),

    # Create a spot for the barplot
    mainPanel(
      plotOutput("phonePlot")
    )

  )
)
----
. The last thing to do before publishing your App is to create an
`app.R` file. This is a script that Domino runs after initializing the host that will serve your App.
It should contain all commands required to launch your App. In this example, the only command you need is:
+
[source,r]
----
shiny::runApp("./", port = 8888, host = "0.0.0.0")
----
+
Make note of two important parameters in this command. Apps in Domino
must run with a host of `0.0.0.0` on port `8888`. This is where Domino
will direct users to your application. Create the `app.R` file the same
way you did for `server.R` and `ui.R`, then save it.


== Publish an app

. Click *Deployments > App* from the project sidebar.
. Give your App an informative title and description, and set
Permissions to *Anyone can access*. This will allow anyone with a
network connection to your Domino deployment to access the App if they
have the URL.
. Select app.R as your app file.
. Click *Publish*.
. After the App status says "Running", click *View App* to load your App.
You should see the interactive bar chart with a Domino toolbar above it
showing the project it's published from, plus buttons to email the App
owner and open the description panel.


== Share and consume

Now, you can set the permissions on your app to *Anyone can access* to share it with colleagues who have access to your Domino instance.
You can try this out yourself by opening a private or incognito browser, or logging out of Domino, and navigating to the *Deployments > App > Copy App Link*.

To browse more apps, click *Deploy > Apps* in the top navigation menu.

----- user_guide/apps/common-app-frameworks/streamlit-app-llm.txt -----
:page-version: 6.1
:page-permalink: 2c3a33
:page-title: Publish a Streamlit LLM app in Domino
:page-sidebar: Streamlit LLM apps
:page-order: 10

This guide walks you through building and deploying a chatbot powered by large language models (LLMs) using the link:https://docs.streamlit.io/[Streamlit framework^] in Domino.

The app combines a couple of models hosted on link:https://huggingface.co/inference-endpoints/dedicated[Hugging Face^] with a model served via a link:8dbc91[Domino endpoint] through a single, interactive UI.

== What you'll do

* Set up API access and compute environment prerequisites
* Use link:https://github.com/dominodatalab/docs_llm_app_example[Domino's example code^] to build a Streamlit chatbot with selectable model backends
* Integrate Hugging Face and Domino endpoints
* Configure app startup and publish it in Domino
* Share the app with others in your organization

== Prerequisites

Before you begin, make sure you have the following in place:

* *Hugging Face API key* +
Get a Hugging Face API key to access models hosted on Hugging Face:

. Create your account on https://huggingface.co/join[Hugging Face^].
. Get your Access token link:https://huggingface.co/settings/tokens[from Hugging Face^].
. In Domino, go to *Account Settings* > *Environment Variables* and add a link:d8dde6[user-level variable]:
.. *Name*: `HUGGING_FACE_API_TOKEN`
.. *Value*: `your key`

* *Domino compute environment* +
If you're using a *Domino Standard Environment (DSE)*, these are already included. Your environment must include:

. Jupyter or JupyterLab
. The https://pypi.org/project/jupyter-server-proxy/[jupyter-server-proxy^] package
. Streamlit and necessary Python libraries

* *Domino endpoint* +
To demonstrate multi-model access, this guide uses:

. Hugging Face-hosted models, such as Llama 3.2 3B and Mistral 7B.
. A model deployed as a Domino-hosted prediction endpoint. It is assumed you already have a Domino endpoint configured and available.

== Step 1: Set up the environment

To run and publish your Streamlit chatbot, first configure a Domino environment with the required packages. Streamlit apps are launched through a streamlit run and are hosted at port `8501` by default. You’ll configure this to use Domino’s required port `8888` later.

. Go to *Environments* > *Create Environment*.
. Enter a name and description.
. Select a base image, such as the *Domino Standard Environment*.
. In the Dockerfile, add the following if not already present:
+
[source,python]
----
USER root
RUN pip install --no-cache-dir \
    streamlit \
    jupyter-server-proxy \
    requests \
    pandas
USER ubuntu
RUN pip install transformers torch streamlit
----
+
. Click *Build* and wait for the environment to finish building.

You now have a compute environment with Streamlit and proxy support, ready to run your chatbot app.

== Step 2: Create the Project and build the App UI

Next, you’ll create a new Domino project and begin building your chatbot app using Streamlit. You’ll start with a basic layout: a title, a sidebar with model selection, and a password field for your Hugging Face API key.

You’ll develop the app interactively inside a Jupyter or JupyterLab workspace and view it live in a browser tab using a URL generated by Domino.

=== Create the Domino Project

First, create the Domino Project you'll use for your app:

. Go to your Domino home page, or your Projects page, and choose *Develop* > *Projects* > *Create Project*.
. Name your project something like `streamlit-llm-chatbot` and click *Create*.
. In the project sidebar, go to *Settings* > *Compute environment* and select the environment you just created.

=== Create the app title and sidebar

Next, create the app title and sidebar for the app:

. Launch a new Jupyter or JupyterLab workspace.
. In the `/mnt` directory, create a new file named `chatbot.py`.
. Add the starter code found here in Domino's link:https://github.com/dominodatalab/docs_llm_app_example[example streamlit LLM app repo^].
. Update the following values in your file:
.. `DOMINO_ENDPOINT_URL`: set to your Domino-hosted model endpoint
.. `DOMINO_MODEL_ACCESS_TOKEN`: your model’s access token

More information about Domino endpoints and tokens can be found in the link:e038f2[Select Domino endpoint authorization mode] documentation.

=== Get your app URL

To preview your app, construct the URL based on your Domino run context.

. Replace `//your-domino-url/` with your actual Domino domain and run:
+
[source,python]
----
echo -e "import os\nprint('https://your-domino-url/{}/{}/notebookSession/{}/proxy/8501/'.format(os.environ['DOMINO_PROJECT_OWNER'], os.environ['DOMINO_PROJECT_NAME'], os.environ['DOMINO_RUN_ID']))" | python3
----
+
*Reminder*: Port `8501` is Streamlit’s default. You’ll remap this to `8888` when publishing.

. Run the app in your workspace:
+
[source,python]
----
streamlit run chatbot.py
----

. Open the app in a browser tab using the generated URL.

You now have a live app preview with a sidebar and model selector ready for user interaction and backend logic.

== Step 3: Add model logic and user interaction

Now that the UI is in place, you’ll add logic to handle chat input, store conversation history, and route model calls based on user selection.

The app will support both a Hugging Face-hosted model and a Domino-hosted endpoint. This step uses Streamlit session state to maintain chat history across user inputs.

=== Store chat messages in session state

This logic is already included in the `chatbot.py` code. It initializes and displays the conversation history.

=== Define model query functions

This app already includes logic to:

* Call Hugging Face-hosted models via InferenceClient.
* Fallback to direct HTTP calls if needed.
* Call your Domino-hosted model endpoint.

All function definitions are in the file, including:

* `chat_with_hf(...)`
* `chat_with_api_direct(...)`
* `query_domino_endpoint(...)`

=== Capture user input and generate a response

User prompts are captured using `st.chat_input`, and responses are routed depending on the selected model. The logic makes sure that:

* Messages are appended to the session state
* Responses are streamed with a spinner
* Fallbacks are handled gracefully

The `chatbot.py` file has the full implementation details.

=== Sync all changes

Once your file is complete and tested interactively in your workspace, you’ll need to link:262fef#sync-changes-to-files[sync all changes].

. Save all files, including `chatbot.py` and any updates to tokes or environment values.
. If you’re ready to move on from development, link:0002fb#stop-workspace[stop your workspace]. This makes sure Domino syncs all file changes back to your project.
. Confirm that both files - `chatbot.py` and `app.sh` - exist in your project root.

Your app is now fully interactive, capturing input, maintaining history, and calling different LLMs based on user selection.

== Step 4: Configure the app file and prepare for publishing

To publish your Streamlit app in Domino, you need a startup script that tells Domino how to launch the app and configure it to use the correct host and port.

* *Problem*: Streamlit defaults to running on port `8501`, but Domino requires apps to listen on `0.0.0.0:8888`.
* *Solution*: Override the default settings using a `config.toml` file inside your `app.sh` startup script.

=== Create the app file

In the root of your project, create a file named app.sh with the following content:

[source,python]
----
#!/bin/bash

mkdir -p ~/.streamlit

cat << EOF > ~/.streamlit/config.toml
[browser]
gatherUsageStats = true

[server]
port = 8888
enableCORS = false
enableXsrfProtection = false
address = "0.0.0.0"
EOF

streamlit run chatbot.py
----

*Verify*: Make sure the file is executable by running `chmod +x app.sh` in the terminal from inside your workspace, if needed.

=== Why this works

* The `config.toml` file tells Streamlit to use the correct port (`8888`) and host (`0.0.0.0`), which is required for apps published in Domino.
* Domino will run `app.sh` automatically when launching your app with no manual intervention needed.

You’re now ready to publish the chatbot as a live Domino app.

== Step 5: Publish the app

Now that your app and startup script are in place, it’s time to publish your Streamlit chatbot using Domino’s built-in app publishing feature.

Domino apps run inside containers based on your project’s environment and are launched by executing the `app.sh` script you configured in the previous step.

=== Open the publishing interface

* In your project sidebar, go to *Deployments* > *App*.
* Fill in an informative title and description, for example, *LLM Chatbot with Hugging Face and Domino Endpoints*.

=== Set access permissions

Choose how others can interact with your app:

* Set the permissions to *Anyone can access* to allow other users on your Domino deployment to view it.
* Leave *Make globally discoverable* selected to let others browse and find it from the *Deploy* > *Apps* view.

You can change these settings later if needed.

=== View the app

To see your fully interactive chatbot with model selection, live input, and responses from either the Hugging Face-hosted or Domino-hosted model:

* Click *Publish Domino App* to deploy the app.
* Once the app status changes to *Running*, click *View App* to open it in a new browser tab.

Your app is now live, discoverable, and running in Domino, accessible to others by URL or from the Launchpad.

== Step 6: Share the app

Once your chatbot is running, you can easily share it with colleagues or test public access to ensure everything works as expected.

Access depends on the permissions you set during publishing. The *Anyone can access* permission allows any authenticated user on your Domino deployment to view the app through its URL.

=== Copy the app link

* From the *Deployments* > *App screen*, click *Copy App Link*.
* Share the link with others who have access to your Domino instance.

=== Test access

Open the app URL in an incognito or logged-out browser window to confirm access settings.

If you're prompted for a Hugging Face token, enter it in the sidebar to test API access as a new user.

=== Browse or discover more apps

* From the top nav, go to *Deploy > Apps* to explore other published tools within your organization.

You’ve now successfully published and shared an LLM-powered Streamlit chatbot with interactive inputs and backend model integration.

== Next steps

* link:e3ec27[Apps in Domino] gives an overview of how apps work within the Domino ecosystem.
* link:cd0095[Create and publish an app] has instructions on creating and publishing your apps, customizing the app's URL, and sharing apps with authorized users.
* link:5bef19[Learn more about how apps run in Domino] and what identity and permissions are used.

----- user_guide/apps/host-html.txt -----
:page-version: 6.1
:page-title: Host HTML pages
:page-permalink: 9b11ea
:page-order: 130

// TR Audit note: This page contained no new functionality TRs, all functionality is covered by TRs created on different pages

[[tr1]]
// Code snippets work as expected in the documentation

This is a simple example on how to host a HTML page on Domino.
There are a number of ways to host web applications of course but this example shows how you can integrate a simple HTTP server using python with Domino.
You could also add javascript and other pages to your project.
The example in this note just shows how you would start the server to support your pages.

== Files

You'll need to create a file in your project (in addition to your files required for your page such as `index.html`).

`app.py`

[source,python]
----
import http.server
import socketserver
PORT = 8888
Handler = http.server.SimpleHTTPRequestHandler
httpd = socketserver.TCPServer(("", PORT), Handler)
print ("serving at port", PORT)
httpd.serve_forever()
----

When publishing the app you will select `app.py` as the app file. When Domino launches your app, it runs the `app.py` file within the container that contains your project files.

== Next steps

* link:e3ec27[Apps in Domino] gives an overview of how apps work within the Domino ecosystem.
* link:cd0095[Create and Publish an App] has instructions on creating and publishing your Apps, customizing the App's URL, and sharing Apps with authorized users.
* link:5bef19[Learn more about how Apps in Domino] run and what identity and permissions are used.


----- user_guide/apps/index.txt -----
:page-version: 6.1
:page-permalink: 71635d
:page-title: Publish Apps
:page-order: 300

link:e3ec27[Apps in Domino] are built for flexibility, letting you run custom web-based tools and expose them to others while Domino manages infrastructure, permissions, and routing.

link:cd0095[Build and publish interactive Applications]::
Learn how to use models, notebooks, and pipelines to publish interactive Apps with minimal setup.

link:b04682[Best practices for Domino Apps]::
Follow proven techniques to keep your Apps clear, maintainable, and scalable as usage grows.

link:a537c2[Common App frameworks]::
See examples of how to run Apps built with Streamlit, Dash, Flask, or Shiny in Domino.

link:5bef19[App execution and authentication]::
Learn more about how apps run in Domino and what identity and permissions are used.

link:6a2215[Request headers and environment variables for Apps]::
Use metadata from the runtime environment to adjust the behavior of your Apps.

link:cb9195[Secure your Apps]::
Apply permissions to control access and protect sensitive content.

link:bb4b24[View App logs and use version control]::
Track usage and troubleshoot issues through built-in log files. Snapshot fully reproducible artifacts each time you publish a new version of your App.

link:f47536[Understand App usage]::
Use built-in usage analytics to help you monitor engagement, track adoption, and spot trends over time.

link:b99d5d[Persist data from Apps]::
Persist data using Datasets or external storage to make your Apps more dynamic and interactive.

link:cfc79e[Configuration settings for Apps]::
Adjust the behavior of your Apps by tuning the settings of configuration records.

link:a29497[Optimize performance and scale]::
Tune your App's performance based on its framework and resource profile.

link:9b11ea[Host HTML pages]::
Publish apps using any HTTP framework.

link:aec000[Whitelist resources for web Apps]::
Configure iFrame or content security policies if required by your organization’s Domino instance.

----- user_guide/apps/persist-data-in-apps.txt -----
:page-permalink: b99d5d
:page-version: 6.1
:page-title: Persist data from Apps
:page-order: 100

Apps in Domino do not automatically save file changes back to your project’s repository. This differs from *Workspaces* or *Jobs*, which can commit outputs at the end of execution.

If your App needs to persist results, such as saving user input, logging events, or generating output files, you'll need to handle persistence directly within your App code.

== Ways to persist data from an App

You can persist results using any of the following approaches:

* *Mounted Datasets*: Datasets provide read/write storage and are versioned independently from your project files. Use them to store:
  ** Generated files
  ** Log outputs
  ** Snapshots or other intermediate results

* *Databases or External Data Sources*: Use standard client libraries to write to:
  ** SQL or NoSQL databases
  ** Cloud object storage (e.g., S3, Azure Blob)
  ** Enterprise data warehouses
  (ensure your runtime environment has credentials and network access)

* *APIs*: Your App can invoke internal or external APIs to:
  ** Record state
  ** Send metrics or events
  ** Trigger workflows
  ** Store data in external services

== Concurrency considerations

Apps can serve **multiple users at the same time**, which means your app code must be designed to handle **concurrent access safely**.

[IMPORTANT]
====
Domino does *not* serialize or isolate access to shared resources across App users. You are responsible for implementing any necessary safeguards in your App code.
====

Here are important safeguards:

* Use locking mechanisms or atomic operations if multiple users write to the same resource.
* Prefer per-user namespaces, directories, files, or database tables to isolate individual user actions.
* Watch for race conditions or state inconsistencies, especially in long-running or interactive Apps.

== Next steps

* link:e3ec27[Apps in Domino] gives an overview of how apps work within the Domino ecosystem.
* link:cd0095[Create and publish an App] has instructions on creating and publishing your Apps, customizing the App's URL, and sharing Apps with authorized users.
* Learn more about link:5bef19[how Apps in Domino run] and what identity and permissions are used.

----- user_guide/apps/publish-an-app.txt -----
:page-permalink: cd0095
:page-version: 6.1
:page-sidebar: Create and Publish an App
:page-title: Create and Publish an App
:page-order: 20

Domino Apps make it easy to turn your project code into a running web application, whether for internal tools, dashboards, or interactive reports. You can define an app, configure its environment, and publish it with access controls, all from within your Domino project.

Each app in Domino has its own compute environment, hardware tier, launch script, and access permissions. Projects can contain multiple apps, each independently configured and accessible through a unique or custom URL.

== Create and publish an App

The Domino App window guides you through the full process from setup to deployment, so that you can go from working code to a live app in just a few steps.

image::/images/6.1/user-guide/app-publishing-wizard.png[alt="App Publishing Window", width=900, role="noshadow"]

. Open an existing project or create a new one.
. From the top menu, go to *Deploy > Apps*, or from the sidebar, go to *Deployments > Apps*.
. Click *Publish Domino App*.
. In the *Details* pane, enter the app name, choose a project, and optional description. 
.. If you are using a Git-based project, you'll be able to select from related options here.
. Click *Next*.
. In *Deployment*, choose the compute environment, hardware tier, and code revision. Click *Next*.
. In *Data*, select any required data sources. Click *Next*.
. In *Access & sharing*, set app visibility, permissions, and optionally a custom URL, such as `/apps/demo`.
. Set the *launch file* (typically a shell, Python or R script) that Domino will use to start the app.
. Click *Publish Domino App* to finish.

Once published, the app will appear in your project’s *Deployments* list. To share it, go to the app’s *Overview* page and copy the link from the *Access & sharing* section.

=== App runtime behavior

* Start a new run based on the App settings.
* Monitor uptime and make the App accessible to authorized users through its published URL.
* Expose the App at a unique URL.

Once published, your App is immediately available to authorized users through the generated or custom URL. You can update the App at any time by editing its configuration or underlying code. To ensure a smooth user experience, verify routing, permissions, and output behavior before sharing the App broadly.

== App permissions

Domino lets you control access to Apps through four flexible sharing modes. You set the permission level when publishing or editing the App.

[IMPORTANT] 
====
Access is always enforced at the proxy level, before any request reaches your App logic.
====

[cols="1,2", options="header"]
|===
| Mode | Who Can Access

| *Anyone (including anonymous users)*
| Anyone with the URL, even without a Domino account.

| *Anyone with an account*
| Any logged-in Domino user.

| *Invited users only*
| Only users you explicitly add to the app’s access list.

| *Invited users (request access)*
| Only invited users can access the app by default, but others can request access for you to approve.
|===

== Next steps

* link:e3ec27[Apps in Domino] gives an overview of how apps work within the Domino ecosystem.
* link:5bef19[Learn more about how Apps in Domino] run and what identity and permissions are used.
* Use link:6d4f60[Best Practices for Apps] to make your apps easier to find and maintain.

----- user_guide/apps/request-headers-and-env-variables-apps.txt -----
:page-permalink: 6a2215
:page-version: 6.1
:page-sidebar: Request headers and environment variables
:page-title: Request headers and environment variables for Apps
:page-order: 60

When Domino publishes and runs an App, it injects useful metadata into the runtime environment. These values can help your App adjust its behavior based on user identity, routing, project settings, or access controls.

== Request headers

Domino includes several custom HTTP headers in each request forwarded to your App. These headers are only available if your app framework supports proxied HTTP headers.

[cols="1,3", options="header"]
|===
|Header |Description

|`domino-username`
|The Domino username of the user making the request. Value is `Anonymous` for unauthenticated users.

|`Authorization`
|(Optional) A JWT token if `SecureIdentityPropagationToAppsEnabled` is enabled. Can be decoded to extract user identity metadata.
|===

NOTE: These headers are supported by Dash and Flask by default. For Shiny, you must use link:https://docs.rstudio.com/shiny-server/#proxied-headers[Shiny Server Pro] to receive them.

== Environment variables

Domino also injects runtime link:6ac5a1[environment variables] when an App is started. These are available to your app logic at startup.

[cols="1,3", options="header"]
|===
|Variable |Description

|`DOMINO_RUN_HOST_PATH`
|The full URL path prefix for your App. Use this for dynamic route configuration.
|===

Additional environment variables may be defined at the project or user level:

* *Project-level variables*: Defined in the project settings.
* *User-level variables*: Tied to the user who starts the App.

These variables can be used to:

* Configure your App's route prefix programmatically.
* Apply logic based on project context or user configuration.
* Pass secrets or credentials securely (via Domino’s environment variable management).

== Next steps

* link:e3ec27[Apps in Domino] gives an overview of how apps work within the Domino ecosystem.
* Learn more about  link:5bef19[how Apps in Domino run] and what identity and permissions are used.
* link:cd0095[Create and Publish an App] has instructions on creating and publishing your Apps, customizing the App's URL, and sharing Apps with authorized users.


----- user_guide/apps/understanding-app-usage.txt -----
:page-permalink: f47536
:page-version: 6.1
:page-title: Understand App usage
:page-order: 90

Domino Apps include built-in usage analytics to help you monitor engagement, track adoption, and spot trends over time. Everything is available directly in the UI, with no additional setup required.

image::/images/6.1/user-guide/apps-usage-tab.png[alt="Accessing the Usage Tab for Apps", width=1200, role="noshadow"]

This section explains how to access the *Usage* tab, interpret available metrics, and apply these insights to improve your App’s performance and reach.

== Accessing the Usage tab

To view usage:

. Open your App’s detail page.
. Click *Usage* (next to *Overview*).
+
At the top of the tab, you'll see:

[cols="1,3", options="header"]
|===
|Metric |Description

|`Deployed timestamp`
|When the current version was published.

|`Uptime`
|How long the App has been running.

|`Started by`
|The user who clicked *Start*.

|`Status`
|Current status: *Running*, *Suspended*, or *Stopped*.
|===

== Total views and period comparison

This section shows how many times your App has been accessed and how that compares to the previous month.

* *Views in past 30 days*: Total number of HTTP requests to your App, for example, `263`.
* *Change over previous period*: Percentage increase or decrease compared to the prior 30-day window, for example, `+30.20%`.

== Daily views chart

This chart helps you visualize usage patterns over time.

* Each bar represents the number of App views on a specific day.
* Hover over a bar to see the exact count, like `14 views on Jun 3`.

== Version-specific filtering

Use the *Version* dropdown to filter metrics:

* *All versions* (default): Aggregated data.
* Select a specific version to view usage for that version only.
* Green bars indicate the currently deployed version.

== Interpret the data

The resource metrics chart provides insight into how your app is being used over time. You can uncover trends that inform performance tuning, capacity planning, and user engagement strategies, by analyzing usage patterns such as spikes, steady growth, or sudden drops.

Use trends in the usage data to understand how your App is being used:

* A steady increase = Growing engagement.
* Spikes = Demos, training sessions, or automation.
* Drops = Autosuspends, errors, or reduced interest.

Review logs or gather user feedback to investigate anomalies. These insights can help you improve performance, plan for growth, and boost adoption.

== Monitor resource consumption

Domino Apps include built-in resource monitoring to help you track real-time CPU and memory usage. This visibility aids in troubleshooting, performance tuning, and choosing the right hardware tier.


image::/images/6.1/user-guide/apps-monitor-consumption.png[alt="Monitor App resource consumption", width=900, role="noshadow"]

The panel provides historical data, visual insights, and best practices for optimization.

=== Access the Resource Consumption panel

You can monitor an app’s hardware usage (such as CPU, memory, and GPU) using the *Resource Consumption* panel available in the app’s *Overview* tab. This helps you understand performance over time, diagnose resource-related issues, or fine-tune compute settings. 

Resource data is retained for the previous 15 days; if no data is available for that window, the panel will display a message indicating that no chart is visible.

=== Understand the chart

The chart displays CPU and memory usage over time for the current App run.

* *Memory Usage (GiB)* → Purple line (left Y-axis).
* *CPU Usage (%)* → Green line (right Y-axis).
* *Time* (last 15 days) → X-axis.

Hover over any point to see timestamp, memory usage, and CPU percentage.

=== Key insights

Beyond general usage patterns, the resource chart can reveal deeper operational insights about your app’s performance. Tracking these signals helps you validate resource sizing, detect inefficiencies, and identify opportunities to optimize memory or compute behavior.

Use the chart to identify these trends in resource usage:

* *Baseline usage*: Confirms App fits within allocated resources.
* *Spikes*: May occur during data loads or visualizations.
* *Gradual increases*: Can indicate memory leaks or inefficiencies.

== Data retention and external monitoring

Domino retains resource data for the **past 15 days only**.

For long-term tracking or alerting:

* Forward metrics to external monitoring systems, such as *Prometheus + Grafana*, or *Splunk*.
* Use the Domino API or a sidecar exporter to stream data.

== Best practices for resource optimization

* *Right-size early*: Scale up if usage consistently exceeds 80%.
* *Optimize hot paths*: Link resource spikes to specific features or actions.
* *Set limits and alerts*: Work with admins to configure quotas and alerts.

By monitoring and optimizing usage, you can improve stability, enhance UX, and ensure efficient use of hardware resources.

== Next steps

* link:e3ec27[Apps in Domino] gives an overview of how apps work within the Domino ecosystem.
* link:cd0095[Create and Publish an App] has instructions on creating and publishing your Apps, customizing the App's URL, and sharing Apps with authorized users.
* link:5bef19[Learn more about how Apps in Domino] run and what identity and permissions are used.

----- user_guide/apps/whitelist-resources.txt -----
:page-version: 6.1
:page-title: Whitelist resources
:page-permalink: aec000
:page-order: 140

Your Domino admin may have to whitelist resources for your web app if your Domino instance has iFrame security enabled or if it requires a content security policy for web apps. Domino admins can see link:71d6ad#tr340[Web app configuration records] for more information on whitelisting URLs.

To give your web app access to external resources, identify and gather the URLs of block resources.

. Open a web browser.
. Go to your web app in Domino.
. Start and open your web app.
. After the app is open and running, open your browser's web console.
. Locate the URLs of the resources your app must access.
In Chrome, they're highlighted in red and are typically preceded by a message like “Refused to connect to…” (or similar).

. Share these URLs with your Domino administrator and request that the URLs be whitelisted.
. After the URLs are whitelisted, stop and restart your Domino web app.
Your web app should now have access to these resources.

== Next steps

link:a29497[Optimize app scalability and performance]

----- user_guide/collaborate/access-controls.txt -----
:page-version: 6.1
:page-permalink: 22a752
:page-title: Access controls and collaboration
:page-order: 60

Different entities in Domino can be permissioned separately. Permissions affect who is allowed to find and use specific entities. To summarize:

[cols="1,3,3",options="header"]
|===
h|
h|Permission options
h|Details and subtleties

|Projects
|The Project visibility level can make Projects accessible to everyone if desired. Otherwise, Projects can have specific users and groups as collaborators. Collaborators can have specific roles that limit what they can do within the Project.
|For Git repos attached to a Project, you'll have to manage access through your Git host. The Discoverable visibility setting can make Projects appear in search results for users who don't have access to the Project's contents.

|Datasets, Data Sources, and External Data Volumes
|These entities can be shared with specific users or groups.
|To use any one of these features within a Project, a user needs access to the Project and to the respective data entity.

|Apps
|Apps are permissioned independently of their parent Project. See link:71635d[Apps] for more details.
|You can permission an App to be accessible by anyone, even users without a Domino user identity. You can also allow users to request access to an App.

|Compute Environments
|
|
|===

== Collaboration-related admin config settings

Some configuration records settings, only accessible by admins, affect aspects of how collaboration on Domino works. For the full list of options, see link:71d6ad[Configuration records].

[cols="2,2,^1",options="header",width="100%"]
|===
h|Config key
h|Description
h|Example value

|`frontend.restrictCollaboratorsToOrganizations`
|If `true`, when adding Project collaborators, you can only specify groups, not individuals.
|`false`

|`publicProjects.enabled`
|Whether users can mark Projects as public.
|`true`

|`defaultProjectVisibility`
|The default “visibility” level for new Projects.
|`Public`

|`search.fileExtensionsToIndexContent`
|Comma-separated list of file extensions to index for search.
|txt, out, tsv, rtf, err, log, tex, latex, README, ipynb, py

|`search.documentTypesEnabledForSearch`
|Comma-separated list of Domino entities to index for search.
|project, file, data_set, comment, model, run, environment

|`search.isElasticSearchEnabled`
|Whether search is enabled.
|`true`

|`search.maxIndexableFileSizeInKilobytes`
|Maximum size of a file (kilobytes) to index for search.
|2048
|===

----- user_guide/collaborate/collaborative-development.txt -----
:page-version: 6.1
:page-permalink: ae6e22
:page-title: Develop collaboratively
:page-order: 10

Domino streamlines team-based data science work with features designed to enhance collaborative efforts. Learn how to effectively add collaborators, manage permissions, and utilize modular work strategies to optimize your project workflow and drive efficient development."

Three features enable collaborative development: Project sharing, Dataset sharing, and Project imports/exports.

== Project sharing

Domino Projects are the fundamental unit of collaboration. A Project is a bundle of all the materials and the record of activities of all collaborators working on a project.

To add collaborators to your Project, go to the Project's *Settings* > *Access & Sharing* and add people or organizations in the *Collaborators & permissions* section.

You can also change a collaborator's role — for example, to restrict them from being able to see (but not modify) the Project's contents.

Collaborators on a Project will all see the same materials within that Project. Contributors will all be able to modify files, run Workspaces, and Jobs, and perform other development tasks.

For more detail see link:7876f1[Collaborator permissions].


== Dataset sharing

If you want to create some modularity in who works on different parts of a Project, you can grant access to Datasets independently from Projects. For more details see link:8f5b7e[Dataset sharing and security].

To summarize: some teams use this approach if they have different people working on preparing data, to be used by another part of the team for modeling or other downstream tasks.

In this scenario, you can grant access to a Dataset to the people working on the data prep, without giving them access to the downstream modeling/analysis Project(s).


== Project imports/exports

Domino lets you use a Project as a component that you can link into another Project. Working this way allows you to modularize work and reduce repetition/duplication. For example, if you have a set of Project Artifacts you want to use in several different Projects, instead of copying those files, you can export a Project with the Artifacts and import it into the downstream Projects.

To import a Project, go to the *Other Projects* tab on the *Code* page in a DFS project or the *Artifacts* page in a Git-based project.

When this Project runs a Job or Workspace, the files and environment variables from the imported Project will be accessible. Files will be read-only.

There are more advanced options, such as controlling which version of a Project you import. For more detail see link:e6ed48[Project imports/exports].

== Next steps

Learn about other ways Domino streamlines collaboration:

- link:bb5e34[Communications and shared context]: Stay informed about what project collaborators are doing, making teamwork more streamlined and creative.
- link:dd8787[Share work and make it accessible]: Share work and use tools to make information more accessible to less technical stakeholders.
- link:0bc056[Knowledge management] and link:d9abb7[Domino search]: Find and reuse work in your Domino Projects.

----- user_guide/collaborate/domino-search.txt -----
:page-version: 6.1
:page-title: Use Domino search
// Do not name this page simply "Search" because this causes it to be redirected to the site search page.
:page-permalink: d9abb7
:page-order: 50

Use Domino's search feature to locate specific files or pieces of text across your entire deployment. Domino will index a variety of content in the platform, such as link:a8e081[Projects], files, link:942549[Jobs], link:08a85b[Models], and link:06ceeb[Comments].

[[tr2]]
Domino limits the search based on the collaboration and privacy settings for
projects. For example, if you do not have read access to a project, then that
project is never listed in your search results.

One advanced feature at the intersection of search and permissions is that Projects can be set to be *Searchable*.

If a Project is set to be *Searchable*, then it will appear in search results for anyone — but its contents will be visible only to explicit collaborators. This is a useful feature if you want to encourage the discovery of work even when that work might be sensitive. With this setting, data scientists could know that relevant past work exists — they would just have to request access to see it. Still, that might prevent wasted effort reinventing the wheel.

NOTE: if you have git repos attached to your Projects, Domino will not index the contents of those repos.

== Use the search feature

[[tr1]]
. In the top navigation pane, type your query into the *Search* bar.
The search results are listed by source, including project names, files, runs, comments, and more.

When Domino searches files, it returns results found in both
filenames and file contents.
However, Domino only indexes the latest revisions of files, so the search results will not contain occurrences of your search term from previous versions.

== Advanced search options

You can use the following commands in your queries to target your search:

[[tr3]]
* `project.tag=`
[[tr4]]
* `project.tag.approved=`
[[tr5]]
* `project.description=`
[[tr6]]
* `project.name=`
[[tr7]]
* `project.user=`

== Next steps

Learn about other ways Domino streamlines collaboration:

- link:ae6e22[Collaborative development]: Domino makes it easy for multiple people to work on the same project together.
- link:bb5e34[Communications and shared context]: Stay informed about what project collaborators are doing, making teamwork more streamlined and creative.
- link:dd8787[Share work and make it accessible]: Share work and use tools to make information more accessible to less technical stakeholders.
- link:0bc056[Knowledge management]: Find and reuse work in your Domino Projects.

----- user_guide/collaborate/index.txt -----
:page-version: 6.1
:page-title: Collaborate in Domino
:page-permalink: d82ba3
:page-order: 210

As data science organizations grow, collaboration becomes critical for speeding up research and development. Many of the highest-performing data science organizations cite collaboration as part of their “secret sauce” that allows them to rapidly innovate and deliver impact from data science work.

== Facets of collaboration

“Collaboration” is a vague term that means different things to different people. Domino enables collaboration in several ways:

* link:ae6e22[Collaborative development]: Domino makes it easy for multiple people to work on the same project together.
* link:bb5e34[Communications and shared context]: Domino helps collaborators stay informed about what each other is doing, making teamwork more streamlined and creative.
* link:dd8787[Make work accessible]: Domino makes it easy to share work and provides tools to make information more accessible to less technical stakeholders.
* link:0bc056[Knowledge management]: Domino makes it easy to find and reuse work.
* link:d9abb7[Use Domino search]: Domino provides a search feature to easily locate specific files or pieces of text across your entire deployment.
* link:22a752[Access controls and collaboration]: Different entities in Domino can be permissioned separately to control who is allowed to find and use specific entities.
* link:526a62[Manage organizations]: Domino makes it easy to give many users permission to your project at once and to add/remove collaborators from multiple projects.

== Domino Projects: The foundational element of collaboration

A Project is Domino's fundamental unit of collaboration. A Project is a bundle of all the materials and context that collaborators use when working on a project together. This includes the code, files (e.g., results or artifacts), data, software packages, and configuration — as well as the historical discussion and record of activity within the Project.

image::/images/collaboration/Domino-project.png[alt="Domino Project", width=600]

Projects make collaboration easy through two simple but powerful characteristics:

*Single source of truth for materials across collaborators.* All the materials you use in data science projects — the code, compute environments, data, and hardware — are centralized and thus identical regardless of who is using them. As a result, Domino solves one of the common problems of collaboration: different team members have different versions of code, software, or data out of sync with each other. Gone are the problems of passing files back and forth between computers or checking versions of packages across different dev environments.

*Automatic tracking of work avoids “writing over” someone else's work.* link:0264e0[Domino's Reproducibility Engine] ensures that work cannot be lost or permanently overwritten. Moreover, git-backed projects make it easy for collaborators to work on multiple branches concurrently, for easier organization of changes.

== Benefits of collaboration and knowledge management

These modes of collaboration drive several benefits, which often differentiate the highest-performing data science organizations.

* *Innovation and breakthroughs.* Much data science work is like scientific research in that exploratory ideas may be paused and put on the shelf. Being able to find past work about a topic, and being able to “page in” the context of past work, can accelerate the process of sparking new ideas. One data science leader we work with told us that all of the most impactful breakthroughs on his team came from someone picking up old work and looking at it in a new way.
* *Productivity.* When data scientists spend less time on mechanics of collaboration — coordinating file changes with each other, and browsing through multiple systems for prior work — they have more time for higher-value work like research and model development.
* *Less duplicate work and analytics “debt”.* A common problem in large organizations is the proliferation of duplicate work, e.g., ten different teams might make a model to do the same thing. This duplication of work becomes a drag on continuous improvement, as different versions diverge in subtle ways, and improvements to one of them aren't automatically propagated to others. By making it easier to reuse past work, Domino helps simplify and minimize your surface area of data science work, increasing maintainability and the speed of continuous improvement.
* *More flexible talent strategies.* As a centralized platform (accessible through the browser) that doesn't require any work on local desktops or laptops, Domino makes remote teams possible, unlocking access to talent in more locations. It also makes work from home much easier. Offering these options can help attract top data scientists, who are in high demand and often have their own choice of employers.
* *More flexible project staffing models.* By making it easy to have multiple people working on a Project together, Domino enables teams to decompose work into different components that can be developed in parallel, to complete projects faster. For example, within a Project, some people could be working on feature engineering tasks while others could be working on visualization and reporting outputs, while others could be working on data prep. Domino provides a single interface to share and integrate multiple parts of a project.

----- user_guide/collaborate/knowledge-management.txt -----
:page-version: 6.1
:page-permalink: 0bc056
:page-title: Manage knowledge
:page-order: 40

Domino’s Search and Project Tagging functionality makes it easy to find past work. From there, the Reproducibility Engine makes it easy to Fork or Copy Projects so that you can resume or reuse work with one click.

== Search 

Domino will index a variety of content in the platform, such as Projects, files, Jobs, Models, and Comments. The “Search” item at the top of Domino’s left navigation menu lets you easily search from anywhere in the platform. For more details see link:d9abb7[Domino search].

== Project Tagging

Searching is powerful when you aren’t quite sure where to look for what you want. In other cases, it can be more efficient to browse through a pre-defined organizational system that your company has put in place to categorize Projects. Domino enables this with Project Tags.

To view all tags in your deployment, go to the project *Overview > Tags & Description* and click *Manage Tags*. From this view, you can see all projects associated with each tag.

You can also use a special search query to find Projects with a specific Tag by searching `project.tag=[tag-name]`.

== Forking & copying Projects

Once you’ve found relevant past work — either by searching or browsing — Domino makes it easy to reuse it. In many cases, you’ll simply begin working. In some cases, you may want to work in an isolated copy, so you don’t disrupt or confuse or pollute the original work. Forking or Copying a Project allows you to do this.

The Overview section of a Project includes buttons to Fork or Copy the Project.

Both of these operations will create new Projects with the same materials — code, compute environment, etc. — so you can resume working where the original Project left off.

Unlike Copying, Forking a Project will maintain metadata indicating that your new Project “originated” from the original one, in case you want to merge changes back later. If a Project is a Fork of another Project, the Overview page will show you the source Project and other “sibling” Projects (i.e., Projects forked from the same parent).

image::/images/6.0/copy-fork.png[alt="Forked Project", width=800, role=noshadow]

== Next steps

Learn about other ways Domino streamlines collaboration:

- link:ae6e22[Collaborative development]: Domino makes it easy for multiple people to work on the same project together.
- link:bb5e34[Communications and shared context]: Stay informed about what project collaborators are doing, making teamwork more streamlined and creative.
- link:dd8787[Share work and make it accessible]: Share work and use tools to make information more accessible to less technical stakeholders.
- link:d9abb7[Domino search]: Find and reuse work in your Domino Projects.

----- user_guide/collaborate/manage-organizations.txt -----
:page-version: 6.1
:page-title: Manage organizations
:page-permalink: 526a62
:page-order: 70

You can create organizations with groups of users so you can give many users permission to your project at once.
You can also easily add/remove collaborators from multiple projects.

. In the top navigation pane, click *Account > Account settings*.
. On the Account Settings page, click *Organization*:

An Organization is associated with one user account that owns and manages the organization.
This user account is referred to as the Admin account.
If you are logged in under this account, you can manage organization membership on your Account page `<Domino URL>/account`.

From there, you can add/remove users from your Organization.

== Typical uses for organizations

=== Manage Project access

The most common use for organizations in Domino is to manage project access.

Domino sees organizations and users as near-equivalents; the only difference is that an organization contains multiple users.
Because of this, organizations simplify project access for groups of users.

For example, you can create an organization with members of a team to invite the team to join a project that you are working. Then, add the organization to the project.
This is also useful if you want to remove the team from the project.
You can also separate an existing organization into smaller groups, and then un-invite one team while retaining another organization.

You can also use organizations to provide restricted access to your projects, instead of full access.
For example, to give a team read-only access to a project, add the organization to your project and set its role to Results Consumer.
If there's a group that should only be able to use Launchers, add the organization and give them Launcher User access.

Organization admins can add or subtract people to and from their organization.
Adding users to (or removing them from) organizations rather than individual projects that the organization owns saves time.
When you update the organization, Domino updates the users' access to each project for you.

=== Organizations for production-ready Projects

You can also use organizations to manage projects that have reached a production grade-level of quality.
You can set up organizations that are specifically intended for projects that are ready to be promoted to production, or that have already been promoted.
Users working with projects owned by this organization will understand that these projects must be carefully documented and kept free of clutter.
Any changes to the project require a fork and merge request process.
Executions in these projects are deliberate, intended to update data or models.

When you use organizations to manage projects promoted to production, the process can unfold in the following ways:

. A project starts as an individual user's project.
That user invites others to this project, either individually or through an Organization.
Over time other users believe this project is useful.
Eventually, it is decided that it must be easier to access the project, and that it should be owned by the organization instead of the individual user.

. A project is initially owned  by an organization.
This usually happens when the project is deemed to be important from the beginning, and the project can best be built up by a group.
When this project is created, all users can access all its resources, and project development proceeds, mostly by forking and merging.
Execution of code is still done in the organization-owned project as a way to check in your analysis or results.
+
Some Domino customers do this for models that are not in production to submit their work to be counted.

=== Share compute Environments

Organizations streamline and simplify the practice of sharing link:f51038[environments].
Project owners who are members of an organization can use all of that organization's compute environments for their own projects.

For example, when a user is a member of a corporate organization, the environments that that organization's admins have created are to the user for their own projects.
Users can be members of multiple organizations.
If a user is a member of two organizations, a corporate organization with production environments and an R&D organization with cutting edge environments, that user has access to the compute environments of both organizations, as well as any global environments and environments the user already owns directly.

If this user were to be removed from an organization that they were a member of, even though several of their projects rely on that organization's environments, the now unavailable environments would be reset to the user's default environment.
Domino sends a notification to affected users.

== Organization permissions for Projects

All members of your organization have `owner-level` access to any projects under the Organization's account.

If you have an Organization account with username `your_org`; it has members Jane, Chris, and Rae.
That means Jane, Chris, and Rae will all have `owner-level` access to any the projects in `your_org`, for example, `your_org/quick-start` and `your_org/project1`.

The members cannot change the organization's membership; only you can do that.


== Organizations as Project collaborators

If you add an organization as a collaborator to one of your projects, all members of the organization will be granted link:7876f1[collaborator-level access].

----- user_guide/collaborate/share-work.txt -----
:page-version: 6.1
:page-permalink: dd8787
:page-title: Share work with stakeholders
:page-sidebar: Share work
:page-order: 30

By hosting work centrally and making it easily accessible through a web browser, Domino simplifies the process of exposing work to less technical colleagues.

Three common formats for sharing work, in order of least interactive to most interactive, are Files, Launchers, and Apps.


== Share files

Share a presentation-mode view of a file with a colleague with the following steps:

. Select the file you'd like to share, then click the *Copy Shareable Link* button.

. This will copy a link to view the file in a simplified UI for the viewer. This is a great way to share charts, PDFs, or other results with non-technical colleagues.

TIP: Domino can render a wide variety of file types directly in the browser, including Jupyter notebooks, and Markdown files! You can send colleagues a link to an ipynb file in Domino, and they'll be able to see the cells and rendered outputs, to view it as a report.


=== Use scheduled Jobs to generate reports for stakeholders

You can combine Domino's file rendering and sharing capabilities with scheduled Jobs and notifications, for a powerful and flexible way to generate reports.

You can configure scheduled Jobs to notify specific recipients when the Job finishes. You can also customize the files that get included in email notifications sent from scheduled Jobs. So you could have a scheduled Job using knitr to generate HTML or PDFs, or render an ipynb file — and email the attachment and/or a link to the resulting file.


== Launchers

link:f4e1e3[Launchers] are a powerful feature for exposing self-serve, templatized analyses to stakeholders. You can expose a simple web form UI on top of a script you've written. Your colleagues can specify input values using the web form, then run your script with their desired inputs and get the results. 


== Apps

Domino makes it easy to publish and host link:71635d[Apps], which can have complex, very flexible interactivity and visualizations. Apps are a great way to expose rich interactive experiences to non-technical colleagues. 

== Next steps

Learn about other ways Domino streamlines collaboration:

- link:ae6e22[Collaborative development]: Domino makes it easy for multiple people to work on the same project together.
- link:bb5e34[Communications and shared context]: Stay informed about what project collaborators are doing, making teamwork more streamlined and creative.
- link:0bc056[Knowledge management] and link:d9abb7[Domino search]: Find and reuse work in your Domino Projects.

----- user_guide/collaborate/shared-context.txt -----
:page-version: 6.1
:page-permalink: bb5e34
:page-title: Communicate in a shared context
:page-order: 20

Domino helps collaborators stay informed about what others are doing, making teamwork more streamlined and creative.

Three features enable shared context: Notifications, comments, and the activity feed.

== Notifications

Collaborators on a Project can opt to get notifications when certain events in the Project take place. This helps them stay informed as their colleagues are working. 

To adjust notification preferences, go to the project *Settings > Access & Sharing > Collaborators & Permissions*. From there, choose the relevant notification preferences for each collaborator. 

== Comments

Collaborators on a Project can leave comments on a variety of items in the Project, such as files or Jobs. Comment threads live inside the Project and can be indexed for Search. @mentioning collaborators will send them notifications via email.

Domino's commenting feature is a powerful way to preserve historical context and keep colleagues abreast of ongoing work. For more see Comments.

To comment on a file, use the text input below the file.

Comments on Jobs will appear in the *Comments* tab of the Job details pane on the Jobs Dashboard.

You can also comment on Results inline in the Results tab.

See the following page for step-by-step instructions to link:06ceeb[Add comments].

== Activity feed

A Project's activity feed makes it easy to see the history of activity on a Project. This is useful if collaborators need to catch up on developments they have missed (e.g., if they were out of the office or in another time zone). Or if someone is picking up an old Project or joining a team and needs to educate themselves on the history of the work.


== Next steps

Learn about other ways Domino streamlines collaboration:

- link:ae6e22[Collaborative development]: Domino makes it easy for multiple people to work on the same project together.
- link:dd8787[Share work and make it accessible]: Share work and use tools to make information more accessible to less technical stakeholders.
- link:0bc056[Knowledge management] and link:d9abb7[Domino search]: Find and reuse work in your Domino Projects.

----- user_guide/customize-environments/access-additional-domains-and-hostnames.txt -----
:page-version: 6.1
:page-title: Access additional domains and hostnames
:page-sidebar: Private domains and hostnames
:page-permalink: f53230
:page-order: 60

[[tr1]]
// As a Domino user, I can configure an Environment to add a custom DNS server and use it to reach hosts via that server.
Use custom DNS name servers to handle internal or private domain names and URLs.
This topic describes how to add a new DNS server to your link:f51038[Domino Environment] to allow access to additional domains and hostnames.

On a Linux host like your Domino executor, a configuration file at `/etc/resolv.conf` controls which DNS servers will be used to resolve hostnames.
You can use a pre-setup script in your Domino Environment to modify this file and add custom DNS servers by following these instructions.

. Open the Environment you want to modify, then click *Edit Definition*.
. Scroll down to the bottom of the definition, then click to expand the *Advanced* section.
. In the *Pre Setup Script* field, enter the following lines of Bash.
Fill in the address of the custom server you want to use where indicated.
+
[source,bash]
----
echo 'nameserver your-custom-server-address' > /tmp/resolv.conf.tmp
cat /etc/resolv.conf >> /tmp/resolv.conf.tmp
cat /tmp/resolv.conf.tmp > /etc/resolv.conf
----
. Click *Build* at the bottom of the definition page to create a new version of the Environment.

Following a successful build, you can use this Environment to reach hosts through the added DNS name server.

----- user_guide/customize-environments/add-packages-to-environments/index.txt -----
:page-version: 6.1
:page-permalink: bfa148
:page-title: Add packages to Environments
:page-order: 20

Add packages to your Environment and manage package versions.

link:a785ff[Package dependency files]::
Use `requirements.txt` or `install.R` to install packages to your Environment every time you start an execution.

link:3d2592[Internal package hosting]::
Use Dockerfile instructions to install internal mirrors of public package repositories.

link:813d89[Preload Environment packages]::
Use Dockerfile instructions to install packages to your Environment permanently with quick execution times.

link:9c4f82[Use requirements.txt]::
Specify additional dependencies, you can add a pip requirements file named `requirements.txt` to the root of your project folder.
(This is for Python only.)

----- user_guide/customize-environments/add-packages-to-environments/package-dependency-files.txt -----
:page-version: 6.1
:page-permalink: a785ff
:page-title: Package dependency files
:page-order: 10

Learn how to manage packages or libraries with a dependency file.

A common term for files like *requirements.txt* in Python and *install.R* in R is a "dependency file" or "package manifest file".  These files are used to specify and manage the dependencies (i.e., external libraries or packages) that a Python or R project requires to run successfully. They list the specific packages and their versions that need to be installed for the Project to work as intended. 


Depending on the programming language and package management system, the file formats and naming conventions may vary, but the purpose is the same: to define and document the dependencies for a Project.

No special formatting is required for Domino compatibility of a dependency file.
If it works outside of Domino, it will work in Domino. 
There are a few specifics about the location of the file which are explained below.

== When to use a package dependency file

Package dependency files can be used in a variety of scenarios.
The following are a few examples.

. Share code using Git-based source code management tools
. Make it easier for other developers to install the correct versions of the required libraries
. Install all the required dependencies in a single command
. Start each Domino Workspace or Job with specific versions, or the latest available versions, of Python, R, or Git repository packages

WARNING: Package dependency files cause packages to install on every execution startup, slowing down the process. If you want your packages to remain permanently installed and reduce startup time, consider link:813d89[preloading Environment packages].

[[python]]
== Python dependency file - requirements.txt

If it doesn't already exist, create a file named `requirements.txt` in the root directory of your Project. 

NOTE: In a link:910370[Git-based Project], `requirements.txt` must be inside your Project's Artifacts folder.

After `requirements.txt` is in place, Domino automatically installs the Python package every time you start a Workspace or Job within the Project.

=== Add Python packages

The requirements file specifies which libraries and any version requirements for them.
For example:

[source,bash]
----
pandas
lxml==3.2.3
numpy>=1.7.1
----

See https://pip.pypa.io/en/stable/cli/pip_install/#requirements-file-format[pip install requirements file format^] for the syntax of the requirements file.

=== Generate a requirements.txt file from an existing Environment

If you're using pip on your local machine, the easiest way to generate the `requirements.txt` file is to run the following command in the root of
your Project folder (or in the Code folder if your Project is Git-based):

[source,bash]
----
~/domino/myProject $ pip freeze > requirements.txt
----

For performance reasons, prune the file so that it includes only the libraries needed for your analysis.

If you're working in a Jupyter Notebook, you can also use `pip` to install dependencies interactively.
In a notebook cell, you can run:

[source,bash]
----
! pip install --user <package>
----

(The '!' tells the notebook to execute the cell as a shell command.)

=== Install packages hosted from a Git repository

CAUTION: This is an advanced topic.

Pip can install Python packages from source by cloning a public Git repository over HTTPs.
See https://pip.pypa.io/en/stable/cli/pip_install/#git[pip install] for reference.
To specify this, you must add something like the following line to your `requirements.txt` file:

[source,shell]
----
-e git+https://git.yourproject.org/you/Project.git#egg=YourProject
----

or

----
package-name@git+https://github.com/repository-name.git
----

The most common host of Git projects is GitHub.
If the package you want to install is publicly accessible, then the previous instructions will work.
However, if you must install private repositories, Domino can securely integrate with GitHub to access those private repositories.
See link:314004[Import Git Repositories] for information about securely storing your GitHub credentials.

CAUTION: Do not embed your GitHub credentials directly in the `requirements.txt` file.
Instead, integrate Domino with GitHub securely by following the instructions in link:314004[Import Git Repositories].

== R dependency file - install.R

[[tr2]]
// As a Domino user, I can install a custom R package using an install.R file with the path to that package in my Project files.

If it doesn't already exist, create a file named `install.R` in the root directory of your Project. 
Add the following line to `install.R`:

[source,r]
----
devtools::install_local('/repos/<repo_name>')
----

For example, if you want to install `ggplot2` from a Git repository, your `install.R` should look like:

image::/images/4.x/R-dependency-file.png[alt="R dependency file", width=300, role=noshadow]

After this `install.R` is in place, enter a line at the beginning of your work within the Project to install the Project for your sessions.

[source,r]
----
source('install.R')
----

----- user_guide/customize-environments/add-packages-to-environments/preload-environment-packages.txt -----
:page-version: 6.1
:page-permalink: 813d89
:page-title: Preload Environment packages
:page-order: 30

= Preload your Environment with packages and libraries

[[tr30]]
// As a Domino user, I can install packages directly to my Environment.

Domino Environments have a *Dockerfile Instructions* section.
Type commands in this section to install packages directly into your Environment.

== Save package install time and improve reproducibility

Preloading packages can save time as it prevents repeated loading in Workspaces and Jobs. Installation is not required after the initial build as Domino Environments are cached. However, the Environment is still loaded with each new execution so only packages that are frequently used should be cached. Preloading too many unnecessary packages will increase the size of the cached which can significantly increase load times.

Dockerfile instructions can load specific versions of packages and libraries for clarity which helps with reproducibility and collaboration. It's a best practice to fix package versions in the Dockerfile instructions.

An alternative to caching packages in a Python project is to use a requirement.txt file. See link:9c4f82[requirements.txt in Domino] for more details.

== Docker command syntax in Domino

Consult the official Docker documentation to learn more about Dockerfiles:

* http://docs.docker.com/engine/reference/builder/[Reference^]
* http://docs.docker.com/engine/articles/dockerfile_best-practices/[Best practices^]

[NOTE]
====
Domino's Docker images must be run by the user, `ubuntu`.
However, Domino recommends installing packages in your Dockerfiles as the `root` user to avoid permissions issues.
To do this, add `USER root` before your Docker commands.
Then, after running the commands, type `USER ubuntu`.
====

Do not start your Dockerfile instructions in Domino with a `FROM` line.
Domino includes the `FROM` line for you, pointing to the base image specified when setting up the Environment.

The most common Dockerfile instructions you'll use are
`RUN`, `ENV`, and `ARG`:

`RUN` commands execute lines of bash.
For example:

[source,dockerfile]
----
USER root

RUN wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.1-bin-hadoop2.6.tgz
RUN tar xvzf spark-1.5.1-bin-hadoop2.6.tgz
RUN mv spark-1.5.1-bin-hadoop2.6 /opt
RUN rm spark-1.5.1-bin-hadoop2.6.tgz
RUN pip install s3fs==2021.6.1 scipy==1.7.1
RUN R --no-save -e "install.packages('remotes')"

USER ubuntu
----

`ARG` commands set build-time variables, and
`ENV` commands set container bash Environment variables.
They will be accessible from runs that use this Environment.
For example:

[source,dockerfile]
----
ENV SPARK_HOME /opt/spark-1.5.1-bin-hadoop2.6
----

If you set variables in the *Environment variables* section of your definition, you can use an `ARG` statement:

[source,dockerfile]
----
ARG SPARK_HOME
----

This will be available for the build step.
If you want the variable to be available in the final compute Environment you must add an `ENV` statement referencing the argument name:

[source,dockerfile]
----
ENV SPARK_HOME=$SPARK_HOME
----

== Examples: Package installation

. When you edit your Environment, click *R Package* or *Python Package* to insert a line to install packages.
. Enter the names of the packages.
. You can also add the commands, as in the following examples:

* R Package Installation, example with the *devtools*
package.
+

[source,dockerfile]
----
USER root
RUN R --no-save -e "install.packages('devtools')"
RUN chown -R ubuntu:ubuntu /usr/local/lib/R/site-library  # ensure ubuntu user can update installed packages
USER ubuntu
----
* Python Package Installation with Pip, for example with the...
numpy package.
+

[source,dockerfile]
----
USER root
RUN pip install numpy
USER ubuntu
----



[[docker-bp]]
== Dockerfile best practices

* Docker optimizes its build process by keeping track of commands it has run and aggressively caching the results.
This means that if it sees the same set of commands as in a previous build, it will assume that it can use the cached version.
A single new command will invalidate the caching of all subsequent commands.
* A Docker image can have up to 127 layers, or commands.
To work around this limit, you can use `&&`: to combine several commands into a single command.
+
[source,dockerfile]
----
RUN 
  wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.1-bin-hadoop2.6.tgz && 
  tar xvzf spark-1.5.1-bin-hadoop2.6.tgz && 
  mv spark-1.5.1-bin-hadoop2.6 /opt && 
  rm spark-1.5.1-bin-hadoop2.6.tgz
----
* If you are installing multiple python packages via pip, it's almost always best to use a single pip install command.
This ensures that dependencies and package versions are properly resolved.
If you install via separate commands, you may end up inadvertently overriding a package with the wrong version, due a dependency specified by a later installation.
For example:
+
[source,dockerfile]
----
RUN pip install luigi nolearn lasagne
----

----- user_guide/customize-environments/add-packages-to-environments/set-up-internal-package-hosting.txt -----
:page-version: 6.1
:page-title: Set up internal package hosting
:page-sidebar: Internal package hosting
:page-permalink: 3d2592
:page-order: 20

Many customers maintain internal mirrors of public package repos.
This is typically done while they use a whitelist to remove access to external, public repositories.
This might be for security or for standardization.

[[tr1]]
// As a Domino user, I can configure my Environment dockerfile instructions to use private mirrors for public package repos.
You can add the following to the Dockerfile instructions in your global link:f51038[Compute Environments] to set Conda, pip, or RStudio to reference your internal mirrors by default:

[source,dockerfile]
----
RUN 
   # Add $CUSTOMER mirrors to condarc config files. Also disabled auto_update since mirrors are not always up to date.
   printf "
channels:
  - $CUSTOMER_CHANNEL_1
  - $CUSTOMER_CHANNEL_1

" >> /opt/conda/.condarc && 
   printf "
allow_other_channels: False
" >> /opt/conda/.condarc && 
   printf "
ssl_verify: False 
" >> /opt/conda/.condarc && 
   printf "
show_channel_urls: True
" >> /opt/conda/.condarc && 
   printf "
auto_update_conda: False
" >> /opt/conda/.condarc && 
   
   # Pointing R to $CUSTOMER CRAN mirror
   printf "
local({ 
  r <- getOption('repos') 
  r['CRAN'] <- '$CRAN_MIRROR_URL' 
 options(repos = r) 
})
" >> /home/ubuntu/.Rprofile && 
   
   # Pointing pip to $CUSTOMER PyPi mirror
   printf "[global]
index-url=$CUSTOMER_PYPI_MIRROR
trusted-host=$IP_OR_FQDN" > /etc/pip.conf && 
   
   chown -R ubuntu:ubuntu /home/ubuntu /opt/conda/
----




----- user_guide/customize-environments/add-packages-to-environments/use-requirements-txt.txt -----
:page-version: 6.1
:page-permalink: 9c4f82
:page-title: Use requirements.txt (Python only)
:page-sidebar: Use requirements.txt
:page-order: 40

// This article has an incredibly high amount of organic traffic. Let's keep it to introduce folks to Domino. Let's be primarily helpful to Domino users but this article ranks 1 for requirements.txt searches, so let's support non-Domino users too.

Unless package persistence is enabled, Domino runs each script or interactive session in a fresh environment. In this case, any custom packages are reinstalled at the start of each execution.

To add Python dependencies to your project:

* Create a link:https://pip.pypa.io/en/latest/user_guide/#requirements-files[requirements.txt] file in the root of your project directory.
* List the required packages using standard pip syntax.

Domino will install these packages automatically at runtime.

Domino provides flexibility, so if you want your changes to remain permanently installed, see link:813d89[Preload Environment packages] to create a custom environment.

[[python]]

Many common modules are installed by default.
To get a list of pre-installed modules, include `help('modules')` at the start of your script or in a new IPython Notebook session.
You can also run `domino run --direct "pip freeze"` with the Domino CLI tool.

== Improve execution startup time

Using a `requirements.txt` file allows you to specify and install Python dependencies at runtime. However, this approach comes with trade-offs:

* For each execution, Domino installs the listed packages from scratch, which can increase startup time.

To optimize performance based on your use case:

* **For Workspaces:** Enable package persistence to install packages once during initial startup. These packages will then be retained between sessions, avoiding repeat installations and significantly reducing startup time for future launches.
* **For Jobs, Apps, and other executions:**  Use preloaded Environment packages to build a custom environment where packages are baked into the image. This ensures packages are available immediately and persist across all executions, avoiding any runtime installation cost.

By selecting the correct installation strategy, you can strike a balance between flexibility and faster execution times.

== Add your own packages

[[tr1]]
// As a Domino user, I can add packages to a workload execution (e.g. batch job, workspace) by specifying them in a requirements.txt file in the project where the workload is to be executed.

To define additional dependencies, create a `requirements.txt` file and place it in the root of your project directory.

NOTE: If your project is link:910370[Git-based], the `requirements.txt` file must be located in the `Code/` folder within your project’s directory.

The `requirements.txt` file should list all required Python packages using standard pip syntax. You can specify exact versions, minimum versions, or allow the latest versions, along with which libraries to use. For example:

[source,bash]
----
pandas
lxml==3.2.3
numpy>=1.7.1
----

Domino will automatically install the listed packages at the start of each execution, unless package persistence or preloaded environments are used to optimize startup time.

=== Install packages interactively in Workspaces

If you're working in a Jupyter Notebook, you can also install packages interactively using pip directly in a notebook cell:

[source,bash]
----
! pip install --user <package>
----

The '!' prefix runs the command in the shell from within the notebook environment.

== Generate a requirements.txt file

If you're using pip on your local machine, the easiest way to generate the requirements.txt file is to run the following command in the root of your project folder (or in the Code folder if your project is Git-based):

[source,bash]
----
~/domino/myProject $ pip freeze > requirements.txt
----

For performance reasons, prune the file to include only the libraries needed for your analysis.

== Install packages hosted from a Git repository

CAUTION: This is an advanced topic.

Pip can install Python packages from source by cloning a public Git repository over HTTPs.
See https://pip.pypa.io/en/stable/cli/pip_install/#git[pip install] for reference.
To specify this, you must add something like the following line to your `requirements.txt` file:

[source,shell]
----
-e git+https://git.yourproject.org/you/Project.git#egg=YourProject
----

The most common host of Git projects is GitHub. If the package you want to install is publicly accessible, then the previous instructions will work. However, if you must install private repositories, Domino can securely integrate with GitHub to access them. See link:314004[Import Git Repositories] for information about securely storing your Github credentials.

CAUTION: Do not embed your GitHub credentials directly in the `requirements.txt` file. Instead, integrate Domino with GitHub securely by following the instructions in link:314004[Import Git Repositories].

----- user_guide/customize-environments/deploy-a-custom-image.txt -----
:page-version: 6.1
:page-permalink: c11e1c
:page-title: Deploy a custom image
:page-order: 50

= Enable a custom image for publishing

[[tr1]]
// As a Domino user, I can publish and call a model using an Environment with basic Domino compatibility and model-specific packages.
When you create an Environment, custom images can be made automatically compatible with Domino Jobs and Workspaces when creating an Environment, as explained in link:5babee[Use external images in Domino Environments].
This topic explains how to use additional Dockerfile instructions to enable custom images for use when publishing models or hosting web applications.

* To do this when creating the Environment, select *Customize before building*.
* To add this capability to an existing Environment, click *Edit Definition* in the top right of the Environment's overview page.

To publish a Domino Domino endpoint, install uWSGI and Flask as follows:

[source,dockerfile]
----
# Install uWSGI pre-requisites
RUN apt update && apt install -y --no-install-recommends gcc && 
    apt-get clean && rm -rf /var/lib/apt/lists/*

# Add missing library for uWSGI
RUN conda install libpython-static==5.11
ARG LDFLAGS=-fno-lto
ENV LDFLAGS=-fno-lto

# Install Flask & uWSGI
RUN pip install --no-cache-dir Flask Flask-Compress Flask-Cors uwsgi six prometheus-client
----

NOTE: Replace `5.11` with the specific Python version being used in your Environment.

To publish a Domino endpoint with R, you must also add `plumber` and `future` to the Environment:

[source,dockerfile]
----
# Make sure R is installed, then add plumber and future
RUN R --no-save -e "install.packages(c('plumber', 'future'))"
----

// Link to Publish a Domino App. However, the link lacks detail. It's better but not perfect.
The Environment modifications needed to host a web application in Domino depend on the framework you are using; see link:71635d[App publishing].

----- user_guide/customize-environments/index.txt -----
:page-version: 6.1
:page-permalink: e46d54
:page-title: Customize Environments
:page-order: 200

link:f51038[Manage Compute Environments]::
Learn how to work with Domino Environments.

link:bfa148[Add packages to Environments]::
Add a package to the Environment used in a Job, Workspace, or App.

link:35ff43[Work with Pluggable Workspaces]::
Leverage a variety of Workspaces based on specific languages, development tools, and software.

link:5babee[Use external images in Domino Environments]::
Use pre-built images from external catalogs.

link:c11e1c[Deploy a custom image]::
Use Dockerfile instructions to enable custom images to publish models or host web applications.

link:f53230[Access additional domains and hostnames]::
Use custom DNS name servers to handle internal or private domain names and URLs.

----- user_guide/customize-environments/manage-compute-environments/1-best-practices-domino-environments.txt -----
:page-permalink: 0e4da9
:page-version: 6.1
:page-title: Best Practices for Domino Environments
:page-order: 10

== Domino Environments

Cumbersome Environment definitions can significantly impact startup times for executions that use that Environment. 

=== Use clear naming conventions and descriptions

*Reason*: A common reason environments get duplicated is that users are unsure if an existing environment meets their needs. Using clear and descriptive names for all environments makes it easier for new users and admins to understand the catalog. This clarity helps make Domino easier to work with and maintain. 

*Solution*: Keep things clear for everyone by using these tips:

* Use clear, descriptive names for your environments.
* Enforce a standard of quality in naming and Dockerfile commenting.
* Make sure you don’t make a copy of an environment without renaming it.

=== Use build propagation to keep environments updated

*Reason*: Prevents users from having to manually update environments downstream.

*Solution*:  Automatically update environments by: 

* Environments that need to be auto-updated are set to *Active (auto-rebuilds)*.
* Testing newly built revisions after updating (ensure *Auto-assign next new revision as Active* is off).
* Once you're happy with the latest revision, set it as *Active*.
* You can view these in the Subscribed Environments view.

=== Limit global environments

*Reason*: Focusing on a few global environments with broad uses is more beneficial than creating many specialized ones. 

*Solution*: Follow these ideas for limiting global environments:

* Use the default compute environment as the base.
* Create a global environment and image that meets 80-90% of the community’s requirements.

=== Create one environment per team to avoid sprawl

*Reason*: When a user asks for a new environment with specific features, consider adding those to an existing global environment instead of making a new environment. Having fewer global environments makes it easier to maintain them.

*Solution*: Here are some ideas to keep sprawl to a minimum:

* Individual users’ environments should be based on a primary global environment.
* Environments should always be based on the latest standard base image.

=== Audit and archive environments regularly

*Reason*: Avoid sprawl to maintain control of your environments and prevent slow executions.

*Solution*: Follow these tips to create a culture of tidiness around environment management: 

* Share responsibility for environment management.
* Clean up old or poorly maintained environments. 
* Be assertive about pruning unnecessary environments. 
* Team leads/SMEs responsible for keeping images up-to-date.
* Have few global environments.
* If several users need to install something larger than 1G, it may be time to update your base image.
* Use Organizations to manage access to restricted environments.

== Building Images
These are our best practices for building more efficient Docker images. 

=== Reduce the Environment image size 

*Reason*: If the environment image needed for a workspace, job, or other task is not on the compute node where it will run, it must first be sent over the network from the image registry in a compressed format. After the transfer, the image is then decompressed on the compute node. Transferring and decompressing the image slows startup time, and the delay increases as the image size grows.

*Solution*: Follow these tips to minimize the Environment image size:

* Choose the smallest base image that suits your needs, either an existing Domino Environment or a custom image.
* Install only essential software via the Dockerfile.
* Consider creating several smaller, special-purpose Environments rather than one larger, general-purpose Environment.

=== Limit dependency file usage

*Reason*: Every package listed in a dependency file, like `requirements.txt`, is installed via a network transfer for every execution. This causes startup latency and is vulnerable to network failures.

*Solution*: Follow these tips to minimize usage of dependency files:

* Move packages from dependency files to the Environment’s Dockerfile. 
* Packages in the Dockerfile are installed just once during Environment creation.
* Eliminate rarely used packages from dependency files. On the rare occasions they are needed, install them manually in the running execution or as part of your execution’s code.

=== Cache global environments on the executor machine image

*Reason*: If users create environments with base images that are very different from what is cached on the machine image, it can take a long time to pull the necessary images when launching executors.

*Solution*: To improve the startup time for your users:

* Cache your global environments in the executor template machine image so that each new executor will start with the cached base Docker image for any environment.


----- user_guide/customize-environments/manage-compute-environments/2-domino-standard-environments.txt -----
:page-version: 6.1
:page-title: Domino Standard Environments
:page-permalink: 0d73c6
:page-order: 20

Every Domino installation includes the default Domino Core Environment (DCE) and the Domino Standard Environment (DSE) setup. These environments come with pre-configured data science tools and libraries for Domino users. They can be used for any Domino execution, such as a Workspace, Job, App, or Domino endpoint.

With each major release, Domino updates its core environments with new libraries and packages. You can still update your core environments with custom libraries and packages. Your Domino administrator can guide you through that process if needed.

DCEs are smaller and more straightforward, making them suitable only for the worker nodes in the cluster. DSEs cannot be used for worker nodes in a cluster.

[[dce]]
== Domino Core Environment (DCE)

Domino provides a security-focused, smaller-footprint image called the Domino Core Environment (DCE). It includes only the essential packages needed to operate within Domino. 

This can serve as a base to help you create a Domino-compatible environment from scratch, add only the packages you want to it, and speed up the process of building the environment and initiating executions.

The Domino Core Environment supports Jupyter and JupyterLab but lacks some packages available in the Domino Standard Environment (DSE). 

Domino recommends the DCE for custom package installations on a base image because its smaller size speeds up builds and minimizes conflicts.

[[dse]]
== Domino Standard Environment (DSE)

The Domino Standard Environment (DSE) is available with standard libraries and packages. This environment has a much larger image size but many common packages for data scientists. It can handle common data science workflows out of the box and includes the most common Python and R packages.

The DSE offers a version that includes FileSystem in Userspace (FUSE) binaries, enabling support for Goofys and SSHFS. You can add these commands to your Dockerfile to enable FUSE functionality:

[source,dockerfile]
----
USER root

# Goofys
ADD https://github.com/kahing/goofys/releases/download/v0.24.0/goofys  /usr/bin/
RUN chmod a+x /usr/bin/goofys

# SSHFS
RUN apt-get update && apt-get install -y sshfs &&
    sed -i "s/^#user_allow_other/user_allow_other/" /etc/fuse.conf

USER ubuntu
----

[[cluster-env]]
== Domino Cluster Environments

Domino Cluster Environments work similarly to Domino Standard Environments (DSEs) but include extra libraries for specific cluster types. They offer the same workspace tools and general packages as DSEs to support data science tasks.

Each Domino environment is designed specifically for the corresponding compute cluster environments. They are most effective when used in a distributed compute cluster alongside a Domino Compute Cluster Environment. 

You can use these environments with any Domino execution, such as a Workspace or Job.  Here are a few things to remember about clusters in Domino:

* All environments must maintain identical Python versions across the cluster.
* A cluster won’t work correctly if the worker nodes are not using the appropriate Domino Cluster Environment for cluster workers.
* A compatible Domino Compute Environment for Job or Workspace is also needed for proper functionality.


The link:799193[Compute Environment catalog] has a complete list of pre-built compute environments for you to use with Domino. 

[cols="1a,4a",options="header"]
|===
|Type|Description

|*Domino Spark Environment*
|This includes Scala and Spark, along with the typical features of DSE. 
link:f11f6a[Work with your Spark cluster] has complete instructions on creating simple Spark clusters for use with Domino Spark environments.

|*Domino Ray Environment*
|This combines Ray features with standard DSE tools. 
link:190175[Create a base Ray cluster environment] has complete instructions on creating simple Ray clusters for use with Domino Ray environments.

|*Domino Dask Environment*
|This environment includes Dask on top of the typical DSE functionality. 
link:aaa2c1[Work with clusters] has complete instructions on creating simple Dask clusters for use with Domino Dask environments.

|*Domino MPI Environment*
|Provision and orchestrate an Open MPI cluster directly on the infrastructure backing your Domino deployment.
link:d60880[Distributed GPUs with Open MPI] has complete instructions on enabling and configuring Open MPI on your environment.
|===

[[dge]]
== Domino GPU Environments

The Domino GPU Environment includes CUDA support and common packages for taking advantage of GPUs. Domino recommends using an Environment with explicit GPU support when using GPU hardware tiers.

The link:799193[Compute Environment catalog] has a complete list of pre-built compute environments for you to use with Domino.

== Next steps

* link:fa8137[Create a new Domino Environment] to meet your language and package needs.
* link:5ecbff[Set default Environments] establishes a starting point for all new projects or an environment for a specific project.
* The link:799193[Compute Environment catalog] has information on accessing Domino Environment images. 


----- user_guide/customize-environments/manage-compute-environments/3-create-an-environment.txt -----
:page-version: 6.1
:page-title: Create a Domino Environment
:page-permalink: fa8137
:page-order: 30

This guide shows you three ways to create a new Domino Environment, whether you want to create a core Domino environment, base it on an existing environment, or start from a custom setup.

== Tour the New Environment window

image::/images/6.0/user-guide/create-new-environment.png[alt="Create a new Environment", width=600]

. *Name*: The name you give your environment.
. *Description (Optional)*: Use this to record details about your environment.
. *Base Environment / Image*: There are two options for a base environment/image:
..  *Start from an existing Environment*: Base your new environment on a Domino environment.
..  *Start from a custom base image*: Input a URI to build a custom environment.
. *Supported Clusters*: Lists the currently supported types of clusters.
. *Visibility*: Choose a visibility level: Private, Available to an Organization, or Globally Accessible (admin only). 
. *Customize before building*: Use this to add custom features to your environment.
. *Create Environment*: Validates and creates your new environment.

== Create environment based on Domino Core Environment
Domino comes with the link:0d73c6[Domino Core Environment] installed, but you can create a new Domino Environment using the available environment images. 

[[tr8]]

. Go to *Govern* > *Environments* and click *+ Create Environment*.
. Fill in the following fields:
.. *Name*: Name your environment according to the naming convention of your organization.
.. *Description (Optional)*: Enter a meaningful description of the environment. For example, you could add details like the OS, Python version, and whether the environment uses Jupyter, JupyterLab, VS Code, etc. 
. Choose *Start from an existing Environment* for your *Base Environment / Image*.
.. The global default for your environment is pre-selected. 
.. Scroll until you can select the *Domino Core Environment*.
. Choose an option for any *Supported Clusters*.
. Decide on your environment's Visibility level: *Private*, *Available to an Organization*, or *Globally Accessible* (admin only).
. Click *Create Environment*. The *Revisions* tab for the environment opens. 

To customize the environment to enable tools, run scripts, or manage environment variables, click *Customize before Building* and use the link:5dd2c1[Customize your Environment] guide. 

You can also click *Edit Definition* to customize the environment later.

== Create environment based on existing environment
You can create a new Domino Environment by duplicating existing environments, or by choosing a base environment from the *Start from existing environment* menu.

After building or editing an environment, use the *Auto-assign next new revision as Active* toggle in the *Revisions* tab to control which revision is active. When enabled, the latest successful build becomes *Active* automatically. When disabled, the environment stays on the current *Active* revision, even as new ones are built.

image::/images/6.0/user-guide/AutoassignRevision.png[alt="Auto-assign next new revision as Active", width=700, role=noshadow]

Suppose your environment updates automatically due to a subscription, but you have locked its *Active* revision to a specific version. In that case, you will see a notice when the base environment’s *Active* revision changes. This reminds you to consider updating your environment's revision.

You can keep a chosen version of your environment as *Active* while still receiving updates on the base environment. This lets you test the new version before connecting your Jobs, Apps, and Model Endpoints to it.

Please note that archived images will not be rebuilt automatically.

=== Create environment by duplication
To create a new environment by duplicating an existing environment:

. Go to *Govern* > *Environments*.
. Choose the environment you want to copy and click the three dots on the right.
. Choose *Duplicate Environment* from the menu.
. Rename it clicking the edit icon and then entering when done.
. You can edit other fields quickly to customize them: 
.. *Edit Description*: Click *Update* when you are done with the description.
.. *Edit Visibility*: Click *Update Visibility* when finished.

Use the link:5dd2c1[Customize your Environment] article for guidance to enable tools, run scripts, or manage environment variables. 

== Create environment based on custom images 
Choose your link:71a047[custom Docker or container image] before creating a new environment and copy the URI. 

. Go to *Govern* > *Environments* and click *+Create Environment*.
. Fill in the following fields:
.. *Name*: Name your environment according to the naming convention of your org
.. *Description (Optional)*: Enter a meaningful description of the environment. For example, you could add details like the OS, Python version, and whether the environment uses Jupyter, Jupyterlab, VScode, etc. 
. Choose *Start from a custom base image* for your *Base Environment / Image*.
.. *FROM*: Paste the custom Docker or container image URI here.
.. *Optional*: Review the information about the *Automatically make compatible* field.
. Choose an option for any *Supported Clusters*.
.  Decide on your environment's Visibility level: *Private*, *Available to an Organization*, or *Globally Accessible* (admin only).
. Click *Create Environment*. The *Revisions* tab for the environment opens.

To customize the environment to enable tools, run scripts, or manage environment variables, click *Customize before Building* and use the link:5dd2c1[Customize your Environment] guide. 

You can also click *Edit Definition* to customize the environment later.

== Archive an environment

You can archive environments as long as they are not the default environment for a project. Archiving an environment does not affect reproducibility; executions based on it will still run. 

. Go to *Govern* > *Environments*.
. Choose an environment from the list.
. There are two ways to archive an environment quickly:
.. You can click the 3 dots within the list and choose *Archive Environment*.
.. Choose *Archive Environment* from the menu. You can also archive from any tab within the *Environment* details. 

== Next Steps

* link:5dd2c1[Customize your Environment] to enable tools, run scripts, or manage environment variables.
* link:5ecbff[Set default Environments] establishes a starting point for all new projects or an environment for a specific project.
* link:00f51f[Use the GitHub Copilot VS Code extension] to enhance your coding in a Domino Environment.

----- user_guide/customize-environments/manage-compute-environments/4-use-environment-revision-controls.txt -----
:page-permalink: 027d91
:page-version: 6.1
:page-title: Use Environment Revision Controls in Domino
:page-sidebar: Use Environment Revision Controls
:page-order: 35

Domino enables you to manage environment inheritance and revision lifecycles with features such as build propagation, revision pinning, and revision selection. These tools help ensure the automatic propagation of environment updates, allowing for safe testing of changes, and giving you control over which revisions are used across your workflows.

This approach improves environment lifecycle management in two key ways:

* **Better governance and compliance**, since environment updates can be cascaded through derived environments.
* **More robust automation**, with the ability to test updated environments before production workloads adopt them.

Together, these capabilities make it easier to scale, standardize, and maintain reliable environments across teams and projects.

== Prerequisites

* You must have permission to create or edit compute environments.
* You should be familiar with how base and derived environments work in Domino.

== Step 1: Select a base revision (revision selection)

When creating or editing an environment:

. Go to **Environments** > **Create Environment** or open an existing one.
. Under **Base Environment**, select a base environment.
. In the **Revision** dropdown, you can:
.. Choose a specific revision of the base, or
.. Select **Active - auto-rebuild** to subscribe to the base's current Active revision.
+
Choosing **Active - auto-rebuild** enables build propagation. Any change to the Active revision of the base environment will trigger an automatic rebuild of this environment.

== Step 2: Manage automatic activation of revisions (revision pinning)

You can control whether the latest revision becomes Active automatically or if activation must be manual.

When creating a new environment::
In the setup form, check or uncheck **Auto-assign latest revision as Active**:

* **Checked:** New revisions are automatically set to Active.
* **Unchecked:** You must manually activate revisions (i.e., the Active revision is pinned).

For existing environments::
If using build propagation, uncheck this option to avoid automatically activating new, untested revisions:

. Go to the **Revisions** tab.
. Uncheck **Auto-assign latest revision as Active** to pin the current Active revision.
. Manually promote future revisions when ready.

== Step 3: Use build propagation

If you selected **Active - auto-rebuild** in Step 1:

* Any time the base environment's Active revision changes, a new revision of your environment will be built automatically.
* If the environment has *Auto-assign latest revision as Active* toggled *on*, this new revision becomes Active.
* If the environment has *Auto-assign latest revision as Active* toggled *off* (i.e., the Active revision is pinned), the new revision must be activated manually.

If the environment has *Auto-assign latest revision as Active* toggled *on*, the new auto-rebuilt revision is automatically set to Active and consequently propagates builds to any of its subscribed environments. This way you can achieve chained propagation across multiple levels of environment inheritance.

== Next Steps

* link:5dd2c1[Customize an Environment] to enable tools, run scripts, or manage environment variables.
* link:[Set a default Environment] to address particular package or IDE requirements.
* Enhance your coding by using the link:00f51f[GitHub Copilot VS Code extension] in a Domino Environment.
* link:6ac5a1[Manage Environment variables] to help configure how the system runs and manage secrets or settings across different areas.
----- user_guide/customize-environments/manage-compute-environments/5-edit-environment-definition.txt -----
:page-version: 6.1
:page-title: Customize your Environment 
:page-permalink: 5dd2c1
:page-order: 40

You can customize an environment to enable tools, run scripts, or manage environment variables. You may need to be an administrator to edit an existing environment.

* Click *Customize before Building* if you just created an Environment and want to enable tools, run scripts, or manage environment variables.
* To edit an existing Environment, go to *Govern* > *Environments* in the navigation pane and click the name of an Environment. Then click *Edit Definition*.

== Order of script execution

Domino runs setup scripts before and after running the `requirements.txt` file. The scripts within an environment are executed in the following order: 

* First, it executes *pre-setup scripts*. 
* Next, it installs packages from the `requirements.txt` file.
* Then Domino runs *post-setup* and *pre-run scripts*. 
* *Post-run scripts* are executed after your Workspace, Job, or App is stopped.

However, no scripts are run before or after tasks in a distributed compute cluster environment, such as Spark, Ray, Dask, or MPI. 

== Define your environment

Before you add items to these scripts and the `requirements.txt` file, think about whether similar instructions should be included directly in the environment's Dockerfile.

This can make it faster to start workspaces, jobs, and other tasks using the environment because the necessary work is already included in the Docker image.

image::/images/6.1/admin-guide/domino-environment-subscribed.png[alt="Domino Environment Overview Tab", width=600, role=noshadow] 

You can start by going to *Govern* > *Environments* and selecting the environment you want to define from the list. Then click *Edit Definition*.

On the *Overview* tab, you can update the *Description* or change the environment's Visibility. You won’t be able to change the *Environment Base* or *Supported Cluster Settings*. 

=== Dockerfile instructions

Enter your Dockerfile layers. Some things to remember: 

* Exclude the ``FROM`` statement.
* Commands can be run as root or other user available in the base image.
* Docker can’t access your project files or user environment variables when building.
* There is a limit of 127 layers.

https://docs.docker.com/engine/reference/builder/[Docker's official site] has more detailed information about Docker.

=== Pluggable workspace tools

Enter the interactive tools needed by any Project that uses this Environment. link:03e062[Add Workspace IDEs] has more information about interactive tools.

=== Run setup scripts

Enter the `bash` code that is executed at runtime. Domino executes these commands at runtime and runs them as root.

* *Pre Run Script*: Enter commands to execute before the Python packages in your Project’s `requirements.txt`are installed.
* *Post Run Script*: Enter commands to execute after your Workspace/Job/App is stopped.

=== Environment variables

Set the *Variable Names* and their *Values*.

=== Advanced

In the advanced section, enter the `bash` code executed at a specified step in your experiment’s lifecycle.

* *Pre Run Script*: Enter commands to execute before installing Python packages in your Project’s `requirements.txt`.
* *Post Run Script*: Enter commands to execute after the Python packages in your Project's `requirements.txt` are installed
* *Enable VPN Networking*: Select this if a VPN is to be used for that environment.

=== Revision History

Enter a short description of the changes to the Environment.

== Next steps

* link:6ac5a1[Use Domino’s environment variables] to store keys and credentials or to inject values into your runs.
* link:fa8137[Create a Domino Environment] using an existing environment or start from a custom setup.
* link:5ecbff[Set default Environments]  establishes a starting point for all new projects or an environment for a specific project.






[[tr12]]

[[tr22]]

[[tr23]]

[[tr24]]
[[tr25]]
[[tr29]]

[[tr26]]

[[tr27]]

[[tr28]]


----- user_guide/customize-environments/manage-compute-environments/6-set-a-default-environment.txt -----
:page-version: 6.1
:page-title: Set a default Environment
:page-permalink: 5ecbff
:page-order: 50

You can set a project-specific environment as the default for your project. This might address particular package or IDE requirements. The project owner can choose the default environment for that project.

When you launch a job or create a new workspace, Domino automatically populates the fields with the compute environment defaults from the project’s settings.

[[set-env-default-project]]
== Set default environment for a project

Adjust the project’s Compute Environment settings to replace the global default with one tailored to your project’s specific needs:

. Go to *Develop* > *Projects*.
. Choose the project that you want to set a default environment for.
. In the navigation pane, click *Settings* > *Hardware & Environment*.
. From *Compute Environment*, select the environment you want to use as the default for your project.
. Click *Manage Environment* to see your environment's details.

== Next steps

* link:fa8137[Create a Domino Environment] using an existing environment or start from a custom setup.
* link:5dd2c1[Edit an Environment] to enable tools, run scripts, or manage environment variables.
* link:00f51f[Use the GitHub Copilot VS Code extension] to enhance your coding in a Domino Environment.

----- user_guide/customize-environments/manage-compute-environments/8-compute-environment-catalog.txt -----
:page-permalink: 799193
:page-version: 6.1
:page-title: Compute Environment Catalog
:page-order: 80

[[compute-env]]

Domino is an open platform for data science which integrates various languages, IDEs, data sources, and tools in one place.

You can add various environments to any Domino installation by visiting quay.io. Download the image from the URL link to the repository. Our documentation provides instructions for setting up these environments.

You can also use Domino to automatically adjust Docker images so they work with Domino Workspaces and Jobs. You can find more information, including the requirements for these images, in link:5babee[Use External Images in Domino Environments].

[cols="^1a,5a",options="header"]
|===
|Partner |Product/Version

//DSE
^| image:/images/logos/domino-new.png[alt="Domino logo", width=100, role=noshadow]
| link:0d73c6#dse[Domino Standard Environment (DSE)]

* `quay.io/domino/domino-standard-environment:ubuntu22-py3.10-r4.5-domino6.1-standard`

//DCE
^| image:/images/logos/domino-new.png[alt="Domino logo", width=100, role=noshadow]
| link:0d73c6#dce[Domino Core Environment (DCE)]

* `quay.io/domino/domino-core-environment:ubuntu22-py3.10-domino6.1-core`

//DME
^| image:/images/logos/domino-new.png[alt="Domino logo", width=100, role=noshadow]
| link:0d73c6[Domino Minimal Environment (DME)]

* `quay.io/domino/domino-minimal-environment:ubuntu22-py3.10-domino6.1-minimal`

//DGE
^| image:/images/logos/domino-new.png[alt="Domino logo", width=100, role=noshadow]
| link:0d73c6#dge[Domino GPU Environment (DGE)]

* `quay.io/domino/domino-gpu-environment:ubuntu22-py3.10-domino6.1-gpu`

//Spark
^| image:/images/logos/domino-new.png[alt="Domino logo", width=100, role=noshadow]
| link:0d73c6#spark-env[Domino Spark Environment]

* `quay.io/domino/domino-spark-environment:ubuntu22-py3.10-r4.5-spark3.5.4-domino6.1` +
* `quay.io/domino/spark-cluster-environment:spark3.5.4-py3.10-domino6.1`

//Dask
[[dask-env]]
^| image:/images/logos/domino-new.png[alt="Domino logo", width=100, role=noshadow]
| link:0d73c6#dask-env[Domino Dask Environment]

* `quay.io/domino/domino-dask-environment:ubuntu22-py3.10-r4.5-dask2024.12.1-domino6.1` +
* `quay.io/domino/dask-cluster-environment:dask2024.12.1-py3.10-domino6.1`

//MPI image
^| image:/images/logos/domino-new.png[alt="Domino logo", width=100, role=noshadow]
| link:d60880[On-Demand Open MPI]

* `quay.io/domino/compute-environment-images:ubuntu20-py3.9-r4.2-mpi-4.1.3-domino-5.2`
* `quay.io/domino/domino-mpi-environment:ubuntu22-py3.10-r4.4-mpi4.1.4-domino6.1`

^| image:/images/logos/intel-ai-logo2.png[alt="Intel AI logo", width=100, role=noshadow]
| link:35f28e[Intel AI Analytics Toolkit (AI Kit)]
* `intel/oneapi-aikit`

^| image:/images/logos/matlab.png[alt="MATLAB logo", width=100, role=noshadow]
| link:f08799[MATLAB 2021a]

* `quay.io/domino/matlab:R2021a`

^| image:/images/logos/matlab.png[alt="MATLAB logo", width=100, role=noshadow]
| link:f08799[MATLAB 2020b]

* `quay.io/domino/matlab:R2020b`

^| image:/images/logos/matlab.png[alt="MATLAB logo", width=100, role=noshadow]
| link:f08799[MATLAB 2020a]

* `quay.io/domino/matlab:R2020a`

^| image:/images/logos/matlab.png[alt="MATLAB logo", width=100, role=noshadow]
| link:f08799[MATLAB 2019b]

* `quay.io/domino/matlab:R2019b`

^| image:/images/logos/nvidia-ngc.png[alt="NVIDIA NGC logo", width=100, role=noshadow]
| * link:6fdcff[Container images from the NVIDIA NGC Catalog]

//Ray
^| image:/images/logos/domino-new.png[alt="Domino logo", width=100, role=noshadow]
| link:0d73c6#dre[Domino Ray Environment]

* `quay.io/domino/domino-ray-environment:ubuntu22-py3.10-r4.5-ray2.43.0-domino6.1` +
* `quay.io/domino/ray-cluster-environment:ray2.43.0-py3.10-domino6.1` +
* `quay.io/domino/ray-cluster-gpu-environment:ray2.43.0-py3.10-domino6.1-gpu`

^| image:/images/logos/snowflake.png[alt="Snowflake logo", width=100, role=noshadow]
| link:d4ef2b[Snowflake Snowpark]

* `quay.io/domino/snowflake:snowpark-latest`

^| image:/images/logos/stata_logo_blue.png[alt="Stata logo", width=100, role=noshadow]
| link:063fca[Stata 17]

* `quay.io/domino/stata:17`
|===

----- user_guide/customize-environments/manage-compute-environments/index.txt -----
:page-version: 6.1
:page-title: Manage Compute Environments
:page-permalink: f51038
:page-order: 10

Domino Environments, or Compute Environments, help teams manage their projects better. They improve processes in MLOps, generative AI, and data science workflows.

In Domino, environments are defined by Docker images that create isolated containers for each execution unit including Workspaces, Runs, Apps, and Models. This setup allows professionals to experiment and customize their work in a contained, versioned, and shareable space, boosting reproducibility and collaboration.

== Basics of Domino Environments

You can create new environments, use those provided by Domino, or modify existing ones. You might choose to create or alter an environment if:

* You must install a package for Python, R, Octave, or some other software dependency.
* You want to cache a package or library so that it's immediately available when you start a run.
* You manage an organization and want to create a default Environment for all projects.

Domino has two main types of Compute Environments: the Domino Standard Environment (DSE) and the Domino Minimal Environment (DME). The DSE offers a complete set of libraries and packages, while the DME is lighter and includes fewer packages.

Domino also provides Cluster Environments, which work like DSEs but are designed for specific types of clusters. The environments are smaller and simpler, suitable only for worker nodes in a cluster, while DSEs cannot be used for worker nodes.

== View accessible environments
You can view the environments you can access by clicking *Govern* > *Environments* in the navigation bar.

In the *Environments Overview* page, you can see a list of:

* Global Environments.
* Subscribed Environments view, which displays environments subscribed to a base, along with their associated build propagation behavior.
* Specific environments used by Projects you link:d7731d[collaborate] on.
* Environments shared with link:526a62[organizations] you belong to.

Click an environment's name to see its details.

image::/images/6.0/environment-overview-pane6.1.png[alt="Environment Overview Pane", width=500, role=noshadow]

[%header,cols="1,2"]
|===
|Field
|Description

|*Overview*
|The Overview tab lists all metadata about your environment or image. If your environment is built on top of another, you may need to click through to the parent environment before seeing the underlying Docker image. Click link:5dd2c1[Edit Definition] if you need to make changes to your environment.

|*Revisions*
|All revisions of your compute environment are listed here, along with their build status, timestamp, and Docker image URI. Each saved change increments the revision number by one. By default, the latest successfully built revision is automatically set as *Active*. If you’d like to keep a specific revision active instead, you can disable the *Auto-assign next new revision as Active* toggle to lock your chosen revision in place. Domino rebuilds the environment and pushes the resulting image to the local Docker registry. By default, the latest successfully built revision becomes the Active revision.

The list also indicates whether a revision was built automatically via build propagation, and includes metadata about its parent revision—specifically, which revision it was derived from and whether that parent is currently Active. This context helps track inheritance chains and understand how updates cascade across environments.

|*Revision details*
|Click the three vertical dots for additional options like viewing build logs, downloading an image source project, or canceling builds.

|*Projects*
|These are listed after being assigned to an environment. You can view details of the project by clicking its name.

|*Endpoints*
|Shows you a list of endpoints for that environment, including the status of the endpoint.

|===

== Next steps

* link:0e4da9[Our Best Practices] will guide you in setting up and maintaining your Domino environments.
* link:0d73c6[Domino Standard Environments] describes the pre-configured environments that come with Domino.
* link:fa8137[Create a Domino Environment] using an existing environment or start from a custom setup.
* link:027d91[Use Environment Revision Controls in Domino] to manage environment inheritance and revision lifecycles.
* link:5dd2c1[Customize your Environment] to enable tools, run scripts, or manage environment variables.
* link:5ecbff[Set default Environments] establishes a starting point for all new projects or an environment for a specific project.
* link:748f25[Use Environment Revision Controls in Domino]to manage environment inheritance and revision lifecycles with build propagation, revision pinning, and revision selection.
* link:799193[Compute Environment Catalog] provides images to add various environments to any Domino installation.
* link:6ac5a1[Manage environment variables] to configure how the system runs and manage secrets or settings across different areas.
----- user_guide/customize-environments/manage-compute-environments/manage-environment-variables/1-compute-environment-variables.txt -----
:page-permalink: b60bf6
:page-version: 6.1
:page-title: Compute Environment Variables
:page-order: 10

Compute Environment variables provide user, project, and hardware information. These are set at the environment level and used for new revisions. 

Environment variables help configure how the system runs and manage secrets or settings across different areas.

== Navigate to environment variables screen

There are two ways to navigate to your environment to add variables.

*Option 1 - From your project*:

. Open your *Project* and click *Settings*. 
. Click *Manage Environment* > *Edit Definition*.

*Option 2 - From Domino Overview*:

. Open Domino and click *Govern* > *Environments*.
. From *Environments Overview*, scroll to locate your Environment.
. Click the three dots and choose *Edit Definition*.

image::/images/6.0/domino-overview-variables.png[alt="Access variables through main", width=600, role=noshadow]

== Add compute environment variables

On the *Edit Definition* page, scroll down to the *Environment variables* section.

image::/images/6.0/add-variables-comp-env.png[alt="Add compute environment variables", width=600, role=noshadow]

. Click the plus icon and add the following: 
.. *Variable Name*: Enter the variable name. For example, API_KEY.
.. *Value*: Enter a value for the variable. The maximum length for a value is 64K.
. Declare the variable in the Dockerfile instructions with an ARG statement. 
https://docs.docker.com/engine/reference/builder/[Docker’s official site] has more detailed information about Docker.
. Click *Build* after entering new variables and updating the Dockerfile instruction.

== Next Steps

* link:0e4da9[Our Best Practices] will guide you in setting up and maintaining your Domino environments.
* link:910b3a[Set up user environment variables] for each user. The system injects these variables at execution time for any run initiated by that user. 
* Learn to link:66b789[create model variables] to add, modify, or delete variables for your models. 
* The link:55fc89[Default Domino Variables] glossary contains details about variables automatically injected by Domino.
----- user_guide/customize-environments/manage-compute-environments/manage-environment-variables/2-build-project-variables.txt -----
:page-permalink: d7bac0
:page-version: 6.1
:page-title: Project Variables
:page-order: 20

Project owners or collaborators can add, modify, or delete project level environment variables.
These variables are stored securely and can hold sensitive information, such as credentials. 

image::/images/6.0/set-environment-variables.png[alt="Set environment variables", width=600, role=noshadow]

. Open your *Project* and click *Settings*. 
. In the *Environment variables* section, add the following: 
.. *Name*: Enter the variable name. For example, `YOUR_NAME_SHARED_KEY`.
.. *Value*: Enter a value for the variable. The maximum length for a value is 64K.
. Click *Set Variable*.

You can send the values exactly as they are without adding escaping characters.

== Next steps

* link:0e4da9[Our Best Practices] will guide you in setting up and maintaining your Domino environments.
* link:910b3a[Set up user environment variables] for each user. The system injects these variables at execution time for any run initiated by that user. 
* Learn to link:66b789[create model variables] to add, modify, or delete variables for your models. 
* The link:55fc89[Default Domino Variables] glossary contains details about variables automatically injected by Domino.
----- user_guide/customize-environments/manage-compute-environments/manage-environment-variables/3-create-model-variables.txt -----
:page-permalink: 66b789
:page-version: 6.1
:page-title: Model Variables
:page-order: 30

Only model owners or editors can create environment variables. When you set environment variables for active Endpoints, you’ll need to restart the Endpoints so they will be recognized. 

In Domino 5.11, when you add a https://docs.dominodatalab.com/en/5.11/user_guide/6ac5a1/environment-variables/[model variable], the values are pushed to all running model versions.

Project-level and user-level environment variables are not used in models and must be set separately for each model. 

image::/images/6.0/model-env-variables.png[alt="Model environment variables", width=800]

. Open your *Model* and click *Settings*. 
. In the *Environment variables* section, add the following: 
.. *Name*: Enter the variable name. For example, `YOUR_NAME_MODEL_KEY`.
.. *Value*: Enter a value for the variable. The maximum length for a value is 64K.
. Click *Set Variable*.

== Next steps

* link:0e4da9[Our Best Practices] will guide you in setting up and maintaining your Domino environments.
* link:910b3a[Set up user environment variables] for each user. The system injects these variables at execution time for any run initiated by that user. 
* Learn to link:d7bac0[build project variables] to add, modify, or delete environment variables for your projects. 
* The link:55fc89[Default Domino Variables] glossary contains details about variables automatically injected by Domino.

----- user_guide/customize-environments/manage-compute-environments/manage-environment-variables/4-setup-user-environment-variables.txt -----
:page-permalink: 910b3a
:page-version: 6.1
:page-title: User Environment Variables
:page-order: 40

In Domino, you can create, update, or delete your private environment variables. These variables are stored securely in Vault and only you can see them. The system injects these variables at execution time for any run initiated by you. 

User environment variables are automatically imported into runs for all projects. However, user-specific environment variables cannot be used in models.

image::/images/6.0/user-environment-variables.png[alt="Set user environment variables", width=600, role=noshadow]

. Click *Account > Account Settings* to open the Account Settings page.
. Choose *User environment variables* from the list.
. In the *Environment variables* section, add the following: 
.. *Name*: Enter the variable name. For example, `YOUR_NAME_ENV_KEY`.
.. *Value*: Enter a value for the variable. The maximum length for a value is 64K.
. Click *Set Variable*.

You can send the values exactly as they are without adding escaping characters. 

== Access Environment variables
Every language reads environment variables uniquely. 

* https://docs.python.org/2/library/os.html#os.environ[In Python], it might look like this:

[source,python]
----
import os
s3 = S3Client(os.environ['S3_KEY'], os.environ['S3_SECRET'])
----
Run this to retrieve an environment variable within your code for Python:
[source,python]
----
import os
os.environ['DOMINO_RUN_ID']
----

* http://stat.ethz.ch/R-manual/R-patched/library/base/html/Sys.getenv.html[In R], it might look like this:

[source,r]
----
makeS3Client(Sys.getenv("S3_KEY"), Sys.getenv("S3_SECRET"))
----
Run this to retrieve an environment variable within your code for R:

[source,r]
----
Sys.getenv("DOMINO_RUN_ID")
----

== Next steps

* link:0e4da9[Our Best Practices] will guide you in setting up and maintaining your Domino environments.
* link:910b3a[Set up user environment variables] for each user. The system injects these variables at execution time for any run initiated by that user. 
* Learn to link:d7bac0[build project variables] to add, modify, or delete environment variables for your projects. 
* The link:55fc89[Default Domino Variables] glossary contains details about variables automatically injected by Domino.
----- user_guide/customize-environments/manage-compute-environments/manage-environment-variables/5-glossary-default-variables.txt -----
:page-permalink: 55fc89
:page-version: 6.1
:page-title: Default Domino Environment Variables Glossary
:page-sidebar: Default Environment Variables Glossary
:page-order: 50

Domino automatically injects the following environment variables whenever it runs your code as part of the context of your run.

Using these variables, you can create artifacts or results, such as a report mentioning the run number. Default variables are not available to use in the Model Manager.

[cols="3a,4a",options="header"]
|===
|Variable|Description

| `AWS_SHARED_CREDENTIALS_FILE`
| Path to your AWS credential file to connect to additional AWS resources (for example, S3, Redshift). See link:eb6a88[AWS credential propagation].

| `DOMINO_API_HOST`
| Use this to use the link:f35c19[Domino API] to access another project.

| `DOMINO_ARTIFACTS_DIR`
| Directory where Artifacts files system is accessed.

| `DOMINO_DATASETS_DIR`
| Directory where Datasets are accessed.

| `DOMINO_GROUP`
| Value of the default Linux group name in the container, i.e. `ubuntu`.

| `DOMINO_GROUP_ID`
| Value of default Linux group ID in the container, i.e. `12574`.

| `DOMINO_HARDWARE_TIER_ID`
| Hardware tier the current run is executing on (new in v1.42).

| `DOMINO_IMPORTED_ARTIFACTS_DIR`
| Directory where imported Artifacts file systems are accessed.

| `DOMINO_IMPORTED_CODE_DIR`
| Directory where imported code repositories are accessed.

| `DOMINO_IMPORTED_DATA_DIR`
| Directory where imported Datasets are accessed.

| `DOMINO_IS_GIT_BASED`
| Boolean value indicating if the Project is Git-based.

| `DOMINO_IS_LOCAL_DATA_PLANE`
| Boolean value indicating if Datasets are associated with a local data plane.

| `DOMINO_IS_WORKFLOW_JOB`
| Boolean value indicating if the job was orchestrated via a Flow.

| `DOMINO_NODE_IP`
| The compute node IP.

| `DOMINO_NODE_NAME`
| Name of the compute node.

| `DOMINO_PROJECT_ID`
| Project ID of the current project.

| `DOMINO_PROJECT_NAME`
| Name of the running project.

| `DOMINO_PROJECT_OWNER`
| Username of the owner of the running project.

| `DOMINO_RUN_ID`
| Run ID of the current run.

| `DOMINO_RUN_NUMBER`
| Run number of the current run.

|  `DOMINO_STARTING_USERNAME`
| Username of the user who began the run (new in v1.43).

| `DOMINO_TOKEN_FILE`
| No longer exists as an environment variable. Instead, link:40b91f[Use a Token for Authentication].

| `DOMINO_TRAINING_SET_PATH` 
| Directory where Domino Model Monitoring training sets are accessed.

| `DOMINO_USER`
| Value of the default Linux username in the container, i.e. `ubuntu`.

| `DOMINO_USER_API_KEY`
| This variable is sunsetting; use the link:40b91f[API Proxy for Domino API authentication] instead.

| `DOMINO_USER_ID`
| Value of the default Linux user ID in the container, i.e. `12674`.

| `DOMINO_USER_NAME`
| Domino Username of the person running the Workspace/Job.

| `DOMINO_WORKING_DIR`
| Working directory for the running project.
|===

== Next steps

* link:0e4da9[Our Best Practices] will guide you in setting up and maintaining your Domino environments.
* link:910b3a[Set up user environment variables] for each user. The system injects these variables at execution time for any run initiated by that user. 
* Learn to link:d7bac0[build project variables] to add, modify, or delete environment variables for your projects. 

----- user_guide/customize-environments/manage-compute-environments/manage-environment-variables/index.txt -----
:page-version: 6.1
:page-title: Manage Environment Variables 
:page-order: 180
:page-permalink: 6ac5a1

Domino allows users to set environment variables at four levels: Compute Environments, Projects, Models, and User. These variables help configure how the system runs and manage secrets or settings across different areas.

* link:b60bf6[Compute Environment variables] provide user, project, and hardware information. These are set at the environment level and used for new revisions.
* link:d7bac0[Project Environment variables] are set at the project level and shared with all project collaborators. They help with configurations specific to the project and ensure consistency in all runs within that project.
* link:66b789[Model Level variables] are used for deployed models. They help manage settings needed for running the model, like API keys or endpoints for external services.
* link:910b3a[User Level variables] are specific to each user and apply to all projects and tasks they start. They are helpful for personal settings like login details or default preferences.

You can create your own or use Domino’s environment variables to add values to your runs. These variables must start with letters and can only include letters, numbers, and underscores. 

== Hierarchy of variables
You can use the same variable in different places. When Domino loads a run or workspace, it pulls environment variables from these sources in this order:

. *Compute Environment variables*
. *Project environment variables*
. *User Level variables*

Here is an example of the expected run results with the same variable in multiple places:

[cols="2a,1a,1a,1a",options="header"]
|===
|Place set|Run #1|Run #2|Run #3

|Compute Environment
|A
|A
|A

|Project
|-
|B
|B

|User Account
|-
|-
|C

|*Run Result*
|*A*
|*B*
|*C*
|===

== Reserved variable names in Domino
Domino uses specific environment variables to configure and manage operations. These variables should not be defined at the user or project level. Here are a few generic ones:

* `USER`
* `LOGNAME`
* `MLFLOW_TRACKING_URI`

In addition to the above, any variable that starts with the following is reserved: 

* `DOMINO_*`
* `KUBERNETES_*`
* `NEW_RELIC_*`
* `DATA_PLANE_*`
* `MINICONDA_*`
* `FLYTE_*`
* `HEPHAESTUS_*`
* `SELDON_*`
* `CONDA_*`
* `DISTRIBUTED_COMPUTE_*`

We recommend using a common prefix, such as YOUR_COMPANY NAME_, for all custom environment variables to avoid collisions with link:55fc89[Domino environment variables].

== Next steps

* The link:55fc89[Default Domino Variables] glossary contains details about variables automatically injected by Domino.
* link:910b3a[Set up user environment variables] for each user. The system injects these variables at execution time for any run initiated by that user. 
* Learn to link:d7bac0[build project variables] to add, modify, or delete environment variables for your projects. 
* Learn to link:66b789[create model variables] to add, modify, or delete variables for your models. 

----- user_guide/customize-environments/manage-compute-environments/use-environment-revision-controls.txt -----
:page-permalink: 748f25
:page-version: 6.1
:page-iversion: 610 
:page-title: Use Environment Revision Controls in Domino
:page-order: 70

Domino enables you to manage environment inheritance and revision lifecycles with build propagation, revision pinning, and revision selection. These tools help ensure the automatic propagation of environment updates, allowing for safe testing of changes, and giving you control over which revisions are used across your workflows.

This approach improves environment lifecycle management in two key ways:

* Better governance and compliance, since environment updates can be cascaded through derived environments.
* More robust automation, with the ability to test updated environments before production workloads adopt them.

Together, these capabilities make it easier to scale, standardize, and maintain reliable environments across teams and projects.

== Prerequisites

* You must have permission to create or edit compute environments.
* You should be familiar with how base and derived environments work in Domino.

== Step 1: Select a base revision (revision selection)

When creating or editing an environment:

. Go to *Environments* > *Create Environment* or open an existing one.
. Under *Base Environment*, select a base environment.
. In the *Revision* dropdown, you can:
* Choose a specific revision of the base, or
* Select *Active - auto-rebuild* to subscribe to the base's current Active revision.

Choosing *Active - auto-rebuild* enables build propagation. Any change to the Active revision of the base environment will trigger an automatic rebuild of this environment.

== Step 2: Manage automatic activation of revisions (revision pinning)

You can control whether the latest revision becomes Active automatically or if activation must be manual.

When creating a new environment:

. In the setup form, check or uncheck *Auto-assign latest revision as Active*:
* *Checked*: New revisions are automatically set to Active.
* *Unchecked*: You must manually activate revisions (i.e., the Active revision is pinned).

For existing environments:

If using build propagation, uncheck this option to avoid automatically activating new, untested revisions:

. Go to the *Revisions* tab.
. Uncheck *Auto-assign latest revision as Active* to pin the current Active revision.
. Manually promote future revisions when ready.

== Step 3: Use build propagation

If you selected *Active - auto-rebuild* in Step 1:

* Any time the base environment’s Active revision changes, a new revision of your environment will be built automatically.
* If revision pinning is off, this new revision becomes Active.
* If revision pinning is on, the new revision must be activated manually.

This supports chained propagation across multiple levels of environment inheritance.

== Next Steps
* link:0d73c6[Domino Standard Environments] describes the pre-configured environments that come with Domino. 
* link:fa8137[Create a Domino Environment] using an existing environment or start from a custom setup.
* link:5dd2c1[Edit an Environment] to enable tools, run scripts, or manage environment variables.
* link:5ecbff[Set default Environments] establishes a starting point for all new projects or an environment for a specific project.
----- user_guide/customize-environments/use-external-images/automatic-custom-image-compatibility.txt -----
:page-version: 6.1
:page-permalink: 23187f
:page-title: Pre-requisites for automatic custom image compatibility
:page-sidebar: Automatic custom image compatibility
:page-order: 70

[[compulsory_deps]]
The following requirements must be present in custom images for use with the automatic compatibility feature:

* `bash` 4.2+
* `shadow-utils`, including
** `groupadd`
** `useradd`
* `glibc` 2.23+, including:
** `libc.so.6`
** `libpthread.so.0`
** `libcrypt.so.1`
** `libutil.so.1`
** `libdl.so.2`
** `libm.so.6`

These requirements are typically satisfied by images based on the following off-the-shelf distributions of Linux:

* CentOS Linux 7 (or later)
* Debian GNU/Linux 10 (or later)
* Red Hat Enterprise Linux 8.2 (or later)
* Ubuntu 16.04.2 LTS (or later)

[NOTE]
====
Alpine Linux distribution images are not officially supported.
Alpine images might work if the previous requirements are met, but this has not been validated.
====

[[workspace_deps]]
The following table shows the requirements that must be present in the image for each of the default Domino Workspace Tools, as well as the requirements for Domino Jobs.
If a tool isn't working, check to see if its requirements are present in the image.
You can modify the base image or add Dockerfile instructions to the Domino Environment if there are missing requirements.
//What are Workspace Tools?

[cols="2a,^1a,^1a",options="header"]
|===
|Workspace Tool |Requires Python |Requires Jupyter

|Domino Jobs |No |No
|VS Code |Yes |No
|JupyterLab (kernels present) |Yes |Yes
|JupyterLab (terminal only) |No |No
|===

[NOTE]
====
* Jobs can be used without Python or Jupyter.
However, if you want to run a Python script in a job, then you need Python in the custom image.
* JupyterLab can be launched even if Python and Jupyter are not installed, but only a basic terminal will be available and the JupyterLab "Launcher" will be empty.
To use a terminal, open JupyterLab and then click *File* > *New* > *Terminal*.
====

----- user_guide/customize-environments/use-external-images/aws-accelerators.txt -----
:page-version: 6.1
:page-title: AWS Trainium and Inferentia silicon accelerators
:page-sidebar: AWS accelerators
:page-permalink: d98a6d
:page-order: 40

Domino supports the use of cost and energy-efficient AWS-designed silicon processors, link:https://aws.amazon.com/machine-learning/trainium/[AWS Trainum^] and link:https://aws.amazon.com/machine-learning/inferentia/[Inferentia^], to accelerate deep-learning model training and AI inference workloads. Use the AWS Neuron SDK to reuse existing code. Learn how to set up Trainium and Inferentia accelerators in your Domino.

Set up involves the following:

. *Node group creation*: Create a new node group for Trainium and Inferentia instances.
. *Device plugin configuration*: Provide hardware-specific settings.
. *Hardware tier setup*: Enable Domino users to use Trainium and Inferentia instances for their workloads.
. *Environment configuration*: Set up the necessary development tools and software libraries.

== Node group creation

To use AWS accelerators, create a new node group that:

- Uses one of the instance types in the following table
- Uses the link:https://docs.aws.amazon.com/eks/latest/userguide/retrieve-ami-id.html[GPU-enabled AMI for your EKS version^]
- Has a unique node pool label identifying its accelerator type

[cols="^1a,^1a,^1a,^1a,^1a",options="header"]
|===
|Name |vCPU |Memory (GiB) |aws.amazon.com/neuron |Total Neuron Memory (GiB)

|inf1.xlarge |4 |8 |1 |8
|inf1.2xlarge |8 |16 |1 |8
|inf1.6xlarge |24 |48 |4 |32
|inf1.24xlarge |96 |192 |16 |128
|inf2.xlarge |4 |16 |1 |32
|inf2.8xlarge |32 |128 |1 |32
|inf2.24xlarge |96 |384 |6 |192
|inf2.48xlarge |192 |768 |12 |384
|trn1.2xlarge |8 |32 |1 |32
|trn1.32xlarge |128 |512 |16 |512
|===

For the cluster-autoscaler to scale your Neuron-based node group successfully, you must tag those autoscaling groups with the Neuron device resource template, like the `k8s.io/cluster-autoscaler/node-template/resources/aws.amazon.com/neuron` tag in the table below:

[cols="3a,3a,^1a",options="header"]
|===
|Key |Value |Tag new instances

|Name |inferentia-test-domino-trn1-Node |Yes
|alpha.eksctl.io/cluster-name |inferentia-test |Yes
|alpha.eksctl.io/eksctl-version | 0.155.0 |Yes
|alpha.eksctl.io/nodegroup-name |domino-trn1 |Yes
|alpha.eksctl.io/nodegroup-type | unmanaged |Yes
|aws:cloudformation:logical-id |NodeGroup |Yes
|aws:cloudformation:stack-id |arn:aws:cloudformation:us-west-2:873872646799:stack/eksctl-inferentia-test-n... |Yes
|aws:cloudformation:stack-name |eksctl-inferentia-test-nodegroup-domino-trn1 |Yes
|eksctl.cluster.k8s.io/v1alpha1/cluster-name |inferentia-test |Yes
|eksctl.io/v1alpha2/nodegroup-name |domino-trn1 |Yes
|k8s.io/cluster-autoscaler/enabled |true |Yes
|k8s.io/cluster-autoscaler/inferentia-test |owned |Yes
|k8s.io/cluster-autoscaler/node-template/label/dominodatalab.com/node-pool |trainium |Yes
|k8s.io/cluster-autoscaler/node-template/resources/aws.amazon.com/neuron |1 |Yes
|kubernetes.io/cluster/inferentia-test |owned |Yes
|===

=== Example eksctl node group config

Here's an example eksctl node group config for Neuron-based node groups:

[source,shell]
----
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: inferentia-test
  region: us-west-2

nodeGroups:
  - name: domino-trn1
    instanceType: trn1.2xlarge
    minSize: 0
    maxSize: 3
    desiredCapacity: 1
    volumeSize: 200
    volumeType: gp3
    availabilityZones: ["us-west-2a"]
    labels:
      "dominodatalab.com/node-pool": "trainium"
    tags:
      k8s.io/cluster-autoscaler/node-template/dominodatalab.com/node-pool: trainium
      k8s.io/cluster-autoscaler/node-template/resources/aws.amazon.com/neuron: 1
    iam:
      withAddonPolicies:
        ebs: true
        efs: true
  - name: domino-inf1
    instanceType: inf1.2xlarge
    minSize: 0
    maxSize: 3
    desiredCapacity: 1
    volumeSize: 200
    volumeType: gp3
    availabilityZones: ["us-west-2a"]
    labels:
      "dominodatalab.com/node-pool": "inferentia"
    tags:
      k8s.io/cluster-autoscaler/node-template/dominodatalab.com/node-pool: inferentia
      k8s.io/cluster-autoscaler/node-template/resources/aws.amazon.com/neuron: 1
    iam:
      withAddonPolicies:
        ebs: true
        efs: true
  - name: domino-inf2
    instanceType: inf2.2xlarge
    minSize: 0
    maxSize: 3
    desiredCapacity: 1
    volumeSize: 200
    volumeType: gp3
    availabilityZones: ["us-west-2a"]
    labels:
      "dominodatalab.com/node-pool": "inferentia2"
    tags:
      k8s.io/cluster-autoscaler/node-template/dominodatalab.com/node-pool: inferentia2
      k8s.io/cluster-autoscaler/node-template/resources/aws.amazon.com/neuron: 1
    iam:
      withAddonPolicies:
        ebs: true
        efs: true
----


== Device plugin deployment

Once your nodes have joined the cluster, deploy the Neuron device plugin DaemonSet using the following specification. You must use version `2.17.3.0` or greater for Domino workloads to be correctly processed by the device plugin.

To deploy this DaemonSet:

. Save the following specification to a file (such as `neuron-device-plugin-ds.yaml`).
. Apply the specification with `kubectl apply -f neuron-device-plugin-ds.yaml`.
+
[source,shell]
----
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: neuron-device-plugin
rules:
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - create
  - patch
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - update
  - patch
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - nodes/status
  verbs:
  - patch
  - update
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: neuron-device-plugin
  namespace: kube-system
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: neuron-device-plugin
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: neuron-device-plugin
subjects:
- kind: ServiceAccount
  name: neuron-device-plugin
  namespace: kube-system
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: neuron-device-plugin-daemonset
  namespace: kube-system
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      name: neuron-device-plugin-ds
  template:
    metadata:
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ""
      creationTimestamp: null
      labels:
        name: neuron-device-plugin-ds
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node.kubernetes.io/instance-type
                operator: In
                values:
                - inf1.xlarge
                - inf1.2xlarge
                - inf1.6xlarge
                - inf1.24xlarge
                - inf2.xlarge
                - inf2.8xlarge
                - inf2.24xlarge
                - inf2.48xlarge
                - trn1.2xlarge
                - trn1.32xlarge
      containers:
      - env:
        - name: KUBECONFIG
          value: /etc/kubernetes/kubelet.conf
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        image: public.ecr.aws/neuron/neuron-device-plugin:2.17.3.0
        imagePullPolicy: Always
        name: k8s-neuron-device-plugin-ctr
        resources: {}
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /var/lib/kubelet/device-plugins
          name: device-plugin
        - mountPath: /run
          name: infa-map
      dnsPolicy: ClusterFirst
      priorityClassName: system-node-critical
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: neuron-device-plugin
      serviceAccountName: neuron-device-plugin
      terminationGracePeriodSeconds: 30
      tolerations:
      - key: CriticalAddonsOnly
        operator: Exists
      - effect: NoSchedule
        key: aws.amazon.com/neuron
        operator: Exists
      volumes:
      - hostPath:
          path: /var/lib/kubelet/device-plugins
          type: ""
        name: device-plugin
      - hostPath:
          path: /run
          type: ""
        name: infa-map
  updateStrategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
    type: RollingUpdate
----
+
. Once the device plugin DaemonSet is deployed, run `kubectl describe node` to confirm that you see device plugin daemons running on your Neuron-based instances, and that they advertise `aws.amazon.com/neuron` resources to Kubernetes.


The following output is an example of a correctly configured Neuron-based node. Note the Neuron device plugin daemon present on the node, the advertised `aws.amazon.com/neuron` resource, and the Domino node pool label identifying the node as Trainium.

[source,shell]
----
Name:               ip-192-168-42-179.us-west-2.compute.internal
Roles:              <none>
Labels:             alpha.eksctl.io/cluster-name=inferentia-test
                    alpha.eksctl.io/instance-id=i-00549360c9911f4f1
                    alpha.eksctl.io/nodegroup-name=domino-trn1
                    beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/instance-type=trn1.2xlarge
                    beta.kubernetes.io/os=linux
                    dominodatalab.com/node-pool=trainium
                    failure-domain.beta.kubernetes.io/region=us-west-2
                    failure-domain.beta.kubernetes.io/zone=us-west-2b
                    k8s.io/cloud-provider-aws=7c4bfb478ecbb2400bead13fc878a3a1
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=ip-192-168-42-179.us-west-2.compute.internal
                    kubernetes.io/os=linux
                    node-lifecycle=on-demand
                    node.kubernetes.io/instance-type=trn1.2xlarge
                    topology.ebs.csi.aws.com/zone=us-west-2b
                    topology.kubernetes.io/region=us-west-2
                    topology.kubernetes.io/zone=us-west-2b

... <snip> ...

Capacity:
  aws.amazon.com/neuron:        1
  aws.amazon.com/neuroncore:    2
  aws.amazon.com/neurondevice:  1
  cpu:                          8
  ephemeral-storage:            209702892Ki
  hugepages-1Gi:                0
  hugepages-2Mi:                0
  memory:                       32338380Ki
  pods:                         58
  smarter-devices/fuse:         20
Allocatable:
  aws.amazon.com/neuron:        1
  aws.amazon.com/neuroncore:    2
  aws.amazon.com/neurondevice:  1
  cpu:                          7910m
  ephemeral-storage:            192188443124
  hugepages-1Gi:                0
  hugepages-2Mi:                0
  memory:                       31321548Ki
  pods:                         58
  smarter-devices/fuse:         20
System Info:
  Machine ID:                 ec240d0453aef36a07d5248a753946c5
  System UUID:                ec240d04-53ae-f36a-07d5-248a753946c5
  Boot ID:                    9d066b81-273b-406d-ad0f-6375adebca5d
  Kernel Version:             5.4.253-167.359.amzn2.x86_64
  OS Image:                   Amazon Linux 2
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.6.19
  Kubelet Version:            v1.26.7-eks-8ccc7ba
  Kube-Proxy Version:         v1.26.7-eks-8ccc7ba
ProviderID:                   aws:///us-west-2b/i-00549360c9911f4f1
Non-terminated Pods:          (11 in total)
  Namespace                   Name                                    CPU Requests  CPU Limits   Memory Requests  Memory Limits  Age
  ---------                   ----                                    ------------  ----------   ---------------  -------------  ---
  domino-compute              run-650b32d5eb80df0d60cec514-kgcc4      5610m (70%)   6600m (83%)  28174Mi (92%)    28174Mi (92%)  3h17m
  domino-platform             aws-ebs-csi-driver-node-lwp9g           30m (0%)      200m (2%)    80Mi (0%)        456Mi (1%)     3h10m
  domino-platform             aws-efs-csi-driver-node-kk7mb           20m (0%)      200m (2%)    40Mi (0%)        200Mi (0%)     3h10m
  domino-platform             docker-registry-cert-mgr-k67bb          0 (0%)        0 (0%)       0 (0%)           0 (0%)         3h10m
  domino-platform             fluentd-ljkhk                           200m (2%)     1 (12%)      600Mi (1%)       2Gi (6%)       3h10m
  domino-platform             image-cache-agent-7ztv6                 0 (0%)        0 (0%)       0 (0%)           0 (0%)         3h10m
  domino-platform             prometheus-node-exporter-vr9bc          0 (0%)        0 (0%)       0 (0%)           0 (0%)         3h10m
  domino-platform             smarter-device-manager-ncsr6            10m (0%)      100m (1%)    15Mi (0%)        15Mi (0%)      3h10m
  kube-system                 aws-node-q89xl                          25m (0%)      0 (0%)       0 (0%)           0 (0%)         3h10m
  kube-system                 kube-proxy-b2qpv                        100m (1%)     0 (0%)       0 (0%)           0 (0%)         3h10m
  kube-system                 neuron-device-plugin-daemonset-h8d7k    0 (0%)        0 (0%)       0 (0%)           0 (0%)         3h10m
----

== Hardware Tier setup

Next, you need to make the node group accessible to your users by creating a Domino hardware tier that does the following:

- Targets the node pool label you've given to your Neuron-based nodes
- Requests a suitable amount of the node vCPU and memory, link:908bd9[allowing for necessary overhead^]
- Requests a custom GPU resource with the name `aws.amazon.com/neuron`

See the following example:

[cols="1a,^1a",options="header"]
|===
|Key |Value

|Cluster Type |Kubernetes
|ID |trainium
|Name |Trainium
|Cores Requested |5.0
|Memory Requested (GiB) |26.0
|Number of GPUs |1
|Use custom GPU name |Yes
|GPU Resource Name |aws.amazon.com/neuron
|Cents Per Minute Per Run |0.0
|Node Pool |trainium
|Restrict to compute cluster |Options: Spark, Ray, Dask, Mpi
|Maximum Simultaneous Executions |
|Overprovisioning Pods |0
|===

== Environment setup

The AWS Neuron SDK is designed for use with fully integrated frameworks like PyTorch and TensorFlow. When setting up a Domino environment for a new version of Neuron or the integrated framework, you should read the documentation on:

- link:https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/index.html?pytorch-install.html=[Getting started with PyTorch Neuron^]

- link:https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/tensorflow/index.html[Getting started with Tensorflow Neuron^]

As an example, and to facilitate testing, here's an environment definition for adding PyTorch Neuron to the Domino 5.7 Standard Environment (`quay.io/domino/compute-environment-images:ubuntu20-py3.9-r4.3-domino5.7-standard`):

[source,shell]
----
# Configure Linux for Neuron repository updates
RUN sudo touch /etc/apt/sources.list.d/neuron.list
RUN echo "deb https://apt.repos.neuron.amazonaws.com focal main" | sudo tee -a /etc/apt/sources.list.d/neuron.list
RUN sudo wget -qO - https://apt.repos.neuron.amazonaws.com/GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB | sudo apt-key add -

# Update OS packages 
RUN sudo apt-get update -y

# Install Neuron Driver
RUN sudo apt-get install aws-neuronx-dkms=2.* -y

# Install Neuron Runtime 
RUN sudo apt-get install aws-neuronx-collectives=2.* -y
RUN sudo apt-get install aws-neuronx-runtime-lib=2.* -y

# Install Neuron Tools 
RUN sudo apt-get install aws-neuronx-tools=2.* -y

# Add PATH
RUN export PATH=/opt/aws/neuron/bin:$PATH

# pip installs
RUN python -m pip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com
RUN python -m pip install --user neuronx-cc==2.* torch-neuronx torchvision
----

== Testing Neuron devices in Domino

To test your setup, start a Jupyter workspace using a Neuron-based *hardware tier* and Neuron-enabled *Workspace Environment*.

Once your workspace has started, open a Python notebook and execute a cell with the command `!/opt/aws/neuron/bin/neuron-ls` to see mounted Neuron devices.

You can now use the Neuron framework you've installed to invoke the mounted accelerator.

== Next steps

Refer to the link:https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/quick-start/index.html[Getting Started with Neuron guide^] for your chosen framework to get started.

----- user_guide/customize-environments/use-external-images/index.txt -----
:page-version: 6.1
:page-title: Use external images in Domino Environments
:page-sidebar: Use external images
:page-permalink: 5babee
:page-order: 40

[[tr1]]
// As a Domino user, I can create an Environment with an external base image and make it compatible with Domino workloads.
[[tr2]]
// As a Domino user, I can execute a workload using a Domino compatible Environment built from an external base image.
Domino can automatically adapt most Docker images and containers
This functionality makes it easy to use pre-built images from external catalogs like the GPU-optimized https://catalog.ngc.nvidia.com/containers[NVIDIA GPU Cloud (NGC)^] containers in Domino.

link:6fdcff[Create an NVIDIA NGC Environment in Domino]::
Get the path for the NGC container and then create the image.

link:063fca[Create a Stata Environment in Domino]::
Create the Environment needed to use Stata in Domino.

link:d98a6d[Use AWS-designed silicon accelerators]::
Use AWS-designed silicon processors to accelerate deep-learning model training and AI inference workloads.

link:35f28e[Create an Intel AI Kit Environment in Domino]::
Use the Intel AI Kit container image natively in Domino.

link:f08799[Create a MATLAB Environment in Domino]::
Set up the Environment for an interactive MATLAB® session in Domino.

link:78ae6b[Manually create an Environment with a pre-built image]::
Customize images for use in Domino without using the automatic compatibility feature.

link:23187f[Pre-requisites for automatic custom image compatibility]::
Requirements for custom images that use the automatic compatibility feature.

----- user_guide/customize-environments/use-external-images/intel-ai-kit.txt -----
:page-version: 6.1
:page-title: Create an Intel AI Kit Environment
:page-sidebar: Intel AI Kit
:page-permalink: 35f28e
:page-order: 50

// As a Domino user, I can create a new Domino-compatible Environment using the Intel AI Kit container image.

The Intel OneAPI and AI Analytics Toolkit (AI Kit) provides familiar Python tools and frameworks to accelerate end-to-end data science and analytics pipelines on Intel® architecture.
The components are built using oneAPI libraries for low-level compute optimizations.
This toolkit maximizes performance from preprocessing through machine learning and provides interoperability for efficient model development.

== Run AI Kit in Domino

Many pre-built AI Kit Docker images run natively in Domino.
See https://hub.docker.com/r/intel/oneapi-aikit[https://hub.docker.com/r/intel/oneapi-aikit] for a list of available images on Dockerhub.

. link:fa8137[Create a new Environment] from a custom image.
. Paste or type the Docker registry path of the image that you want to use in the *FROM* field. For example, `intel/oneapi-aikit:latest`.
+
The Environment includes the following packages:
+
----
intel-aikit-getting-started
intel-basekit-getting-started
intel-level-zero-gpu
intel-oneapi-advisor
intel-oneapi-ccl-devel
intel-oneapi-common-licensing
intel-oneapi-common-vars
intel-oneapi-compiler-dpcpp-cpp
intel-oneapi-dal-devel
intel-oneapi-dev-utilities
intel-oneapi-dnnl-devel
intel-oneapi-dpcpp-debugger
intel-oneapi-ippcp-devel
intel-oneapi-ipp-devel
intel-oneapi-libdpstd-devel
intel-oneapi-mkl-devel
intel-oneapi-model-zoo
intel-oneapi-onevpl-devel
intel-oneapi-python
intel-oneapi-pytorch
intel-oneapi-tbb-devel
intel-oneapi-tensorflow
intel-oneapi-vtune
intel-opencl-icd
level-zero
level-zero-dev
----
. After the environment is built, link:e6e601[create a new Workspace].

See the following for more information:

* https://www.intel.com/content/www/us/en/developer/tools/oneapi/ai-analytics-toolkit.html[Intel AI Analytics Toolkit (AI Kit)]
* https://hub.docker.com/r/intel/oneapi-aikit[AI Kit containers on Dockerhub]
* https://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics[Code samples] on GitHub

----- user_guide/customize-environments/use-external-images/manually-create-an-environment.txt -----
:page-version: 6.1
:page-permalink: 78ae6b
:page-title: Manually create an Environment with a pre-built image
:page-sidebar: Manually create an Environment
:page-order: 60

[CAUTION]
====
This is an advanced topic to assist with customizing images for use in Domino without using the automatic compatibility feature.
====

//Is this supposed to tell the difference of what the other option does vs what this topic is describing? It is not clear what this topic does that is different.
When you create an Environment with the automatic compatibility feature selected, Domino adds several Dockerfile commands so that the custom images can launch Job and Workspace containers.

At a high level, these instructions perform the following actions:

* Create a non-root Domino user.
* Install `sudo` and `curl`.
+
For these images to work in Domino, you must have `Sudo` and `curl` installations.
`curl` transfers data.
Use `sudo` to install additional packages and libraries in the container, depending on the permissions set up by your Domino administrator.

* Add an isolated directory, `/opt/domino/`, with the tools required to run Domino Workspaces.

If you have enabled non-root executions, refer to link:#create-image-for-non-root-execution[Create the image compatible with non-root executions].

[[create-image]]
== Create the image
. Use the following Dockerfile instructions to replicate these steps when building a custom image.
+
[source,dockerfile]
----
# Ensure you are injecting as the root user
USER root

# Add a Domino user and group ID
ARG DOMINO_USER='domino'
ARG DOMINO_GROUP='domino'

# The variable names that can be used in generation of Environment scripts
ARG ALLENV="$INSTALL_DIR,$INSTALL_BIN,$CONDA_DIR,$DOMINO_UID,$DOMINO_GID,$DOMINO_USER,$DOMINO_GROUP,$VSCODE_DIR,$VSCODE_EXT_DIR"
RUN groupadd -g 12574 ${DOMINO_GROUP} && \
  useradd -u 12574 -g 12574 -m -N -s /bin/bash ${DOMINO_USER};

# Install common dependencies for the compiler and setting things up
ARG INSTALL_DIR=/opt/domino
ARG INSTALL_BIN=${INSTALL_DIR}/bin
ARG CONDA_DIR=${INSTALL_DIR}/conda
RUN apt-get update && \
    apt-get -y install \
    build-essential \
    gettext-base \
    gnupg && \
  apt-get clean && \
  rm -rf /var/lib/apt/lists/* && \
  mkdir -p \
      ${INSTALL_DIR} \
      ${INSTALL_BIN} \
      ${CONDA_DIR}

# Recommended: Add domino user account to sudoers
RUN echo "${DOMINO_USER}    ALL=NOPASSWD: ALL" >> /etc/sudoers

# Set to a Domino supported language to prevent unrecognized character input
RUN echo "export LANG=${LANG}" >> /home/${DOMINO_USER}/.domino-defaults && \
    echo "export LC_ALL=${LANG}" >> /home/${DOMINO_USER}/.domino-defaults && \
    echo 'export PYTHONIOENCODING=utf-8' >> /home/${DOMINO_USER}/.domino-defaults && \
    chown -R ${DOMINO_USER}.${DOMINO_GROUP} "/home/${DOMINO_USER}"

# Load Domino defaults
RUN if [ -f /home/${DOMINO_USER}/.domino-defaults ]; then \
        echo "source /home/${DOMINO_USER}/.domino-defaults" >> /home/${DOMINO_USER}/.bashrc; \
    fi

# Install Sudo and Curl
# Note that this will only work with Debian and Ubuntu
RUN apt update && apt install -y --no-install-recommends \
        curl \
        sudo \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*
----

. If you want JupyterLab or any IDE in this workspace, define a start script for each.
See link:4e7f25[Replace Default Environment Tools] for an example.
These start scripts are stored in `/opt/domino/workspaces` by default.
. Add a new field to your Environment's *Pluggable Workspace Tools* for the associated tool and set the *start* field to the location of your start script.

[[create-image-for-non-root-execution]]
== Create the image compatible with non-root executions

Images shouldn’t add users to sudoers, according to the non-root executions principle. It is recommended to build environment images with the packages you need, rather than relying on a root user for runtime package installation, for increased reproducibility and security in your environment.

. Use the following Dockerfile instructions to replicate these steps when building a custom image.
+
[source,dockerfile]
----
# Add a Domino user and group ID
ARG DOMINO_USER='domino'
ARG DOMINO_GROUP='domino'

# The variable names that can be used in generation of environment scripts
ARG ALLENV="$INSTALL_DIR,$INSTALL_BIN,$CONDA_DIR,$DOMINO_UID,$DOMINO_GID,$DOMINO_USER,$DOMINO_GROUP,$VSCODE_DIR,$VSCODE_EXT_DIR"
RUN groupadd -g 12574 ${DOMINO_GROUP} && \
  useradd -u 12574 -g 12574 -m -N -s /bin/bash ${DOMINO_USER};

# Install common dependencies for the compiler and setting things up
ARG INSTALL_DIR=/opt/domino
ARG INSTALL_BIN=${INSTALL_DIR}/bin
ARG CONDA_DIR=${INSTALL_DIR}/conda
RUN apt-get update && \
  apt-get -y install \
      build-essential \
      gettext-base \
      gnupg && \
  apt-get clean && \
  rm -rf /var/lib/apt/lists/* && \
  mkdir -p \
      ${INSTALL_DIR} \
      ${INSTALL_BIN} \
      ${CONDA_DIR}

# Set to a Domino supported language to prevent unrecognized character input
RUN echo "export LANG=${LANG}" >> /home/${DOMINO_USER}/.domino-defaults && \
    echo "export LC_ALL=${LANG}" >> /home/${DOMINO_USER}/.domino-defaults && \
    echo 'export PYTHONIOENCODING=utf-8' >> /home/${DOMINO_USER}/.domino-defaults && \
    chown -R ${DOMINO_USER}.${DOMINO_GROUP} "/home/${DOMINO_USER}"

# Load Domino defaults
RUN if [ -f /home/${DOMINO_USER}/.domino-defaults ]; then \
        echo "source /home/${DOMINO_USER}/.domino-defaults" >> /home/${DOMINO_USER}/.bashrc; \
    fi

# Install Curl
# Note that this will only work with Debian and Ubuntu
RUN apt update && apt install -y --no-install-recommends \
        curl \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*
----

. If you want JupyterLab or any IDE in this workspace, define a start script for each.
See link:4e7f25[Replace Default Environment Tools] for an example.
These start scripts are stored in `/opt/domino/workspaces` by default.
. Add a new field to your environment's *Pluggable Workspace Tools* for the associated tool and set the *start* field to the location of your start script.

== Optional: Enable JupyterLab

If you want to use JupyterLab in a Domino workspace that uses a custom image, use the following instructions to install with `miniconda`.
The commands to install Tensorboard are included, and the comments indicate which lines to remove if you do not need Tensorboard in your image.

[source,dockerfile]
----
# Install JupyterLab and Tensorboard using Conda
# A proper combination of these versions is important!
ARG CONDA_URL=https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh
ARG PYTHON_VERSION=3.9
ARG JUPYTER_VERSION=2.3

# Remove the following line if you are not installing Tensorboard:
ARG TENSORBOARD_VERSION=2.2

# You must create a start script with configurations for JupyterLab
ARG JUPYTER_SCRIPT=jupyterlab-start.sh

# Install Conda and Python
# (Download signature not available)
RUN curl -o conda-install.sh -LSsf ${CONDA_URL} && \
  /bin/bash conda-install.sh -fbp ${CONDA_DIR} && \
  ${CONDA_DIR}/bin/conda update -y --update-all && \
  ${CONDA_DIR}/bin/conda install -y python=${PYTHON_VERSION}

# Install NodeJS if installing Tensorboard
RUN ${CONDA_DIR}/bin/conda install -y nodejs && \
  mkdir -p ${INSTALL_DIR}/node && \
  ln -rs ${CONDA_DIR}/bin/node ${INSTALL_DIR}/node/ && \
  ln -rs ${CONDA_DIR}/bin/npm ${INSTALL_DIR}/node/

# Install and configure Jupyterlab and extensions
RUN ${CONDA_DIR}/bin/conda install -y jupyterlab=${JUPYTER_VERSION} && \
  ${CONDA_DIR}/bin/conda install -y tensorboard=${TENSORBOARD_VERSION} && \
  ${CONDA_DIR}/bin/conda install -y -c conda-forge jupytext && \
  ${CONDA_DIR}/bin/conda install -y nodejs && \
  ${CONDA_DIR}/bin/pip install jupyter_tensorboard && \
  PATH=${CONDA_DIR}/bin ${CONDA_DIR}/bin/jupyter labextension install jupyterlab_tensorboard

# Create Juptyerlab workspace Environment scripts
RUN envsubst "${ALLENV}" < ${JUPYTER_SCRIPT} > ${INSTALL_BIN}/${JUPYTER_SCRIPT} && \
  chmod 755 ${INSTALL_BIN}/${JUPYTER_SCRIPT}
----

== Optional: Enable VS Code

To use VS Code in a Domino workspace, add the following instructions.

[source,dockerfile]
----
# Install VS Code

#Set Versions
ARG VSCODE_VERSION=3.10.2
ARG VSCODE_URL=https://github.com/cdr/code-server/releases/download/v${VSCODE_VERSION}/code-server-${VSCODE_VERSION}-linux-amd64.tar.gz
ARG VSCODE_PYTHON_VERSION=2021.5.926500501
ARG VSCODE_PYTHON_URL=https://github.com/microsoft/vscode-python/releases/download/${VSCODE_PYTHON_VERSION}/ms-python-release.vsix

# Where it will be deployed
ARG VSCODE_DIR=${INSTALL_DIR}/vscode
ARG VSCODE_EXT_DIR=${VSCODE_DIR}/extensions

# You must create a start script with configurations for VS Code
ARG VSCODE_SCRIPT=vscode-start.sh
RUN curl -LSsf ${VSCODE_URL} | tar -xz --no-same-permissions && \
  mv -f code-server-${VSCODE_VERSION}-linux-amd64 ${VSCODE_DIR} && \
  curl -o python.vsix -LSsf ${VSCODE_PYTHON_URL} && \
  mkdir -p ${VSCODE_EXT_DIR} && \
  ${VSCODE_DIR}/bin/code-server \
      --install-extension python.vsix \
      --extensions-dir ${VSCODE_EXT_DIR}

# Create VS Code workspace Environment scripts
RUN envsubst "${ALLENV}" < ${VSCODE_SCRIPT} > ${INSTALL_BIN}/${VSCODE_SCRIPT} && \
  chmod 755 ${INSTALL_BIN}/${VSCODE_SCRIPT}
----
----- user_guide/customize-environments/use-external-images/matlab.txt -----
:page-version: 6.1
:page-title: MATLAB
:page-permalink: f08799
:page-order: 60



[[tr1]]
// As a Domino user, I can start an interactive MATLAB session using a properly configured Environment.
To use an interactive MATLAB® session in Domino, you must configure a Compute Environment and interactive Workspace.
If your administrators have already configured such an Environment, you can use it by setting your Project to use that Environment.

== Configure a MATLAB Environment in Domino

This topic describes how to create a Compute Environment if it does not already exist.
You might need help from a Domino or MATLAB admin.

[[Prerequisites]]
== Prerequisites

Network licensing structure for MATLAB::

You must have a network license to configure the Compute Environment.
Domino recommends a network concurrent license.
A network-named license might work if the usernames specified in the license matches the usernames in Domino.
If you don't have a network concurrent license, contact your MATLAB license administrator.
You can have a single license server or a set of three license servers.
The following are examples of how to provide the license server information:

=== Method 1

This method requires only the port and host for your license servers.
For example, `27000@lic1.customer.com` (and optionally `27000@lic2.customer.com`, `27000@lic3.customer.com`).
This is recommended for most cases.

=== Method 2
Use this method if you have a `network.lic` file.
For example:
[source,shell]
----
SERVER lic1.customer.com 000000000000 27000 SERVER lic2.customer.com 000000000010 27000 SERVER lic3.customer.com 000000000020 27000 USE_SERVER
----
This is recommended only if you have several sets of license servers (and therefore, several license files). For example, you would have separate license servers for MATLAB Parallel Server.

See https://www.mathworks.com/matlabcentral/answers/116637-what-are-the-differences-between-the-license-lic-license-dat-network-lic-and-license_info-xml-lic[Mathworks
help^] for more information about the options or talk to your MATLAB license administrator.

(Optional) Hardware Tiers with the correct GPU drivers::

To use GPU capabilities, the hardware tier in Domino must have GPU drivers installed that can support the version of MATLAB that you want to use.
Use the following to determine compatibility:
* https://www.mathworks.com/help/solutions/parallel-computing/gpu-support-by-release.html[GPU Support by Release table^] which describes the minimum CUDA Toolkit version requirement for each version of MATLAB.
* https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility__table-toolkit-driver[CUDA compatibility^] which describes the minimum NVIDIA driver version requirement for each version of the CUDA Toolkit.

The MATLAB base images provided by Domino already have a suitable version of the CUDA Toolkit installed.
You must configure the NVIDIA drivers at the Hardware Tier level.
If you already have a GPU-enabled hardware tier configured, it is likely that it already has suitable NVIDIA drivers.
See the link:#faq-and-ts[Troubleshooting section] for instructions about how to check the driver version during a MATLAB session to verify it is configured correctly.

If you do not have the correct NVIDIA drivers configured on your GPU Hardware Tier,
contact your Domino administrator and refer them to the documentation to link:07075d#install-nvidia-docker-2-0-gpu-amis-only[configure NVIDIA drivers in EKS].
talk to your Domino administrator.
They must install and/or upgrade the NVIDIA drivers, and (if your deployment uses Amazon Web Services) update the executor AMI following a procedure similar to the one for link:e79fe6[Creating a new executor AMI].
If you are a Domino administrator looking for guidance on this process, contact your Domino account team or email us at support@dominodatalab.com.

== Environment setup

[[tr2]]
// As a Domino user, I can create an Environment with the necessary content and license to support MATLAB Workspaces.

If a MATLAB-specific Environment does not already exist in your Domino deployment, you must create one.
Read about link:f51038[Environment Management] if you aren't already familiar.

[[BaseImage]]
=== Choose a base image

When creating a new Environment, choose a base image that has MATLAB installed.
The following are the most current options available from Domino.

[cols="1a,2a",options="header"]
|===
|Version |Repo Link

|MATLAB 2021a
|
`quay.io/domino/matlab:R2021a`

|MATLAB 2020b
|
`quay.io/domino/matlab:R2020b`

|MATLAB 2020a
|
`quay.io/domino/matlab:R2020a`

|MATLAB 2019b
|
`quay.io/domino/matlab:R2019b`
|===

=== Edit the Dockerfile

. Add the following lines to your *Dockerfile Instructions*, editing the license server information to match your actual license servers.
. Edit the `MATLAB_VERSION` variable to match the MATLAB version of the base image you chose in the previous step.
. If you have port and host information for your license servers (recommended method):
+
[source,bash]
----
# This MATLAB_VERSION variable is used later in persisting Add-ons and preferences
ENV MATLAB_VERSION='R2019b'
ENV MLM_LICENSE_FILE=27000@lic1.customer.com,27000@lic2.customer.com,27000@lic3.customer.com
----
+
Or, if you have multiple license files and you have a `network.lic` file you can adapt this example:
+
[source,bash]
----
USER root

ENV MATLAB_VERSION='R2019b'
# Here we are simply pasting the information in the license file
# If you have the license file hosted somewhere accessible to Domino, you could instead download it to the same location via wget or similar methods
RUN mkdir -p /usr/local/MATLAB/$MATLAB_VERSION/licenses && 
    printf "SERVER lic1.customer.com 000000000000 27000
SERVER lic2.customer.com 000000000010 27000
SERVER lic3.customer.com 000000000020 27000
USE_SERVER
" > /usr/local/MATLAB/$MATLAB_VERSION/licenses/network.lic

USER ubuntu
----

=== Add pluggable Workspace tools definition

. Add the "Pluggable Properties" to the *Properties for Workspaces*.
+
These are documented alongside the base image descriptions here:
https://github.com/dominodatalab/Domino_Base_Images/tree/master/MATLAB_Base_Images.
+
The MATLAB base images are built on top of the
link:0d73c6[Domino Standard Environment],
so you can add the Workspace configurations such as Jupyter or RStudio for those standard Environments also.

For example, if you are using the `quay.io/domino/matlab:r2019b-20200521` base image, add the following lines for the MATLAB Workspace:

[source,yaml]
----
matlab-rdp:
  title: "RDP"
  iconUrl: "https://raw.githubusercontent.com/dominodatalab/partner-resources/main/assets/workspaces/Mathworks/MATLAB/matlab-simulink.png"
  start: [ "/opt/domino/workspaces/matlab/start" ]
  httpProxy:
    port: 8080
    internalPath: "/{{ownerUsername}}/{{projectName}}/{{sessionPathComponent}}/{{runId}}/#/?username=domino&password=domino"
    rewrite: false
    requireSubdomain: false
matlab:
  title: "MATLAB"
  iconUrl: "https://raw.githubusercontent.com/dominodatalab/partner-resources/main/assets/workspaces/Mathworks/MATLAB/matlab.png"
  start: [ "/opt/domino/workspaces/matlab-webdesktop/start" ]
  httpProxy:
    port: 8888
    internalPath: "/{{ownerUsername}}/{{projectName}}/{{sessionPathComponent}}/{{runId}}/matlab/index.html"
    rewrite: false
    requireSubdomain: false
----

Because it is built on top of `quay.io/domino/base:Ubuntu18_DAD_Py3.6_R3.6_20190916`, you can add any of the following sections for Workspaces from that Environment:

[source,yaml]
----
jupyter:
  title: "Jupyter (Python, R, Julia)"
  iconUrl: "/assets/images/workspace-logos/Jupyter.svg"
  start: [ "/opt/domino/workspaces/jupyter/start" ]
  supportedFileExtensions: [ ".ipynb" ]
  httpProxy:
    port: 8888
    rewrite: false
    internalPath: "/{{ownerUsername}}/{{projectName}}/{{sessionPathComponent}}/{{runId}}/{{#if pathToOpen}}tree/{{pathToOpen}}{{/if}}"
    requireSubdomain: false
jupyterlab:
  title: "JupyterLab"
  iconUrl: "/assets/images/workspace-logos/jupyterlab.svg"
  start: [  "/opt/domino/workspaces/jupyterlab/start" ]
  httpProxy:
    internalPath: "/{{ownerUsername}}/{{projectName}}/{{sessionPathComponent}}/{{runId}}/{{#if pathToOpen}}tree/{{pathToOpen}}{{/if}}"
    port: 8888
    rewrite: false
    requireSubdomain: false
vscode:
  title: "vscode"
  iconUrl: "/assets/images/workspace-logos/vscode.svg"
  start: [ "/opt/domino/workspaces/vscode/start" ]
  httpProxy:
    port: 8888
    requireSubdomain: false
rstudio:
  title: "RStudio"
  iconUrl: "/assets/images/workspace-logos/Rstudio.svg"
  start: [ "/opt/domino/workspaces/rstudio/start" ]
  httpProxy:
    port: 8888
    requireSubdomain: false
----

=== (Optional) Persist MATLAB preferences and add-ons between sessions

To persist your MATLAB Preferences and Add-ons between Domino sessions, you can define a *Post Run Script* to automatically copy them to your Project Files when you *Stop* and *Sync* a session, and define a *Pre Run Script* to copy them from your Project Files into the locations MATLAB will recognize at the start of subsequent sessions.

Put the following lines into your *Pre Run Script*.

[source,bash]
----
if [ -d /mnt/matlab-addons ]; then
    mkdir -p /home/ubuntu/Documents/MATLAB/SupportPackages/$MATLAB_VERSION/
    rsync -r /mnt/matlab-addons/ /home/ubuntu/Documents/MATLAB/SupportPackages/$MATLAB_VERSION
fi
if [ -d /mnt/matlab-preferences/$DOMINO_STARTING_USERNAME ]; then
    mkdir -p /home/ubuntu/.matlab/$MATLAB_VERSION/
    rsync -r /mnt/matlab-preferences/$DOMINO_STARTING_USERNAME/ /home/ubuntu/.matlab/$MATLAB_VERSION
fi
----

Put the following lines into your *Post Run Script*.

[source,bash]
----
if [ -d /home/ubuntu/Documents/MATLAB/SupportPackages/$MATLAB_VERSION ]; then
    mkdir -p /mnt/matlab-addons
    rsync -r /home/ubuntu/Documents/MATLAB/SupportPackages/$MATLAB_VERSION/ /mnt/matlab-addons
fi
if [ -d /home/ubuntu/.matlab/$MATLAB_VERSION ]; then
    mkdir -p /mnt/matlab-preferences/$DOMINO_STARTING_USERNAME
    rsync -r /home/ubuntu/.matlab/$MATLAB_VERSION/ /mnt/matlab-preferences/$DOMINO_STARTING_USERNAME
fi
----

These scripts will save add-ons *per project*, so that any collaborators launching a Workspace in the same Project will have the same add-ons.
For preferences, however, they will be saved *per user* in the Project so that each user can have a different set of preferences.

[[Proxy]]
== (Optional) Generate a startup script for proxy settings

If your company uses a proxy to connect to external networks, you might already have lines like the following in many of your Compute Environment Dockerfiles:

[source,bash]
----
ENV http_proxy='http://someusername:password123@proxy.company.com:80'
ENV https_proxy='http://someusername:password123@proxy.company.com:80'
ENV HTTP_PROXY='http://someusername:password123@proxy.company.com:80'
ENV HTTPS_PROXY='http://someusername:password123@proxy.company.com:80'
----

Without setting the proxy information in MATLAB as well, you may not be able to connect to external networks from MATLAB (for example, for downloading community Add-ons).
Proxy information can be set manually in MATLAB under *Preferences* > *Web*, and these will be persisted across sessions if the *Pre* and *Post Run Scripts* from the previous section are included in your Environment.

Alternatively, you can define a `startup.m` file in your Project to ensure the proxy settings are correct at every launch of MATLAB.
The file would look something like the following.

[source,matlab]
----
% This startup file was created from the Environment pre-run script, to set proxy settings for MATLAB
disp('Setting proxy configuration via startup.m file')
com.mathworks.mlwidgets.html.HTMLPrefs.setUseProxy(true)
com.mathworks.mlwidgets.html.HTMLPrefs.setProxyHost('proxy.company.com')
com.mathworks.mlwidgets.html.HTMLPrefs.setProxyPort('80')
com.mathworks.mlwidgets.html.HTMLPrefs.setUseProxyAuthentication(true)
com.mathworks.mlwidgets.html.HTMLPrefs.setProxyUsername('someusername')
com.mathworks.mlwidgets.html.HTMLPrefs.setProxyPassword('password123')
----

Put the following lines into your *Pre Run Script* to automatically generate such a file in any Project where it does not exist, whenever this Compute Environment is used to launch a session.
(Omit the Authentication, Username, and Password lines if your proxy does not use them.)

[source,bash]
----
if [ ! -f /mnt/startup.m ]; then
    echo "% This startup file was created from the Environment pre-run script, to set proxy settings for MATLAB" > /mnt/startup.m
    echo "disp('Setting proxy configuration via startup.m file')" >> /mnt/startup.m
    echo "com.mathworks.mlwidgets.html.HTMLPrefs.setUseProxy(true)" >> /mnt/startup.m
    echo "com.mathworks.mlwidgets.html.HTMLPrefs.setProxyHost('proxy.company.com')" >> /mnt/startup.m
    echo "com.mathworks.mlwidgets.html.HTMLPrefs.setProxyPort('80')" >> /mnt/startup.m
    echo "com.mathworks.mlwidgets.html.HTMLPrefs.setUseProxyAuthentication(true)" >> /mnt/startup.m
    echo "com.mathworks.mlwidgets.html.HTMLPrefs.setProxyUsername('someusername')" >> /mnt/startup.m
    echo "com.mathworks.mlwidgets.html.HTMLPrefs.setProxyPassword('password123')" >> /mnt/startup.m
fi
----

=== Build and test the Environment

Click *Build* when you have finished editing the above sections.
The Revisions page for the Environment opens.

If the new revision builds successfully, you are ready to test.
Go to the Project Settings for a Project where you want to use MATLAB and change the Compute Environment to use this new Environment. After you do, you will see a MATLAB icon as an option for Workspaces in that Project.

See the link:#faq-and-ts[Troubleshooting] section for information about known issues and frequently encountered problems if anything does not work as expected.

[[faq-and-ts]]
== FAQ and troubleshooting

=== GPU functionality does not work

The most common problem with GPU functionality is not having the correct NVIDIA driver version for your Environment configured in your Hardware Tier.
The following shell command should print some information about the GPU driver versions so you can verify it with the compatibility tables linked in the `Prerequisites` section.

[source,matlab]
----
>> !nvidia-smi
Wed May 13 00:00:16 2020
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  Off  | 00000000:00:1B.0 Off |                    0 |
| N/A   39C    P0    50W / 300W |      0MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-SXM2...  Off  | 00000000:00:1C.0 Off |                    0 |
| N/A   41C    P0    50W / 300W |      0MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-SXM2...  Off  | 00000000:00:1D.0 Off |                    0 |
| N/A   40C    P0    52W / 300W |      0MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-SXM2...  Off  | 00000000:00:1E.0 Off |                    0 |
| N/A   40C    P0    54W / 300W |      0MiB / 16160MiB |      4%      Default |
+-------------------------------+----------------------+----------------------+
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
----

=== MATLAB Workspace fails to launch

==== License configuration problems

License issues are the most common cause of problems launching a MATLAB Workspace.
To debug, start a Jupyter or RStudio Workspace (still using the MATLAB Environment) and open a terminal.
Check if your license server information has been correctly entered into the `network.lic`
file, in the correct part of the MATLAB directory structure (i.e. matching your MATLAB version):

[source,bash]
----
$ which matlab
/usr/local/MATLAB/R2019b/bin/matlab
$ cat /usr/local/MATLAB/R2019b/licenses/network.lic
----

If that contains the license server information you expect, but there are still problems, you can use the `lmutil` utility to check that the license server is reachable and contains the correct information.

[source,bash]
----
$ lmutil lmstat -a -c 27000@lic1.customer.com
----

It should output information on the licenses being managed by that license server, including the MATLAB licenses.

=== Advanced troubleshooting

The Workspace launch process for MATLAB respects several Environment variables related to logging, which may help you debug more obscure or advanced problems.
You can set these as Project-level Environment variables, similar to how you would link:6ac5a1[set Environment variables for secure credential storage], to enable toggling them on or off without having to edit the Compute Environment.

* `WS_LOGGING=[MATLAB]|ALL|DEBUG`.
By default, only the MATLAB logs are shown.
** `ALL` shows logs for the underlying screen management *xterm* and
*xpra*.
** `DEBUG` shows everything from `ALL`, and activates a flag to show all commands.
* `TTY_MODE=[disabled]|PRE|POST`.
Runs *ttyd* before or after the Workspace.
It's a terminal that runs in a browser so you can easily inspect things.
** `PRE` will run *ttyd* before the Workspace launch.
** `POST` will run *ttyd* after the Workspace launch fails (if it fails).
* `START_LOGGING=[false]|true`.
Increases logging verbosity in certain stages.

=== MATLAB Workspace is laggy

In poor network conditions you may notice some lag in typing and general interactions with the MATLAB UI.
This is to be expected in certain circumstances.
Other common Domino Workspaces like Jupyter or RStudio use browser-based IDEs, while this MATLAB Environment is running a (Linux) desktop version of MATLAB in a remote desktop style interface.
This results in more sensitivity to slow network conditions than a browser-based IDE.

For these situations, if you encounter them frequently, you may find it useful to install the link:e21e55[Domino CLI] on your local machine to facilitate editing files locally, then syncing them to Domino.

=== Keyboard shortcuts for copy/paste do not work

MATLAB in Domino runs the Linux version of MATLAB, which uses the "Emacs" set of keyboard shortcuts by default.
To enable keyboard shortcuts `Ctrl+C` and `Ctrl+V` for copy and paste, go to *Preferences* > *Keyboard* > *Shortcuts* and change this from *Emacs* to *Windows*.
See the
https://www.mathworks.com/matlabcentral/answers/103161-why-am-i-unable-to-use-copy-ctrl-c-and-paste-ctrl-v-keyboard-short-cuts-in-the-text-editing-mode[MATLAB
help^] for more details.

=== Connection error when downloading Add-ons or using urlread

If you get network errors when downloading Add-ons or using `urlread` to download files, it may be due to proxy settings.
See the `Proxy` section above for options for setting your proxy in MATLAB either within the current session or more globally.

=== Toolbox is not available

A selection of Toolboxes is pre-installed in each MATLAB base image, but this might not include every Toolbox you have on your local installation.
You can see the list of Toolboxes by running the `ver` command within MATLAB, and the list is also documented for each link:#BaseImage[base image].
You can contact your Domino account team with any specific requests for Toolboxes to include.

In addition to having the Toolbox installed in the base image, you must have an available license for using it on your
link:#Prerequisites[license server].

== Known issues and release notes

If your problem is not listed in this FAQ, it may be worth checking the release notes for the link:#BaseImage[base image] you are using.
Especially if your Environment is built on an older image tag, checking the release notes of newer images may give you a quick answer about whether rebuilding on the most current image will help.

_MATLAB and Simulink are registered trademarks of The MathWorks, Inc.
See_ http://mathworks.com/trademarks[http://mathworks.com/trademarks^] _for a list of additional trademarks._

----- user_guide/customize-environments/use-external-images/nvidia-ngc.txt -----
:page-version: 6.1
:page-permalink: 6fdcff
:page-title: NVIDIA NGC
:page-order: 10

== NVIDIA GPU Cloud (NGC) offers pre-built containers

They have both general-purpose and domain-specific offerings for machine learning and deep learning workloads on NVIDIA GPUs.

Domino enhances NGC containers for use in Domino and for general data science work.

Add Domino compatibility:: Domino automatically manages code and data versioning as part of the container lifecycle and as a result we require specific additional software in the container.

Add Data Source drivers:: Domino adds drivers for Data Sources such as Snowflake, Oracle, and Microsoft SQL to make it easy to connect to a Data Source without needing to manually add drivers.

Add Workspaces:: When an NGC container doesn't include an interactive notebook, Domino adds Jupyter to the image so you can interactively develop in the image.
When there's a notebook already included, Domino configures the existing notebook to work.

Domino builds the latest versions of NGC containers with a preference for Ubuntu base images.
If there's an Environment that you're looking for which is not available, request it from ngc-request@dominodatalab.com.

== Add an NGC image to Domino

[[tr1]]
// As a Domino user, I can create a new Domino-compatible Environment using an NGC base image.

. Choose the NGC container from the list of link:#available-containers[available containers].
. Create a new Compute Environment in Domino.
. Use the link from the container you selected as the base compute Environment (for example, `quay.io/domino/ngc-pytorch:20.12-py3`)
. Enter the following in the *Workspace Definition* area:

[source,yaml]
----
jupyter:
 title: "Jupyter (Python, R, Julia)"
 iconUrl: "/assets/images/workspace-logos/Jupyter.svg"
 start: [ "/opt/domino/workspaces/jupyter/start" ]
 httpProxy:
   port: 8888
   rewrite: false
   internalPath: "/{{ownerUsername}}/{{projectName}}/{{sessionPathComponent}}/{{runId}}/{{   requireSubdomain: false
jupyterlab:
 title: "JupyterLab"
 iconUrl: "/assets/images/workspace-logos/jupyterlab.svg"
 start: [  "/opt/domino/workspaces/jupyterlab/start" ]
 httpProxy:
   internalPath: "/{{ownerUsername}}/{{projectName}}/{{sessionPathComponent}}/{{runId}}/{{   port: 8888
   rewrite: false
   requireSubdomain: false
----

[[available-containers]]
== Available containers

=== RAPIDS

[[tr2]]
// As a Domino user, I can create a new Environment using an NGC RAPIDS base image.

[cols="1a,4a",options="header"]
|===
| <|https://ngc.nvidia.com/catalog/containers/nvidia:rapidsai:rapidsai

|Domino registry path:
|`quay.io/domino/ngc-rapids:0.18-cuda11.0-runtime-ubuntu20.04-py3.8`

|Versions:
|RAPIDS 0.18, Ubuntu 18.04, CUDA 11
|Base Image:
|`docker pull nvcr.io/nvidia/rapidsai/rapidsai:0.18-cuda11.0-base-ubuntu18.04`
|Notes:
|N/A

|===

=== PyTorch

[[tr3]]
// As a Domino user, I can create a new Environment using an NGC PyTorch base image.

[cols="1a,4a",options="header"]
|===
| <|https://ngc.nvidia.com/catalog/containers/nvidia:pytorch
|Domino registry path: |`quay.io/domino/ngc-pytorch:20.12-py3`
|Versions:
|Ubuntu 18.04, CUDA 11
|Base Image:
|`docker pull nvcr.io/nvidia/pytorch:21.02-py3`
|Notes:
|N/A

|===

=== TensorFlow

[[tr4]]
// As a Domino user, I can create a new Environment using an NGC TensorFlow base image.

[cols="1a,4a",options="header"]
|===
| <|https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow
|Domino registry path: |`quay.io/domino/ngc-tensorflow:20.12-tf1-py3`

|Versions:
|TensorFlow v21.03, Ubuntu 18.04, CUDA 11
|Base Image:
|`docker pull nvcr.io/nvidia/tensorflow:21.03-tf1-py3`
|Notes:
|N/A

|===

=== Clara Train

[[tr5]]
// As a Domino user, I can create a new Environment using an NGC Clara Train base image.

[cols="1a,4a",options="header"]
|===
| <|https://ngc.nvidia.com/catalog/containers/nvidia:clara-train-sdk
|Domino registry path: |`quay.io/domino/ngc-clara-train:v3.1.01`
|Versions:
|Clara v3.1.01, Ubuntu 18.04, CUDA 11
|Base Image:
|`docker pull nvcr.io/nvidia/clara-train-sdk:v3.1.01`
|Notes:
|N/A

|===

=== MXNet

[[tr6]]
// As a Domino user, I can create a new Environment using an NGC MXNet base image.

[cols="1a,4a",options="header"]
|===
| <|https://ngc.nvidia.com/catalog/containers/nvidia:mxnet
|Domino registry path: |`quay.io/domino/ngc-mxnet:20.12-py3`

|Versions:
|MXNet v21.03, Ubuntu 18.04, CUDA 11
|Base Image:
|`docker pull nvcr.io/nvidia/mxnet:21.03-py3`
|Notes:
|N/A

|===

[[tr7]]
// As a Domino user, I can execute a workload on a NGC image based Environment using a GPU-based hardware tier.

----- user_guide/customize-environments/use-external-images/stata.txt -----
:page-version: 6.1
:page-title: Stata
:page-permalink: 063fca
:page-order: 30

= Create a Stata Environment in Domino

[[tr1]]
// As a Domino user, I can start an interactive Stata session using a properly configured Environment.

Stata® statistical software provides all the features you need for data science and inference–data manipulation, exploration, visualization, statistics, reporting, and reproducibility.

Stata has the following versions:

* Basic Edition (BE)
* Standard Edition (SE)
* Multiprocessor Edition (MP)

NOTE: To learn more about Stata, see https://www.stata.com/features/[Stata features^],  https://www.stata.com/why-use-stata/[Stata benefits^], and https://www.stata.com/products/which-stata-is-right-for-me/[compare versions of Stata^].

== Get data from license file

[[tr2]]
// As a Domino user, I can create an Environment with the necessary content and license to support Stata Workspaces.

To use a Stata workspace in Domino you must have a valid Stata license file.
The license from Stata comes in a PDF file.
You need the following data from this file:

* Licensed software version (either `MP`, `SE`, or `BE`)
* Serial number (12-digit number)
* Code (series of nine 4-character codes)
* Authorization (4-character code)

== Create a license file

. Open the Stata Environment in Domino.
. Right-click the background and go to *applications > shells > bash*.
. Go to the Stata program directory:
+
----
cd /usr/local/stata
----
. As superuser, run the Stata init program:
+
----
sudo ./stinit
----

. Follow the prompts to enter the serial number, code, and authorization.

. Follow the additional prompts to enter the organization name and location.
+
NOTE: These values are not validated and can be your company name and headquarters location.

Completing this process generates the Stata license file `stata.lic` in the `stata` directory.
Copy the contents of this file to a safe location.

== Create Environment

. When you create a Domino workspace Environment for Stata, use the following custom base image:
+
----
quay.io/domino/stata:17
----
. Update the Dockerfile instructions with the following Docker commands.
+
[source,shell]
----
ENV STATA_VERSION="MP"
RUN apply-license “<url_to_license_file>”
----
+
Ensure the `STATA_VERSION` matches the customers' licensed version.
The version must be `MP`, `SE`, or `BE`.
+
. Paste the following code into the Environments pluggable workspace properties.
+
[source,yaml]
----
stata:
    title: "Stata"
    iconUrl: "https://raw.githubusercontent.com/dominodatalab/partner-resources/main/assets/workspaces/stata/statamp.png"
    start: [ "/opt/domino/workspaces/stata/start" ]
    httpProxy:
        port: 8080
        internalPath: "/{{ownerUsername}}/{{projectName}}/{{sessionPathComponent}}/{{runId}}/#/?username=domino&password=domino"
        rewrite: false
        requireSubdomain: false
----
+
See the following for more resources:

* https://www.stata.com/support/[Stata Support Center^]
* https://www.stata.com/features/documentation/[Stata Documentation^]
* https://www.stata.com/learn/[Stata Training^]
* https://www.stata.com/links/examples-and-datasets/[Stata Examples and Datasets^]
* https://www.stata.com/python/pystata18/[PyStata—Python and Stata^]

----- user_guide/customize-environments/work-with-pluggable-workspaces/add-a-scala-kernel.txt -----
:page-version: 6.1
:page-title: Add a Scala kernel
:page-permalink: d8e0a6
:page-order: 30

By default the link:0d73c6[Domino Standard Environment] includes kernels for Python and Julia.
However, you can add new kernels to your Environment in just a few steps.
In this topic, you will learn how to add a Scala kernel to your Environment.

[[tr1]]
// As a Domino user, I can install additional kernels to my Environment.
== Install dependencies

[source,dockerfile]
----
USER root

# Install scala (requires java to already be installed)
RUN echo "deb https://repo.scala-sbt.org/scalasbt/debian all main" | sudo tee /etc/apt/sources.list.d/sbt.list && 
curl -sL "https://keyserver.ubuntu.com/pks/lookup?op=get&search=0x2EE0EA64E40A89B84B2DF73499E82A75642AC823" | sudo apt-key add && 
apt-get update && apt-get install sbt && 
rm -rf /var/lib/apt/lists/* && 
rm -Rf /tmp/*

https://xxx[USER] ubuntu

# Install scala kernel (almond)
RUN sudo curl -Lo coursier https://git.io/coursier-cli && 
    sudo chmod +x coursier && 
    ./coursier launch --fork almond:0.10.9 --scala 2.12 -- --install && 
    sudo rm -f coursier
----

After adding these commands to your Environment Dockerfile you can start Scala notebooks and run Scala commands directly from the JupyterLab console.

You can add new kernels to Domino Environments like you added the scala kernel.
See https://almond.sh/docs/quick-start-install[https://almond.sh/docs/quick-start-install^] for more information about how to install the Scala kernel.


----- user_guide/customize-environments/work-with-pluggable-workspaces/add-workspace-ides.txt -----
:page-version: 6.1
:page-title: Add Workspace IDEs
:page-permalink: 03e062
:page-order: 10

Domino is interoperable with your favorite languages, development tools and software. Learn how to bring your favorite IDEs to Domino.

When you add a Workspace IDE to your Compute Environment, you can:

* Upgrade to a newer version of currently supported Domino tools such as Jupyter or RStudio.
* Add new web-based tools like JupyterLab.
* Manage the standard default tool for your team or organization across all projects.

//move this info to admin guide
NOTE: Your administrator must be aware that when the `ShortLived.PluggableInteractiveSessionSubdomains` feature flag is set to `false`, the `httpProxy.requireSubdomain` becomes required, and must be set to `false`.
Any pluggable definitions without this flag explicitly set will be invalid and are not usable.


== Configure an IDE

You must set up the Environment's Docker image installation instructions and define how Domino will serve the tool.

NOTE: Typically, an administrator or advanced user does this work. Domino recommends that you contact your administrator for help defining the tool you want to use.

[[tr1]]
// As a Domino user, I can install and configure a new Workspace IDE tool in my Environment Dockerfile instructions.

. In your Environment, enter the instructions to install and configure a tool in the Dockerfile instructions:

=== For Python version > 2.7.9

--
[source,Dockerfile]
----
###Remove any old workspaces
RUN 
apt-get remove rstudio-server -y && 
 rm -rf /usr/local/lib/rstudio-server/rstudio-server && 
 rm -rf /var/opt/workspaces
###Setup workspaces directory and retrieve workspace configs
RUN mkdir /var/opt/workspaces
RUN cd /tmp && wget https://github.com/dominodatalab/workspace-configs/archive/2018q2-v1.9.zip && unzip 2018q2-v1.9.zip && cp -Rf workspace-configs-2018q2-v1.9/. /var/opt/workspaces && 
rm -rf /var/opt/workspaces/workspace-logos && rm -rf /tmp/workspace-configs-2018q2-v1.9
#add update .Rprofile with Domino customizations
RUN 
mv /var/opt/workspaces/rstudio/.Rprofile /home/ubuntu/.Rprofile && 
chown ubuntu:ubuntu /home/ubuntu/.Rprofile
# # # #Install Rstudio from workspaces
RUN chmod +x /var/opt/workspaces/rstudio/install
RUN /var/opt/workspaces/rstudio/install
# # # # # #Install Jupyterlab from workspaces
RUN chmod +x /var/opt/workspaces/Jupyterlab/install
RUN /var/opt/workspaces/Jupyterlab/install
# # #Install Jupyter from workspaces
RUN chmod +x /var/opt/workspaces/jupyter/install
RUN /var/opt/workspaces/jupyter/install
# Clean up temporary files
RUN 
 rm -Rf /var/lib/apt/lists/* && 
 rm -Rf /tmp/*
----
--

=== For Python version < 2.7.9

--
[source,dockerfile]
----
###Remove any old workspaces
RUN 
apt-get remove rstudio-server -y && 
 rm -rf /usr/local/lib/rstudio-server/rstudio-server && 
 rm -rf /var/opt/workspaces

###Setup workspaces directory and retrieve workspace configs
RUN mkdir /var/opt/workspaces
RUN cd /tmp && wget https://github.com/dominodatalab/workspace-configs/archive/2018q2-v1.9.zip && unzip 2018q2-v1.9.zip && cp -Rf workspace-configs-2018q2-v1.9/. /var/opt/workspaces && 
rm -rf /var/opt/workspaces/workspace-logos && rm -rf /tmp/workspace-configs-2018q2-v1.9

#add update .Rprofile with Domino customizations
RUN 
mv /var/opt/workspaces/rstudio/.Rprofile /home/ubuntu/.Rprofile && 
chown ubuntu:ubuntu /home/ubuntu/.Rprofile

# # # #Install Rstudio from workspaces
RUN chmod +x /var/opt/workspaces/rstudio/install
RUN /var/opt/workspaces/rstudio/install

# # # # # #Install Jupyterlab from workspaces (pinned to avoid working directory bug in Jupyterlab)
RUN pip install jupyterlab==0.31.12

# # #Install Jupyter from workspaces
RUN chmod +x /var/opt/workspaces/jupyter/install
RUN /var/opt/workspaces/jupyter/install
# Clean up temporary files
RUN 
 rm -Rf /var/lib/apt/lists/* && 
 rm -Rf /tmp/*
----
--

. Notebook properties are stored as `yaml` data mapping notebook names to their definitions.
Enter this in the
*Pluggable Workspace Tools*
field in the Environment definition.
+
For example:

[source,yaml]
----
jupyter:
  title: "Jupyter (Python, R, Julia)"
  iconUrl: "/assets/images/workspace-logos/Jupyter.svg"
  start: [ "/var/opt/workspaces/jupyter/start" ]
  supportedFileExtensions: [ ".ipynb" ]
  httpProxy:
    port: 8888
    rewrite: false
    internalPath: "/{{ownerUsername}}/{{projectName}}/{{sessionPathComponent}}/{{runId}}/{{#if pathToOpen}}tree/{{pathToOpen}}{{/if}}"
    requireSubdomain: false
jupyterlab:
  title: "JupyterLab"
  iconUrl: "/assets/images/workspace-logos/jupyterlab.svg"
  start: [  "/var/opt/workspaces/Jupyterlab/start.sh" ]
  httpProxy:
    internalPath: "/{{ownerUsername}}/{{projectName}}/{{sessionPathComponent}}/{{runId}}/{{#if pathToOpen}}tree/{{pathToOpen}}{{/if}}"
    port: 8888
    rewrite: false
    requireSubdomain: false
vscode:
  title: "vscode"
  iconUrl: "/assets/images/workspace-logos/vscode.svg"
  start: [ "/var/opt/workspaces/vscode/start" ]
  httpProxy:
    port: 8888
    requireSubdomain: false
rstudio:
  title: "RStudio"
  iconUrl: "/assets/images/workspace-logos/Rstudio.svg"
  start: [ "/var/opt/workspaces/rstudio/start" ]
  httpProxy:
    port: 8888
    requireSubdomain: false
----

----- user_guide/customize-environments/work-with-pluggable-workspaces/configure-native-workspaces.txt -----
:page-version: 6.1
:page-permalink: 4e7f25
:page-title: Configure native Workspaces
:page-order: 20

[CAUTION]
====
This is an advanced topic.
Use this only if you want to change the way the default tools are run, or if you want to use Workspace Tools that were not configured by default.
====

To replace the default tools in a Domino Environment (such as JupyterLab or VS Code) with the tools in your image, you can add a startup script for each tool to your Environment.
To do this, you can either:

* Add the script to the base image.
* Add the script to the Dockerfile instructions.

[NOTE]
====
Domino recommends using JupyterLab instead of Jupyter.
It is a newer offering with superior features and performance.
====

See the following instructions for these tools:

== VS Code
See link:fbb1db[Using Visual Studio Code in Domino Workspaces].

== JupyterLab

The following example script shows how the JupyterLab script must configure access to files in the Environment, set up the working directory, and define the necessary routing.

[source,shell]
----
BASE_URL="/$DOMINO_PROJECT_OWNER/$DOMINO_PROJECT_NAME/notebookSession/$DOMINO_RUN_ID"
CONF_FILE="$HOME/.jupyter/jupyter_lab_config.py"
mkdir -p $(dirname "$CONF_FILE")

cat > "$CONF_FILE" << EOF
c = get_config()

# Lab has access to all files in the Environment
c.NotebookApp.notebook_dir='/'

# Lab starts in the domino working dir
c.NotebookApp.default_url='/lab/tree${DOMINO_WORKING_DIR}'

# Routing, networking and access
c.NotebookApp.base_url='${BASE_URL}/'
c.NotebookApp.tornado_settings={'headers': {'Content-Security-Policy': 'frame-ancestors *'}, 'static_url_prefix': '${BASE_URL}/static/'}
c.NotebookApp.token=u''
c.NotebookApp.iopub_data_rate_limit=10000000000

# The default cell execution timeout in nbconvert is 30 seconds, set it to a year
c.ExecutePreprocessor.timeout=365*24*60*60

EOF
----

== Other tools

See link:03e062[Domino pluggable notebooks].

After adding the script to the Environment, modify the *Pluggable Workspace Tools* section of the Environment definition to define the entrypoint for Workspace tool startup:

. Go to the Environment you want to edit.
. Click *Edit Definition*.
. Go to *Pluggable Workspace Tools*.
. Modify the entry so that the *start* field points to your tool's launch script.

----- user_guide/customize-environments/work-with-pluggable-workspaces/index.txt -----
:page-version: 6.1
:page-title: Work with Pluggable Workspaces
:page-permalink: 35ff43
:page-order: 30

Domino is interoperable with your favorite languages, development tools and software. Learn how to bring your own tools to Domino.

link:03e062[Add Workspace IDEs]::
Steps to configure an IDE for your Workspace.

link:4e7f25[Configure native Workspaces]::
Change the way the default tools are run, or use tools not configured by default.

link:d8e0a6[Add a Scala kernel]::
Create Scala notebooks.

link:74ed74[Use TensorBoard in Jupyter Workspace]::
Install and enable the Jupyter-TensorBoard
server extension in your Compute Environment.

link:e7805a[Use SAS as a Workspace]::
Use SAS for Containers on Domino.

----- user_guide/customize-environments/work-with-pluggable-workspaces/sas-workspace.txt -----
:page-version: 6.1
:page-title: SAS on Domino
:page-permalink: e7805a
:page-order: 50

Learn how to bring your SAS projects to Domino to unify your data science teams and provide all the benefits of the Domino platform to SAS users. Domino supports link:https://www.sas.com/en_us/software/analytics-pro/features-list.html[SAS Analytics Pro, SAS Analytics Pro - Advanced Programming], link:https://documentation.sas.com/doc/en/containers/9.4/n133nr0ok71e5pn1oy96124cg1iz.htm[SAS Analytics for Containers], and link:https://github.com/sassoftware/model-container-recipes[SAS Data Science programming-only], with support for Viya and SAS platforms.

To use a SAS Workspace, your administrator must configure the Compute Environment and Workspace for you. Once configured, you can switch your Project to the SAS-enabled Environment. With containerization, multiple SAS users can work on the same SAS code using separate Workspaces.

Contact your Domino representative for more information about configuring SAS Workspaces on Domino.


== SAS Analytics Pro 

This section provides an overview of the setup process for SAS Analytics Pro on Domino (including Advanced Programming). Contact your Domino representative to learn more.

. When you receive your license from SAS, you'll receive a credential token to access the SAS container repository. Use the token to pull the base image for SAS Analytics Pro onto your local machine.
. Use a Dockerfile, provided by your Domino representative, to build a Domino-compatible image.
. Include your SAS license file, `license.jwt`, in the same location as the Dockerfile when building the Docker image.
+
The license file will be copied into the container and the SAS license utility will run to license the software.
+
. Push your newly created Docker image to a repository that your Domino deployment can access.
+
This may be a repository hosted by your organization or a private repository within the Domino cluster.
+
. Once you have created the Docker image, create a new Domino Environment using the custom image path.
. Add the pluggable properties provided by your Domino representative, and make sure the “start:” parameter points to the start script created in the Dockerfile `/opt/domino/workspaces/sas/start`.
. After building the Environment in Domino, you will be able to select it in the Workspace launch menu when starting a new Workspace in your Project.


== SAS Analytics for containers on Domino

Learn how to build SAS as a container. The following instructions are similar to the official link:https://documentation.sas.com/doc/en/containers/9.4/n08crk346hlahon17mj16pmdj8t5.htm[SAS instructions for containerization], but with a few key differences specific to Domino. To learn about the benefits of running SAS as a container, see link:https://documentation.sas.com/doc/en/containers/9.4/n133nr0ok71e5pn1oy96124cg1iz.htm[SAS: Build and run a container].

In this guide, you learn how to:

. Get your SAS license.
. Create a TAR file of SAS Studio.
. Create a Docker image that includes:
.. SAS Studio TAR file.
.. Dockerfile with instructions to build the Docker image.
.. Start script provided by Domino.
. Use the Docker image to create pluggable Workspaces in Domino.
. Optional: Persist SAS preferences and add-ons.

== Get your SAS license

To use a SAS Workspace in Domino, you must have a valid SAS license file.

. Go to `my.sas.com` to download the license file.
. Save the license file securely, you will use the license file to build the SAS Docker image later.

== Create the SAS Studio TAR file

Bundle SAS Studio in a TAR file to run in your Docker container.

. Run the SAS Deployment Wizard to install SAS Studio on a Linux machine with the same version and distribution as the Domino Compute Environment that you want to run the SAS container on.
. During the installation, change the default location for the SAS Studio installation to `/usr/local/SASHome`.
. Manually create a TAR file of SAS Studio that includes the `SASHome` directory.
[source,bash]
----
tar — cvf SASHomeTar.tar /usr/local/SASHome
----

[[dockerfile-instructions]]
== Create the Dockerfile instructions

Ensure that your Dockerfile includes a command to download and apply the valid SAS license file.

The license file must be hosted on a server accessible through an HTTP request from the Domino cluster.
When building the Environment image, this file must be available to the build host without authentication restrictions.

Enter the following command in your *Dockerfile Instructions*:
[source,docker]
----
RUN apply-license <url_to_license_file>
----

Base64 example
[source,docker]
----
ENV SAS_LICENSE=”UgbWFnbmEgYWx...pcXVhLgo=”

RUN apply-license $SAS_LICENSE
----

Project file example

In your *Pre Run Script*:
[source,bash]
----
apply-license /mnt/my-sas-license.jwt
----

== Create a start script

Create a start script in your Docker image to start SAS Studio.

The following start script is a basic example, reach out to your Domino representative for guidance on creating a start script to suit your specific needs.

[source,bash]
----
#!/usr/bin/env bash
set -o errexit -o pipefail

# Update SAS Settings to start in Domino Project directory
sed -i "s#webdms.customPathRoot=#webdms.customPathRoot=$DOMINO_WORKING_DIR#g" 
    /usr/local/SASHome/SASStudioBasic/3.8/war/config/config.properties

# Set the prefix path to the SAS web service
PREFIX_PATH="${DOMINO_PROJECT_OWNER}/${DOMINO_PROJECT_NAME}/notebookSession/${DOMINO_RUN_ID}"
PREFIX_FILE="${DOMINO_PROJECT_OWNER}#${DOMINO_PROJECT_NAME}#notebookSession#${DOMINO_RUN_ID}"
sed -i "s#path="/SASStudio"#path="/${PREFIX_PATH}"#g" /usr/local/SASHome/sas/appserver/studio/conf/Catalina/localhost/SASStudio.xml
mv /usr/local/SASHome/sas/appserver/studio/conf/Catalina/localhost/SASStudio.xml 
   "/usr/local/SASHome/sas/appserver/studio/conf/Catalina/localhost/${PREFIX_FILE}#SASStudio.xml"

# Start SAS Studio and keep the container alive while it runs
sudo -E bash -c "/usr/local/SASHome/sas/sasstudio.sh start && while true ; do :; sleep 60 ; done"
----

== Create the Docker image

Create the Docker image in a repository that Domino can access that includes the following:

- SAS Studio TAR file
- Domino-provided start script
- Dockerfile with instructions

== Enable SAS Workspace with pluggable Workspace properties

Paste the following code into *Pluggable Workspace Tools* to enable the SAS workspace.

[source,yaml]
----
sas:
    title: "SAS"
    iconUrl: "https://upload.wikimedia.org/wikipedia/commons/1/10/SAS_logo_horiz.svg"
    start: [ "/opt/domino/workspaces/sas/start" ]
    httpProxy:
        internalPath: "/{{ownerUsername}}/{{projectName}}/{{sessionPathComponent}}/{{runId}}/SASStudio/"
        port: 8888
        rewrite: false
        requireSubdomain: false
----

== Optional: Persist SAS preferences and add-ons

To persist your SAS Studio preferences between Domino sessions, you can define a *Post Run Script* to automatically copy them to your Project Files when you *Stop and Sync* a session, and define a *Pre Run Script* to copy them from your Project Files into locations that SAS Studio recognizes at the start of subsequent sessions.

. Add the following lines in the *Pre Run Script*.
+
[source,bash]
----
# Copy preferences to Workspace
PREFS_DIR="$DOMINO_WORKING_DIR/.sasstudio5"
mkdir -p $PREFS_DIR
rsync -rpq $PREFS_DIR $HOME
----
. Add the following lines in the *Post Run Script*.
+
[source,bash]
----
# Copy preferences from Workspace
PREFS_DIR="$HOME/.sasstudio5"
mkdir -p $PREFS_DIR
rsync -rpq $PREFS_DIR $DOMINO_WORKING_DIR
----
+
These scripts save add-ons per Project, so collaborators launching a Workspace in the same Project will have the same add-ons.
However, the scripts save preferences per user in the Project so that each user can have a different set of preferences.

== SAS Data Science Programming 3.5

To run SAS Data Science Programming 3.5 on Domino, contact your Domino representative to learn more. The following steps provide an overview of the process:

. Build the SAS Data Science Programming 3.5 container using the link:link:https://github.com/sassoftware/model-container-recipes[SAS container recipes repository^]. The Dockerfile expects a centos7 base image with the SAS 3.5 components installed (CAS, SAS Studio, Base SAS, etc.).
. The SAS license file, `license.jwt`, will need to be in the same location as the Dockerfile when building the Docker image.
+
The license file will be copied into the container and the SAS license utility will run to license the software.
+
. Push your newly created Docker image to a repository that your Domino deployment can access.
+
This may be a repository hosted by your organization or a private repository within the Domino cluster.
+
. Once you have created the Docker image, create a new Domino Environment using the custom image path.
. Add the pluggable properties provided by your Domino representative, and make sure the “start:” parameter points to the start script created in the Dockerfile `/opt/domino/workspaces/sas/start`.
. After building the Environment in Domino, you will be able to select it in the Workspace launch menu when starting a new Workspace in your Project.

== Next steps

Learn how to link:23a442[access Domino Data Sources from SAS Workspaces].

----- user_guide/customize-environments/work-with-pluggable-workspaces/use-tensorboard-in-jupyter-workspaces.txt -----
:page-version: 6.1
:page-title: Use TensorBoard in Jupyter Workspaces
:page-sidebar: TensorBoard in Jupyter Workspaces
:page-permalink: 74ed74
:page-order: 40

// The version-specific differences in this topic look suspect and should be verified or corrected.

https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard[TensorBoard^] is a tool for visualizing TensorFlow data.
TensorBoard operates by reading events files, which contain summary data that generated by TensorFlow.
You can visualize your TensorFlow graph, plot quantitative metrics about graph, and show additional data that passes through the graph.

You can install and enable the
https://github.com/lspvic/jupyter_tensorboard[Jupyter-TensorBoard^]
server extension in your compute Environment to use TensorBoard in your Domino Jupyter.

== Set up your Environment

[[tr1]]
// As a Domino user, I can configure my Environment to be able to use TensorBoard in a Jupyter Workspace.

You must create or modify an Environment to enable this extension in your Domino Workspaces.
See link:f51038[Environment Management].

. In the navigation bar, click *Environments*.
. Click *Create Environment*.
. Give the Environment a name, and then select a base image that has
Python 3.8
installed.
You can use an Environment as a base image if it uses this Domino standard:
+
`quay.io/domino/domino-gpu-environment:ubuntu22-py3.10-domino6.1-gpu`
. After setting the *Visibility* and entering a description, click *Create Environment*.
The Environment's Overview page opens.
. Go to *Docker Settings* and click *Edit Dockerfile*.
. Add the following lines to the *Dockerfile Instructions*:
+
[source,dockerfile]
----
USER root

RUN apt-get update
RUN apt-get install -y --no-install-recommends libnode-dev npm nodejs
RUN npm install -g configurable-http-proxy

USER ubuntu
----

. Add the following lines to the *Pluggable Workspace Tools* to add JupyterLab workspace:
+
[source,dockerfile]

----
jupyterlab:
  title: "JupyterLab"
  iconUrl: "/assets/images/workspace-logos/jupyterlab.svg"
  start: [  "/opt/domino/workspaces/jupyterlab/start" ]
  httpProxy:
    internalPath: "/{{ownerUsername}}/{{projectName}}/{{sessionPathComponent}}/{{runId}}/{{#if pathToOpen}}tree/{{pathToOpen}}{{/if}}"
    port: 8888
    rewrite: false
    requireSubdomain: false
----

. Add the following lines to the *Pre-run script* if the Project you plan to use in this Compute Environment is a Domino Project:
+
[source,dockerfile]

----
#!/bin/bash
set -e

# Ensure required packages are installed (optional safety step)
pip install --quiet --upgrade jupyter-server-proxy tensorboard "notebook<7"

# Create Jupyter config directory
CONF_DIR="${HOME}/.jupyter"
mkdir -p "${CONF_DIR}"

# Write valid server proxy config for TensorBoard
cat << EOF > "${CONF_DIR}/jupyter_notebook_config.py"
c.ServerProxy.servers = {
    "tensorboard": {
        "command": [
            "tensorboard",
            "--logdir", "/domino/datasets/local/${DOMINO_PROJECT_NAME}/tensorboard_logs",
            "--host", "0.0.0.0",
            "--port", "{port}"
        ],
        "timeout": 30,
        "launcher_entry": {
            "title": "TensorBoard"
        }
    }
}
EOF
----
+
. Add the following lines to the *Pre-run script* if the Project is a Git-based Project:
+
[source,dockerfile]

----
#!/bin/bash
set -e

# Ensure required packages are installed (optional safety step)
pip install --quiet --upgrade jupyter-server-proxy tensorboard "notebook<7"

# Create Jupyter config directory
CONF_DIR="${HOME}/.jupyter"
mkdir -p "${CONF_DIR}"

# Write valid server proxy config for TensorBoard
cat << EOF > "${CONF_DIR}/jupyter_notebook_config.py"
c.ServerProxy.servers = {
    "tensorboard": {
        "command": [
            "tensorboard",
            "--logdir", "/mnt/data/${DOMINO_PROJECT_NAME}/tensorboard_logs",
            "--host", "0.0.0.0",
            "--port", "{port}"
        ],
        "timeout": 30,
        "launcher_entry": {
            "title": "TensorBoard"
        }
    }
}
EOF
----


. Click *Build*.
+
The *Revisions* page opens.
If the new revision builds successfully, you can use this Environment.

== Use Jupyter-TensorBoard

[[tr2]]
// As a Domino user, I can use TensorBoard in a Jupyter workspace running in a properly configured Environment.

. Open the Project you want to use with Jupyter-Tensorboard.
. In the navigation pane, click *Settings*.
. From *Compute Environment*, select the Environment you created previously.
A notification opens to verify that the new Environment is now set.
. In the navigation pane, click *Workspaces*, then select
*Jupyterlab*
and launch a new workspace.

. Select *Tensorboard* from the Launcher.

See the link:https://github.com/tensorflow/tensorboard/blob/master/README.md[TensorBoard README^] to learn how to consume TensorFlow events.

== Troubleshooting

. If you encounter issues loading Tensorboard initially, confirm that Tensorflow is operating properly.
Loading the application depends on Tensorflow.

. By default, Domino's standard Compute Environments have `tensorflow-gpu` installed (for example, `pip install tensorflow-gpu`).
Therefore, Tensorboard and Tensorflow will not work on a CPU hardware tier.
If you'd like to use Tensorboard on a CPU, make sure that CPU optimized Tensorflow is installed (for example, `pip install tensorflow`).


----- user_guide/data-source-connectors/connect-to-amazon-s3-with-aws-glue.txt -----
:page-version: 6.1
:page-permalink: 37a0a6
:page-title: Connect to Amazon S3 Tabular with AWS Glue
:page-sidebar: Amazon S3 Tabular with AWS Glue
:page-order: 40

Administrators can create a Data Source for https://docs.aws.amazon.com/athena/latest/ug/data-sources-glue.html[tabular S3 with AWS Glue^].  See the link:72362d[Admin Guide] for details.

After your administrator has created the Data Source, you can link:fa5f3a[work with it in your project] as usual.

== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].
* link:33ea62[Share this Data Source] with your collaborators.

----- user_guide/data-source-connectors/connect-to-amazon-s3.txt -----
:page-version: 6.1
:page-title: Connect to Amazon S3
:page-sidebar: Amazon S3
:page-order: 30
:page-permalink: dd93d4

This topic describes how to connect to https://aws.amazon.com/s3/[Amazon Simple Storage Service (S3)^] from Domino.
You must have network connectivity between S3 and your Domino deployment.

The easiest way to connect to an Amazon S3 instance from Domino is to create a Domino Data Source as described below.

NOTE: Administrators can create a Data Source for https://docs.aws.amazon.com/athena/latest/ug/data-sources-glue.html[tabular S3 with AWS Glue^]. See the link:72362d[Admin Guide] for details.

== Create an Amazon S3 Data Source

. From the navigation pane, click *Data > Data Sources*.
. Click *Create a Data Source*.
. In the New Data Source window, from *Select Data Store*, select *Amazon S3*.
. Enter the *Bucket*.
. Optional: Enter a *Subfolder Path*, such as `myFolder` or `data/tests`, to restrict the Data Source to only list data from and write to the specified folder.
. Enter the *Region*.
. Enter the *Data Source Name*.
. Optional: Enter a *Description* to explain the purpose of the Data Source to others.
. Click *Next*.
. Enter the credentials to authenticate to S3.
+
By default, Domino supports basic authentication; the Domino secret store backed by HashiCorp Vault securely stores the credentials.
If your administrator enabled it, link:6a8639#iam-auth[IAM credential propagation] might be available.
+
NOTE: IAM-authenticated connections can be used only to execute jobs and workspaces.
Other execution types, such as scheduled jobs and Domino endpoints, require basic authentication.

. Click *Test Credentials*.
. If the Data Source authenticates, click *Next*.
. Select who can view and use the Data Source in projects.
. Click *Finish Setup*.


== Alternate way to connect to an Amazon S3 Data Source

WARNING: This section describes an alternate method to connect to the Amazon S3 Data Source.
Domino does not officially support this method.



//ifeval::[{iversion} >= 401]
Use one of the following methods to authenticate with S3 from Domino.
// One of which methods?  Are the headings below messed up?

Both follow the common naming convention of environment variables for AWS packages so you don't have to explicitly reference credentials in your code.


== Setup credentials

. Use a short-lived credential file obtained through Domino's link:eb6a88[AWS Credential Propagation] feature.
+
After your administrator configures this feature, Domino automatically populates any Run or Job with your AWS credentials file.
These credentials will be periodically refreshed throughout the Workspace duration to ensure they don't expire.
+
Following common AWS conventions, you will see an environment variable
`AWS_SHARED_CREDENTIALS_FILE` which contains the location of your credential files which will be stored at `/var/lib/domino/home/.aws/credentials`.
+
Learn more about https://aws.amazon.com/blogs/security/a-new-and-standardized-way-to-manage-credentials-in-the-aws-sdks/[using a credential file with the AWS SDK^].

. Store your AWS access keys securely as environment variables.
+
To connect to the S3 buckets to which your AWS account has access, enter your
link:https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html[AWS Access Key and AWS Secret Key^] to the AWS CLI.
By default, AWS utilities https://docs.aws.amazon.com/cli/latest/userguide/cli-environment.html[look for these in your environment variables^].
+
Set the following as Domino environment variables on your user account:
+
--
* `AWS_ACCESS_KEY_ID`
* `AWS_SECRET_ACCESS_KEY`
--
+
See link:d8dde6[Store Project credentials] to learn more about Domino environment variables.




=== Get a file from an S3-hosted public path



. If you have files in S3 that are set to allow public read access, use https://www.gnu.org/software/wget/[Wget^] from the OS shell of a Domino executor to fetch the files.
The request for those files will look similar to the following:
+
[source,bash]
----
wget https://s3-<region>.amazonaws.com/<bucket-name>/<filename>
----
+
This method is simple but doesn't require authentication or authorization.
Do not use this method with sensitive data.


=== AWS CLI



. Use the https://docs.aws.amazon.com/cli/latest/reference/index.html#cli-aws[AWS CLI^] for a secure method to read S3 from the OS shell of a Domino executor.
Making the AWS CLI work from your executor, install it in your environment and enter your credentials.
. Get the AWS CLI as a Python package from https://pypi.org/project/pip/[pip^].
. Use the following Dockerfile instruction to install the CLI and automatically add it to your system PATH.
You must have https://pypi.org/project/pip/[pip^] installed.

+
[source,dockerfile]
----
USER root
RUN pip install awscli --upgrade
USER ubuntu
----

. After your Domino environment and credentials are set up, fetch the contents of an S3 bucket to your current directory by running:
+
[source,bash]
----
aws s3 sync s3://<bucket-name> .
----

+
. If you are using an AWS credential file with multiple profiles, you might need to specify the profile.
(The `default` profile is used if none is specified.)
+
[source,bash]
----
aws s3 sync s3://<bucket-name> . --profile <profile name>
----
+
See the https://docs.aws.amazon.com/cli/latest/reference/s3/index.html[official AWS CLI documentation^] on S3 for more commands and options.


=== Python and boto3



. To interact with AWS services from Python, Domino recommends https://aws.amazon.com/sdk-for-python/[boto3^].
. If you're using a link:0d73c6[Domino Standard Environment], boto3 will already be installed.
If you want to add boto3 to an environment, use the following Dockerfile instructions.
+
This instruction assumes you already have https://pypi.org/project/pip/[pip^] installed.

+
[source,dockerfile]
----
USER root
RUN pip install boto3
USER ubuntu
----
. To interact with S3 from boto3, see https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html[the
official boto3 documentation^].
The following is an example of downloading a file where:

* You have set up your credentials as instructed above
* Your account has access to an S3 bucket named `my_bucket`
* The bucket contains an object named `some_data.csv`
+
[source,python]
----
import boto3
import io
import pandas as pd

# Create a new S3 client
client = boto3.client('s3')

# Download some_data.csv from my_bucket and write to ./some_data.csv locally
file = client.download_file('my_bucket', 'some_data.csv', './some_data.csv')
----
+
Alternatively, for users using a credential file, use the following code:
+
[source,python]
----
import boto3

# Specify your profile if your credential file contains multiple profiles
session = boto3.Session(profile_name='<profile name>')

# Specify your bucket name
users_bucket = session.resource('s3').Bucket('my_bucket')

# 'list' bucket should succeed
for obj in users_bucket.objects.all():
   print(obj.key)

# Download a file
users_bucket.download_file('some_data.csv', './some_data.csv')
----
+
This code does not provide credentials as arguments to the client constructor, since it assumes:
+
* Credentials will be automatically populated at `/var/lib/domino/home/.aws/credentials` as specified in the environment variable `AWS_SHARED_CREDENTIALS_FILE`
* You have already set up credentials in the `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` environment variables.

After running the previous code, a local copy of _$$some_data.csv$$_ exists in the same directory as your Python script or notebook.
You can load the data into a https://pandas.pydata.org/[pandas^] dataframe.

[source,python]
----
df = pd.read_csv('some_data.csv')
----

See link:0ed610[Get your files and data] in the link:9a69d9[Get Started with Python] tutorial for a more detailed example of working with CSV data in Python.


=== R and aws.s3



. To connect to S3 from R, Domino recommends the https://cloudyr.github.io/[cloudyr project's^] package called https://github.com/cloudyr/aws.s3[aws.s3^].
. If you're using a link:0d73c6[Domino Standard Environment], aws.s3 will already be installed.
To add aws.s3 to an environment, use the following Dockerfile instructions.

+
[source,dockerfile]
----
USER root

RUN R -e 'install.packages(c("httr","xml2"), repos="https://cran.r-project.org")'
RUN R -e 'install.packages("aws.s3", repos = c("cloudyr" = "http://cloudyr.github.io/drat"))'

USER ubuntu
----

. For basic instructions about using aws.s3 see the https://github.com/cloudyr/aws.s3/blob/master/README.md[package README^].
The following is an example of downloading a file where:

* You have set up the environment variables with credentials for your AWS account
* Your account has access to an S3 bucket named `my_bucket`
* The bucket contains an object named `some_data.csv`

+
[source,r]
----
# Load the package
library("aws.s3")

# If you are using a credential file with multiple profiles. Otherwise, this can be excluded.
Sys.setenv("AWS_PROFILE" = "<AWS profile>")

# Download some_data.csv from my_bucket and write to ./some_data.csv locally
save_object("some_data.csv", file = "./some_data.csv", bucket = "my_bucket")
----

. After running the previous code, a local copy of _$$some_data.csv$$_ exists in the same directory as your R script or notebook.
Read from that local file to work with the data it contains.
+
[source,r]
----
myData <- read.csv(file="./some_data.csv", header=TRUE, sep=",")
View(myData)
----


== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].
* link:33ea62[Share this Data Source] with your collaborators.

----- user_guide/data-source-connectors/connect-to-azure-blob-storage.txt -----
:page-version: 6.1
:page-permalink: 3ca4ff
:page-title: Connect to Azure Blob Storage
:page-sidebar: Azure Blob Storage
:page-order: 50

Azure Blob Storage stores massive amounts of unstructured object data, such as text or binary data. Domino Data Sources is the easiest way to connect to Azure Blob Storage from Domino.

== Create an Azure Blob Storage Data Source

. From the navigation pane, click *Data > Data Sources*.
. Click *Create a Data Source*.
. In the New Data Source window, from *Select Data Store*, select *Azure Blob Storage*.
. Enter unique names for the account, container, and Data Source. Optionally, enter a *Description* to explain the purpose of the Data Source to others.
. Enter your Azure Blob Storage link:https://learn.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage?tabs=azure-portal#view-account-access-keys[Access Key^].
The Domino secret store backed by HashiCorp Vault securely stores the credentials.
. Click *Test Credentials* to validate authentication.
. Select who can view and use the Data Source in Projects.
. Click *Finish Setup*.

== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].
* link:33ea62[Share this Data Source] with your collaborators.

----- user_guide/data-source-connectors/connect-to-azure-data-lake-storage.txt -----
:page-version: 6.1
:page-permalink: 06b9bd
:page-title: Connect to Azure Data Lake Storage
:page-sidebar: Azure Data Lake Storage
:page-order: 60

Azure Data Lake Storage (ADLS) is a cloud-based data-warehouse.
This topic describes how to connect to https://docs.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage?tabs=azure-portal#view-account-access-keys[account access keys^] from Domino.
You must have network connectivity between Azure Data Lake Storage and your Domino deployment.

The easiest way to connect to a Azure Data Lake Storage instance from Domino is to create a Domino Data Source as described below.

== Create an ADLS Data Source

. From the navigation pane, click *Data > Data Sources*.
. Click *Create a Data Source*.
. In the New Data Source window, from *Select Data Store*, select *ADLS* for *Azure Data Lake Store*.
. Enter the unique storage account name in *Account Name*.
. Enter the name of the *Container*.
. Enter the *Data Source Name*.
. Optional: Enter a *Description* to explain the purpose of the Data Source to others.
. Click *Next*.
. Enter your link:https://learn.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage?tabs=azure-portal#view-account-access-keys[Access Key^] to connect to Azure Data Lake Storage.
The Domino secret store backed by HashiCorp Vault securely stores the credentials.
. Click *Test Credentials*.
. If the Data Source authenticates, click *Next*.
. Select who can view and use the Data Source in projects.
. Click *Finish Setup*.

== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].
* link:33ea62[Share this Data Source] with your collaborators.

----- user_guide/data-source-connectors/connect-to-bigquery.txt -----
:page-version: 6.1
:page-title: Connect to BigQuery
:page-sidebar: BigQuery
:page-permalink: a6daac
:page-order: 70

Domino can connect to and query any common database, including Google BigQuery.
The easiest way to connect to a https://cloud.google.com/bigquery/[Google BigQuery^] instance from Domino is to create a Domino Data Source as described below.

You must have network connectivity between BigQuery and your Domino deployment.

== Create a BigQuery Data Source

. From the navigation pane, click *Data > Data Sources*.
. Click *Create a Data Source*.
. In the New Data Source window, from *Select Data Store*, select *Google BigQuery*.
. Optional: Enter the unique identifier for your project in *GCP Project ID*.
. Enter the *Data Source Name*.
. Optional: Enter a *Description* to explain the purpose of the Data Source to others.
. Click *Next*.
. Copy your *Private Key (JSON format)*.
See https://cloud.google.com/docs/authentication/getting-started#creating_a_service_account[creating a service account^] for instructions about creating a service account and downloading the JSON credentials file.
You must copy the entire content of the file.
The Domino secret store backed by HashiCorp Vault securely stores the credentials.
. Click *Test Credentials*.
. When your Data Source authenticates, click *Next*.
. Enter the users and organizations who can view and use the Data Source in projects.
. Click *Finish Setup*.
+
image::/images/5.2/bigquery-setup.png[alt="The BigQuery setup", width=800, role=noshadow]

== Alternate way to connect to a Google BigQuery Data Source

You can use the link:https://cloud.google.com/bigquery/docs/reference/libraries-overview[BigQuery API^] to query any Dataset and table to which you have access, whether it's public or private, by authenticating with your Google Cloud private key.

WARNING: This section describes an alternate method to connect to the Google BigQuery Data Source.
Domino does not officially support this method.

. Go to Google's Service Accounts page.
Select a previous project or create a new project.
+
image::/images/4.x/mceclip2.png[alt="Select a project", width=500, role=noshadow]
+
If you selected *Create*, the New Project page opens:
+
image::/images/4.x/mceclip4.png[alt="Create a new project", width=500, role=noshadow]

. *Create a Service account* for your project.
+
image::/images/4.x/mceclip5.png[alt="Create a service account", width=500, role=noshadow]
. Define the access that the Service account must have to BigQuery.
See https://cloud.google.com/bigquery/docs/access-control[Google's Access Control documentation^] for more information.
+
image::/images/4.x/mceclip6.png[alt="Define the access", width=800, role=noshadow]
. Confirm that your Service account has been created.
+
image::/images/4.x/mceclip7.png[alt="New service account has been created", width=500, role=noshadow]
. On the Service Accounts page, create a new key.
+
image::/images/4.x/mceclip8.png[alt="Create a new key", width=600, role=noshadow]

. Download the JSON key and keep it in a safe place.
You will use this key later to programmatically authenticate to Google.
+
image::/images/4.x/mceclip10.png[alt="Obtain the JSON key", width=500, role=noshadow]

. To enable the BigQuery API, click the *Google APIs* logo.
+
image::/images/4.x/mceclip11.png[alt="Click the Google APIs logo", width=300, role=noshadow]

. In the Library page, select the *Big Query API*.
+
image::/images/4.x/mceclip13.png[alt="Select BigQuery API", width=300, role=noshadow]

. If it is not enabled, click *Enable*.
+
image::/images/4.x/mceclip14.png[alt="Enable the BigQuery API", width=300, role=noshadow]


=== Activate your credentials from Domino



Google Cloud uses the https://cloud.google.com/sdk/[Google Cloud SDK^] to activate your credentials.
This is already installed in the Domino Default environment.

Execute the following bash command:

----
/home/ubuntu/google-cloud-sdk/bin/gcloud auth activate-service-account <service account name> --key-file <key file path>
----

For example:

----
/home/ubuntu/google-cloud-sdk/bin/gcloud auth activate-service-account big-query-example@example-big-query-170823.iam.gserviceaccount.com --key-file key.json
----

You can use a custom Domino compute environment and enter this command in Domino pre-setup script to activate the credentials before each run.
Otherwise, you can execute them in workspace sessions.
See link:d8dde6[how to store your credentials securely].


=== Authenticate and query using Python



You need the `gcloud` and `oauth2client==1.4.12` Python packages.
Use the following package to install them in your custom Domino compute environment or in your workspace session.

----
pip install --user gcloud oauth2client==1.4.12
----

Use the following code to authenticate your Google credentials and query a public BigQuery table:

[source,python]
----
from oauth2client.client import GoogleCredentials
from googleapiclient.discovery import build

# Grab the application's default credentials from the environment.
credentials = GoogleCredentials.get_application_default()

# Construct the service object for interacting with the BigQuery API.
bigquery_service = build('bigquery', 'v2', credentials=credentials)

query_request = bigquery_service.jobs()
query_data = {
    'query': (
    'SELECT TOP(corpus, 10) as title, '
    'COUNT(*) as unique_words '
    'FROM [publicdata:samples.shakespeare];')
}

query_response = query_request.query(
    projectId='example-big-query-170823', # Substitute your ProjectId
    body=query_data).execute()

print('Query Results:')
for row in query_response['rows']:
    print('	'.join(field['v'] for field in row['f']))
----



== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].
* link:33ea62[Share this Data Source] with your collaborators.

----- user_guide/data-source-connectors/connect-to-clickhouse.txt -----
:page-version: 6.1
:page-permalink: 2c1c20
:page-title: Connect to ClickHouse
:page-sidebar: ClickHouse
:page-order: 80


Your link:f32f69[administrator must create a Data Source for this data store] for you to use. 

After your administrator has created the Data Source, you can work with it in your project as usual.

== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].
* link:33ea62[Share this Data Source] with your collaborators.

----- user_guide/data-source-connectors/connect-to-databricks.txt -----
:page-version: 6.1
:page-title: Connect to Databricks
:page-sidebar: Databricks
:page-permalink: 63393f
:page-order: 90

Learn how to connect to a https://docs.databricks.com/en/clusters/index.html[Databricks Cluster^] or https://docs.databricks.com/en/sql/admin/create-sql-warehouse.html[Databricks SQL Warehouse^] from Domino.

== Create a Databricks Data Source

. From the navigation pane, click *Data > Data Sources*.
. Click *Create a Data Source*.
. In the New Data Source window, from *Select Data Store*, select *Databricks*.
. Enter the server *Host*, *Port*, and cluster/warehouse *HTTP Path*.
Details on where to find these can be found on the link:https://docs.databricks.com/en/integrations/compute-details.html[Databricks Documentation^].
The host should not contain the protocol. For example, a valid host format is "name.cloud.databricks.com".
.. Optional: Enter the initial Catalog and/or Schema to use for this session.
If you include schema, you may need to include the corresponding catalog so it can be identifiable.
. Enter the Data Source *Name* and a *Description* to explain the purpose of the Data Source to others.
. Enter your *Personal Access Token* to connect to Databricks. The Domino secret store backed by HashiCorp Vault securely stores the credentials.
+
NOTE: If this Personal Access Token expires after you created the Data Source, you can generate a new
Personal Access Token on Databricks and edit this field with the new value.
. Click *Test Credentials* to validate the authentication.
. Select who can view and use the Data Source in projects.
. Click *Finish Setup*.

If your users have Domino permissions to use the Data Source and enter their credentials, they can now use the Domino Data API to retrieve data with the connector.

See link:fa5f3a[Retrieve data] for more information.

== Connect using the Databricks SDK

NOTE: Domino recommends that you connect to Databricks using Data Sources to take advantage of
flexible access controls, the link:140b48[Domino Data API], and receive technical support.
However, you can also connect using the link:https://docs.databricks.com/en/dev-tools/sdk-python.html[Databricks SDK^] if your business requires it.
This SDK allows for Databricks workspace and compute management among other data operations.

=== Prepare your workspace environment

* Ensure you have a workspace environment that uses a Python compatible link:03e062[workspace IDE] like Jupyter Notebook.
* Add Python 3.8 or above and the Databricks SDK package "databricks-sdk" to the environment.
For more information, please see link:bfa148[Add Packages to Environments].

NOTE: You may want to use the link:0d73c6[Domino Standard Environment] base image as a starting environment,
which is compatible with Python workspace IDEs and contains a Python version above 3.8.

=== Databricks authentication.

. Set up a https://docs.databricks.com/en/dev-tools/auth.html#config-profiles[Databricks configuration profile^].
. Add the ".databrickscfg" file mentioned in the link above to your project.
. Set the DATABRICKS_CONFIG_PROFILE link:6ac5a1#_define_custom_environment_variables[environment variable] to be the name of the custom configuration profile you want to use.
. Launch your workspace.
. Add the following lines at the top of your code file:

[source,python]
----
from databricks.sdk import WorkspaceClient

w = WorkspaceClient()
----

Alternatively, set environment variables for your workspace using your Databricks workspace host, token, and any other relevant configurations.
Pass them in as parameters when calling the WorkspaceClient function:

[source,python]
----
from databricks.sdk import WorkspaceClient

w = WorkspaceClient(
  host  = MY_HOST_VARIABLE,
  token = MY_TOKEN_VARIABLE
)
----

The Databricks SDK is ready to be used in your workspace.
For more information, please see the https://databricks-sdk-py.readthedocs.io/en/latest/[Databricks SDK documentation^].

== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].
* link:33ea62[Share this Data Source] with your collaborators.

----- user_guide/data-source-connectors/connect-to-datarobot.txt -----
:page-version: 6.1
:page-title: Connect to DataRobot
:page-sidebar: DataRobot
:page-permalink: 452cff
:page-order: 100

This topic describes how to connect directly to DataRobot from Domino.
You must have network connectivity between DataRobot and your Domino deployment.

Use the following commands to install the DataRobot Python client in the Dockerfile instructions section of your environment:

[source,dockerfile]
----
RUN pip install "datarobot==<version>"
----

== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].

* link:33ea62[Share this Data Source] with your collaborators.
----- user_guide/data-source-connectors/connect-to-druid.txt -----
:page-version: 6.1
:page-permalink: 738514
:page-title: Connect to Druid
:page-sidebar: Druid
:page-order: 110


Your link:f32f69[administrator must create a Data Source for this data store] for you to use. 

After your administrator has created the Data Source, you can work with it in your project as usual.

== Next steps
* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].
* link:33ea62[Share this Data Source] with your collaborators.

----- user_guide/data-source-connectors/connect-to-gcs.txt -----
:page-version: 6.1
:page-permalink: 95d78a
:page-title: Connect to Google Cloud Storage
:page-sidebar: Google Cloud Storage
:page-order: 130

This topic describes how to connect to https://cloud.google.com/storage/[Google Cloud Storage (GCS)^] from Domino.
You must have network connectivity between GCS and your Domino deployment.

The easiest way to connect to a GCS instance from Domino is to create a Domino Data Source as described below.

== Create a Google Cloud Storage Data Source

. From the navigation pane, click *Data > Data Sources*.
. Click *Create a Data Source*.
. In the New Data Source window, from *Select Data Store*, select *GCS* for *Google Cloud Storage*.
. Enter the name of the *Bucket*.
. Enter the *Data Source Name*.
+
NOTE: Although Google Cloud allows storage bucket names that include underscores, Domino only allows lowercase letters, numbers, periods, and hyphens.
+
. Optional: Enter a *Description* to explain the purpose of the Data Source to others.
. Click *Next*.
. Copy the *Private Key (JSON format)*.
See https://cloud.google.com/docs/authentication/getting-started#creating_a_service_account[creating a service account^] for instructions about creating a service account and downloading the JSON credentials file.
You must copy the entire content of the file.
The Domino secret store backed by HashiCorp Vault securely stores the credentials.
. Click *Test Credentials*.
. If the Data Source authenticates, click *Next*.
. Select who can view and use the Data Source in projects.
. Click *Finish Setup*.

== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].
* link:33ea62[Share this Data Source] with your collaborators.

----- user_guide/data-source-connectors/connect-to-generic-s3.txt -----
:page-version: 6.1
:page-title: Connect to Generic S3
:page-sidebar: Generic S3
:page-permalink: 947ddd
:page-order: 120

This topic describes how to connect to a generic S3 service (such as https://min.io/[MiniO^]) from Domino.
You must have network connectivity between your S3 service and your Domino deployment.

The easiest way to connect to a generic S3 service from Domino is to create a Domino Data Source as described below.

== Create a Generic S3 Data Source

. From the navigation pane, click *Data > Data Sources*.
. Click *Create a Data Source*.
. In the New Data Source window, from *Select Data Store*, select *Generic S3*.
. Enter the *Bucket*.
. Optional: Enter a *Subfolder Path*, such as `myFolder` or `data/tests`, to restrict the Data Source to only list data from and write to the specified folder.
. Enter the *Host*.
. Enter the *Data Source Name*.
. Optional: Enter a *Description* to explain the purpose of the Data Source to others.
. Click *Next*.
. Enter the *Access Key ID* and *Secret Access Key* to connect to Generic S3.
The Domino secret store backed by HashiCorp Vault securely stores the credentials.
. Click *Test Credentials*.
. If the Data Source authenticates, click *Next*.
. Select who can view and use the Data Source in projects.
. Click *Finish Setup*.


== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].
* link:33ea62[Share this Data Source] with your collaborators.

----- user_guide/data-source-connectors/connect-to-greenplum.txt -----
:page-version: 6.1
:page-permalink: 27cba9
:page-title: Connect to Greenplum
:page-sidebar: Greenplum
:page-order: 140


Your link:f32f69[administrator must create a Data Source for this data store] for you to use. 

After your administrator has created the Data Source, you can work with it in your project as usual.

== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].
* link:33ea62[Share this Data Source] with your collaborators.

----- user_guide/data-source-connectors/connect-to-ibm-db2.txt -----
:page-version: 6.1
:page-title: Connect to IBM db2
:page-sidebar: IBM db2
:page-permalink: f64ea1
:page-order: 150

You can connect to IBM db2 using a Data Source or directly.

== IBM db2 Data Source


Your link:f32f69[administrator must create a Data Source for this data store] for you to use. 

After your administrator has created the Data Source, you can work with it in your project as usual.

== Direct connection

Connect directly to https://www.ibm.com/products/db2[IBM db2^] from Domino.
You must have network connectivity between IBM db2 and your Domino deployment.

WARNING: Domino does not officially support this method. We provide this information as a courtesy.

== Python and ibm_DB2

Domino recommends the https://www.ibm.com/support/knowledgecenter/bg/SSEPGG_9.7.0/com.ibm.swg.im.dbclient.python.doc/doc/t0054368.html[ibm_DB2^] package for interacting with DB2 databases from Python.

=== Environment setup

Use the following Dockerfile instruction to add https://pypi.org/project/ibm_db/[ibm_db^] to your environment.

This instruction assumes you already have https://pypi.org/project/pip/[pip^] installed.

[source,dockerfile]
----
RUN pip install ibm_db ibm_db_sa
----



=== Credential setup

Set the following Domino environment variables to store secure information about your DB2 connection.

* `db_username`
* `db_password`

See link:d8dde6[Store Project credentials] to learn more about Domino environment variables.

=== Usage

See https://github.com/ibmdb/python-ibmdb[Python support for IBM Db2 for LUW, IBM Informix and IBM Db2 for z/OS^] for detailed information about how to use the package.
The following is an example for connecting to DB2 with ibm_db where:

* You have set up the environment variables with the `db_username` and `db_password`.
* You've replaced `my.host.name` with the host name for your machine.

[source,python]
----
import ibm_db
import ibm_db_dbi
import pandas as pd

hostname = 'my.host.name'
port = 50001
username = os.environ['db_username']
password = os.environ['db_password']

def query_db(sql):
 ibm_db_conn = ibm_db.connect("DATABASE=IBMPROD;HOSTNAME={};PORT={};PROTOCOL=TCPIP;UID={};PWD={};".format(hostname, port, username, password), "", "")
 conn = ibm_db_dbi.Connection(ibm_db_conn)

 df = pd.read_sql_query(sql, conn)
 ibm_db.close(ibm_db_conn)
 return df

sql_cmd = """

SELECT
 *
FROM
 table

"""

df_cmd = query_db(sql_cmd)

df_cmd
----

See https://www.ibm.com/support/knowledgecenter/bg/SSEPGG_9.7.0/com.ibm.swg.im.dbclient.python.doc/doc/t0054368.html[Connecting to an IBM database server in Python^].

== R and ibmdbR

Domino recommends the https://www.ibm.com/support/knowledgecenter/en/SS6NHC/com.ibm.swg.im.dashdb.doc/connecting/connect_connecting_rstudio.html[imbdbR^] library for interacting with DB2 databases from R.

=== Environment setup

Use the following Dockerfile instruction to add ibmdbR to your environment.

[source,dockerfile]
....
RUN R -e 'install.packages("ibmdbR")'
....

== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].

* link:33ea62[Share this Data Source] with your collaborators.
----- user_guide/data-source-connectors/connect-to-ibm-netezza.txt -----
:page-version: 6.1
:page-title: Connect to IBM Netezza
:page-sidebar: IBM Netezza
:page-permalink: 5a641d
:page-order: 160

You can connect to IBM Netezza using a Data Source or directly.

== IBM Netezza Data Source


Your link:f32f69[administrator must create a Data Source for this data store] for you to use. 

After your administrator has created the Data Source, you can work with it in your project as usual.

== Direct connection

Connect directly to https://www.ibm.com/products/netezza[IBM Netezza^] from Domino.
You must have network connectivity between Netezza and your Domino deployment.

WARNING: Domino does not officially support this method. We provide this information as a courtesy.

== Python and nzpy

Domino recommends the https://pypi.org/project/nzpy/[nzpy^] package for interacting with Netezza from Python.

=== Environment setup

Use the following Dockerfile instruction to add https://pypi.org/project/nzpy/[nzpy^] to your environment.

This instruction assumes you already have https://pypi.org/project/pip/[pip^] installed.

....
RUN pip install nzpy
....


=== Credential setup

You must set the following Domino environment variables to store secure information about your Netezza connection


* `db_username`
* `db_password`

See link:d8dde6[Store Project credentials] to learn more about Domino environment variables.

=== Usage

See the https://github.com/IBM/nzpy[nzpy docs^] for detailed information about how to use the package.
The following is an example for connecting to Netezza with nzpy where:

* You have set up environment variables noted above with the
`db_username` and `db_password`
* You've replaced `my.host.name` with the host name for your machine
* You've replaced `my.database.name` with the database name for your machine

[source,python]
----
import nzpy
import os
import pandas as pd

hostname = 'my.host.name'
database_name = 'my.database.name'
port = 5480
username = os.environ['db_username']
password = os.environ['db_password']

def query_db(sql):
   conn = nzpy.connect(user=username, password=password, host=hostname, port=port, database=database_name)
   with conn.cursor() as cursor:
      cursor.execute(sql)
      df = cursor.fetchall()
      return df

sql_cmd = """

SELECT
 *
FROM
 table

"""

df_cmd = query_db(sql_cmd)

df_cmd
----

== R and RJDBC

Domino recommends the RJDBC package and nzjdbc jar for interacting with Netezza from R.

=== Environment setup

Use the following Dockerfile instruction to add RJDBC to your environment.

....
RUN R -e 'install.packages("RJDBC")'
....



=== Credential setup

You must set the following Domino environment variables to store secure information about your Netezza connection


* `db_username`
* `db_password`

See link:d8dde6[Store Project credentials] to learn more about Domino environment variables.

=== Usage

Test if the `nzjdbc` jar is working as expected by running the below in a workspace terminal

`java -jar <path.to.the.nzjdbc.jar> -t -h my.host.name -p 5480 -u <username> -db <database_name>`

The following is a simple example for connecting to Netezza with RJDBC and
`nzjdbc` jar where:

* `nzjdbc` jar is present in the project files (`/mnt/nzjdbc3`)
* You have set up environment variables noted above with the
`db_username` and `db_password`
* You've replaced `my.host.name` with the host name for your machine
* You've replaced `my.database.name` with the database name for your machine

[source,R]
----
library(RJDBC)
drv <- RJDBC::JDBC(driverClass = "org.netezza.Driver", classPath = "/mnt/nzjdbc3.jar")
conn <- dbConnect(drv, 'jdbc:netezza://my.host.name:5480/my.database.name;logLevel=2',Sys.getenv('db_username'), Sys.getenv('db_password'))
dbListTables(conn)
----

== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].
* link:33ea62[Share this Data Source] with your collaborators.

----- user_guide/data-source-connectors/connect-to-ignite.txt -----
:page-version: 6.1
:page-permalink: f9c9a2
:page-title: Connect to Ignite
:page-sidebar: Ignite
:page-order: 170


Your link:f32f69[administrator must create a Data Source for this data store] for you to use. 

After your administrator has created the Data Source, you can work with it in your project as usual.

== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].
* link:33ea62[Share this Data Source] with your collaborators.

----- user_guide/data-source-connectors/connect-to-impala.txt -----
:page-version: 6.1
:page-title: Connect to Impala
:page-sidebar: Impala
:page-permalink: f200d6
:page-order: 180

This topic describes how to connect directly to http://impala.apache.org/[Apache Impala^] from Domino.
You must have network connectivity between Impala and your Domino deployment.

WARNING: Domino does not officially support this method. We provide this information as a courtesy.

== Use Impala ODBC Connector for Cloudera Enterprise with pyodbc

Domino recommends using the https://www.cloudera.com/downloads/connectors/impala/odbc/2-6-0.html[Impala ODBC Connector for Cloudera Enterprise^] in concert with the https://pypi.org/project/pyodbc/[pyodbc^] library for interacting with Impala from Python.

=== Environment setup

. Visit the Cloudera downloads page to download the Impala ODBC Connector for Cloudera Enterprise to your local machine.
For default Domino images of Ubuntu 16.04, download the 64-bit Debian package.
Keep track of where you save this file, as you will need it in a later step.
+
image::/images/4.x/cloudera-odbc-connector.png[alt="Cloudera ODBC Connector"]
. Create a new _public_ project in your Domino instance to host the driver files for use in Domino environments.
+
image::/images/4.x/create-public-project.png[alt="Create a public project", width=600]
. In the new project, click *browse for files* and select the driver file you downloaded earlier to queue it for upload.
Click *Upload* to add it to the project.
+
image::/images/4.x/upload-driver.png[alt="Upload the driver"]
. After the driver file has been added to your project files, click the gear next to it in the files list, then right-click **Download** and click **Copy link address**.
Save this address as you will need it when setting up your environment.
+
image::/images/4.x/copy-link-address.png[alt="Copy the link address"]
. Add the following Dockerfile instructions to install the driver and pyodbc in your environment, pasting the URL you copied earlier where indicated on line 5.
+
[source,dockerfile,subs="attributes"]
----
USER root

# download the driver from your project
RUN mkdir /ref_files
RUN 
cd /ref_files && 
wget --no-check-certificate [paste-download-url-from-previous-step-here] && 
gzip -d clouderaimpalaodbc_2.6.0.1000-2_amd64.deb.gz

# install the driver
RUN gdebi /ref_files/clouderaimpalaodbc_2.6.0.1000-2_amd64.deb --n

# update odbc.ini file for impala driver
RUN 
echo "

[Cloudera ODBC Driver for Impala] 
 
Driver=/opt/cloudera/impalaodbc/lib/64/libclouderaimpalaodbc64.so 
 
KrbFQDN=_HOST 
 
KrbServiceName=impala 
" >> /etc/odbcinst.ini

# set up impala libraries
RUN export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/cloudera/impalaodbc/lib/64
RUN ldd /opt/cloudera/impalaodbc/lib/64/libclouderaimpalaodbc64.so

# install pyodbc
RUN pip install pyodbc

USER ubuntu
----

=== Credential setup

Set up the following Domino environment variables to store secure information about your Impala connection.

* `IMPALA_HOST`

Hostname where your Impala service is running.
Make sure your Impala service and network firewall are configured to accept connections from Domino.

* `IMPALA_PORT`

The port your Impala service is configured to accept connections on.

* `IMPALA_KERB_HOST`

Hostname of your Kerberos authentication service.

* `IMPALA_KERB_REALM`

The name of the Kerberos realm used by the Impala service.

See link:d8dde6[Store Project credentials] to learn more about Domino environment variables.

=== Usage

See the https://github.com/mkleehammer/pyodbc/wiki[pyodbc documentation^] for detailed information about how to use the package to interact with a database.
The following are some examples for how to set up a connection.

[source,python]
----
import pyodbc
import os

# fetch values from environment variables
hostname = os.environ['IMPALA_HOST']
service_port = os.environ['IMPALA_PORT']
kerb_host = os.environ['IMPALA_KERB_HOST']
kerb_realm = os.environ['IMPALA_KERB_REALM']

# create connection object
conn = pyodbc.connect('Host=hostname;'
                     +'DRIVER={Cloudera ODBC Driver for Impala};'
                     +'PORT=service_port;'
                     +'KrbRealm=kerb_realm;'
                     +'KrbFQDN=kerb_host;'
                     +'KrbServiceName=impala;'
                     +'AUTHMECH=1',autocommit=True)

# if you see:
# 'Error! Filename not specified'
# while querying Impala using the connection object,
# add the following configuration line:
#
# conn.setencoding(encoding='utf-8', ctype=pyodbc.SQL_CHAR)


# if your Impala uses SSL, add SSL=1 to the connection string
# conn = pyodbc.connect('Host=hostname;'
#                      +'DRIVER={Cloudera ODBC Driver for Impala};'
#                      +'PORT=service_port;'
#                      +'KrbRealm=kerb_realm;'
#                      +'KrbFQDN=kerb_host;'
#                      +'KrbServiceName=impala;'
#                      +'AUTHMECH=1;'
#                      +'SSL=1;'
#                      +'AllowSelfSignedServerCert=1', autocommit=True)
----


== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].
* link:33ea62[Share this Data Source] with your collaborators.

----- user_guide/data-source-connectors/connect-to-mariadb.txt -----
:page-version: 6.1
:page-permalink: 5f5ae0
:page-title: Connect to MariaDB
:page-sidebar: MariaDB
:page-order: 190


Your link:f32f69[administrator must create a Data Source for this data store] for you to use. 

After your administrator has created the Data Source, you can work with it in your project as usual.

== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].
* link:33ea62[Share this Data Source] with your collaborators.

----- user_guide/data-source-connectors/connect-to-mongodb.txt -----
:page-version: 6.1
:page-permalink: cadeb5
:page-title: Connect to MongoDB
:page-sidebar: MongoDB
:page-order: 200

Your administrator must create a Data Source for https://www.mongodb.com/[MongoDB^] that you can access.
See link:ed290d[Connect to MongoDB] in the link:b35e66[Admin Guide].


After your administrator has created the Data Source, you can link:fa5f3a[work with it in your project] as usual.




// tag::unnest-ds[]

[NOTE]
====
Domino data sources do not support querying nested objects.
The workaround is to https://count.co/sql-resources/bigquery-standard-sql/unnest[`UNNEST`^] the object in the SQL query.

The following is an example `UNNEST` query:
----
res = ds.query("""
select account_id, t1
from sample_analytics.transactions
cross join unnest (transactions)
as t(t1, t2, t3, t4, t5, t6)
""")
----
====
// end::unnest-ds[]


== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].
* link:33ea62[Share this Data Source] with your collaborators.

----- user_guide/data-source-connectors/connect-to-mssql.txt -----
:page-version: 6.1
:page-title: Connect to MSSQL
:page-sidebar: MSSQL
:page-permalink: 634c13
:page-order: 210

This topic describes how to connect to a https://www.microsoft.com/en-us/sql-server/sql-server-2016[Microsoft SQL Server^] (MSSQL) from Domino.
You must have network connectivity between Impala and your Domino deployment.


The easiest way to connect to MSSQL from Domino is to create a Domino Data Source as described below.

== Create a MSSQL Data Source

. From the navigation pane, click *Data > Data Sources*.
. Click *Create a Data Source*.
. In the New Data Source window, from *Select Data Store*, select *SQL Server*.
. Enter the *Host*. Valid values are `<host string>:3306` or `<host string>`.
. Enter the *Port*.
. Enter the name of the *Database*.
. Enter the *Data Source Name*.
. Optional: Enter a *Description* to explain the purpose of the Data Source to others.
. Click *Next*.
. Enter the *Username* and *Password* to connect to SQL Server.
The Domino secret store backed by HashiCorp Vault securely stores the credentials.
. Click *Test Credentials*.
. If the Data Source authenticates, click *Next*.
. Select who can view and use the Data Source in projects.
. Click *Finish Setup*.



== Alternate way to connect to an MSSQL Data Source

WARNING: This section describes an alternate method to connect to the MSSQL Data Source.
Domino does not officially support this method.



. To interact with MSSQL databases from Python, Domino recommends the http://www.pymssql.org/[pymssql^] package.
. Use the following Dockerfile instruction to
http://www.pymssql.org/intro.html[install pymssql^] in your environment.
+
This instruction assumes you already have https://pypi.org/project/pip/[pip^] installed.
+
[source,dockerfile]
....
RUN pip install pymssql
....




+
. You must set up the following Domino environment variables to store secure information about your MSSQL connection.
* `DB_SERVER`
* `DB_USERNAME`
* `DB_PASSWORD`
+
See link:d8dde6[Store Project credentials] to learn more about Domino environment variables.
+
See the http://www.pymssql.org/pymssql_examples.html[pymssql documentation^] for detailed information about how to use the package.
The following is an example of connecting to MSSQL with Python where the following is true:

* You have set up environment variables noted previously.
* The server hosts a database named `myData` with a table named `addresses`.
+
[source,python]
----
from os import getenv
import pymssql

server = getenv("DB_SERVER")
user = getenv("DB_USERNAME")
password = getenv("DB_PASSWORD")

conn = pymssql.connect(server, user, password, "myData")
cursor = conn.cursor()

cursor.execute('SELECT * FROM addresses')
row = cursor.fetchone()
while row:
    print("ID=%d, Name=%s" % (row[0], row[1]))
    row = cursor.fetchone()

conn.close()
----



=== R and RODBC to MSSQL



. To interact with MSSQL databases from R, Domino recommends the https://cran.r-project.org/web/packages/odbc/index.html[RODBC^] library. You can use an alternative package if you'd like.

. Use the following Dockerfile instructions to add the MSSQL drivers to your Ubuntu 16.04 environment.
+
[source,dockerfile]
----
RUN curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -
RUN curl https://packages.microsoft.com/config/ubuntu/16.04/prod.list > /etc/apt/sources.list.d/mssql-release.list
RUN apt-get update
RUN ACCEPT_EULA=Y apt-get install msodbcsql17
----

. See Microsoft's documentation for the Dockerfile instructions to add the MSSQL drivers to your Domino-supported Ubuntu environment.
. See the https://db.rstudio.com/databases/microsoft-sql-server/[RStudio
RODBC documentation^] for information about how to use the package.

== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].
* link:33ea62[Share this Data Source] with your collaborators.

----- user_guide/data-source-connectors/connect-to-mysql.txt -----
:page-version: 6.1
:page-title: Connect to MySQL
:page-sidebar: MySQL
:page-permalink: 69b09a
:page-order: 220

This topic describes how to connect to https://www.mysql.com/[MySQL^] from Domino.
You must have network connectivity between MySQL and your Domino deployment.


The easiest way to connect to MySQL from Domino is to create a Domino Data Source as described below.

== Create a MySQL tabular Data Source

. From the navigation pane, click *Data > Data Sources*.
. Click *Create a Data Source*.
. In the New Data Source window, from *Select Data Store*, select *MySQL*.
. Enter the *Host*. Valid values are `<host string>:<port>` or `<host string>`.
+
If no port is specified, the default is 3306.
. Enter the name of the *Database*.
. Enter the *Data Source Name*.
. Optional: Enter a *Description* to explain the purpose of the Data Source to others.
. Click *Next*.

. Specify the credentials for authenticating to MySQL.
+
Basic authentication is supported by default.  IAM credential propagation might also be available if your administrator has link:6a8639#iam-auth[enabled it].
+
NOTE: IAM-authenticated connections can be used only for executing jobs and workspaces.  Other execution types, such as scheduled jobs and Domino endpoints, require basic authentication.
. Click *Next* (or *Skip for Now* to configure authentication later).


. Select whether *Everyone* can access this Data Source or just *Specific users or organizations*.

. Select who can view and use the Data Source in projects.
. Click *Finish Setup*.


== Alternate way to connect to a MySQL Data Source

WARNING: This section describes an alternate method to connect to the MySQL Data Source.
Domino does not officially support this method.



. Domino recommends the https://dev.mysql.com/doc/connector-python/en/[mysql-connector-python^] library to interact with MySQL databases from Python.
. Use the following Dockerfile instruction to https://www.psycopg.org/docs/install.html[install psycopg2^] in your environment.
You must have https://pypi.org/project/pip/[pip^] installed.
+
[source,dockerfile]
----
USER root
RUN pip install mysql-connector-python
USER ubuntu
----
. You must set up the Domino environment variables to store secure information about your MySQL connection.
* `MYSQL_HOST`
+
Hostname where your MySQL service is running.
Make sure your MySQL service and network firewall are configured to accept connections from Domino.
* `MYSQL_USER`
+
The MySQL user you want to authenticate as.
* `MYSQL_PASSWORD`
+
The password for the user chosen previously.
+
See link:d8dde6[Store Project credentials] to learn more about Domino environment variables.
. See the https://dev.mysql.com/doc/connector-python/en/[mysql-connector-python
documentation^] for information about how to use the package.
The following is an example to connect to MySQL with `mysql-connector-python` where:

* You have set up environment variables with the host, user, and password.
* Your user has access to a database named  `db1` in the target MySQL instance.
* The `db1` database contains a table called `employees`.
+
[source,python]
----
from mysql.connector import (connection)
import os

# fetch values from environment variables and set the target database
hostname = os.environ['MYSQL_HOST']
username = os.environ['MYSQL_USER']
password = os.environ['MYSQL_PASSWORD']
dbname = 'db1'

# establish connection to db1 database in your mysql service
cnx = connection.MySQLConnection(user=username,
                                 password=password,
                                 host=hostname,
                                 database=dbname)

# create cursor for passing queries to database
cursor = cnx.cursor()

# define query
query = ("SELECT * FROM employees")

# execute query
cursor.execute(query)

# print results
for row in cursor:
  print(row)

# close connection
cnx.close()
----


=== R and RMySQL



. To interact with MySQL services from R, Domino recommends the
https://www.rdocumentation.org/packages/RMySQL/versions/0.10.16[RMySQL^] library.
. Use the following Dockerfile instructions to add RMySQL to your environment.
+
[source,dockerfile]
----
RUN sudo apt-get install -y libmariadb-client-lgpl-dev
RUN R -e 'install.packages("RMySQL")'
----
. Set the following Domino environment variables to store secure information about your MySQL connection.
* `MYSQL_HOST`
+
Hostname where your MySQL service is running.
Make sure your MySQL service and network firewall are configured to accept connections from Domino.
* `MYSQL_USER`
+
The MySQL user you want to authenticate as.
* `MYSQL_PASSWORD`
+
The password for the user chosen previously.
+
See link:d8dde6[Store Project credentials] to learn more about Domino environment variables.
. See the https://www.rdocumentation.org/packages/RMySQL/versions/0.10.16[RMySQL
documentation^] for information about how to use the package.
The following is an example for connecting to MySQL with RMySQL where:

* You have set up environment variables  with the host, user, and password.
* Your user has access to a database named `db1` in the target MySQL instance.
* The database contains a table named `employees`.
+
[source,r]
----
# load the library
library(RMySQL)

# fetch values from environment variables and set the target database
hostname <- Sys.getenv['MYSQL_HOST']
username <-  Sys.getenv['MYSQL_USER']
password <- Sys.getenv['MYSQL_PASSWORD']
database <- 'db1'

# set up a driver and use it to create a connection to your database
con <- dbConnect(RMySQL::MySQL(), host = hostname,
 user = username, password = password, dbname = database)

# run a query and load the response into a dataframe
df_mysql <- dbGetQuery(con, "SELECT * FROM employees")

# close your connection when finished
dbDisconnect(con)
----

== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].
* link:33ea62[Share this Data Source] with your collaborators.

----- user_guide/data-source-connectors/connect-to-oracle.txt -----
:page-version: 6.1
:page-title: Connect to Oracle Database
:page-sidebar: Oracle Database
:page-permalink: 77bf79
:page-order: 240

This topic describes how to connect to https://www.oracle.com/database/[Oracle Database^] from Domino.
You must have network connectivity between Oracle and your Domino deployment.

The easiest way to connect to Oracle from Domino is to create a Domino Data Source as described below.

== Create an Oracle Data Source

. From the navigation pane, click *Data > Data Sources*.
. Click *Create a Data Source*.
. In the New Data Source window, from *Select Data Store*, select *Oracle*.
. Enter the *Host*, *Port*, and name of the *Database*.
. Enter the *Data Source Name*.
. Optional: Enter a *Description* to explain the purpose of the Data Source to others.
. Click *Next*.
. Enter the *Username* and *Password* to connect to Oracle.
The Domino secret store backed by HashiCorp Vault securely stores the credentials.
. Click *Test Credentials*.
. If the Data Source authenticates, click *Next*.
. Select who can view and use the Data Source in projects.
. Click *Finish Setup*.


== Alternate way to connect to Oracle

WARNING: This section describes an alternate method to connect to Oracle.
Domino does not officially support this method.



. Before you can connect to Oracle, you must install the following client software  your environment:
** The basic package: `instantclient-basic-linux.x64-<oracle-version>dbru.zip`
** The SDK package: `instantclient-sdk-linux.x64-<oracle-version>dbru.zip`
+
You must download these files from the https://www.oracle.com/technetwork/topics/linuxx86-64soft-092277.html[Instant Client Downloads page^].

.. Use your Oracle customer login.
.. Host an internal mirror of the files somewhere accessible to your Domino hosts.
.. Install the software manually.
In the example environments in this topic, these files are retrieved from a private S3 bucket with
https://www.gnu.org/software/wget/[Wget].
You must make them available in a similar manner for your Domino deployment.


=== Python and cs_Oracle




. To interact with Oracle databases from Python, Domino recommends the
https://oracle.github.io/python-cx_Oracle/[cx_Oracle^] library.
. Use the following Dockerfile instruction to install the Oracle client drivers and cx_Oracle in your environment.
+
NOTE: You cannot copy and paste this Dockerfile directly, because you must set up your own internal host of the Oracle clients and modify the `wget` step shown here to retrieve them.

+
[source,dockerfile]
----
USER root

RUN \
    wget https://download.oracle.com/otn_software/linux/instantclient/2350000/instantclient-sdk-linux.x64-23.5.0.24.07.zip -O /home/ubuntu/instantclient-sdk-linux.x64-23.5.0.24.07.zip && \
    cd /home/ubuntu && \
    unzip instantclient-sdk-linux.x64-23.5.0.24.07.zip && \
    mv instantclient_23_5 /usr/local/lib && \
    rm instantclient-sdk-linux.x64-23.5.0.24.07.zip && \
    apt-get install -y libaio1

RUN \
    echo 'export OCI_LIB=/usr/local/lib/instantclient_23_5' \
          >> /home/ubuntu/.domino-defaults && \
    echo 'export OCI_INC=/usr/local/lib/instantclient_23_5/sdk/include' \
          >> /home/ubuntu/.domino-defaults && \
    echo 'export LD_LIBRARY_PATH=/usr/local/lib/instantclient_23_5:$LD_LIBRARY_PATH' \
          >> /home/ubuntu/.domino-defaults

RUN \
    cd /usr/local/lib/instantclient_23_5 && \
    ln -sf libclntsh.so.23.5 libclntsh.so && \
    chown -R ubuntu:ubuntu /usr/local/lib/instantclient_23_5

RUN \
    echo '/usr/local/lib/instantclient_23_5' \
          > /etc/ld.so.conf.d/oracle-instantclient.conf && \
    ldconfig -v

RUN pip install cx_Oracle --upgrade

USER ubuntu
----
. Set the following Domino environment variables to store secure information about your Oracle connection.
* `ORACLE_HOST`
+
Hostname where your database is running.
Make sure your Oracle host and network firewall are configured to accept connections from Domino.
* `ORACLE_SERVICE`
+
The service name of the Oracle service running on the target host.
* `ORACLE_USER`
+
The Oracle user you want to authenticate as.
* `ORACLE_PASSWORD`
+
Password for the user specified previously.
+
See link:d8dde6[Store Project credentials] to learn more about Domino environment variables.
. See the https://cx-oracle.readthedocs.io/en/latest/index.html[cx_Oracle
documentation^] for information about how to use the package.
The following is an example to connect to Oracle with cx_Oracle where:
+
--
* You have set up environment variables with the hostname, service name, username, and password.
* Your user has access to a database named `houses` in the target Oracle instance.
--
+
[source,python]
----
from __future__ import print_function
import cx_Oracle
import os

# fetch values from environment variables and set the target database
hostname = os.environ['ORACLE_HOST']
service  = os.environ['ORACLE_SERVICE']
username = os.environ['ORACLE_USER']
password = os.environ['ORACLE_PASSWORD']
connection_string = hostname + "/" + service

# Connect as user "hr" with password "welcome" to the "oraclepdb" service running on this computer.
connection = cx_Oracle.connect(username, password, connection_string)

cursor = connection.cursor()
cursor.execute("""
    SELECT address
    FROM houses
    WHERE zip = 90210""")
for address in cursor:
    print("Address:", address)
----



=== R and ROracle



. To interact with Oracle databases from R, Domino recommends the https://cran.r-project.org/web/packages/ROracle/index.html[ROracle^] library.
. Use the following Dockerfile instruction to install the Oracle client drivers and RODBC in your environment.
+
NOTE: You cannot copy and paste this Dockerfile directly because you must set up your own internal host of the Oracle clients and modify the `wget` step shown here to retrieve them.

+
[source,dockerfile]
----
USER root

RUN \
    wget https://download.oracle.com/otn_software/linux/instantclient/2350000/instantclient-sdk-linux.x64-23.5.0.24.07.zip -O /home/ubuntu/instantclient-sdk-linux.x64-23.5.0.24.07.zip && \
    cd /home/ubuntu && \
    unzip instantclient-sdk-linux.x64-23.5.0.24.07.zip && \
    mv instantclient_23_5 /usr/local/lib && \
    rm instantclient-sdk-linux.x64-23.5.0.24.07.zip && \
    apt-get install -y libaio1

RUN \
    echo 'export OCI_LIB=/usr/local/lib/instantclient_23_5' \
          >> /home/ubuntu/.domino-defaults && \
    echo 'export OCI_INC=/usr/local/lib/instantclient_23_5/sdk/include' \
          >> /home/ubuntu/.domino-defaults && \
    echo 'export LD_LIBRARY_PATH=/usr/local/lib/instantclient_23_5:$LD_LIBRARY_PATH' \
          >> /home/ubuntu/.domino-defaults

RUN \
    cd /usr/local/lib/instantclient_23_5 && \
    ln -s libclntsh.so.23.5 libclntsh.so && \
    chown -R ubuntu:ubuntu /usr/local/lib/instantclient_23_5

RUN \
    echo '/usr/local/lib/instantclient_23_5' \
          > /etc/ld.so.conf.d/oracle-instantclient.conf && \
    ldconfig -v

RUN \
    cd /home/ubuntu && \
    wget https://cran.r-project.org/src/contrib/ROracle_1.3-1.tar.gz && \
    R CMD INSTALL --configure-args='--with-oci-inc=/usr/local/lib/instantclient_23_5/sdk/include --with-oci-lib=/usr/local/lib/instantclient_23_5' ROracle_1.3-1.tar.gz

USER ubuntu
----

. Set up the following Domino environment variables to store secure information about your Oracle connection.
* `ORACLE_HOST`
+
Hostname where your database is running.
Make sure your Oracle host and network firewall are configured to accept connections from Domino.
* `ORACLE_SERVICE`
+
The service name of the Oracle service running on the target host.
* `ORACLE_USER`
+
The Oracle user you want to authenticate as.
* `ORACLE_PASSWORD`
+
Password for the user specified previously.
+
See link:d8dde6[Store Project credentials] to learn more about Domino environment variables.
. See the https://www.rdocumentation.org/packages/ROracle/versions/1.3-1/topics/dbConnect-methods[ROracle documentation^] for usage details.

== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].
* link:33ea62[Share this Data Source] with your collaborators.

----- user_guide/data-source-connectors/connect-to-palantir.txt -----
:page-version: 6.1
:page-permalink: ccd264
:page-title: Connect to Palantir Foundry
:page-sidebar: Palantir Foundry
:page-order: 250

Learn how to connect to link:https://www.palantir.com/platforms/foundry/[Palantir Foundry^] from Domino.
You must have network connectivity between Palantir and your Domino deployment.


== Create a Palantir Foundry Data Source

The easiest and most secure way to connect to Palantir from Domino is to have your administrator link:a932d7[create a Data Source for Palantir Foundry] for users to access.

After your administrator has created the Data Source, you can link:fa5f3a[use the Data Source] as usual.

[NOTE]
====
You must specify the dataset path, obtained from the link:https://www.palantir.com/docs/foundry/data-integration/dataset-preview/index.html[Dataset Preview^] in Palantir, in your query as shown in this example:

----
example res =  ds.query('SELECT * FROM "/namespace/project"."/namespace/project/path/to/your/dataset"')
----

====


== Connect using individual user credentials

You can also choose to use individual user credentials to connect to Palantir. However, you should contact your Palantir admin to ensure this method is compliant with your organization's governance policies.

To connect to Palantir Foundry using this method, you must have the following:

* A Palantir Foundry account.
* An access token.
* A path to the Dataset.

=== Create an access token

. Sign in to Palantir Foundry.
. In the Foundry sidebar, click *Account* and select *Settings*.
. Under *User settings*, click *Tokens*.
+
image::/images/palantir-datasets/domino-user-tokens.png[alt="Palantir UI on the Tokens page"]

. Click *Create token*.
. Copy the user token and save the token string.
You can store it as an environment variable in Domino.


=== Copy the dataset path

. In the Palantir Foundry application, go to the *Project* containing the dataset and select the dataset.
. In the *About* section, copy the *Location* attribute.
+
TIP: You can also copy and use the *RID* field as a permalink to this dataset.

. Save the location path or RID string to store as an environment variable in Domino.

=== Add environment variables to Domino

You can store your Palantir connection information securely in Domino as environment variables in your user or project settings.

. Sign in to Domino.
. Click *User* or *Project* settings.
. Click *Environment Variables*.
. Add the `PALANTIR_TOKEN` environment variable with the value of the user token you created in Foundry.
. Add the `PALANTIR_HOSTNAME` environment variable with the value of your Foundry instance hostname.

See link:d8dde6[Store Project credentials] to learn more about Domino environment variables.

=== Add the Palantir-SDK package to your environment

Some Domino Workspace Environments have the `palantir-sdk` package installed by default.
You can also add Dockerfile commands to the environment definition to add packages:

[source,shell]
----
RUN pip install -user palantir-sdk
----

You can also create a cell at the top of a Jupyter Notebook and run the following:

[source,shell]
----
pip install palantir-sdk
----

=== Run your code

You can now access your Palantir Foundry data through Python, from a Jupyter notebook or VS Code.

image::/images/palantir-datasets/palantir-connection-in-jupyter.png[alt="Jupyter notebook accessing a Palantir dataset and using the Palantir SDK"]

== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].
* link:33ea62[Share this Data Source] with your collaborators.

----- user_guide/data-source-connectors/connect-to-pinecone.txt -----
:page-version: 6.1
:page-permalink: 5c64ef
:page-title: Connect to Pinecone
:page-sidebar: Pinecone
:page-order: 260

Domino's Pinecone vector database Data Source connector enables easy, secure access to vectorized content stored in link:https://www.pinecone.io/[Pinecone^].

== Create a Pinecone Data Source

You will need the following information about your Pinecone database:

- link:https://docs.pinecone.io/docs/quickstart#2-get-your-api-key[Pinecone API key^]

To create a Pinecone Data Source:

. From the navigation pane, click *Data > Data Sources*.
. Click *Create a Data Source*.
. In the New Data Source window, from *Select Data Store*, select *Pinecone*.
. Provide the link:https://docs.pinecone.io/docs/quickstart#2-get-your-api-key[Pinecone API Key^] for your Pinecone database.
. Create a *Data Source Name* and an optional *Description*.

Domino stores your Pinecone API Key, along with other secrets, in a link:d8dde6[secure secret store backed by HashiCorp Vault], so you can have confidence that your secrets are safe. You can test the API key before finalizing your Data Source:

. Click *Test Credentials*.
. Confirm that the Data Source authenticates.
. Select who can view and use the Data Source in Projects.

== Use the Pinecone Data Source

To use your Pinecone Data Source, you must install the prerequisite Python libraries in the compute Environment you want to run Pinecone commands in and configure the Pinecone client using a Domino-specific configuration.

=== Install the Pinecone Python library and Domino Data SDK

To use the Pinecone Python library in Domino, you must install the following libraries in your compute Environment. Domino standard environments have these libraries installed by default. If you are using a customized environment, link:5dd2c1[Edit your compute Environment] and add the following lines to the Dockerfile instructions:

IMPORTANT: You must replace the placeholder `{x.y.z}` with the compatible version of your Domino deployment. Refer to link:https://pypi.org/project/dominodatalab-data/#history[all released packages^].

[source,dockerfile]
----
USER root

RUN python -m pip install pinecone-client

RUN python -m pip install dominodatalab-data=={x.y.z}

USER ubuntu
----

=== Configure the Pinecone Python client

To connect to your Pinecone service in a Domino execution, you must initialize your Pinecone connection using a Domino-specific Environment. To find the configuration code snippets:

. link:fa5f3a#add_an_existing_data_source_to_a_project[Add the Pinecone Data Source to your Project].
. In your Workspace, go to *Data* > *Data Sources* > *Code snippet* > *Python*.
. Copy the code snippet, paste it into your code, and modify it as needed.

NOTE: Domino now only supports accessing the Pinecone data source through Python pinecone-client>=3.0.0.

* The code snippet for pinecone-client>=3.0.0:
+
[source,python]
----
from domino_data.vectordb import domino_pinecone3x_init_params, domino_pinecone3x_index_params
from pinecone import Pinecone

datasource_name = "pinecone"
pc = Pinecone(**domino_pinecone3x_init_params(datasource_name))
print(pc.list_indexes())

# Replace the place holder {{index_name}} below with the index name.
index_name = "{{index_name}}"
index = pc.Index(**domino_pinecone3x_index_params(datasource_name, index_name))
print(index.describe_index_stats())

# More Pinecone Python library-supported operations can be found at https://docs.pinecone.io/docs/python-client
# gRPC client is currently not supported.
----

Once your Pinecone connection is initialized in the Domino execution, you can use the link:https://docs.pinecone.io/docs/python-client[Pinecone Python client^] as usual.

== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].

* link:33ea62[Share this Data Source] with your collaborators.
----- user_guide/data-source-connectors/connect-to-postgres.txt -----
:page-version: 6.1
:page-title: Connect to PostgreSQL
:page-sidebar: PostgreSQL
:page-permalink: f095e7
:page-order: 270

This topic describes how to connect to https://www.postgresql.org/[PostgreSQL^] from Domino.
You must have network connectivity between PostgreSQL and your Domino deployment.

The easiest way to connect to PostgreSQL from Domino is to create a Domino Data Source as described below.

== Create a Postgres Data Source

. From the navigation pane, click *Data > Data Sources*.
. Click *Create a Data Source*.
. In the New Data Source window, from *Select Data Store*, select *PostgreSQL*.
. Enter the unique storage account name in *Account Name*.
. Enter the *Host*, *Port*, and *Database* name.
. Enter the *Data Source Name*.
. Optional: Enter a *Description* to explain the purpose of the Data Source to others.
. Click *Next*.

. Specify the credentials for authenticating to Postgres.
+
Basic authentication is supported by default.  IAM credential propagation might also be available if your administrator has link:6a8639#iam-auth[enabled it].
+
NOTE: IAM-authenticated connections can be used only for executing jobs and workspaces.  Other execution types, such as scheduled jobs and Domino endpoints, require basic authentication.
. Click *Next* (or *Skip for Now* to configure authentication later).


. Enter the users and organizations who can view and use the Data Source in projects.
. Click *Finish Setup*.

== Alternate way to connect to a Postgres Data Source

WARNING: This section describes an alternate method to connect to the Postgres Data Source.
Domino does not officially support this method.




This instruction assumes you already have https://pypi.org/project/pip/[pip^] installed.

. For Python and psycopg2, Domino recommends the https://pypi.org/project/psycopg2/[psycopg2^] library to interact with PostgreSQL databases from Python.
. Use the following Dockerfile instruction to https://www.psycopg.org/docs/install.html[install psycopg2^] in your environment.
+
[source,dockerfile]
----
RUN pip install psycopg2
----

. Set the following as Domino environment variables to store secure information about your PostgreSQL connection.
* `POSTGRES_HOST`
+
Hostname where your DB is running.
Make sure your PostgreSQL DB and network firewall are configured to accept connections from Domino.
* `POSTGRES_USER`
+
The PostgreSQL user you want to authenticate as.
* `POSTGRES_PASSWORD`
+
The password for the user chosen previously.
+
See link:d8dde6[Store Project credentials] to learn more about Domino environment variables.
. See the https://www.psycopg.org/docs/[psycopg2 documentation^] for detailed information about how to use the package.
The following is an example to connect to PostgreSQL with `psycopg2` where:

* You have set up environment variables with the hostname, username, and password.
* Your user has access to a database named `db1` in the target PostgreSQL instance.
* The database contains a table named `metrics`.
+
[source,python]
----
import psycopg2
import os

# fetch values from environment variables and set the target database
hostname = os.environ['POSTGRES_HOST']
username = os.environ['POSTGRES_USER']
password = os.environ['POSTGRES_PASSWORD']
dbname = 'db1'

# set up a connection object with parameters for your database
conn = psycopg2.connect(
  host=hostname,
  port=5432,
  user=username,
  password=password,
  database=dbname, )

# create a cursor in your connection
cur = conn.cursor()

# execute a query on the metrics table and store the response
cur.execute("SELECT * FROM metrics;")
results = cur.fetchall()

# display the contents of the response
print(results)
----
+
The results object created in the previous example is a Python array of entries from the queried table.


=== R and RPostgreSQL



. To connect to R and RPostgreSQL, Domino recommends the https://www.rdocumentation.org/packages/RPostgreSQL/versions/0.6-2[RPostgreSQL^] library to interact with PostgreSQL databases from R.
. Use the following Dockerfile instruction to add RPostgreSQL to your environment.
+
[source,dockerfile]
----
RUN R -e 'install.packages("RPostgreSQL")'
----

. Set up the Domino environment variables to store secure information about your PostgreSQL connection.
* `POSTGRES_HOST`
+
Hostname where your DB is running.
Make sure your PostgreSQL DB and network firewall are configured to accept connections from Domino.
* `POSTGRES_USER`
+
The PostgreSQL user you want to authenticate as.
* `POSTGRES_PASSWORD`
+
The password for the user chosen previously.
+
See link:d8dde6[Store Project credentials] to learn more about Domino environment variables.
. See the https://www.rdocumentation.org/packages/RPostgreSQL/versions/0.6-2[RPostgreSQL documentation^] for information about how to use the package.
The following is an example for connecting to PostgreSQL with RPostgreSQL where:

* You have set up environment variables with the hostname, username, and password.
* Your user has access to a database named `db1` in the target PostgreSQL instance.
* The database contains a table named `metrics``.
+
[source,r]
----
# load the library
library(RPostgreSQL)

# fetch values from environment variables and set the target database
hostname <- Sys.getenv['POSTGRES_HOST']
username <-  Sys.getenv['POSTGRES_USER']
password <- Sys.getenv['POSTGRES_PASSWORD']
database <- 'db1'

# set up a driver and use it to create a connection to your database
drv <- dbDriver("PostgreSQL")
conn <- dbConnect(
  drv,
  host=hostname,
  port=5432,
  user=username,
  password=password,
  dbname=database )

# run a query and load the response into a dataframe
df_postgres <- dbGetQuery(conn, "SELECT * from metrics;")

# close your connection when finished
dbDisconnect(conn)
----

== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].
* link:33ea62[Share this Data Source] with your collaborators.

----- user_guide/data-source-connectors/connect-to-qdrant.txt -----
:page-version: 6.1
:page-permalink: c2364c
:page-title: Connect to Qdrant
:page-sidebar: Qdrant
:page-order: 280


Domino's Qdrant vector database Data Source connector enables easy, secure access to vectorized content stored in link:https://qdrant.tech/[Qdrant^].

== Create a Qdrant Data Source

You will need the following information about your Qdrant database:

- Host
- Port
- link:https://qdrant.tech/documentation/cloud/authentication/#create-api-keys[Qdrant API key^]

To create a Qdrant Data Source:

. From the navigation pane, click *Data > Data Sources*.
. Click *Create a Data Source*.
. In the New Data Source window, from *Select Data Store*, select *Qdrant*.
. Provide the Qdrant *Host* name, *Port*, and link:https://qdrant.tech/documentation/cloud/authentication/#create-api-keys[Qdrant API Key^] for your Qdrant database.
. Specify a *Data Source Name* and an optional *Description*.

Domino stores your Qdrant API Key, along with other secrets, in a link:d8dde6[secure secret store backed by HashiCorp Vault], so you can have confidence that your secrets are safe. You can test the API key before finalizing your Data Source:

. Click *Test Credentials*.
. Confirm that the Data Source authenticates.
. Select who can view and use the Data Source in Projects.

== Use the Qdrant Data Source

To use your Qdrant Data Source, you must install the prerequisite Python libraries in the compute Environment you want to run Qdrant commands in and configure the Qdrant client using a Domino-specific configuration.

=== Install the Qdrant Python library

To install the Qdrant Python library in your compute Environment by link:5dd2c1[edit your compute Environment] and add the following lines to the Dockerfile instructions:

[source,dockerfile]
----
USER root

RUN pip install --user qdrant-client

USER ubuntu
----


=== Configure the Qdrant Python client

To connect to your Qdrant service in a Domino execution, you must initialize your Qdrant connection using a Domino-specific Environment. To find the configuration code snippets:

. link:fa5f3a#_add_an_existing_data_source_to_a_project[Add the Qdrant Data Source to your Project].
. In your Workspace, go to *Data* > *Data Sources* > *Code snippet* > *Python*.
. Copy and paste the following code snippet into your code, and modify it as needed.

[source,python]
----
import os

from qdrant_client import QdrantClient

# Create the Qdrant client
qdrant = QdrantClient(
    url=os.environ.get("DOMINO_DATA_API_GATEWAY", "http://127.0.0.1:8766"),
    metadata={"X-Domino-Datasource": "qdrant"},
)

# List all collections
print(qdrant.get_collections())
----

Once your Qdrant connection is initialized in the Domino execution, you can use the link:https://python-client.qdrant.tech/[Qdrant Python client^] normally.

== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].

* link:33ea62[Share this Data Source] with your collaborators.

----- user_guide/data-source-connectors/connect-to-redshift.txt -----
:page-version: 6.1
:page-title: Connect to Redshift
:page-sidebar: Redshift
:page-permalink: 444abc
:page-order: 290


This topic describes how to connect to https://aws.amazon.com/pm/redshift/[Amazon Redshift^] from Domino.
You must have network connectivity between Redshift and your Domino deployment.

The easiest way to connect to Redshift from Domino is to create a Domino Data Source as described below.

== Create a Redshift Data Source

. From the navigation pane, click *Data > Data Sources*.
. Click *Create a Data Source*.
. In the New Data Source window, from *Select Data Store*, select *Amazon Redshift*.
. Enter the *Host*, *Port*, and name of the *Database*.
. Enter the *Data Source Name*.
. Optional: Enter a *Description* to explain the purpose of the Data Source to others.
. Click *Next*.

. Specify the credentials for authenticating to Redshift.
+
Basic authentication is supported by default.  IAM credential propagation might also be available if your administrator has link:6a8639#iam-auth[enabled it].
+
NOTE: IAM-authenticated connections can be used only for executing jobs and workspaces.  Other execution types, such as scheduled jobs and Domino endpoints, require basic authentication.


. Select who can view and use the Data Source in projects.
. Click *Finish Setup*.

== Alternate way to connect to a Redshift Data Source

WARNING: This section describes an alternate method to connect to the Redshift Data Source.
Domino does not officially support this method.





=== Prerequisites

* Domino recommends storing your database username and password as link:6ac5a1[environment variables] in your project. This lets you access them at runtime without including them in your code.

=== Python

To establish a connection to Redshift with the `psycopg2` library:

[source,python]
----
import psycopg2
import os

HOST = os.environ['REDSHIFT_HOST']
PORT = 5439 # redshift default
USER = os.environ['REDSHIFT_USER']
PASSWORD = os.environ['REDSHIFT_PASSWD']
DATABASE = 'mydatabase'

def db_connection():
    conn = psycopg2.connect(
        host=HOST,
        port=PORT,
        user=USER,
        password=PASSWORD,
        database=DATABASE,
    )
    return conn

example_query = "SELECT * FROM my_table LIMIT 5"

conn = db_connection()
try:
    cursor = conn.cursor()
    cursor.execute(example_query)
    results = cursor.fetchall() # careful, the results could be huge
    conn.commit()
    print results
finally:
    conn.close()

# using pandas
import pandas as pd
conn = db_connection()
try:
    df = pd.read_sql(example_query, conn)
    df.to_csv('results/outfile.csv', index=False)
finally:
    conn.close()
----

=== R

To establish a connection to Redshift with the RPostgreSQL library:

[source,r]
----
install.packages("RPostgreSQL")
library(RPostgreSQL)

redshift_host <- Sys.getenv("REDSHIFT_HOST")
redshift_port <- "5439"
redshift_user <- Sys.getenv("REDSHIFT_USER")
redshift_password <- Sys.getenv("REDSHIFT_PASSWORD")
redshift_db <- "mydatabase"

drv <- dbDriver("PostgreSQL")
conn <- dbConnect(
    drv,
    host=redshift_host,
    port=redshift_port,
    user=redshift_user,
    password=redshift_password,
    dbname=redshift_db)

tryCatch({
    example_query <- "SELECT * FROM my_table LIMIT 5"
    results <- dbGetQuery(conn, example_query)
    print(results)
}, finally = {
    dbDisconnect(conn)
})
----


== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].
* link:33ea62[Share this Data Source] with your collaborators.

----- user_guide/data-source-connectors/connect-to-saphana.txt -----
:page-version: 6.1
:page-permalink: 675a44
:page-title: Connect to SAP HANA 2
:page-sidebar: SAP HANA 2
:page-order: 300


Your link:f32f69[administrator must create a Data Source for this data store] for you to use. 

After your administrator has created the Data Source, you can work with it in your project as usual.

== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].
* link:33ea62[Share this Data Source] with your collaborators.

----- user_guide/data-source-connectors/connect-to-singlestore.txt -----
:page-version: 6.1
:page-permalink: 7753d6
:page-title: Connect to SingleStore (MemSQL)
:page-sidebar: SingleStore (MemSQL)
:page-order: 310


Your link:f32f69[administrator must create a Data Source for this data store] for you to use. 

After your administrator has created the Data Source, you can work with it in your project as usual.

== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].
* link:33ea62[Share this Data Source] with your collaborators.

----- user_guide/data-source-connectors/connect-to-snowflake.txt -----
:page-version: 6.1
:page-title: Connect to Snowflake
:page-sidebar: Snowflake
:page-permalink: d4ef2b
:page-order: 320

This topic describes how to connect to https://www.snowflake.com/[Snowflake^] from Domino.

The easiest way to connect to Snowflake from Domino is to create a Domino Data Source as described below.

== Prerequisites

You must have network connectivity between Snowflake and your Domino deployment.

To use Snowflake code integrations, such as link:https://docs.snowflake.com/en/developer-guide/snowpark/index.html[Snowpark^], you must agree to the https://docs.snowflake.com/en/developer-guide/udf/python/udf-python-packages.html#getting-started[Snowflake third party terms^].
To agree to these terms, you must have a Snowflake account with the link:https://docs.snowflake.com/en/user-guide/organizations-gs.html#enabling-the-orgadmin-role-for-an-account[ORGADMIN^] role.
If you don't have access to a Snowflake account with the ORGADMIN role, link:https://community.snowflake.com/s/article/How-To-Submit-a-Support-Case-in-Snowflake-Lodge[submit a Snowflake support ticket^].

== Create a Snowflake Data Source

Domino recommends that you use a link:fbb41f[Domino Data Source] to connect to a Snowflake instance from Domino.

. From the navigation pane, click *Data > Data Sources*.
. Click *Create a Data Source*.
. In the New Data Source window, from *Select Data Store*, select *Snowflake*.
+
Account Name:: If the Domino deployment and Snowflake Data Source, are in the same region, enter the *Account Name* as `<account name>`.
However, if the Domino deployment and Snowflake Data Source are in different regions, enter the *Account Name* as `<account name>.<region>`.
For example, `abc.us-east`. (For Azure, these might take the form of `def.east-us-2.azure`.)
Optional: Database:: The name of the Snowflake database that contains the data.
Optional: Schema:: The name of the active schema for the session.
Optional: Warehouse:: The name of all the compute resource clusters that provide the resources in Snowflake.
Optional: Role:: The role that has privileges to the Data Source.
Data Source Name:: The name that identifies the Data Source.
Optional: Description:: The purpose for the Data Source.

. Click *Next*.

. Specify the credentials for authenticating to Snowflake.
+
--
* *Basic authentication* is supported by default.
The Domino secret store backed by HashiCorp Vault securely stores the credentials.
* *OAuth authentication* is available if your administrator has link:6a8639#snowflake-oauth[enabled this option].
+
NOTE: OAuth-authenticated connections can be used for any execution type _except_ Domino endpoints.
--
. If you are using basic authentication, click *Test Credentials* to validate your username and password.

. Click *Next* (or *Skip for Now* to configure authentication later).

. Select who can view and use the Data Source in projects.

. Click *Finish Setup*.

If your users have Domino permissions to use the Data Source and enter their credentials, they can now use the Domino Data API to retrieve data with the connector.

See link:fa5f3a[Retrieve Data] for more information.

== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].
* link:33ea62#share-a-datasource[Share this Data Source] with your collaborators.

----- user_guide/data-source-connectors/connect-to-starburst-trino.txt -----
:page-version: 6.1
:page-permalink: 22474d
:page-title: Connect to Starburst/Trino
:page-sidebar: Starburst/Trino
:page-order: 330

Use the Starburst/link:https://trino.io/docs/current/[Trino^] Data Source to connect to an existing Starburst/Trino cluster. Once connected, you can use the Data Source to:

* Execute queries against the Starburst/Trino cluster using the data stores it's connected to.
* Run Starburst/Trino federated queries supported by your Starburst/Trino cluster.

See the https://trino.io/docs/current/connector.html[list of Trino connectors^] Trino connectors for supported data stores.

You must have network connectivity between the Starburst/Trino cluster and your Domino deployment.

NOTE: In a hybrid deployment, Trino Data Sources are only available in executions that use a hardware tier in the local link:95520d[data plane].

== Create a Trino Data Source

Domino recommends that you use a link:fbb41f[Domino Data Source] to connect to a Trino instance.

. From the navigation pane, click *Data > Data Sources*.
. Click *Create a Data Source*.
. In the New Data Source window, from *Select Data Store*, select *Trino*.
. Enter the details about your Data Source and click *Next*:
+
Host:: The URL, hostname, or IP address of your Trino host.
Optional: Networking Proxy:: The URL (host and port) of your networking proxy that points to your Trino cluster, if applicable.
Port:: The port number for the connection, such as 443.
Catalog:: The name of the Trino catalog where your data is located.
For example, if you plan to use the table `hive.test_data.test` then the catalog name is `hive`.
Optional: Schema:: The name of the Trino schema to use.
For example, if you plan to use the table `hive.test_data.test` then the schema name is `test_data`.
Data Source Name:: The name that identifies the Data Source.
Optional: Data Source Description:: The purpose for the Data Source.

. Specify the credentials for authenticating to Trino.
+
Basic authentication is supported by default.
The Domino secret store backed by HashiCorp Vault securely stores the credentials.
+
. Click *Test Credentials*.
. If the Data Source authenticates, click *Next*.
. Select who can view and use the Data Source in projects.
. Click *Finish Setup*.


== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].
* link:33ea62[Share this Data Source] with your collaborators.

----- user_guide/data-source-connectors/connect-to-synapse.txt -----
:page-version: 6.1
:page-permalink: 609f68
:page-title: Connect to Synapse
:page-sidebar: Synapse
:page-order: 340


Your link:f32f69[administrator must create a Data Source for this data store] for you to use. 

After your administrator has created the Data Source, you can work with it in your project as usual.

== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].
* link:33ea62[Share this Data Source] with your collaborators.

----- user_guide/data-source-connectors/connect-to-teradata.txt -----
:page-version: 6.1
:page-title: Connect to Teradata
:page-sidebar: Teradata
:page-permalink: 4c1129
:page-order: 350

This topic describes how to connect to https://www.teradata.com/[Teradata^] from Domino.

Teradata Vantage is a multi-cloud data platform providing access to data lakes, data warehouses, and analytics.
You must have network connectivity between Teradata and your Domino deployment.

The easiest way to connect to Teradata from Domino is to create a Domino Data Source as described below.

== Create a Teradata Data Source

Administrators can create a Data Source for Teradata that Domino users can access.  Domino recommends this method.  See the link:0fc615[Admin Guide] for details.

After your administrator has created the Data Source, you can link:fa5f3a[query it].

[NOTE]
====
You must specify the database name in your query, as in this example where `td_demo` is the database name and `yellow_cab` is the table name:

----
example res = ds.query("select * from td_demo.yellow_cab")
----
====


== Alternate way to connect to a Teradata Data Source

WARNING: Domino does not officially support this method. We provide this information as a courtesy.

Domino recommends the https://pypi.org/project/teradatasql/[Teradata SQL Driver for Python^].

=== Environment setup

The Teradata SQL driver for Python comes pre-installed in several Domino environments. If your environment does not have it installed by default, you can use the following Dockerfile instructions to add it to your environment:

----
RUN pip install teradatasql
----

=== Credential setup

Set the following Domino environment variables to store secure information about your Teradata connection.

* `TERADATA_USER`
* `TERADATA_PASSWORD`
* `TERADATA_HOST`

See link:d8dde6[Store Project credentials] to learn more about Domino environment variables.

=== Usage

See the https://github.com/Teradata/python-driver[Teradata SQL Driver for Python documentation^] for information about how to use the package.
The following is an example.

[source,python]
----
import teradatasql
import os

con = teradatasql.connect(
    user=os.environ['TERADATA_USER'],
    password=os.environ['TERADATA_PASSWORD'],
    host=os.environ['TERADATA_HOST']
    )
cur = con.cursor()
try:
    cur.execute ('{fn teradata_nativesql}Driver version {fn teradata_driver_version}  Database version {fn teradata_database_version}')
    print (cur.fetchone () [0])
finally:
    cur.close()
con.close()
----

== Additional resources for Teradata SQL Driver

*Python*:

* https://docs.teradata.com/r/I5P9sy9O8T4Bsv1ndwGMbg/y3UhMbfkXZVQkYdQlf92lA[Official documentation^]
* https://github.com/Teradata/python-driver[Repository^]
* https://pypi.org/project/teradatasql/[Package^]

*R*:

* https://docs.teradata.com/r/I5P9sy9O8T4Bsv1ndwGMbg/dVYM0P9c1un4po_WBJCFYg[Official documentation^]
* https://github.com/Teradata/r-driver[Repository^]
* https://github.com/Teradata/r-driver#Installation[Packages^]

== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].
* link:33ea62#share-a-datasource[Share this Data Source] with your collaborators.

----- user_guide/data-source-connectors/connect-to-vertica.txt -----
:page-version: 6.1
:page-permalink: 778bc1
:page-title: Connect to Vertica
:page-sidebar: Vertica
:page-order: 360


Your link:f32f69[administrator must create a Data Source for this data store] for you to use. 

After your administrator has created the Data Source, you can work with it in your project as usual.

== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources].
* link:33ea62[Share this Data Source] with your collaborators.

----- user_guide/data-source-connectors/data-source-from-sas.txt -----
:page-version: 6.1
:page-permalink: 23a442
:page-title: Access Data Sources from SAS workspaces
:page-sidebar: SAS Data Sources
:page-order: 20

[[access_data_sources_from_sas]]
[[tr12]]
You can access link:634c13[Microsoft SQL Server], link:77bf79[Oracle], and link:63393f[Databricks] Data Sources from your link:e7805a[SAS Workspaces]. Domino uses the SAS/ACCESS interface to ODBC to access these Data Sources.

[[scope_limitations]]
== Scope limitations

* Domino only supports link:634c13[Microsoft SQL Server], link:77bf79[Oracle], and link:63393f[Databricks] Data Sources through the SAS/ACCESS interface to ODBC.

* Parameterized queries aren't supported.

[[prerequisites]]
== Prerequisites

* Your Data Sources must be created before you launch your Workspace. Restart your Workspace to see new Data Sources.
* The environment must have the Arrow Flight SQL ODBC driver installed.
+
To install the Arrow Flight SQL ODBC driver, add the following lines to your Dockerfile.
+
[source,dockerfile]
----
USER root

# ODBC Driver for Arrow Flight SQL

RUN curl -s -o arrow-flight-sql-odbc-driver.rpm https://download.dremio.com/arrow-flight-sql-odbc-driver/0.9.1/arrow-flight-sql-odbc-driver-0.9.1.168-1.x86_64.rpm

RUN dnf clean packages
RUN dnf config-manager --add-repo https://repo.almalinux.org/almalinux/8/BaseOS/x86_64/os/
RUN dnf install -y --nogpgcheck --repo "repo.almalinux.org_almalinux_8_BaseOS_x86_64_os_" libnsl
RUN sudo dnf -y --nogpgcheck localinstall arrow-flight-sql-odbc-driver.rpm

RUN dnf clean all && rm arrow-flight-sql-odbc-driver.rpm

# Configure SAS odbc manager
RUN echo "export ODBCSYSINI=/etc" >> /opt/sas/viya/config/etc/workspaceserver/default/workspaceserver_usermods.sh && \
    echo "export ODBCINI=odbc.ini" >> /opt/sas/viya/config/etc/workspaceserver/default/workspaceserver_usermods.sh && \
    echo "export ODBCINSTINI=odbcinst.ini" >> /opt/sas/viya/config/etc/workspaceserver/default/workspaceserver_usermods.sh && \
    echo "export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/lib64" >> /opt/sas/viya/config/etc/workspaceserver/default/workspaceserver_usermods.sh

USER domino
----

[[code_snippet]]
== Code snippet

The following SAS snippet shows you how to access Domino Data Sources using the SAS/ACCESS interface to ODBC and SQL pass-through to connect and send statements directly to the DBMS.
Enter your Domino Data Source name in the `datasrc` field.

[source,sas]
----
options set=EASYSOFT_UNICODE=YES;

proc sql;
connect to odbc as mycon(datasrc="sqlserver");

select *
   from connection to mycon
      (select top 10 * from yellow_cab);

disconnect from mycon;
quit;

----

[[troubleshooting]]
== Troubleshooting

Troubleshoot SAS errors by viewing the error message and stack trace in the SAS Studio UI log.

Here are some common errors and solutions:

* If a Data Source is created after launching the Workspace, it won't generate an ODBC configuration causing the error, "Data source name not found and no default driver specified". To resolve the error, restart the Workspace.
* The error "Flight returned internal error" indicates issues with the SQL statement or the use of an invalid SQL dialect.

== Next steps

* After connecting to your Data Source, learn how to link:fa5f3a[Use Data Sources^].
* link:33ea62[Share this Data Source^] with your collaborators.

----- user_guide/data-source-connectors/index.txt -----
:page-version: 6.1
:page-title: Work with Data Source Connectors
:page-sidebar: Data Source Connectors
:page-permalink: fbb41f
:page-order: 110

Domino can connect to your external data sources, such as databases, data warehouses, and data lakes.

Domino automatically creates an link:ae2e6b[audit trail] for Domino Data Sources to let you audit and track all Data Source-related activities.


== Domino Data Source vs. direct connections

There are two ways to connect your external data:

* *Domino Data Source connectors* are available for some types of data sources.
Data Source connectors are the simplest way to connect to supported data sources.
It takes just a few clicks to create a Data Source, and you can re-use it in any project.

* *Direct connections* are available for all types of data sources.
This approach is useful if you want to use a data source that is not supported by a Domino connector, or if you want to use a different version of the connector than the one that is installed on the Domino server.
However, direct connections require more configuration than Domino connectors and are not officially supported.


NOTE: In a link:c65074[Domino Nexus deployment], Data Sources can be accessed on both the `Local` and remote link:e6e601[data planes], with the exception of Starburst Trino which can only be accessed on the Local data plane.
Data sources may not be usable in every data plane due to network restrictions.


The table below compares the two methods for accessing external data sources:

[cols="1a,1a",options="header"]
|===
| Domino Data Source | Direct connection

| Fetch data from the Data Source into a data frame using SQL.
| Fetch data using native library functions.

| Optimized for read-heavy workflows.

Data sources support synchronous read/write operations. However, Data Source connectors don't return metadata about the results of a write operation, such as the number of rows affected.

| Write-heavy workflows.

Direct connections can support asynchronous read/write operations

| Easy to set up and use.

Once configured a few simple steps or
programmatically,
you can embed the Data Sources into your notebooks and fetch data into your data frames through SQL.

| Use of specialized library functions is a priority.

This can be important in non-traditional data sources or in data lakes, where you might rely on specialized functions to process different shapes of data to embed them into your CI/CD pipelines.

| Sharing is a priority.

Domino Data Sources are easy to share with other users across Domino projects.
They also support service accounts that authenticate all users, in addition to individual accounts where users provide their own credentials.
| Sharing is not a priority.

You must set up connectivity to your Data Sources on your own and explicitly share them with other users.

| Easy security configuration.

Domino Data Sources allow for secure safe-keeping of individual and service account credentials in HashiCorp Vault; there is no risk of those secrets being shared with or leaked to other users.

| Secure connectivity must be established through custom solutions using Kubernetes secrets or through integration with a centralized vault/safe-keeping mechanism.

| Portability is not a priority.

Domino Data Sources do not prioritize support for integrated workflows/pipelines that pull/process data outside of Domino.
Although you can call the link:140b48[Domino Data API] from outside of Domino, it does not offer the same benefits and safeguards that native libraries provide when talking directly to services outside of Domino.

| Portability is a priority.

If you need to reuse your code outside of Domino or you want broad functionality when communicating with your data services from outside of Domino, using native libraries might be preferable.

|===

== Domino connectors

Domino comes with connectors for many external data sources. See full list of supported connectors in the left navigation under Data Source Connectors.

=== Starburst JDBC Data Source connectors

Only Domino administrators can create link:ae2e6b[Data Sources for Starburst JDBC] data entities to extend Domino's connectivity through Starburst's extensive JDBC capabilities. Once your admin has created the Starburst-powered Data Source, link:fa5f3a[add it to your Project] like any other Data Source.

For a list of data entities compatible with Starburst JDBC drivers, refer to https://docs.starburst.io/latest/connector.html[Starburst's documentation^].


== Connect directly to an external data source

// tag::direct-connections[]

The following data sources are not supported by a Domino Data Source connector, but you can still connect directly to these data sources. Domino does not provide support for any issues related to direct connections.

* link:452cff[DataRobot]
* link:f200d6[Impala]
// end::direct-connections[]

== Next steps

link:fa5f3a[Use Data Sources.]

----- user_guide/data-source-connectors/use-data-sources.txt -----
:page-version: 6.1
:page-title: Use Data Sources
:page-permalink: fa5f3a
:page-order: 10

[[tr7]]
// Project owner should be able to add Data Source to the Project
[[tr8]]
// Project contributor should be able to add Data Source to the Project
[[tr9]]
// When the Data source is not explicitly added users with authenticated access should be able to connect to the Data Source in any Project execution
Data sources have a global scope in a Domino deployment and are accessible to anyone with the appropriate permissions in any project.

== Connect to a Data Source

Some data stores require additional steps. The link:fbb41f[Data Source connector] page has more details about specific connections.

NOTE: Data Sources can be set up to use either service account credentials or individual user credentials. Verify the credentials you'll need with your administrator before connecting to data sources.

These steps show you how to connect to a Data Store:

. From the navigation pane, click *Data* > *Data Sources* > *Add a Data Source*.
. Click *Create New Data Source*, then click *Create a Data Source*.
. Choose an option from the *Select Data Store* dropdown.
. Enter credentials for the Data Source and choose from the options.
. Click *Finish Setup*.

[[add_an_existing_data_source_to_a_project]]
== Add an existing Data Source to a project

You can add data sources to a project in two ways. First, you can add a data source directly to the project's data page. Second, a data source can be added automatically when used in the code within the project.

// Project owner should be able to add Data Source to the Project

// Project contributor should be able to add Data Source to the Project

You can add a data source to a project if you have access to that data source and it is set up. This step is not required, but it helps you see which data sources are used in your projects.

// When the Data source is not explicitly added users with authenticated access should be able to connect to the Data Source in any Project execution
If you don't add a Data Source to a project, you can still use it in your code if you have permission to access it.

//tag::add-data-source[]
. In your project, go to *Data* > *Data Sources* > *Add a Data Source*.
. Select an existing Data Source from the list.
. Click *Add to Project*.
//end::add-data-source[]

== Use the Domino Data API

[[tr12]]
After a Data Source is properly configured, use the link:140b48[Domino Data API^] to retrieve data without installing drivers or Data Source-specific libraries.


The auto-generated code snippets provided in your workspace are based on the Domino Data API, which supports tabular and file-based Data Sources. The API supports Python and R.

The Data API comes pre-packaged in the Domino Standard Environment (DSE). You can link:5267b0[install the Data API] in custom environments if needed.

The Data API's Data Source client uses environment variables available in the workspace to automatically authenticate your identity.  You can override this behavior using link:9edb62[custom authentication].

=== Get code snippets

[[tr13]]
// Snippet for Tabular Data Source
[[tr14]]
// Snippet for File-based Data Source
Domino creates code snippets to help you access data sources for your project using the Domino Data API. Code snippets are available for Python and R and customized for tabular and file-based Data Sources. The Data Source must be link:f37ae6#add-data[added to your project] to enable snippets.

Here's how to get a code snippet that you can copy and paste into your workspace:

. In your workspace, go to *Data* > *Data Sources*.
// R studio code snippets were not supported until 5.7 (DOM-44571)
// Using new Domino Logo for 5.10 and beyond
. Click the copy icon to display language options.
+
image::/images/6.0/data-source-code-snippet1.png[alt="Code snippet button for Python or R", width=600]
. Select *Python* or *R* to copy the code snippet in the desired language.
. Paste the copied snippet into your own code, and modify it as needed.

// tag::unnest-ds[]

[NOTE]
====
Domino data sources do not support querying nested objects.
The workaround is to https://count.co/sql-resources/bigquery-standard-sql/unnest[`UNNEST`^] the object in the SQL query.

The following is an example `UNNEST` query:
----
res = ds.query("""
select account_id, t1
from sample_analytics.transactions
cross join unnest (transactions)
as t(t1, t2, t3, t4, t5, t6)
""")
----
====
// end::unnest-ds[]

=== Query a tabular store

You can query a tabular store with the following code, assuming a data source named `redshift-test` is configured with valid credentials for the current user:

[source,python]
----
from domino.data_sources import DataSourceClient

# instantiate a client and fetch the datasource instance
redshift = DataSourceClient().get_datasource("redshift-test")

query = """
     SELECT
         firstname,
         lastname,
         age
     FROM
         employees
     LIMIT 1000
 """

 # res is a simple wrapper of the query result
 res = redshift.query(query)
 # to_pandas() loads the result into a pandas dataframe
 df = res.to_pandas()
 # check the first 10 rows
 df.head(10)
----

[source,r]
----
library(DominoDataR)
client <- DominoDataR::datasource_client()

query <- "
  SELECT
    firstname,
    lastname,
    age
  FROM
    employees
  LIMIT 1000"

# table is a https://arrow.apache.org/docs/r/reference/Table-class.html
table <- DominoDataR::query(client, "redshift-test", query)
#> Table
#> 1000 rows x 3 columns
#> $firstname <string not null>
#> $lastname <string not null>
#> $age <int32 not null>
----

=== Read/write to an object store

==== List
Get the data source from the client:

[source,python]
----
from domino.data_sources import DataSourceClient

s3_dev = DataSourceClient().get_datasource("s3-dev")
----
[source,r]
----
library(DominoDataR)
client <- DominoDataR::datasource_client()
----

You can list objects available in the data source. You can also specify a prefix:

[source,python]
----
objects = s3_dev.list_objects()

objects_under_path = s3_dev.list_objects("path_prefix")
----
[source,r]
----
keys <- DominoDataR::list_keys(client, "s3-dev")

keys_under_path <- DominoDataR::list_keys(client, "s3-dev", "path_prefix")
----

By default the number of returned objects is limited by the underlying data source. You can specify how many keys you want as an optional parameter:

[source,python]
----
objects = s3_dev.list_objects(page_size = 1500)
----

[source,r]
----
keys = DominoDataR::list_keys(client, "s3-dev", page_size = 1500)
----

==== Read

You can get object content, without having to create object entities, by using the data source API and specifying the `Object` key name:

[source,python]
----
# Get content as binary
content = s3_dev.get("key")

# Download content to file
s3_dev.download_file("key", "./path/to/local/file")

# Download content to file-like object
f = io.BytesIO()
s3_dev.download_fileobj("key", f)
----
[source,r]
----
# Get content as raw vector
content <- DominoDataR::get_object(client, "s3-dev", "key")

# Get content as character vector
content <- DominoDataR::get_object(client, "s3-dev", "key", as = "text")

# Download content to file
DominoDataR::save_object(client, "s3-dev", "key", "./path/to/local/file")
----

You can also get the data source entity content from an object entity (Python only):

[source,python]
----
# Key object
my_key = s3_dev.Object("key")

# Get content as binary
content = my_key.get()

# Download content to file
my_key.download_file("./path/to/local/file")

# Download content to file-like object
f = io.BytesIO()
my_key.download_fileobj(f)
----

==== Write

Similar to the read/get APIs, you can also write data to a specific object key. From the data source:

[source,python]
----
# Put binary content to given object key
s3_dev.put("key", b"content")

# Upload file content to specified object key
s3_dev.upload_file("key", "./path/to/local/file")

# Upload file-like content to specified object key
f = io.BytesIO(b"content")
s3_dev.upload_fileobj("key", f)
----
[source,r]
----
# Put raw or character vector to object key
DominoDataR::put_object(client, "s3-dev", "key", what = "content")

# Upload file content to specified object key
DominoDataR::upload_object(client, "s3-dev", "key", file = "./path/to/local/file")
----

You can also write from the object entity (Python only).

[source,python]
----
# Key object
my_key = s3_dev.Object("key")

# Put content as binary
my_key.put(b"content")

# Upload content from file
my_key.upload_file("./path/to/local/file")

# Upload content from file-like object
f = io.BytesIO()
my_key.upload_fileobj(f)
----

=== Write to a local file

==== Parquet

Because Domino uses https://arrow.apache.org/docs/python/[PyArrow^] to serialize and transport data, the query result is easily written to a local parquet file. You can also use pandas as shown in the CSV example.

[source,python]
----
redshift = DataSourceClient().get_datasource("redshift-test")

res = redshift.query("SELECT * FROM wines LIMIT 1000")

# to_parquet() accepts a path or file-like object
# the whole result is loaded and written once
res.to_parquet("./wines_1000.parquet")
----
[source,r]
----
client <- DominoDataR::datasource_client()
table <- DominoDataR::query(client, "redshift-test", "SELECT* FROM wines LIMIT 1000")

# We can use https://arrow.apache.org/docs/r/reference/write_parquet.html since we leverage the arrow library
arrow::write_parquet(table, "./wines_1000.parquet")
----

==== CSV

Because serializing to a CSV is lossy, Domino recommends using the https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html#pandas-dataframe-to-csv[Pandas.to_csv] API so you can leverage its multiple options.

[source,python]
----
redshift = DataSourceClient().get_datasource("redshift-test")

res = redshift.query("SELECT * FROM wines LIMIT 1000")

# See Pandas.to_csv documentation for all options
csv_options = {header: True, quotechar: "'"}

res.to_pandas().to_csv("./wines_1000.csv", **csv_options)
----
[source,r]
----
client <- DominoDataR::datasource_client()
table <- DominoDataR::query(client, "redshift-test", "SELECT* FROM wines LIMIT 1000")

# We can use https://arrow.apache.org/docs/r/reference/write_csv_arrow.html since we leverage the arrow library
arrow::write_csv_arrow(table, "./wines_1000.csv")
----

Your identity is verified automatically to make sure you have the right permissions to use the Domino Data API. The system will try to use a link:40b91f[Domino JWT token]. If that’s not available, it will use a link:40b91f[user API key] instead.

* In a link:c65074[Domino Nexus deployment], Data Sources can be accessed on both the `Local` and remote link:e6e601[data planes], with the exception of Starburst Trino which can only be accessed on the Local data plane.
Data sources may not be usable in every data plane due to network restrictions.
* Connectivity issues may originate anywhere between your Domino deployment and the external data store.
Consult your administrator to verify that the Data Source is accessible from your Domino deployment.

== Next steps

* link:440de9[Create training sets] using the Domino Data API.
* link:942549[Run or schedule a job] to train your model or update your model's predictions using the latest data.
* Learn more about link:3afed0[developing] and link:08a85b[deploying models] in Domino.
* Use link:715969[model monitoring] to link:b08113[detect data drift].

----- user_guide/data/index.txt -----
:page-version: 6.1
:page-permalink: 16d9c1
:page-title: Access Data in Domino
:page-order: 100
:page-separator: true
:page-section: Data

Data is at the core of every data science project.
Data varies across different use cases and lives in many places within an organization.
Domino provides the flexibility to access all of your data, whatever and wherever it may be.

image::/images/diagrams/access-data-diagram.png[alt="Data overview", width=1000, role=noshadow]
//Source: https://docs.google.com/presentation/d/1aY8S8O1PAiBFQQ4KnckuXXODEIdgokcJHrEfteHuY4M/edit#slide=id.g33f9bbbb2a7_1_0
Each of these data access and storage mechanisms comes with pros and cons, suited to different use cases.
The topics in this section explain how to connect your data to Domino and use it in your projects.

== Access all your data, anywhere

Domino provides a single interface for accessing all of your data, wherever it lives.

Domino Data Source Connectors provide easy setup for connections to popular data services, or you can connect Domino directly to any data service using the same code you use in your local environment.
Domino Datasets enable you to upload, store and manage data within the Domino system.

You can also mount external storage volumes to Domino and use your Git repositories as project file stores.

The following table compares Domino's data access and storage methods:

// tag::comparison-table-head[]
[cols="2a,2a,2a,2a,2a",options="header"]
|===
|
|link:ba5bad[Domino Datasets]
|link:fbb41f[Data Source connectors]
|link:f12554[External Data Volumes]
|link:63ac71#save-artifacts[Project artifacts]
// end::comparison-table-head[]

|*Description*
|Read/Write managed folders shareable within Domino.
Can be versioned with snapshots for reproducibility.
|Managed data connectors that can connect to SQL and file-type stores.
Or, connect directly to any data service using a library.
|Managed external file store mounts.
|A special, version-controlled folder that works similarly to https://git-lfs.com/[Git LFS^].

|*Location*
|Network File System (NFS) storage or Amazon Elastic File System.
|Any external data service.
// Need to confirm that that's technically true.
// That is, in addition to our connectors is there a generic connection method that will work with any otherwise-unsupported data store?
|NFS or Server Message Block (SMB) storage.
|Domino File System (DFS).

|*Intended use cases*
|
* Training sets that can't easily be shared/controlled outside of Domino.
* Derived data that's an input to a downstream processing step in a pipeline.
|Accessing data in existing external stores such as databases.
|Exposing existing IT data storage interfaces within Domino.
|Storing outputs (such as charts, serialized model files, output CSVs, and so on).

|*Intended data sizes*
|Up to ~1TB per Dataset and hundreds of TB across Datasets.
|Constrained only by what you query and pull into the machines executing your code.
|Constrained by your existing network storage.
|Up to ~10GB.

|*Advantages*
|Supports much larger data than project artifacts, with snapshots for reproducibility.
|
Connectors provide an easy and secure way to connect to external data without drivers or configuration.
Direct connections use the same code you would use outside of Domino, with the flexibility to access files or data however you want.
|Use the same code you would use outside of Domino.
|Simple to use and share.

|*Limitations*
|Snapshots must be managed to minimize storage costs.
|
Code for accessing connector-supported data sources is not portable outside Domino.
Domino does not automatically track snapshots for reproducibility.
|Will not automatically track snapshots for reproducibility.
|Not performant at scale of data size or many thousands of files.

|*Query methods*
// Or "Access methods"
|Path to the mounted Dataset.
|
* link:fa5f3a[Code snippets] for Domino data sources.
* link:140b48[Domino Data API].
* link:3d3103[Direct access].

// link needed
|Path to the mounted volume and files.
|Path to the project's files.

// tag::comparison-table-executions[]
|*Supported executions*
|
// The list below is currently missing from the Dataset docs but is found at https://university.dominodatalab.com/domino-for-data-analysts/1579000
* Workspaces
* Jobs
* Launchers
* Apps
|
* Workspaces
* Jobs
* Launchers
* Apps
* Domino endpoints
// Are the above support for "direct access" or only for data sources?
|
* Workspaces
* Jobs
* Launchers
* Apps
* On-demand Spark clusters
|All
// end::comparison-table-executions[]

|*Access control*
|link:8f5b7e[Role based].
|
* link:33ea62[Domino data sources]: Based on Domino user/group or a service account.
* link:3d3103[Direct access]: Based on your data service's access control (IAM token propagation is supported for some AWS data services).
// link needed
|Based on Domino user/organization as link:7ec608[configured by an admin].
|Per-project link:7876f1[collaborator permissions].

|===

== Data reproducibility

When your project requires reproducible data, you can link:dbdbff[create Dataset snapshots] that are immutable and versioned.
You can also use the Data API to link:9c4dec[create training sets] based on any of your data in any location.
If your external data sources or network filesystems support dataframes, snapshots, or versioning, you can use those features in your code in Domino.

== Share your data securely

Data in Domino is reusable across multiple projects, with full control over who can access it.
You can share data with your collaborators and decide who has read-only access or the ability to edit the data.

See link:df7044[Data sharing and security] to learn how to share your data with collaborators and keep it secure.

// TO DO: Find a place to discuss training sets.  They're mentioned in the API guide:
// https://docs.dominodatalab.com/en/latest/api_guide/440de9/trainingsets-use-cases/

// TO DO: Add content from "Working with Data in On-Demand Compute Clusters"
// https://university.dominodatalab.com/domino-201-advanced-features-v55/1548445

----- user_guide/datasets/create-and-manage-datasets.txt -----
:page-version: 6.1
:page-title: Create and manage Datasets
:page-permalink: 0a8d11
:page-order: 10


Domino Datasets provides high-performance, versioned, and structured filesystem storage in Domino.
You can use Datasets to build multiple curated collections of data in one Project and share them with your collaborators to use in their Projects.
Likewise, you can mount Datasets from other Projects in your own Project if they are shared with you.
You can modify the contents of a Dataset through the Domino application or through workload executions.

A Dataset always reflects the most recent version of the data.
For reproducibility, you can create read-only snapshots of your Dataset at any moment in time.
Snapshots are associated with the Dataset they version.


== Create a new Dataset

If the Dataset you need is not yet in Domino, you can create it in your Project:

[[tr1]]
// Create Read-Write Dataset
[[tr2]]
// Pause and resume the Upload
[[tr29]]
// Upload files to the Datasets

[#ui-1]
.In the UI
--
. In your Project, go to *Data* > *Domino Datasets* > *Create New Dataset*.
. Enter a *Dataset Name* and *Description*, then click *Next*.
. Enter the users or organizations to give them permission to the Dataset.
+
TIP: To give all Project members access to the Dataset, click the *Add all project members* link.

. Specify the user or organization's link:8f5b7e[role].
. Click *Add*.
. Repeat steps 6-8 as needed.
+
TIP: In the *Current Permissions* area, you can modify the link:8f5b7e[role] as needed, or click the trash icon to delete permissions.

. Click *Finish*.

--
[#pydom]
.In `python-domino`
--
You can use the link:c5ef26[python-domino library] to create a Dataset using `datasets_create`:
[source,python]
----
datasets_create(dataset_name, dataset_description)
----

* `dataset_name`: Name of the new Dataset. The name must be unique.
* `dataset_description`: Description of the Dataset.
--
[#api-1]
.In the REST API
--
.Endpoint:
....
/api/datasetrw/v1/datasets
....
.Body:
[source,json]
----
{
    "name": "<new-dataset-name>",
    "description": "<dataset-description>",
    "projectId": "<project-id>",
    "grants": []
}
----

.Example:
Example request:
[source,python]
----
import requests
import json

url = "test14995.test-api.tech/api/datasetrw/v1/datasets"

payload = json.dumps({
  "name": "test-dataset4",
  "description": "A new dataset",
  "projectId": "649cb3b0dab75c31b6e9daf6",
  "grants": []
})
headers = {
  'X-Domino-Api-Key': 'a944d3612a76f6c44ad9c680c25234b2ef580ac6b5d2649fd3fc0a259cb62255',
  'Content-Type': 'application/json'
}

response = requests.request("POST", url, headers=headers, data=payload)

print(response.text)
----
Example response:
[source,json]
----
{
    "dataset": {
        "id": "649cb4dedeb75c31b6e7dafc",
        "name": "test-dataset4",
        "description": "A new dataset",
        "projectId": "649cb3b0dab75c31b6e9daf6",
        "createdAt": "2023-06-28T22:31:58.042Z",
        "snapshotIds": [
            "649cb4dedeb75c31b6e7dafb"
        ],
        "tags": {}
    },
    "metadata": {
        "requestId": "956ef589-8f4d-4c6d-b7f3-1c28d0201248",
        "notices": []
    }
}
----
--

=== Create a new Dataset from a snapshot

You can create as many snapshots as you need, but you cannot modify existing snapshots.
Instead, you can create a Dataset from an existing snapshot, modify the new Dataset, and then create a new snapshot:

[#ui-2]
.In the UI
--
. Go to the existing snapshot.
. Click *Copy to New Dataset*.
. Complete the fields as needed.
. Click *Upload files* to add files to the Dataset.
. Click *Take Snapshot* > *Include all files*.
--

[#cli-1]
.In the CLI
--
.Syntax:
[source,shell]
----
domino create-dataset-from-snapshot <project-owner>/<project-name>/<dataset-name> <snapshot-number> <new-dataset-name>
----
--

[#api-2]
.In the API
--
.Endpoint:
....
/api/datasetrw/v1/datasets
....
.Body:
[source,json]
----
{
    "name": "test-dataset5",
    "description": "A new dataset based on a snapshot",
    "grants": [],
    "snapshotId": "649cbe38fe7c5443a9cb1b62"
}
----
.Example: Create a new Dataset from a snapshot
Example request:
[source,python]
----
import requests
import json

url = "test14995.test-api.tech/api/datasetrw/v1/datasets"

payload = json.dumps({
  "name": "test-dataset5",
  "description": "A new dataset based on a snapshot",
  "grants": [],
  "snapshotId": "649cbe38fa7c5443a7cb1b62"
})
headers = {
  'X-Domino-Api-Key': 'a944d3612a76f6c44ad9c680c25234b2ef580ac6b5d2649fd3fc0a259cb62255',
  'Content-Type': 'application/json'
}

response = requests.request("POST", url, headers=headers, data=payload)

print(response.text)
----
Example response:
[source,json]
----
{
    "dataset": {
        "id": "649cb689deb75c31b6e7dafe",
        "name": "test-dataset5",
        "description": "A new dataset based on a snapshot",
        "projectId": "649cb2b0deb75c31b6e7daf6",
        "createdAt": "2023-06-28T22:39:05.788Z",
        "snapshotIds": [
            "649cb689deb75c31b6e7dafd"
        ],
        "tags": {}
    },
    "metadata": {
        "requestId": "3d87b111-8052-4965-9044-0531ba49a93a",
        "notices": []
    }
}
----
--

=== Upload files to a Dataset

[[tr3]]
// Max upload
Use the Domino UI to upload up to 50GB or 50,000 individual files.
For larger uploads, use the Domino CLI for your upload.

[#ui-3]
.In the UI
--

. In your Project, go to *Data* > *Domino Datasets*.
. Open the Dataset to which you want to add files, then click *Upload files*:
+
image::/images/6.0/dataset-upload.png[alt="Upload files to a Dataset", width=800]
+
You can browse your local filesystem or drag and drop files to upload.
+
TIP: To preserve the filesystem structure of your uploads, use the drag-and-drop option.
You can pause and resume the upload as needed.
+
NOTE: Uploading to a folder can fail if you do not have permissions to write to it. For example, a folder created from a
Workspace might not be writable through the UI without appropriate changes to permissions.
--

[#cli-2]
.In the CLI
--
[[tr4]]
// CLI Upload - left out [source,bash] as it is buggy and put the command above the text
[[tr15]]
Upload all the files in a folder from your local machine to an existing dataset.

CAUTION: This command overwrites existing files unless you use the `&#8209;&#8209;fileUploadSetting` option.

.Syntax:
[source,shell]
----
domino [--fileUploadSetting Ignore|Overwrite|Rename] [--targetRelativePath <destination path>] <project-owner>/<project-name>/<dataset-name> <folder path>
----

The optional `fileUploadSetting` flag handles path collisions:

* `Ignore`: If a file already exists in the Dataset, ignore the new file.
* `Overwrite`: If a file already exists in the Dataset, overwrite the existing file with the new file.
* `Rename`: If a file already exists in the Dataset, append `_1` to the uploaded filename.
For example, if `/Users/myUser/data/file.txt` already exists then the newly-uploaded file becomes `/Users/myUser/data/file_1.txt`.

The optional `targetRelativePath` flag enables uploading to a subdirectory in the Dataset:

* `<destination path>`: This is the path, relative to the root directory of the Dataset,  where files will be uploaded.
Note that other users with access to the Dataset can alter the file structure, which may require you to modify the destination path accordingly.
+
If `targetRelativePath` is not specified, files will be uploaded to the root directory of the Dataset.

NOTE: `fileUploadSetting`, `targetRelativePath`, and their values are case-sensitive.

.Example:
[source,shell]
----
domino upload-dataset --fileUploadSetting Overwrite --targetRelativePath Experiments/Run1/Analysis jsmith/global-predictions/global-data /Users/myUser/data
----

If you don't have the CLI installed, see link:e21e55[Install the Domino Command Line (CLI)] for instructions.
--

=== Download files from Datasets

Use the Domino
UI or
CLI to download files and folders from your Projects.

[#ui-4]
.In the UI
--

. In your Project, go to *Data* > *Domino Datasets*.
. Open the Dataset you want to download files from.
. Navigate to the directory where your files are located.
. Either:
** Click on the vertical dots next to the file or folder you want to download. Click *Download*.
If you are downloading a folder, the downloaded file will be a ZIP or TAR archive, which can be toggled via the link:71d6ad#tr310[Configuration records] `com.cerebro.domino.dataset.batchDownloadArchiveFormat`.
Otherwise, the file downloads directly.

** Click on the checkboxes to the left of the files and directories you want to download, and then click *Download Selected Items*.
If you are downloading a folder or multiple files, the downloaded file will be a ZIP file (default) or TAR archive, which can be toggled via the link:71d6ad#tr310[Configuration records] key `com.cerebro.domino.dataset.batchDownloadArchiveFormat`.
Otherwise, the file downloads directly.

--
[#cli-3]
.In the CLI
--
`download` will download the latest copy of your files from the cloud into your current project folder.
If you have made changes that conflict with changes in the cloud, you will see both versions of the conflicting file side-by-side.

There are two reasons files in the cloud might change: first, your collaborators on a Project might make changes; second, you might have executed a run that produced new output files.

.Example
[source,shell]
----
domino download
----

To download the output files of the given run, please see the link:8ec1ce[Download files with the CLI] page.

If you don't have the CLI installed, see link:e21e55[Install the Domino Command Line (CLI)] for instructions.
--

== Modify Datasets

You can always modify the contents of a Dataset or rename the Dataset.

TIP: Always create a snapshot before modifying the contents of a Dataset so that you can always return to the previous version of the data.

=== Rename a Dataset

You can change the name of a Dataset.
//SME can there be any repercussions from doing this?

. In the navigation pane, click *Data*.
. Click *Domino Datasets*.
. Click the Dataset to rename.
. Go to *More Actions > Rename Dataset*.
. Enter a *New Name* and click *Rename*.

=== Delete a Dataset

[[tr19]]
// Mark Dataset for deletion
If you no longer need the entire Dataset, you can mark it for deletion.
When you mark a Dataset for deletion, it removes the Dataset and its associated snapshots from the originating Project and from all projects that it was shared with.
Domino executions will not be able to use the Dataset.
A Domino administrator must perform the final deletion.

. In the navigation pane, click *Data*.
. Click *Domino Datasets*.
. Click the name of the Dataset to delete.
. Go to *More Actions* > Click *Delete Dataset*.
. Click *Delete Dataset* to confirm that you want to mark the Dataset for deletion.
Your administrator will permanently link:6be870[delete the Dataset].


=== Add or remove files

You can add or delete files in a Dataset using the Domino UI.
With the CLI, you can add all the files in a folder to a Dataset.
// Currently there's no way to use the API to do those things.

[#ui-5]
.In the UI
--
In the navigation pane, click *Data*, then click the name of the Dataset to change.

* To add files, click *Upload files*.
* To delete files, select the files to delete, then click *Delete Selected Items*.
[[tr14]]
// Rename Dataset
* To rename the Dataset, click *Rename Dataset*, enter the new name, then click *Rename*.
//The UI says changing the name of a Dataset can have unintended consequences and pointed to the documentation but there was no info. We should add info or remove the note from the UI.

Before deleting a file with a special character like a backslash () in its name, you need to rename it first.
You can use a tilde (~) or colon (:) anywhere in a filename, except at the beginning.
If the file that you want to delete has a tilde or colon at the beginning of its name, rename it.

--

[#cli-4]
.In the CLI
--
Upload all the files in a folder from your local machine to an existing dataset.

CAUTION: This command overwrites existing files unless you use the `&#8209;&#8209;fileUploadSetting` option.

.Syntax:
[source,shell]
----
domino [--fileUploadSetting Ignore|Overwrite|Rename] [--targetRelativePath <destination path>] <project-owner>/<project-name>/<dataset-name> <folder path>
----

The optional `fileUploadSetting` flag handles path collisions:

* `Ignore`: If a file already exists in the Dataset, ignore the new file.
* `Overwrite`: If a file already exists in the Dataset, overwrite the existing file with the new file.
* `Rename`: If a file already exists in the Dataset, append `_1` to the uploaded filename.
For example, if `/Users/myUser/data/file.txt` already exists then the newly-uploaded file becomes `/Users/myUser/data/file_1.txt`.

The optional `targetRelativePath` flag enables uploading to a subdirectory in the Dataset:

* `<destination path>`: This is the path, relative to the root directory of the Dataset,  where files will be uploaded.
Note that other users with access to the Dataset can alter the file structure, which may require you to modify the destination path accordingly.
+
If `targetRelativePath` is not specified, files will be uploaded to the root directory of the Dataset.

NOTE: `fileUploadSetting`, `targetRelativePath`, and their values are case-sensitive.

.Example:
[source,shell]
----
domino upload-dataset --fileUploadSetting Overwrite --targetRelativePath Experiments/Run1/Analysis jsmith/global-predictions/global-data /Users/myUser/data
----
--

[[rename_files_and_folders]]
=== Rename files and folders

You can change the name of the latest version of a file or folder in a Dataset.
Domino also does not rename files or folders in snapshots.
// What are we renaming below if not the latest version of an item in a Dataset?

WARNING: You must update references to the original file or folder.
If you don't, your Project might not work.
For example, you might see inconsistencies in text files and documentation.

. Go to a Project that uses a Dataset.
. In the navigation pane, click *Data*.
. To rename the file or folder, go to the end of the row and click the three vertical dots.
. Click *Rename*.
. In the Rename window, enter the *New Name* and click *Rename*.


[[automatically-pipe-data-from-external-sources-into-domino]]
== Schedule Jobs to update a Dataset

If you have data in an external source from which you want to periodically fetch and load into Domino, you can set up link:5dce1f[scheduled jobs] to write to Datasets.

Suppose you have data stored in an link:28ddf4[external Data Source] that is periodically updated.
If you wanted to fetch the latest state of that file once a week and load it into a Domino Dataset, you could set up a scheduled Run:

. Create a Dataset to store the data from the external source.
. Write a script that fetches the data and writes it to the Dataset.
. Create a scheduled Job to run your script with the new Dataset configuration.
+
The following is a detailed example showing how to fetch a large, dynamic data file from a
link:947ddd[private S3 bucket]
with a scheduled Run once a week.
+
. Create a Dataset to hold the file.
This example shows the Dataset named `fetched-from-s3`.
+
image::/images/6.0/datasets-best-practices-from-s3.png[alt="Best practices to fetch Datasets from S3", width=700]
+
For this example, assume the S3 bucket is named `my_bucket` and the file you want is named `some_data.csv`.
You can set up your script like this:
+
.fetch-data.py
+
[source,shell]
----
import boto3
import io

# Create new S3 client
client = boto3.client('s3')

# Download some_data.csv from my_bucket and write to latest-S3 output mount
file = client.download_file('my_bucket',
                            'some_data.csv',
                            '/domino/datasets/fetched-from-s3/some_data.csv')
----
+
. Set up a link:5dce1f[scheduled Job] that executes this script once a week with the correct Dataset configuration.


== Next steps

* link:6942ab[Find a Dataset's path]
* link:dbdbff[Create and manage Dataset snapshots]
* link:9c819c[Manage large Datasets]
* link:8f5b7e[Share a Dataset]

----- user_guide/datasets/index.txt -----
:page-version: 6.1
:page-title: Work with Domino Datasets
:page-permalink: ba5bad
:page-order: 130

Domino Datasets provide high-performance, versioned, and structured filesystem storage in Domino.
You can use Datasets to build multiple curated collections of data in one project and share them with your collaborators to use in their projects.
Likewise, you can mount Datasets from other projects in your own project if they are shared with you.
You can modify the contents of a Dataset through the Domino application or through executions.

A Dataset always reflects the most recent version of the data.
For reproducibility, you can create read-only snapshots of your Dataset at any moment in time.
Snapshots are useful when you need to ensure that the data you're working on does not change.

From your code, you can access the latest version of a Dataset or a snapshot of a Dataset.
The topics in this section show you how to use Datasets in your projects.

link:0a8d11[Create and manage Datasets]::
Find shared Datasets and make them part of your project.

link:6942ab[Use Datasets and Snapshots]::
Use the Dataset's path to access the Dataset from your code.

link:dbdbff[Version data with Snapshots]::
Create a read-only, immutable version of the Dataset.
Add tags to create a friendly, stable path to snapshots that hold the desired state for your data.

link:305721[Work with local data]::
Store data in a Domino Dataset and reserve your project files for the scripts and documents that you want to work with locally.

link:9c819c[Manage large Datasets]::
Use Domino Datasets or Project data compression to handle large data.

----- user_guide/datasets/local-data.txt -----
:page-version: 6.1
:page-title: Work with local data
:page-permalink: 305721
:page-order: 40

If you use the link:e21e55[Domino CLI] to work with projects on your local machine, you might find that storing large data files slows your link:9355a5#to-download-the-latest-version-of-your-files[download] and
link:9355a5#to-synchronize-the-files-on-your-computer-with-the-server[sync] operations, and fills up a lot of your local disk storage.
To prevent this, store data in a Domino Dataset, and reserve your project files for the scripts and documents that you want to work with locally.

NOTE: In a link:c65074[Domino Nexus deployment], CLI access to Datasets is only available in the `Local` link:95520d[data plane].

To simplify your local workflow:

. link:0a8d11[Create a Dataset] in your project, and write your large data files to it.
. After the files have been written to the Dataset, remove them from your project files.
. link:9355a5#to-get-a-project-from-the-server-onto-your-computer-for-the-first-time[Fetch] a clean, lighter-weight copy of your project.
. Update your code to reference your data files in their new location, at:
+
`/domino/datasets/local/<dataset-name>/`
. When everything is working properly, delete copies of the project that have the large data files in them from your local machine.

== Next steps

* link:6942ab[Get the path to a snapshot]
* Learn about link:305721[Dataset best practices]

----- user_guide/datasets/manage-large-data.txt -----
:page-version: 6.1
:page-title: How Domino handles large Datasets
:page-sidebar: Manage large Datasets
:page-permalink: 9c819c
:page-order: 50

//There is duplicate info in here and in the best practices topic, however in the bp topic, there are three items and here there are two. Need to find out which is accurate and possibly combine these.

When you start an execution, Domino copies your project files to the executor that is hosting the execution.
After every execution in Domino, by default, Domino will try to write all files in the working directory back to the project as a new revision.
When working with large volumes of data, this presents two potential problems:

* The number of files that are written back to the project might exceed the configurable limit.
By default, the file limit for Domino project files is 10,000 files.
* The time required for the write process is proportional to the size of the data.
It can take a long time if the size of the data is large.

The following table shows the recommended solutions for these problems.
They are described in further detail after the table.

[cols="2a,^1a,^1a,^1a,2a",options="header"]
|===
|Case |Data size |# of files |Static / Dynamic |Solution

|Large volume of *static* data |Unlimited |Unlimited |Static |Domino
Datasets

|Large volume of *dynamic* data |Up to 300GB |Unlimited |Dynamic  |Project
Data Compression
|===

== Domino Datasets

When working on image recognition or image classification deep learning projects, you often need a training Dataset of thousands of images.
The total Dataset size can easily become tens of GB.
For these types of projects, the initial training also uses a static Dataset.
The data is not constantly being changed or updated.
Furthermore, the actual data that is used is normally processed into a single large tensor.

Store your processed data in a Domino Dataset.
Datasets can be mounted by other Domino projects, where they are attached as a read-only network filesystem to that project's runs and workspaces.

== Project data compression

Sometimes, you must work with logs as raw text files.
Typically, new log files are constantly being updated, so your Dataset is dynamic.
You might encounter both problems described previously at the same time:

. The number of files are over the 10k limit.
. There are long times to prepare and sync data.

Domino recommends that you store these files in a compressed format.
If you need the files to be in an uncompressed format during your Run, you can use Domino Compute Environments to prepare the files.
In the pre-run script, you can uncompress your files:
----
tar -xvzf many_files_compressed.tar.gz
----

Then in the post-run script, you can re-compress the directory:
----
tar -cvzf many_files_compressed.tar.gz /path/to/directory-or-file
----

If your compressed file is still large, the time to prepare and sync might still be long, depending on how large your compressed file is.
Consider storing these files in a
link:0a8d11[Domino Dataset]
to minimize the time to copy.

== Dataset quotas and limits

Admins can link:bc084f[set quotas and limits on Dataset storage] in configuration records. Contact your admin to configure quotas.

As you approach your quota limit, you receive notifications, emails, and warnings on Dataset pages.

Your Dataset storage usage is the sum of the size of all active Datasets that you own. If a Dataset has multiple owner, then the size of that Dataset counts towards the storage usage of each owner. For more information, see link:8f5b7e#_dataset_roles[Dataset Roles].

== Next steps

* link:6942ab[Get the path to a snapshot]
* Learn about link:305721[Dataset best practices]
----- user_guide/datasets/use-datasets-and-snapshots.txt -----
:page-version: 6.1
:page-permalink: 6942ab
:page-title: Use datasets and snapshots
:page-order: 20

Domino Datasets provide easy-to-use, fast, and organized storage for files in the Domino platform. You can create different sets of data within a project and share them with your teammates.

If you mount a dataset, others can use it in their own projects. If someone shares their Datasets with you, you can also mount them for use in your own project.

== Mount a dataset

To mount a dataset, you'll need the appropriate link:8f5b7e#dataset-roles[Dataset role] and you must be link:d7731d[an owner or contributor] on the Project to which you want to mount the dataset.

. Launch Domino and open the *Project* that you want to work with.
. From the left navigation pane, click *Data* > *Datasets*.
. On the *Domino Datasets* page, click the *Mount Shared Dataset* button.
. Choose the dataset that you want to mount from the *Dataset Name* menu, then click *Mount Dataset*.

The dataset that you selected should now appear in the *Shared Datasets* list. You can find the directory path for that dataset under *Name*.

== Unmount a shared dataset

Unmounting a shared dataset will not remove it from existing executions until the execution completes.
However, the dataset will not be available for new executions in this Project.

To unmount a dataset, you'll need the appropriate link:8f5b7e#dataset-roles[Dataset role] and you must be link:d7731d[an owner or contributor] on the Project from which you want to unmount the dataset.

. Open the *Project* that you want to unmount the dataset from.
. From the left navigation pane, click *Data* > *Datasets*.
. Look for the dataset you want to unmount in the *Domino Dataset* list.
. At the end of its row, click the three vertical dots and click *Unmount*.

== Render a dataset

Domino supports tabulated previews for a number of file formats to streamline your data analysis and workflows. This minimizes downloading files and artifacts from Domino, while keeping sensitive data inside the Domino platform.

As a prerequisite for viewing in a tabulated format, make sure that the *Define.xml* file and its associated *style sheet (.xsl file)* are in the same directory. There are a couple of ways to view these:

* From within a *Domino Dataset*: Double-click the *Define.xml* file to render it in a readable format.
* From within Domino artifacts:
** View *Styled XML*: click *Show Rendered XML* to render it in a readable format.

These are the supported file types for rendering:

[cols="2a,4a",options="header"]
|===
|File formats|File types

|*Microsoft Office*
|* .doc, .docx, .ppt, .pptx, .xls, .xlsx

|*OpenDocument*
|* .odg, .odp, .ods, .odt

|*Other*
|* .csv, .pdf, .rtf, .xml

|*SAS*
|* .sas7bdat

|===

== Find a dataset path to access it in your code

In order to access a dataset or a snapshot in your code, you need to know its path.

[[find-the-path]]
There are several ways to find the path to a dataset or any of its snapshots in Domino.

* From a Workspace:
** link:e6e601[Before launching a Workspace]: from the left navigation, click *Workspaces* > *Settings*.
** In a link:0002fb[running Workspace]: from the left navigation, click *Data* > *Datasets*.

* From a Job:
** When link:af97b7[launching a Job]: from the left navigation, click *Jobs*, choose the job from the list, then click *Details* > *Data*.
** In a running Job, click *Details* > *Data*.

== Understand dataset paths

Domino executions (workspaces, jobs, apps, and launchers) automatically make datasets and their associated snapshots from a Project available.

The following configuration demonstrates the conventions followed by a predefined path:

// If there is no tag, the numeric value of the snapshot is used as path when mounting the snapshot
* Dataset called `clapton` (local to the project)
** Snapshot 1 (tagged with `tag1`)
** Snapshot 2 (not tagged)
* Dataset called `mingus` (local to project)
** Snapshot 1 (tagged with `tag2`)
** Snapshot 2 (not tagged)
* Dataset called `ella` (shared from another project)
** Snapshot 1 (tagged with `tag3`)
** Snapshot 2 (not tagged)
* Dataset called `davis` (shared from another project)
** Snapshot 1 (tagged with `tag4`)
** Snapshot 2 (not tagged)

=== Dataset paths in DFS projects

For a Domino File System (DFS) project, datasets and snapshots are available in the following hierarchy:

[source,console]
----
/domino
   |--/datasets
      |--/local               <== Local datasets and snapshots
         |--/clapton          <== Read-write dataset for owner and editor, read-only for reader
         |--/mingus           <== Read-write dataset for owner and editor, read-only for reader
         |--/snapshots        <== Snapshot folder organized by dataset
            |--/clapton       <== Read-write for owner and editor, read-only for reader
               |--/tag1          <== Mounted under latest tag
               |--/1             <== Always mounted under the snapshot number
               |--/2
            |--/mingus
               |--/tag2
               |--/1
               |--/2
      |--/ella                <== Read-write shared dataset for owner and editor, Read-only for reader
      |--/davis               <== Read-write shared dataset for owner and editor, Read-only for reader
      |--/snapshots           <== Shared datasets snapshots organized by dataset
         |--/ella             <== Read-write for owner and editor, read-only for reader
            |--/tag3          <== Mounted under latest tag
            |--/1             <== Always mounted under the snapshot number
            |--/2
         |--/davis
            |--/tag4
            |--/1
            |--/2
----

=== Dataset paths in Git-based projects

For Git-based Projects, the datasets and snapshots are available in the following hierarchy:

[source,console]
----
/mnt
   |--/data                  <== Local datasets and snapshots
     |--/clapton             <== Read-write dataset for owner and editor, read-only for reader
     |--/mingus              <== Read-write dataset for owner and editor, read-only for reader
     |--/snapshots           <== Snapshot folder organized by dataset
        |--/clapton          <== Read-write for owner and editor, read-only for reader
           |--/tag1          <== Mounted under latest tag
           |--/1             <== Always mounted under the snapshot number
           |--/2
        |--/mingus
           |--/tag2
           |--/1
           |--/2
   |--/imported
     |--/data
        |--/ella             <== Read-write shared dataset for owner and editor, read-only for reader
        |--/davis            <== Read-write shared dataset for owner and editor, read-only for reader
        |--/snapshots        <== Shared dataset snapshots organized by dataset
           |--/ella          <== Read-write for owner and editor, read-only for reader
              |--/tag3       <== Mounted under latest tag
              |--/1          <== Always mounted under the snapshot number
              |--/2
           |--/davis
              |--/tag4
              |--/1
              |--/2
----

== Next steps

* link:dbdbff[Create and manage Dataset Snapshots] for data reproducibility.
* Learn about link:305721[Dataset best practices].

----- user_guide/datasets/version-data-with-snapshots.txt -----
:page-version: 6.1
:page-title: Version data with Snapshots
:page-permalink: dbdbff
:page-order: 30

[[tr10]]
// Create the snapshot with all files
[[tr11]]
// Create the snapshot with selected files

Data reproducibility requires versioning the contents of a Dataset so that you can ensure you are analyzing the same data.
// That sentence needs some work.
Snapshots are read-only, immutable states of the Dataset.

== Versioning

When you want to reproduce a training experiment, you can version a Domino Dataset so that you can return to a specific version used in the past.

To do this with Domino:

* Create a snapshot to create versions of a Domino Dataset.
* Use a naming convention and a folder hierarchy to organize data your way in the read/write portions of a Dataset.

== Create a snapshot

NOTE: While a snapshot is in progress, do not modify the files in the Dataset.

[#ui-snapshot]
.In the UI
--
. From the Datasets page of your project, click the name of the Dataset you want to version to open its overview page.
. Click *Take Snapshot* and select one of the following options:
+
* *Include all files* to create a snapshot that copies all files in the Dataset.
* *Include only selected files* to select a subset of the files and folders.
+
[[tr12]]
// Tag the snapshot
. Optionally, you can enter a user-friendly tag name for the snapshot.
. Click *Confirm* to initiate the snapshot.
+
[[tr13]]
// Cancel snapshot creation
TIP: While a snapshot is in progress you can click *Cancel* to cancel the snapshot and automatically delete any partial snapshot data.
--

[#cli-snapshot]
.In the CLI
--
.Syntax:
[source,shell]
----
domino create-snapshot <project-owner>/<project-name>/<dataset-name>
----
--

[#api-snapshot]
.In the REST API
--
.Endpoint:
....
/api/datasetrw/v1/datasets/<dataset-id>/snapshots
....
.Body:
[source,json]
----
{
  "relativeFilePaths": [
    "string"
  ]
}
----
.Example: Snapshot all files in the Dataset
Request:
[source,python]
----
import requests
import json

url = "test14995.test-api.tech/api/datasetrw/v1/datasets/649cb4dedeb75c31b6e7dafc/snapshots"

payload = json.dumps({
  "relativeFilePaths": [
    "/*"
  ]
})
headers = {
  'X-Domino-Api-Key': 'a944d3612a76f6c44ad9c680c25234b3ef580ac6b5d2649fd3fc0a259eb62255',
  'Content-Type': 'application/json'
}

response = requests.request("POST", url, headers=headers, data=payload)

print(response.text)
----
Response:
[source,json]
----
{
    "snapshot": {
        "id": "64935f78867ba25a6903c6c4",
        "datasetId": "6488daa88548661d66844e23",
        "creatorId": "64876299e4941004222e2b93",
        "createdAt": "2023-06-21T20:37:12.95Z",
        "status": "pending"
    },
    "metadata": {
        "requestId": "adc8ecc5-9887-46ef-a38c-5569311bf4cd",
        "notices": []
    }
}
----
--

== Add tags to a snapshot

Tags create a friendly path when you mount a snapshot in an execution.
The owner of the Dataset can move a tag between different snapshots to provide a stable path to whichever snapshot holds the desired state of the data.
[[tr17]]
// When snapshot has been tagged, the most recent tag is used as path when mounting the snapshot
NOTE: If more than one tag is used, the last added tag will be used for mounting.

[[tr16]]
// Add remove snapshot tag

. From the Domino Datasets page of your project, click the name of a Dataset to open its overview page.
. From *Snapshots*, select the snapshot to tag.
. Click *+Tag Snapshot*.
. Enter a *Tag Name* and click *Add*.

To remove the tag, click the *X* next to the tag name.

== Create a new Dataset from a snapshot
// tag::create-dataset-from-snapshot[]
You can create as many snapshots as you need, but you cannot modify existing snapshots.
Instead, you can create a Dataset from an existing snapshot, modify the new Dataset, and then create a new snapshot:

[#ui]
.In the UI
--
. Go to the existing snapshot.
. Click *Copy to New Dataset*.
. Complete the fields as needed.
. Click *Upload files* to add files to the Dataset.
. Click *Take Snapshot* > *Include all files*.
--

[#cli]
.In the CLI
--
.Syntax:
[source,shell]
----
domino create-dataset-from-snapshot <project-owner>/<project-name>/<dataset-name> <snapshot-number> <new-dataset-name>
----
--

[#api]
.In the API
--
.Endpoint:
....
/api/datasetrw/v1/datasets
....
.Body:
[source,json]
----
{
    "name": "test-dataset5",
    "description": "A new dataset based on a snapshot",
    "grants": [],
    "snapshotId": "649cbe38fe7c5443a9cb1b62"
}
----
.Example: Create a new Dataset from a snapshot
Example request:
[source,python]
----
import requests
import json

url = "test14995.test-api.tech/api/datasetrw/v1/datasets"

payload = json.dumps({
  "name": "test-dataset5",
  "description": "A new dataset based on a snapshot",
  "grants": [],
  "snapshotId": "649cbe38fa7c5443a7cb1b62"
})
headers = {
  'X-Domino-Api-Key': 'a944d3612a76f6c44ad9c680c25234b2ef580ac6b5d2649fd3fc0a259cb62255',
  'Content-Type': 'application/json'
}

response = requests.request("POST", url, headers=headers, data=payload)

print(response.text)
----
Example response:
[source,json]
----
{
    "dataset": {
        "id": "649cb689deb75c31b6e7dafe",
        "name": "test-dataset5",
        "description": "A new dataset based on a snapshot",
        "projectId": "649cb2b0deb75c31b6e7daf6",
        "createdAt": "2023-06-28T22:39:05.788Z",
        "snapshotIds": [
            "649cb689deb75c31b6e7dafd"
        ],
        "tags": {}
    },
    "metadata": {
        "requestId": "3d87b111-8052-4965-9044-0531ba49a93a",
        "notices": []
    }
}
----
--
// end::create-dataset-from-snapshot[]

== Download a snapshot

. From the Domino Datasets page of your project, click the name of a Dataset to open its overview page.
. From *Snapshots*, select the snapshot to download.
. Click *Download Snapshot* to begin downloading.
If the selected snapshot contains a folder or multiple files, it will download as a ZIP file (default) or TAR archive,
which can be toggled via the link:71d6ad#tr310[Configuration records] key `com.cerebro.domino.dataset.batchDownloadArchiveFormat`.
Otherwise, the file downloads directly.
image::/images/5.4/dataset-snapshot-download.png[alt="Download Dataset snapshot button", width=1200]


== Delete a snapshot
[[tr18]]
// Mark snapshot for deletion
When you no longer need a snapshot, you can mark it for deletion.
These snapshots will no longer be mounted in subsequent executions.
The snapshot will be flagged to a Domino administrator as ready for deletion, but will not be fully deleted until the administrator deletes it.

. From the Domino Datasets page of your project, click the name of a Dataset to open its overview page.
. From *Snapshots*, select the snapshot to delete.
. Click *Mark Snapshot for Deletion*.
. Click *OK* to confirm that you want to mark the snapshot for deletion.

== Next steps

* link:6942ab[Get the path to a snapshot]
* Learn about link:305721[Dataset best practices]

----- user_guide/deploy-models/batch-scoring.txt -----
:page-version: 6.1
:page-permalink: 68629b
:page-title: Configure batch scoring Job
:page-order: 20

[[Overview]]

Domino makes it easy to deploy models for batch scoring to make inferences (predictions) in bulk over a large batch of records. Learn how to use Domino jobs to perform batch prediction with the web UI and API.

Batch scoring jobs are a good choice for "fire-and-forget" scoring jobs that don't require a strict SLA (Service Level Agreements) for results. If you need a scoring endpoint with better response times, especially for customer-facing applications, use the link:8dbc91[Domino endpoint REST endpoints].

[[Configure-a-batch-scoring-job]]
== Configure a batch scoring Job

. Set up the executable inference file.
.. Create a prediction script that is the entry point for the batch scoring program.
.. Use the prediction script as the executable when you configure a new job run. See details for how to start a Domino job link:942549[here].
. Configure execution and compute resources.
.. Select a custom Domino environment if you require additional packages not supported by the default environment.
.. Select the size of the storage volume in the job.
.. Select the appropriate compute resources needed. For simplification, Domino offers a list of available Hardware Tiers that represents a pool of nodes your Domino Administrator configures for such executions.
.. If your scoring code works with large-scale compute clusters like Spark, Ray, or Dask, attach one of those compute clusters to the job to enable their use. Domino provides on-demand Spark, Ray, Dask, and OpenMPI clusters made available to the executable in the job.

== Deploy the scoring Job

Run the job created in the previous section. Optionally, link:5dce1f[run the job on a schedule] to process periodically generated data that requires batch scoring.

[[Monitor-the-scoring-job]]

== Monitor the scoring Job

Batch job durations depend on the volume of processed data and the available compute resources. link:111eae[Monitor the job’s progress] on the jobs dashboard.

[[Retrieve-results]]

== Retrieve results

Configure the notifications settings to receive the results of an execution. By default, you are notified of all failed runs via email. On successful execution, you receive an email with the last few lines of stdout and up to ten results files from your execution. Domino detects which files were added or changed during your execution and captures those as the execution's results. You can also link:5b84c5[set up custom emails] as part of the notification.

The results of an execution are the set of files that your scoring code generates or modifies when it runs. View the results of a given execution through the *Runs* tab of your Project or on the *Results* tab of your Job. See link:e1c37a[Execution results] to learn how to retrieve results, customize the results dashboard, or automate the delivery of results via an Airflow pipeline.

[[Troubleshooting]]

== Troubleshooting

To troubleshoot your scoring job, Domino provides access to log files related to the execution. If you need help, contact link:88cefb[Domino Support] to review and resolve execution issues.

* For errors that occur before execution, i.e., during setup, download the Setup Output files.
* Download the User Output log file for errors in the scoring job to examine and address any changes to your scoring code.
* For runtime errors related to the deployment itself, download the Deployment Logs file to analyze the failure and share it with Domino Support if needed.

== Next steps

For model deployments that require interactivity, see link:8dbc91[Deploy model endpoints].

----- user_guide/deploy-models/deploy-sagemaker.txt -----
:page-version: 6.1
:page-permalink: 02ec6d
:page-title: Deploy models to SageMaker
:page-sidebar: Deploy to SageMaker
:page-order: 40

Domino provides seamless model deployment to SageMaker. Leverage Domino's flexible development experience to build your models before deploying them for production on SageMaker, while still maintaining centralized governance and tracking of all of your models. Administrators apply guardrails around cost, performance, and security, and the platform picks the resources needed to meet them. 

New versions of your models are automatically published to the target environment, validated, and made available for production after they clear the necessary measures.

NOTE: Your admin must set up an external deployment target before you can deploy models to SageMaker. See the link:57edf2[admin documentation] for more details. Deploying models to SageMaker is only supported when the Domino control plane is running on an link:https://aws.amazon.com/eks[AWS EKS cluster^].

Domino also provides an intelligent system that can profile your models and expected scale of usage and recommend a hosting strategy. This helps enable:

* A smoother transition from development to production through automated packaging, transfer, and deployment of models to SageMaker.
* Unified governance and tracking for all models through a single interface, regardless of deployment environment.
* The advanced capabilities of SageMaker facilitate the scalable and cost-effective deployment of the latest models, including LLMs.

== Prerequisites 

Before you deploy a model to SageMaker, make sure that you have an existing registered model. For details on how to register a model, see the link:19df62[model registry] documentation. 

Verify that your admin set up an external deployment target so you can deploy endpoints to SageMaker. See the link:57edf2[admin documentation] for more details.

== Create the endpoint

. Navigate to the *Endpoints* page and click on the *External* pivot. Then click on *Create external endpoint*.
+
image::/images/6.0/sagemaker/endpoints-list.png[alt="Endpoint Landing Page", width=1000, role=noshadow]

. Provide a *Name* for the endpoint, select the *Type* and optionally enter a *Description*.
+
image::/images/6.0/sagemaker/create-endpoint-details.png[alt="Create Endpoint - Details", width=800, role=noshadow]

. Select the *Model* that you want to deploy, followed by the *Environment* that is required to run the model.
+
image::/images/6.0/sagemaker/create-endpoint-model.png[alt="Create Endpoint - Model", width=800, role=noshadow]

. Choose the *Deployment Target* and *Resources* that you want to use for the deployment.
+
image::/images/6.0/sagemaker/create-endpoint-deployment.png[alt="Create Endpoint - Deployment", width=800, role=noshadow]

. Select whether to enable *Streaming* and the *Minimum* / *Maximum* number of instances for your endpoint. 
+
image::/images/6.0/sagemaker/create-endpoint-configuration.png[alt="Create Endpoint - Configuration", width=800, role=noshadow]
+
NOTE: Setting a higher maximum than minimum value will enable auto-scaling. If auto-scaling is enabled, it will trigger a scale-up when the *CPU utilization* exceeds the target threshold of *80% utilization*. To use a different scaling policy or add additional scaling policies, modify the *Scaling Policies* section.

. Set the *Visibility* of the endpoint and add additional *Collaborators* if you like.
+
image::/images/6.0/sagemaker/create-endpoint-access.png[alt="Create Endpoint - Access & Sharing", width=800, role=noshadow]

== View, test, and edit the endpoint

Once the endpoint has been created, you can click into it to view details for the endpoint. You can also *Edit* or *Delete* it.

From this view, you can get a code snippet for testing the endpoint. Paste this code snippet into a workspace and replace the request body with a valid inference request to test the deployment.

In the same view, you can also stop the endpoint. If you click *Stop endpoint*, the SageMaker endpoint will be deleted and a new one will be created when it is started up again.

image::/images/6.0/sagemaker/endpoint-details.png[alt="Endpoint Details", width=1000, role=noshadow]
----- user_guide/deploy-models/domino-endpoints/async-example-client.txt -----
:page-version: 6.1
:page-permalink: 64515c
:page-title: Example Python client: Asynchronous Domino endpoint
:page-sidebar: Asynchronous Domino endpoint
:page-order: 70

This example shows a Python client application that creates a prediction request from an asynchronous Domino endpoint, polls periodically for completion, and retrieves the result.

[source,python]
----
import json
import logging
import requests
import sys
import time

logging.basicConfig(stream=sys.stdout, level=logging.INFO)  # change logging setup as required

# TO EDIT: update the example request parameters for your model
REQUEST_PARAMETERS = {
    "param1": "value1",
    "param2": "value2",
    "param3": 3
}
# TO EDIT: copy these values from "Calling your endpoint" on the Domino endpoint overview page
DOMINO_URL = "https://domino.mycompany.com:443"
MODEL_ID = "5a4131c5aad8e00eefb676b7"
MODEL_ACCESS_TOKEN = "o2pnVAqFOrQBEZMCuzt797d676E6k4eS3mZMKJVKbeid8V6Bbig6kOdh6y9YSf3R"

# DO NOT EDIT these values
MODEL_BASE_URL = f"{DOMINO_URL}/api/modelApis/async/v1/{MODEL_ID}"
SUCCEEDED_STATUS = "succeeded"
FAILED_STATUS = "failed"
QUEUED_STATUS = "queued"
TERMINAL_STATUSES = [SUCCEEDED_STATUS, FAILED_STATUS]
PENDING_STATUSES = [QUEUED_STATUS]
MAX_RETRY_DELAY_SEC = 60


### CREATE REQUEST ###

create_response = None
retry_delay_sec = 0
while (
        create_response is None
        or (500 <= create_response.status_code < 600)  # retry for transient 5xx errors
):
    # status polling with a time interval that backs off up to MAX_RETRY_DELAY_SEC
    if retry_delay_sec > 0:
        time.sleep(retry_delay_sec)
    retry_delay_sec = min(max(retry_delay_sec * 2, 1), MAX_RETRY_DELAY_SEC)

    create_response = requests.post(
        MODEL_BASE_URL,
        headers={"Authorization": f"Bearer {MODEL_ACCESS_TOKEN}"},
        json={"parameters": REQUEST_PARAMETERS}
    )

if create_response.status_code != 200:
    raise Exception(f"create prediction request failed, response: {create_response}")

prediction_id = create_response.json()["asyncPredictionId"]
logging.info(f"prediction id: {prediction_id}")


### POLL STATUS AND RETRIEVE RESULT ###

status_response = None
retry_delay_sec = 0
while (
        status_response is None
        or (500 <= status_response.status_code < 600)  # retry for transient 5xx errors
        or (status_response.status_code == 200 and status_response.json()["status"] in PENDING_STATUSES)
):
    # status polling with a time interval that backs off up to MAX_RETRY_DELAY_SEC
    if retry_delay_sec > 0:
        time.sleep(retry_delay_sec)
    retry_delay_sec = min(max(retry_delay_sec * 2, 1), MAX_RETRY_DELAY_SEC)

    status_response = requests.get(
        f"{MODEL_BASE_URL}/{prediction_id}",
        headers={"Authorization": f"Bearer {MODEL_ACCESS_TOKEN}"},
    )

if status_response.status_code != 200:
    raise Exception(f"prediction status request failed, response: {create_response}")

prediction_status = status_response.json()["status"]
if prediction_status == SUCCEEDED_STATUS:  # succeeded response includes the prediction result in "result"
    result = status_response.json()["result"]
    logging.info(f"prediction succeeded, result:
{json.dumps(result, indent = 2)}")
elif prediction_status == FAILED_STATUS:  # failed response includes the error messages in "errors"
    errors = status_response.json()["errors"]
    logging.error(f"prediction failed, errors:
{json.dumps(errors, indent = 2)}")
else:
    raise Exception(f"unexpected terminal prediction response status: {prediction_status}")
----

----- user_guide/deploy-models/domino-endpoints/domino-endpoint-authorization.txt -----
:page-version: 6.1
:page-title: Select Domino endpoint authorization mode
:page-sidebar: Domino endpoint authorization mode
:page-permalink: e038f2
:page-order: 60

Domino offers authorization options to support various Domino endpoint applications.

== Authorize your Domino endpoint in unrestricted or restricted mode:


. Go to the Domino endpoint's *Settings* page and click *Invocation*.
. Select one of the following:
* *Unrestricted*:
[[tr1]]
// Deploy a Domino endpoint with unrestricted mode authorization
+
If your Domino endpoint is unrestricted, anyone with network access to Domino can call your Domino endpoint.
No access token is required when sending a request.

* *Restricted*:
[[tr2]]
// Deploy a Domino endpoint with restricted mode authorization
+
If your Domino endpoint is restricted, callers must send a valid access token with their requests to use the endpoint.
// This should be updated and linked with the consolidation work.
See the code examples in the overview tab to see how to send access tokens with your requests.
+
[NOTE]
====
There is a difference in how to invoke synchronous and asynchronous restricted Domino endpoints:

* Synchronous Domino endpoints require the caller to use HTTP basic authentication with the username and password both set to the value of the access token.
+
For example, if the token is `c2b2532ed234f54`, then the Domino endpoint must be invoked with the HTTP username `c2b2532ed234f54` and password `c2b2532ed234f54`.

* Asynchronous Domino endpoints require the caller to use HTTP bearer (or token) authentication with the value of the access token.
+
For example, if the token is `c2b2532ed234f54`, then the Domino endpoint must be invoked with the HTTP header `Authorization: Bearer c2b2532ed234f54`.
====
+
[[tr3]]
// Deploy a Domino endpoint with Access token authorization
. Click *Add Model Access Token* to generate access tokens for your Domino endpoint.
You can generate as many access tokens as you want.
Use the name field to help you track which tokens have been issued, to whom, and for what purpose.
+
To revoke or regenerate a token, click the gear button.

----- user_guide/deploy-models/domino-endpoints/index.txt -----
:page-version: 6.1
:page-permalink: 8dbc91
:page-title: Deploy Domino endpoints
:page-order: 10

[[Overview]]

Domino makes it easy to deploy your model as an HTTP endpoint. Domino streamlines deployment so you can start sending HTTP requests to your endpoint to get predictions from your models quickly. Learn how to:

- Deploy a model for synchronous inference (real-time).
- Deploy a model for asynchronous inference.
- Access Domino assets in a Domino endpoint image.
- Request predictions from a Domino endpoint.
- Retrain a model.
- See an example of an asynchronous inference application.

== How deploy works

When you deploy a model as a Domino endpoint, Domino packages the project's files as a Flask application. By default, Domino copies all project files into the endpoint image, including the compute environment, project files, a Flask/Plumber harness that exposes the HTTP interface, and an authentication and load balancing layer.

A Domino endpoint is an HTTP API endpoint wrapped around your inference code. You supply the arguments to your inference code as parameters in the HTTP request payload, and the response from the endpoint includes the prediction. When a Domino Endpoint is published, Domino runs the script that contains the function. As the process waits for input, any objects or functions from the script remain in memory. Each request sent to the endpoint runs the function. Since the script is only sourced at publish time, expensive initialization processes occur only once rather than on each request.

== Synchronous vs asynchronous

Decide if your model needs to be made available as a synchronous or asynchronous Domino endpoint.

Synchronous inference::
Deploy the model so it can receive an HTTP request, make a prediction, and return results synchronously with low latency. This is useful for interactive applications where predictions must be made synchronously.

Asynchronous inference::
Deploy the model so it can process and make predictions asynchronously for computationally intensive workloads. The asynchronous HTTP interface polls for results after a request is queued. After completion, the endpoint returns the result as a response to the polling request.

Domino limits the size of the request and response packets to 10 KB, so users are expected to send and receive the payloads by reference. After Domino picks up the processing request, it keeps the request alive for 30 minutes. Predictions are written to an external data store that you define.

== Package Requirements

[#Python]
--
To support the Flask harness that exposes a model written in Python, the following packages are required in your endpoint image:

* uWSGI
* Flask
* Six
* Prometheus-client

Domino will return an error if these packages are not found in your compute environment.

Additionally, asynchronous Domino endpoints require the `seldon-core` package.

When your model is registered with Domino's Model Registry feature, the `mlflow` Python package will attempt to infer package requirements for your model and write them to a `requirements.txt` file. Domino will install these requirements during the build process.
At the moment, no other `requirements.txt` files are installed when endpoint images are built.

All of the required packages are pre-installed in all the flavors of Domino's Compute Environment images, except the minimal image, which does not have `seldon-core` or `mlflow`.
--

[#R]
--
To support the Plumber harness that exposes a model written in R, the following packages are required in your endpoint image:

* plumber
* future
* openmetrics

Domino will try to install these packages from CRAN if they are not found in your compute environment.

Domino does not currently support publishing asynchronous endpoints in R or registering them with the Model Registry feature, so there is no requirement for the `seldon-core` or `mlflow` packages.
--

[[Deploy-a-model]]
== Deploy a model

Domino supports a few different methods for deploying a model as a Domino endpoint:

- *Deploy from the UI* - For quick, one time deployments.
- *Deploy with a scheduled job* - Schedule a job and deploy a model from the job. Especially useful for keeping models trained on the latest data on a regular cadence.
- *Deploy with the API* - Use the Domino API to schedule jobs from other applications.

=== Deploy from the UI

Use the Domino UI to deploy your model directly from your browser.

. In your project, go to *Deployments > Endpoints > Create Domino Endpoint*.
. Provide a name and description, and select the prediction code that executes when the model is called.
. (Optional) Select a custom compute environment to build the deployed model container.
. (Optional) Configure scaling. The number of instances and compute resources attached to each instance. See link:9dece2[Scale model deployments] for more information.
+
NOTE: You can also include a GPU if your administrator link:3813fa[configured GPUs in your node pool].
+
. Under *Request Type*, select *Sync* for real-time predictions or *Async* for long-running predictions.


=== Update from a scheduled Job

You can use link:5dce1f[scheduled jobs] to update an existing endpoint. This is especially useful for keeping models trained on the latest data. Select the Domino endpoint from the *Update Domino Endpoint* field when scheduling a job. This setting uses the state of the project's files after the run to build and deploy a new version of an existing Domino endpoint.

You can use automated deploy jobs to automatically keep your Domino endpoint up to date by using a training script in the job that trains on fresh data.

=== Deploy with the Domino API

Use Domino's APIs to programmatically build and deploy models. For more information, see link:8c929e#_createModelApi[Domino Platform API reference].

Set the `isAsync` parameter to true in the API call to create an asynchronous inference endpoint.

== Deploy Domino Endpoints on a remote data plane

Users may prefer to host their endpoints in a separate cluster or data plane than where the model was built, either for scalability or data proximity reasons.
Domino facilitates this through its *Create Endpoint* feature, allowing models to be packaged and deployed close to where they'll have the most impact.

When deploying Model Endpoints to a remote data plane, select the hardware tier associated with the desired local or remote data plane from the *Deployment Target* list (previously resource quotas) in the *New Model Endpoint* form.

Domino containerizes the model (MLflow or Project) as a Domino endpoint image, deploys it as an endpoint, and exposes a URL local to that remote cluster where a Nexus data plane has been deployed.
Users can invoke it via the exposed URL and data movement is restricted to this remote data plane only.

*Limitations:*
Currently, Domino endpoints deployed to a remote data plane do not support asynchronous requests (described below) and link:2a7c3b[integrated model monitoring].

== Access Domino artifacts from endpoint images

You can still access Domino artifacts from endpoint images. However, there are nuances to each artifact type. Learn how to use Domino artifacts from your endpoint image and keep them up to date.

[[use-environments-for-domino-endpoints]]
=== Domino endpoint environments

When you deploy a Domino endpoint, select the link:f51038[compute environment] to include in the endpoint image. The environment bundles packages required by the inference script execution ahead of time.

* Domino endpoint hosts don't read `requirements.txt` files or execute commands defined in the pre-setup, post-setup, pre-run, or post-run scripts of your environment. If your project uses `requirements.txt` or any setup scripts to install specific packages or repositories, add them to the Dockerfile instructions of your environment.

* Your Domino endpoint doesn't inherit environment variables set at the project level. However, you can set Domino endpoint-specific environment variables on the Domino endpoint settings page. This separation decouples the management of projects and deployed models. See link:d8dde6[Store Project credentials] for more details.

* Domino endpoints run using the `uid` and `gid` of `12574` (the `domino` user).  A user with this `uid` and `gid` must exist in the selected environment for the Domino endpoint.  See link:78ae6b#create-image[Create a Domino environment image] for more details and instructions on how to use an existing image to create an environment.

* Domino endpoint image builds run using the `USER` directive last specified in the `Dockerfile` for the selected compute environment.  Please ensure that the environment image sets the desired user for the Domino endpoint image build (e.g. `USER root` if you want the Domino endpoint image build to run as `root`).

[[Use-Project-files-and-Environments]]
=== Domino endpoint project files

Your Domino endpoint can access files from the project. Domino loads the project files onto the Domino endpoint host, similar to a Run or Workspace executor host, with a few important differences:

* Domino adds project files to the image when you build a Domino endpoint. Starting or stopping the Domino endpoint won't change the files on the model host. To update the model host files, you need to create a new version.

* Domino pulls link:314004[Git repositories] attached to projects when you build a Domino endpoint. Starting or stopping the Domino endpoint won't change the files on the model host. To update the model host files, you need to create a new version.

* The Domino endpoint host mounts project files at `/mnt/<username>/<project_name>`. This location differs from the default location for Runs or Workspaces, `/mnt/`. You can use the link:6ac5a1[Domino environment variable] `DOMINO_WORKING_DIR` to reference the directory where your project is mounted.

* The Domino endpoint image excludes project files listed in the `.modelignore` file that are located in the project's root directory. Excluded files are not mounted to the Domino endpoint host.

[[add-volumes]]
=== Add a Kubernetes volume to a synchronous Domino endpoint container

When you load inference data or write the response to an external volume, you can add Kubernetes volumes:

. Select a Domino endpoint from the *Endpoints* page.
. Go to *Settings* > *Advanced* > *Add Volume*.
+
NOTE: Only hostPaths are supported.
. Enter the values required.
.. *Name* - Kubernetes volume name.
.. *Mount Path* - mount point in the Domino endpoint container.
.. *path* - the path of the Kubernetes host node that must be mounted in the Domino endpoint container, as configured by your administrator.
.. *Read Only?* - the read/write permission of the mounted volume.

See the https://kubernetes.io/docs/concepts/storage/volumes/[Kubernetes documentation^] for more details.

[[Request-a-prediction]]
== Request predictions

After you deploy the Domino endpoint, and its status changes to *Running*, try test inputs with the *Tester* tab in the Domino endpoint UI.

TIP: Use the *Request* window to make calls to the Domino endpoint from the Domino web application. You will find additional tabs with code samples to send requests to the Domino endpoint with other tools and in various programming languages.

=== JSON requests

Send your requests as JSON objects. Depending on how you wrote your prediction script, you need to format the JSON request as follows:

* If you use named parameters in your function definition, use a dictionary or parameter array in your JSON request. For example, `my_function(x, y, z)`, use `{"data": {"x": 1, "y": 2, "z": 3}}` or `{"parameters": [1, 2, 3]}`.
* If you use a dictionary in your function definition, use only a parameter array. For example, `my_function(dict)` and your function then uses `dict["x"]`, `dict["y"]`. Send the request: `{"parameters": [{"x": 1, "y": 2, "z": 3}]}`.
* In Python, you can also use kwargs to pass in a variable number of arguments. If you do this: `my_function(x, **kwargs)` and your function then uses `kwargs["y"]` and `kwargs["z"]`, you can use a data dictionary to call your endpoint: `{"data": {"x": 1, "y": 2, "z": 3}}`.

Domino converts JSON data types to the following R and Python data types.

[cols="^1a,^1a,^1a",options="header"]
|===
|*JSON Type* |*Python Type* |*R Type*
|dictionary |dictionary |named list
|array |list |list
|string |str |character
|number (int) |int |integer
|number (real) |float |numeric
|true |True |TRUE
|false |False |FALSE
|null |None |N/A
|===

The endpoint returns the result object, which is a literal, array, or dictionary.

=== Synchronous requests

* *Request a prediction and retrieve the result*: Pass the URL of the Domino Domino endpoint, authorization token, and input parameters. The response object contains the status, response headers, and result.
+
[source, python]
----
response = requests.post("{DOMINO_URL}/models/{MODEL_ID}/latest/model",
   auth=("{MODEL_ACCESS_TOKEN}", "{MODEL_ACCESS_TOKEN}"),
   json={"data": {"start": 1, "stop": 100}}
)

print(response.status_code)
print(response.headers)
print(response.json())
----

=== Asynchronous requests

[NOTE]
====
* Domino cannot guarantee the processing order of predictions. The order in which predictions are handled can differ from the order of requests.
* The same prediction request may execute them more than once, so the prediction functions must be idempotent (not affected by repeated executions).
* Use model environment variables for secrets.

====

* *Request a prediction*: Pass the URL of the Domino Domino endpoint, authorization token, and input parameters. Users typically pass a reference to the payload location for large payloads (>10 KB). A prediction identifier is returned for the user to poll for completion and retrieve the result.
+
[source, python]
----
prediction_id = requests.post(
   "{DOMINO_URL}/api/modelApis/async/v1/{MODEL_ID}",
   headers={"Authorization": f"Bearer {MODEL_ACCESS_TOKEN}"},
      json={"parameters": {
"input_file": "s3://example/filename.ext"}})
----

* *Poll for completion / retrieve the results*: Use the `prediction_id` from the request to retrieve the result. The response includes one of the following statuses: `SUCCEEDED`, `FAILED`, or `QUEUED`. The results field contains the output from the predict function for a successful completion.
+
[source, python]
----
status_response = requests.get(
    f"{MODEL_BASE_URL}/{prediction_id}",
    headers={"Authorization": f"Bearer {MODEL_ACCESS_TOKEN}"},
)


if prediction_status == SUCCEEDED_STATUS:  # succeeded response includes the prediction result in "result"
   result = status_response.json()["result"]
----


[[Continuous-improvement]]
== Retrain a model

Deploy a new version::
After you retrain your model with new data or switch to a different machine learning algorithm, publish a new version of the Domino endpoint. To follow best practices, stop a previous version of the Domino endpoint and then deploy a new version.

[[explore-an-asynch-example]]
== Asynchronous example

This example shows a Python client application that creates a prediction request from an asynchronous Domino endpoint, polls periodically for completion, and retrieves the result.

[source,python]
----
import json
import logging
import requests
import sys
import time

logging.basicConfig(stream=sys.stdout, level=logging.INFO)  # change logging setup as required

# TO EDIT: update the example request parameters for your model
REQUEST_PARAMETERS = {
    "param1": "value1",
    "param2": "value2",
    "param3": 3
}
# TO EDIT: copy these values from "Calling your Model" on the Domino endpoint overview page
DOMINO_URL = "https://domino.mycompany.com:443"
MODEL_ID = "5a4131c5aad8e00eefb676b7"
MODEL_ACCESS_TOKEN = "o2pnVAqFOrQBEZMCuzt797d676E6k4eS3mZMKJVKbeid8V6Bbig6kOdh6y9YSf3R"

# DO NOT EDIT these values
MODEL_BASE_URL = f"{DOMINO_URL}/api/modelApis/async/v1/{MODEL_ID}"
SUCCEEDED_STATUS = "succeeded"
FAILED_STATUS = "failed"
QUEUED_STATUS = "queued"
TERMINAL_STATUSES = [SUCCEEDED_STATUS, FAILED_STATUS]
PENDING_STATUSES = [QUEUED_STATUS]
MAX_RETRY_DELAY_SEC = 60

### CREATE REQUEST ###

create_response = None
retry_delay_sec = 0
while (
        create_response is None
        or (500 <= create_response.status_code < 600)  # retry for transient 5xx errors
):
    # status polling with a time interval that backs off up to MAX_RETRY_DELAY_SEC
    if retry_delay_sec > 0:
        time.sleep(retry_delay_sec)
    retry_delay_sec = min(max(retry_delay_sec * 2, 1), MAX_RETRY_DELAY_SEC)

    create_response = requests.post(
        MODEL_BASE_URL,
        headers={"Authorization": f"Bearer {MODEL_ACCESS_TOKEN}"},
        json={"parameters": REQUEST_PARAMETERS}
    )

if create_response.status_code != 200:
    raise Exception(f"create prediction request failed, response: {create_response}")

prediction_id = create_response.json()["asyncPredictionId"]
logging.info(f"prediction id: {prediction_id}")

### POLL STATUS AND RETRIEVE RESULT ###

status_response = None
retry_delay_sec = 0
while (
        status_response is None
        or (500 <= status_response.status_code < 600)  # retry for transient 5xx errors
        or (status_response.status_code == 200 and status_response.json()["status"] in PENDING_STATUSES)
):
    # status polling with a time interval that backs off up to MAX_RETRY_DELAY_SEC
    if retry_delay_sec > 0:
        time.sleep(retry_delay_sec)
    retry_delay_sec = min(max(retry_delay_sec * 2, 1), MAX_RETRY_DELAY_SEC)

    status_response = requests.get(
        f"{MODEL_BASE_URL}/{prediction_id}",
        headers={"Authorization": f"Bearer {MODEL_ACCESS_TOKEN}"},
    )

if status_response.status_code != 200:
    raise Exception(f"prediction status request failed, response: {create_response}")

prediction_status = status_response.json()["status"]
if prediction_status == SUCCEEDED_STATUS:  # succeeded response includes the prediction result in "result"
    result = status_response.json()["result"]
    logging.info(f"prediction succeeded, result:
{json.dumps(result, indent = 2)}")
elif prediction_status == FAILED_STATUS:  # failed response includes the error messages in "errors"
    errors = status_response.json()["errors"]
    logging.error(f"prediction failed, errors:
{json.dumps(errors, indent = 2)}")
else:
    raise Exception(f"unexpected terminal prediction response status: {prediction_status}")
----

=== Use Spot instances for Domino Endpoints (PREVIEW)

We support serving Domino endpoints using cost-effective link:3813fa[Spot instances]. Select a hardware tier that uses node pool using Spot instances.

If AWS interrupts a spot instance, endpoints deployed on that instance will be affected and either stop responding, if there are no remaining replicas of the endpoint on other AWS instances, or the remaining replicas still running on unaffected instances will experience an increased load and may have their performance degraded or also stop working, depending on the runtime characteristics of the model.
If this happens, the remediation is to change the hardware tier of the endpoint to use a non-spot node pool until AWS spot instances of the requested type become available again.

== Next steps

* link:88a0ef[Access and security for Domino endpoints].
* link:8c79ad[Scale and route Domino endpoints].
* link:d788b5[Deployment logs and health checks].
* link:e038f2[Select Domino endpoint authorization mode].
* link:64515c[Example of a Python client application that uses an asynchronous Domino endpoint].

----- user_guide/deploy-models/domino-endpoints/logs-monitor-health.txt -----
:page-version: 6.1
:page-permalink: d788b5
:page-title: Domino endpoint health and logs
:page-order: 50

Keep an eye on your deployed models with Domino endpoint health checks and logs. Domino can make sure that your endpoints are running and can alert you when they are down. Use logs to troubleshoot and audit your Domino endpoints.

[[Modify-health-settings]]

== Health check settings

Domino monitors every Domino endpoint's health and ability to respond to new inference requests.
When you update the health check settings, the Domino endpoint automatically restarts.

. Navigate to *Endpoints*.
. Select a Domino endpoint, then adjust the fields in *Settings* > *Advanced*:

Initial delay::
The time (in seconds) that Domino waits before a new Domino endpoint can receive incoming requests.
Change the value of this setting to delay the initialization of a Domino endpoint.

Health check period::
How often (in seconds) Domino checks the Domino endpoint health.
*Health check period* x *Failure threshold* must be greater than the *Override request timeout* from the timeout settings.

Timeout settings::
The time (in seconds) that Domino lets an inference request take before timing it out. In the timeout case, Domino responds with `504 Gateway Timeout`. The default is 60 seconds. You must restart the Domino endpoint for timeout setting changes to take effect.

Health check timeout::
The length of time (in seconds) that Domino waits before it considers a health check request as failed.

Failure threshold::
If this number of consecutive health check requests fails, Domino considers the Domino endpoint instance unrecoverable and restarts it.

== Domino endpoint logs

Domino offers multiple logs for troubleshooting and auditing your Domino endpoints. 

Check the *Logs* column for a specific Domino endpoint version to view build, export, instance, or deployment logs.

* *Build Logs* - Events that happened to build the image. See the build definition and metadata needed to complete the build.
* *Export Logs* - Export details for the Domino endpoint.
* *Instance Logs* - Logs related to individual containers for a given Domino endpoint instance. View all Domino endpoints and all containers or filter the information by Domino endpoint and container.
* *Deployment Logs* - Chronological events related to the deployment. These events include heartbeats, jobs, deployments, and Kubernetes events. Inspect payloads that contain pod and status information. Container status information identifies where images are in the deployment and indicates their state.

== Next steps

- link:88a0ef[Model deployment security]
- link:8c79ad[Scale model deployments]

----- user_guide/deploy-models/domino-endpoints/scale.txt -----
:page-version: 6.1
:page-permalink: 8c79ad
:page-title: Domino endpoint scaling and routing
:page-order: 40

Scale link:8dbc91[Domino endpoints] horizontally and vertically for optimal performance. You can also use Domino endpoint routing to deploy simultaneous APIs for testing and production.

== Scale horizontally

Scale horizontally for throughput-constrained Domino endpoints. Typically these are endpoints that have many concurrent users. Consider horizontal scaling when downstream applications see long queues and running times from your endpoint.

By default, Domino schedules endpoint instances to run on separate nodes when possible for availability and fault tolerance. However, you can set a model instance to run strictly on different nodes by setting the `strictNodeAntiAffinity` parameter to true in the API call when creating a new Domino endpoint.

Endpoint instances are also scheduled to run in separate availability zones when possible.


When you publish a Domino endpoint, select the number of Domino endpoint instances that you want to run at any given time. Domino automatically load-balances requests to the endpoint between these instances. A minimum of two instances (default) provides a high-availability setup. Domino supports up to 32 instances per Domino endpoint.

NOTE: Domino admins use the link:71d6ad#tr180[com.cerebro.domino.modelmanager.instances.defaultNumber Configuration records key] to change the default number of instances.

== Scale vertically

Scale vertically for resource-constrained Domino endpoints. Consider whether your endpoint requires complex tasks with more processing power. Scale Domino endpoints vertically when downstream applications see long-running jobs for complex processes.

When you publish a Domino endpoint, select a link:9dece2[hardware tier] that determines the amount of RAM and CPU/GPU resources available to each Domino endpoint instance.

TIP: The scaling settings are under *Endpoints* > *<endpoint name>* > *Settings* > *Deployment*.

NOTE: If you make changes to scale the Domino endpoint, you must restart it.


You can set the degree of parallelism and scale all _Python_ Domino endpoints:

. From the Admin screen, go to *Platform settings* > *Configuration records*.
. Click the pencil icon for the `com.cerebro.domino.modelmanager.uWsgi.workerCount` config key to update it.
+
TIP: If the `com.cerebro.domino.modelmanager.uWsgi.workerCount` config key is not listed on the Configuration Management page, click *Add Record* and enter the config key name at the end of the list in the key column.
+
. In the value column for `com.cerebro.domino.modelmanager.uWsgi.workerCount`, set the value to greater than 1, which is the default. See the https://uwsgi-docs.readthedocs.io/en/latest/[uWSGI project] for more information.
. Click *Save* if you edit the key or *Create* if you add a new one.
. Under the *Configuration Management* page title, the system displays: “Changes here do not take effect until services are restarted. Click here to restart services.” Click the *here* link and follow the directions to restart the services.



[[Route-your-domino-endpoint]]

== Route your Domino endpoint

Domino supports basic and advanced routing modes to help you manage development and test deployments. To change routing modes, go to *Settings* > *Deployment* for each Domino endpoint.

Basic mode::

In basic mode, one exposed endpoint always points to the latest successfully-deployed Domino endpoint version.
When you deploy a new version, the old version is shut down and replaced with the new one to maintain availability.
Basic mode routes have the following signature:

*Latest:* `/models/<modelId>/latest/model`

Advanced mode::

[[tr2]]
// Deploy an endpoint that uses both promoted and latest routing (advanced mode routing)

In advanced mode, a promoted version and the latest version exist simultaneously.
Advanced mode lets you point your clients to the promoted, production version, while giving you the ability to test with the latest version.
When the latest version is ready for production, seamlessly switch it to the promoted version without downtime.
Advanced mode routes have this signature:

*Latest:* `/models/<modelId>/latest/model`

*Promoted:* `/models/<modelId>/labels/prod/model`

== Next steps
- link:02ec6d[Export models to SageMaker]
- link:5cef47[Export models to NVIDIA fleet command]
- link:a1b168[Export models to use with Snowflake queries]

----- user_guide/deploy-models/domino-endpoints/security.txt -----
:page-version: 6.1
:page-permalink: 88a0ef
:page-title: Domino endpoint security
:page-order: 30

Domino offers visibility and authorization controls for link:8dbc91[deployed Domino endpoints] to limit who can see and interact with your endpoints. Learn how to control access and add contributors to Domino endpoints.

== Public or private Domino endpoint

Configure your endpoint to be accessible by certain authorized users (private) or by anyone with access to your Domino deployment (public).

On the Domino endpoint page, go to *Settings* > *Access and Sharing*.

Public::
Anyone with access to your Domino deployment can search, discover, and view your Domino endpoint. Only collaborators can modify or deploy versions or settings.
Private::
Only collaborators can search, discover, and view your Domino endpoint. Only collaborators can modify or deploy versions or settings.

[[Authorization]]

== Unrestricted or restricted authorization

Authorization settings specify which users can access the Domino endpoint's prediction endpoint.

Restricted::
A restricted Domino endpoint only authorizes specific users with valid access tokens to request predictions. Users must send the valid token with their requests. Code examples in the endpoint's *Overview* tab show sample requests.
+
Generate an endpoint access token from the Domino endpoint's *Settings* > *Invocation* tab. Use the name field to track which tokens are issued, to whom, and for what purpose.

Unrestricted::
Unrestricted endpoints allow anyone who can access Domino remotely to request predictions. No access token is required.

[NOTE]
====
Restricted Domino endpoint's have different invocation methods depending on if they are synchronous or asynchronous:

* Synchronous Domino endpoints require the caller to use HTTP basic authentication with the username and password both set to the access token.
+
For example, if the token is `c2b2532ed234f54`, then the Domino endpoint must be invoked with the HTTP username `c2b2532ed234f54` and password `c2b2532ed234f54`.

* Asynchronous Domino endpoints require the caller to use HTTP bearer (or token) authentication with the access token.
+
For example, if the token is `c2b2532ed234f54`, then the Domino endpoint must be invoked with the HTTP header `Authorization: Bearer c2b2532ed234f54`.
====

[cols="1a,3a,3a",options="header"]
|===
|  |Public  |Private

|*Unrestricted*  
|
* Anyone with access to your Domino deployment can search, discover, and view your Domino endpoint. 
* Only collaborators can modify or deploy versions or settings.
* No access token is required to request a prediction.
|
* Only collaborators can search, discover, and view your Domino endpoint. 
* Only collaborators can modify or deploy versions or settings.
* No access token is required to request a prediction.

|*Restricted*    
|
* Anyone with access to your Domino deployment can search, discover, and view your Domino endpoint. 
* Only collaborators can modify or deploy versions or settings.  
* An access token is required to request a prediction.
|
* Only collaborators can search, discover, and view your Domino endpoint. 
* Only collaborators can modify or deploy versions or settings.
* An access token is required to request a prediction.

|===

== Add collaborators

You can add *Domino endpoint collaborators* to view and manage specific Domino endpoints (but not the whole project). Domino endpoint collaborators can also invoke private prediction endpoints.

To add *Domino endpoint collaborators*:
. Go to the Domino endpoint page > *Settings* > *Access and Sharing*. 
. Add new collaborators by their username or email address. 
You can also add organizations as collaborators and grant permissions to all members. 

If you are the project owner, you can set the following access levels for collaborators:

Viewers::
Viewers can only view the Domino endpoint versions and logs. They cannot view settings, edit settings, or publish new versions. A viewer cannot see access tokens.
Editors::
Editors with collaborator access to the underlying project can deploy new versions. They can view logs, view audit history, and change most settings. They cannot invite new collaborators or change Domino endpoint visibility. An editor can see all access tokens and create new ones.
Owners::
Owners have all of the above permissions, and they can invite new collaborators, change the visibility, and transfer ownership. An owner can see and revoke all access tokens and create new ones.

You can also add *Project collaborators* to grant them access to Domino endpoints within a project.
Project collaborators assigned the `Contributor` role can create Domino endpoints, becoming the endpoint owner with the permissions listed above.
All project collaborators can list Domino endpoints in the project, but they do not have access to the endpoints unless they are the endpoint owner or explicitly granted access.

== Next steps

- link:8c79ad[Scale your deployment]
- link:d788b5[Monitor your deployment health and logs]

----- user_guide/deploy-models/export-nvidia.txt -----
:page-version: 6.1
:page-permalink: 5cef47
:page-title: Export models to NVIDIA Fleet Command
:page-sidebar: Export to NVIDIA Fleet Command
:page-order: 50

[[Overview]]

// Briefly explain what this feature is, how it benefits customers, how it fits into the data science workflow, and how Domino does it. Each mention of another concept or feature should be linked inline to a relevant topic elsewhere in the docs.

Domino's integration with NVIDIA Fleet Command simplifies model deployment to the edge.

Scaling AI at the edge is a critical way of deploying data science work, and is useful for scenarios like anomaly detection at cellphone towers and quality control in manufacturing.

[[How-it-works]]

== Export to NVIDIA Fleet Command

. link:8dbc91[Deploy a Domino endpoint] from any project and run tests to ensure your Domino endpoint is functioning properly.
. After the deployment status changes to *Ready to Run*, invoke the Domino API call to register the endpoint image with NVIDIA.
+
`/:modelId/:modelVersionId/exportImageForNvidia`
+
. Wait for the API to register the Domino endpoint container image and an associated Helm chart to the Fleet Command registry.
+
See the Domino endpoint details to learn how to invoke the export API to push the image to Fleet Command and check the status endpoint to track the progress of the export.
+
. Use Fleet Command to configure the distributed edge system.

With Domino's model export catalog, you can see a unified view of all exported models and their performance. With this view, you can track and manage all production assets from a single pane.

== Invoke the export API

[source, shell]
----
curl =
--location --request POST 'https://<deployment>/v4/models/<model-id>/<model-version-id>/exportImageForNvidia' 
--header 'Content-Type: application/json' 
--header 'X-Domino-Api-Key: <domino-api-key>' 
--data-raw '{
 "containerRepository": "<path-to-container-repo>",
 "tag": "<image-tag>",
 "helmRepository": "<path-to-helm-repo>",
 "helmVersion": "<version>",
 "ngcApiKey": "<ngc-api-key>"
}'
----
Here is a description of the parameters:

* `model-id`: unique identifier for the Domino endpoint
* `Model-version-id`: version ID for the Domino endpoint
* `containerRepository`: Repository name for a private container repository in NVIDIA NGC. For example, *<organization-name>/<team-name>/<container-repository-name>*. The user can provide a container repository that already exists in NVIDIA NGC. Here are the acceptable elements for a Container repository name:
** Alphanumeric characters
** Lowercase characters
** Dashes and slashes (for organization and team names)
** No spaces
* `tag`: Tag for the container. The tag must be alphanumeric. It can also contain underscores, periods, and dashes.
* `helmRepository`: Repository name for a private helm repository in NVIDIA NGC. For example, <organization-name>/<team-name>/<container-repository-name>. The user can provide a helm repository that already exists in NVIDIA NGC. Here are the acceptable elements in a Helm repository name:
** Alphanumeric characters
** Lowercase characters
** Dashes and slashes (for organization and team names)
** No spaces
* `helmVersion`: Version of the Helm chart. Helm versions must be in link:https://semver.org/[Semver^] format.
* `ngcApiKey`: NVIDIA NGC API key. See link:https://docs.nvidia.com/ngc/gpu-cloud/ngc-overview/index.html#generating-api-key[generate an NVIDIA NGC API Key^].
+
[NOTE]
====
* The port for the endpoint on the edge device is hardcoded to 30008. If you deploy two Domino models on the same edge device, the second application deployed fails. Override the port in `values.yaml` in the NVIDIA Fleet Command UI to prevent this failure.
* The model cannot use anything Domino-specific like predictive sets or domino datasets.
* The model must be built in Domino first, and there must be a version of the Model.
* If you export a model with an image tag or Helm chart version number that already exists, the export fails.
* You cannot deploy different versions of the same NVIDIA Application to the same edge device. If you want to deploy multiple Domino models on a single-edge device, override the port when you deploy.
====

== Track the status of the export

Use the `export-id` you get as the return value from the export endpoint to monitor the export status.

Here is a sample command:

[source, shell]
----
curl --include 
    --header 'Content-Type: application/json' 
    --header 'Accept: application/json' 
    --header 'X-Domino-Api-Key: <domino-api-key>' 
 'https://<deployment>/v4/models/<export-id>/getExportImageStatus'
----

== Next steps

link:ba503b[Integrate endpoint deployment with CI/CD workflows]

----- user_guide/deploy-models/index.txt -----
:page-version: 6.1
:page-permalink: 08a85b
:page-title: Deploy models
:page-order: 280
:page-separator: true
:page-section: Deploy

[[Deploy-Models]]

link:8dbc91[Deploy models as REST API endpoints] with a single click for asynchronous (batch) or synchronous (real-time) predictions on new data.

Domino offers built-in security, scaling, and management features:

* link:88a0ef[Endpoint security].
* link:8c79ad[Endpoint scaling and routing].
* link:d788b5[Endpoint health and logs].
* link:e038f2[Endpoint authorization mode].
* link:64515c[Asynchronous endpoints].

In addition, you can score large batches of data with link:68629b[batch scoring Jobs].

== Deploy models to third-party platforms

Domino gives you the flexibility to use your models and endpoints on other supported platforms.

* link:02ec6d[Deploy models to SageMaker] to leverage auto-scaling and streaming capabilities.
* link:5cef47[Export to NVIDIA Fleetcommand] for edge deployments.
* link:a1b168[Export models to Snowflake] to bring the model to where your data lives.

== Integrate Domino models with existing CI/CD workflows

Domino can deploy natively, export to third party platforms, or link:ba503b[integrate model deployments with existing CI/CD workflows] with the Model Export API. The choice is yours.

----- user_guide/deploy-models/integrate-ci-cd-workflows.txt -----
:page-version: 6.1
:page-permalink: ba503b
:page-title: Integrated deployment to CI/CD workflows
:page-sidebar: CI/CD integrated deployment
:page-order: 30

[[Overview]]

// Briefly explain what this feature is, how it benefits customers, how it fits into the data science workflow, and how Domino does it. Each mention of another concept or feature should be linked inline to a relevant topic elsewhere in the docs.

Domino can integrate with existing CI/CD workflows and registries outside of Domino. Use Domino's Model Export API to integrate model development in Domino with other production environments.

With a single API call, Domino turns your trained model into a container image with a REST interface that you can deploy into a separate production environment. You may need to host models outside of Domino for specific scenarios such as:

* Support for sudden bursts in traffic
* Strict latency and uptime SLAs
* Security, legal, or locality restrictions 


[[How-it-works]]

== Model Export API

Domino's Model Export API exports Docker images of Domino-built models, model artifacts, project files, dependent packages, base environments, and more. The API registers the images with Domino's internal image registry. You then fetch it, customize it, and deploy it inside your runtime environment with existing CI/CD pipelines to facilitate tests and production-readiness steps.

By default, all files present in a project are copied into the generated model image. The default image includes the specified compute environment, project files, a Flask/Plumber harness that exposes the REST interface, and an authentication and load balancing layer.

image::/images/model-apis/cicd-how-it-works-1.png[alt="CI/CD pipeline workflow", width=800, role=noshadow]

To control the sizes of such images, include names of specified files in the `.modelignore` file in the project's root folder to omit them. Filenames, folder names, or UNIX shell regular expressions are allowed in this file.

image::/images/model-apis/cicd-how-it-works-2.png[alt="CI/CD pipeline workflow continued", width=550, role=noshadow]

== 1. Train the model in Domino
A data scientist creates a project in Domino, associates an environment with it, and conducts multiple experiments to identify the best production model. The data scientist or MLOps engineer then configures the CI/CD pipeline to run through the next steps to deploy the model from the project.

== 2. Build the model image
Your CI/CD pipeline calls the *Build Model Image API* to trigger a build process. This call builds and saves the generated model image in Domino's internal container registry. The API response includes the model ID and the model version ID, which are used to poll for the build status or to fetch build logs. The model build process comprises Preparing, Building, Complete, or Failed.

.Invoke the build process:

[source, script]
----
curl --location --request POST 'https://<YOUR_DOMINO_APP_PATH>/v4/models/buildModelImage' 
--header 'Content-Type: application/json' 
--header 'X-Domino-Api-Key: <YOUR_DOMINO_API_TOKEN>' 
--data-raw '{
  "projectId": "5e5efb400cf22210b88f36df",
  "inferenceFunctionFile": "model.py",
  "inferenceFunctionToCall": "predict",
  "environmentId": "5e4c6cc6cec7a117386a5c0a",
  "modelName": "ModelExportDemo",
  "logHttpRequestResponse": true,
  "description": ""
}'
----

.Poll for completion:

[source, script]
----
curl --location --request GET 'https://<YOUR_DOMINO_APP_PATH>/v4/models/<modelID>/<modelVersionId>/getBuildStatus' 
--header 'Content-Type: application/json' 
--header 'X-Domino-Api-Key: <YOUR_DOMINO_API_TOKEN>'
----

== 3. Export the model image to your registry
After the image is built and registered in Domino's internal registry, export it to your registry with Domino's *Export Model Image API*. The API responds with an export process ID, which is used to poll for the status or fetch export operation logs. As part of a production workflow, ensure the image is first deployed for staging.

.Invoke the build process:

[source, script]
----
curl --location --request POST 'https://<YOUR_DOMINO_APP_PATH>/v4/models/<modelID>/<modelVersionId>/exportImageToRegistry' 
--header 'Content-Type: application/json' 
--header 'X-Domino-Api-Key: <YOUR_DOMINO_API_TOKEN>' 
--data-raw '{
  "registryUrl": "quay.io/sample_url",
  "repository": "sample_prod_repo",
  "tag": "sample_tag",
  "username": "username",
  "password": "secret_token"
}'
----

.Poll for completion:

[source, script]
----
curl --location --request GET 'https://<YOUR_DOMINO_APP_PATH>/v4/models/<exportId>/getExportImageStatus' 
--header 'Content-Type: application/json' 
--header 'X-Domino-Api-Key: <YOUR_DOMINO_API_TOKEN>' 
----

[[Run-the-exported-image]]
== 4. Run the exported image in a staging environment

Stand up the image in your test environment, which is often a separate K8s cluster dedicated to stage and test workflows. You can also configure your CI/CD pipeline to customize the image to prepare it for staging.

Deploy an exported model Image on an existing K8s cluster with these steps. This export assumes your K8s cluster is running and kubectl is installed. Then create the K8s manifest files, which describe your application & service and use kubectl to deploy them.

Use these Sample K8s manifest files and procedures to deploy the Image.

.my-model-deployment.yaml

[source, YAML]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mymodel
  labels:
    app: mymodel
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mymodel
  template:
    metadata:
      labels:
        app: mymodel
    spec:
      containers:
        - name: mymodel
          image: docker.io/myrepo:mytag
          ports:
            - name: mymodel
              containerPort: 8888
----

.my-model-service.yaml

[source, YAML]
----
apiVersion: v1
kind: Service
metadata:
  name: mymodel-service
spec:
  type: NodePort
  selector:
    app: mymodel
  ports:
  - nodePort: 31440
    name: mymodel-port
    port: 90
    targetPort: 8888
----

NOTE: If you pull from a private registry, link:https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/[add a secret^] to your K8s cluster so the pod can authenticate with the docker registry.

== 5. Start the deployment and service

[source, shell]
----
$ kubectl apply -f my-model-deployment.yaml
$ kubectl apply -f my-model-service.yaml
----

.Get the URL:

[source, shell]
----
# If the kubernetes is in minikube then get the minikube IP
$ MYMODEL_IP="$(minikube ip)"
----

[source, shell]
----
# Else get IP of the mymodel-service
$ MYMODEL_IP="$(kubectl get service/mymodel-service -o jsonpath='{.spec.clusterIP}')"
----

.Get the port:

[source, shell]
----
MYMODEL_PORT="$(kubectl get service/mymodel-service -o jsonpath='{.spec.ports[?(@.name=="mymodel-port")].nodePort}')"
----

.Get the Model URL:

[source, shell]
----
echo http://$MYMODEL_IP:$MYMODEL_PORT/model
>> http://192.168.99.100:31440/model
----

[[Test-the-exported-Model]]
=== Test the exported Model
After the image is up and running, test the deployed model.

.Sample invocation to test your python model:

[source, shell]
----
curl --location --request POST 'http://192.168.99.100:31440/model' --header 'Content-Type: application/json' --data-raw '{"data": {"start": 1, "stop": 100}}'
----

The content of the “data” JSON object is the input to your model.


== 6. Deploy to a production environment
After you test the model in the staging environment and it clears the requisite production criteria, your CI/CD pipeline can deploy the Domino endpoint image to a production environment. If you want to use a separate K8s cluster for this environment, substitute your desired cluster in the link:#Run-the-exported-image[Run the exported image in a staging environment] and link:#Test-the-exported-Model[Test the exported model] steps. 

// == Troubleshooting

----- user_guide/deploy-models/snowflake-queries.txt -----
:page-version: 6.1
:page-permalink: a1b168
:page-title: Snowflake queries
:page-order: 60

[[Overview]]

// Briefly explain what this feature is, how it benefits customers, how it fits into the data science workflow, and how Domino does it. Each mention of another concept or feature should be linked inline to a relevant topic elsewhere in the docs.

Snowflake is one of the most prominent data storage services. When you move large volumes of data to the model, it increases costs and involves security risks. As a result, many shift to an approach where the model comes to the data, also known as in-database scoring. Domino pushes models down to Snowflake via Snowpark integration and connects the data back to the Domino platform to monitor those models for drift and accuracy.

[[How-it-works]]
== How it works

After you train your model in Domino and write the necessary inference logic that you want to execute from within Snowflake, Domino lets you push model artifacts and associated Project Files necessary to execute the inference code in Snowflake as a Snowpark UDF.

[NOTE]
====
link:https://www.snowflake.com/en/data-cloud/snowpark/[Snowpark^] is a library that lets users build applications (with a high level language like Scala) that run in Snowflake compute environments where the data resides. The core abstraction in Snowpark is the DataFrame. In addition to built-in capabilities in the Snowpark library, users create user-defined functions (UDFs) for custom operations. Both the DataFrame and UDFs are optimized to execute lazily on the Snowflake server.
====

[[meet-the-requirements]]
== Meet the requirements

The model that you create in Domino must meet these requirements to be hosted in Snowflake:

* Create your model in Python 3.8.
// Nastasia confirms that in the case of projects with Snowflake-hosted models, DOM-34650 does not apply re the location of requirements.txt; it only applies to Domino-hosted models.
* Create a file named `requirements.txt` in the root folders (for example, the `root` folder of the main Git-based project, an imported repository, or imported projects)
+
This file defines the list of required packages for the model. See https://github.com/binder-examples/requirements/blob/master/requirements.txt[https://github.com/binder-examples/requirements/blob/master/requirements.txt^] for an example and link:bfa148[Add Packages to Environments] for more details.
+
** These packages must be in the Snowflake Anaconda list: https://repo.anaconda.com/pkgs/snowflake/[https://repo.anaconda.com/pkgs/snowflake/^].
** If you use external packages, your Snowflake instance must enable the usage of https://repo.anaconda.com/pkgs/snowflake/[Anaconda^] packages.

* Create Python-type hints for the model function input and return variables.

For the model to be monitored, you must meet these requirements:

* A `timestamp` column in your prediction dataset
* The name of the table that contains the prediction data

//Do we need to mention anything similar to what we did in the other sections?
//NOTE: You can register prediction data for a model that uses a Snowflake data source once per table.
//Domino automatically retrieves new data every 24 hours at 12:00 UTC as long as you register the `timestamp` variable with the prediction dataset. See link:bb88ca[Monitoring Config JSON].
//When you deregister the model, Domino stops retrieving the data.
//If you don't register the `timestamp` variable, the prediction data is ingested once, and for every new batch, you must register a new dataset.
Domino looks for entries in the table with new timestamps to continuously ingest new data in batches.

[[Procedure]]
== Push models to Snowflake

. Train a model with Domino Workbench to produce a model artifact and associated Project Files needed to invoke an inference with the model.
. Instrument the inference function in the form of a Snowpark UDF with Snowflake packages. (See example below)
.. The prediction UDF typically accepts a DataFrame as an input to a prediction.
.. The result of the prediction (also typically a DataFrame) is returned as a response to the caller of the UDF
. With a Domino publish flow (via the Domino web application), deploy the model as a Snowpark UDF to a Snowflake endpoint. The presented dialog prompts you to enter the necessary information to create a session with Snowflake and upload the custom function and necessary artifacts to create a Snowpark UDF.
+

TIP: In *Snowflake URL*, enter the URL that uniquely identifies the Snowflake account in your organization in the form `https://<account-id>.snowflakecomputing.com`.

.. Information to create a Snowflake session includes key identifiers for the account (URL, role, credentials via username, and password) and JDBC settings (stage, warehouse, DB, and schema).
.. Domino adds the model artifacts (model file, UDF contained python file, and dependent python modules) to the Snowflake session.
.. Domino registers the UDF with Snowflake.
. (Optional) If you make changes, deploy a new version. This action packages the current state of the project (also known as the HEAD commit on the main branch) as a new version of the exported model. Then, Snowflake overwrites the published UDF.
.. From the *Exports* page, click the name of the model for which you want to create a new version.
.. Go to *Actions > Push new version*.
.. Enter the name of the file that contains the predict function and the name of the predict function to call from the external provider.
.. If necessary, select a different *Environment* in which to run the model.
.. On the *Next* page, enter your *Snowflake Username*, enter you *Snowflake Password*, and select a *User Role* for the external provider. Then, click *Authenticate & Export*.
. (Optional) After your exported model has been used for inference and it has saved model inputs and predictions to a supported data store, configure the model to monitor for data drift and model quality in Domino. See link:#meet-the-requirements[Model Requirements] and link:679cc1[Set up Model Monitor].
.. Go to *Grafana Monitoring* > *Configure Monitoring > Data*.
.. From the Configure Data window, select the *Training Data* and the *Version* for the Domino training set on which the model was trained. See link:fbb41f[Domino Training Sets].
.. From *Model type*, select *Classification* or *Regression* depending on your model type. If your Training Set code includes prediction data defined in target_columns, select the model type that matches your Training Set:
* If target_columns is a categorical column, select *Classification*.
* If target_columns is a numerical column, select *Regression*.
.. After the model is registered, the system shows options to add *Prediction Data* and *Ground Truth Data*.
+
NOTE: You must wait until Training Data is ingested to add this information.
+
After the data is ingested, click the *open this model* link for the data to add.

* See link:86bc1f[Set Up Drift Detection].
* See link:3851db[Set Up Model Quality Monitoring]
.. Click *Save*.
.. See link:66f1f0[Test defaults] to set the targets for your data drift and model quality metrics and click *Next* when you're done.
.. See link:afc767[Set Scheduled Checks] to define the schedule for when monitoring results are calculated and updated and click *Next* when you're done.
.. To send email notifications if thresholds are breached based on the Scheduled checks, in *Send alerts to these email addresses*, type or paste a comma- or semicolon-separated list of email addresses.
//TODO this button name might not be accurate as I was unable to see it.
.. Click *Save & Test*.
. (Optional) Use the Exports page to monitor your externally hosted data science models. The model monitoring feature detects and tracks data drift in the model’s input features and output predictions. See link:d42ae6[Use Monitoring] for more information about model monitoring.
.. In the navigation pane, click *Exports* to review the Drift and Model Quality columns.
.. Inspect the results.
* A green circle with a checkmark indicates that the drift and model quality metrics are healthy.
* A red circle with a number indicates the number of alerts that exceed the threshold for the drift or metrics during the most recent check.
* A gray circle without any icon or number indicates a null value; no metrics are available.

After this setup, call the UDF from inside your Snowpark Python application to perform inference against a Domino trained model.

[[Troubleshooting]]
== Troubleshooting

* To gain the most insight, examine logs for the export:

. Go to the Jobs dashboard of the Project and examine the last Domino Job for the export.
. Use the Export UI (view export logs) to navigate to the Domino Job that was used for this export.
. Examine the User Output under the Job.
+
NOTE: For Snowflake models, when you hover over the command on the Jobs page, the password `SNOWFLAKE` represents an environment variable, not your actual password.
* If your Domino model uses an external file, such as a pickle file in the predict function, you must specify a different directory path for Snowflake than for Domino. See https://docs.snowflake.com/en/developer-guide/snowpark/python/creating-udfs.html#reading-files-from-a-udf[Reading Files from a UDF^].

* Authentication or networking issues might cause exports to fail.
//Retry didn't make it in but I am leaving the content for the next version.
//If you retry the export, the system exports the same version.
//For example, the first time you export Model A, it exports version 1. If you export Model A again, you will be exporting version 1 of the model again.
+
If an export to Snowflake fails because of code issues, you might have to update the model. Then, you can link:#Procedure[push a new version].

NOTE: If you push a new version, the version number increments the next time you export the model.

//future.. Depending on the logs, click *Retry* or see link:27d6ca[push a new version].
//build failures are not making it into 5.2

== Next steps

link:ba503b[Integrate model deployment with CI/CD workflows]

----- user_guide/deployment-offerings/domino-cloud.txt -----
:page-version: 6.1
:page-title: Domino Cloud
:page-permalink: 5dc7d5
:page-order: 10


Domino Cloud offers the Domino enterprise MLOps platform to data science teams that want a no-maintenance, secure, production-ready cloud environment that is available quickly. It is a SaaS offering, hosted and managed by Domino Data Lab.

== What is Domino Cloud?

Domino Cloud is a fully-managed SaaS offering that gets customers' data science teams up and running quickly on the Domino MLOps platform, hosted and managed by Domino. The goal of this offering is to enable data science teams to focus on data science work without worrying about the day-to-day operations of managing a data science platform.

To learn more about Domino Cloud, please visit our website: https://www.dominodatalab.com/product/domino-cloud

== Benefits

- *Accelerate data science impact*: Get your entire team of data scientists up and running in 24 hours with full access to the complete ecosystem of professional data science tools and scalable infrastructure they need to drive immediate business impact. This means your team can spend more time on data science rather than waiting on other teams to deploy tools and provision infrastructure.
- *Lower total cost of ownership*: Maximize the impact of your limited manpower and budget with Domino Cloud. Eliminate the headache of implementing and supporting the growing ecosystem of data science tools. Pay only for the compute your team uses while still getting access to scalable compute including GPUs, and distributed compute frameworks.
- *Governed and secure*: Domino Cloud includes isolation, single sign-on, comprehensive role-based access controls, and more to ensure enterprise-grade security. Get immediate access to the full suite of Domino governance capabilities including full reproducibility, a system of record with full documentation, and project management capabilities.

== Limitations

Domino Cloud includes the main functionality of on-premises Domino installations and most features, but some features are not available:

- Attaching external data volumes
- Some administrator capabilities:
** Searching over the configuration
** Triggering restarts of Domino's backend services
** Access to Domino's own MongoDB database
** Customizing hardware tiers, resource quotas, configuration records, feature flags, and email settings
- Access to view the underlying infrastructure, including the Kubernetes cluster and the VM instances
- Keycloak access for user management

NOTE: Because Domino Data Lab upgrades the product over the time, this list is always evolving.

----- user_guide/deployment-offerings/domino-nexus.txt -----
:page-version: 6.1
:page-permalink: 95520d
:page-title: Domino Nexus
:page-order: 20

If you are using a link:c65074[Domino Nexus deployment], some features depend on data locality, supported through _data planes_.
Data might only be available in certain data planes due to geographic access restrictions or the cost of moving data between data centers.

Data planes are attached to hardware tiers.
Depending on the data you need, you must select a hardware tier with the correct data plane when you create a job, workspace, or compute cluster.

Your Domino admin configures the data planes and hardware tiers.
They also associate external data volumes (EDVs) with data planes.

EDVs are the primary method for accessing large data sets in Domino Nexus; they have a first-class notion of data locality.

== Local vs remote data planes

There are two kinds of data planes: local and remote.

* The _local data plane_ is a namespace within the same Kubernetes cluster as the Domino control plane (often called the `compute` namespace).
* A _remote data plane_ is a namespace residing within a separate Kubernetes cluster from the control plane.

link:22474d[Starburst-powered data sources] are not available in _remote_ data planes.

Other types of data sources can be accessed on both the `Local` and remote data planes.

== Use data planes


You can launch link:942549[Jobs], link:e6e601[Workspaces], link:71635d[Apps], and link:8dbc91[Domino endpoints] on a specific data plane by selecting a hardware tier that is configured with that data plane.

You can also link:ee8d01[mount EDVs] that are configured for specific data planes, and launch link:8b4418[compute clusters] alongside your executions.

* Select the `Local` data plane to use Datasets, or Starburst/Trino data sources in your execution.
* You can use a local or remote data plane if you do not need those features.


----- user_guide/deployment-offerings/index.txt -----
:page-version: 6.1
:page-permalink: cece0f
:page-title: Deployment offerings
:page-order: 50

Learn which Domino deployment offering best fits your infrastructure management preferences and security needs. Domino can be deployed at differing levels of infrastructure ownership from fully managed Software as a service (SaaS) to on-premise deployments, including hybrid infrastructure setups.

== Deployment options

Each of Domino's deployment offerings gives distinct advantages in terms of deployment location and infrastructure management requirements.

=== Domino Cloud

Domino Cloud is for customers interested in a Domino-hosted SaaS offering. Domino Cloud operates in a private, single-tenant, secure cloud environment operated by Domino in Amazon Web Services (AWS). Domino manages security patching, upgrades, backups, billing, and more. For more information see link:5dc7d5[Domino Cloud].

=== Domino Cloud for Life Sciences

Domino Cloud for Life Sciences is a single-tenant SaaS offering designed for life sciences and pharmaceutical customers interested in a GxP-compatible and audit-ready Domino.

=== Domino virtual private cloud (VPC)

Domino VPC is for customers interested in self-managing and self-hosting in their public cloud VPC such as AWS, Azure, and GCP using a link:7b2cbe[compatible version] of the native cloud provider distribution of Kubernetes (EKS, AKS, and GKE). It is ideal for customers who want to use their existing public cloud infrastructure and cloud-native expertise to control and self-manage the Domino platform.

=== Domino On-Prem

Domino On-Prem is for customers interested in self-managing and self-hosting their on-premise infrastructure, including bare-metal servers and virtual machines, typically in the corporate data center. This category may also include customers with OpenShift or other Kubernetes platforms on a VPC.

== Nexus Data Planes hybrid deployments

Any of the above offerings can optionally run link:95520d[Domino Nexus] with one or more Data Planes (add-on clusters).

Domino Nexus is a “single pane of glass” capability that lets you run data science workloads across multiple compute clusters — in any cloud, region, or on-premises — by attaching add-on data planes to the main Domino control plane. Nexus is optional and well-suited for customers with teams concerned with data locality in various regions, or teams interested in using on-premise DGX GPU servers or multiple cloud providers.

image::/images/5.5/hybrid/hybrid-architecture.png[alt="The hybrid architecture", role=noshadow, width=1000]

Each additional Data Plane requires installation in a separate, additional Kubernetes cluster where workloads can be executed. These Data Planes can be deployed and used in any cloud (AWS, Azure, or GCP) or on-premise infrastructure and are connected to the core Domino platform, ready to run workloads.

== Next steps

- link:5dc7d5[Domino Cloud]
- link:95520d[Nexus Data Planes]
- link:a0b173[Domino architecture]

----- user_guide/develop-models/develop-train-models.txt -----
:page-version: 6.1
:page-permalink: 786d3c
:page-title: Develop and train models
:page-order: 10

With Domino, you have the freedom to develop models and analytics solutions using the tools and libraries of your choice. As a result, there are many paths to development in Domino. In this overview, you learn about a model development workflow that uses interactive workspaces.

A typical data science workflow for model training iterates through these steps:

. Import, visualize, and prepare data.
. Explore model training approaches.
. Review and evaluate results with collaborators.
. Formalize your model through deployment, automation, or training on larger datasets.

image::/images/develop-models/develop-and-train-models-iterative.png[alt="Data science workflow", width=800, role=noshadow]

Domino closely aligns with this workflow. Use interactive workspaces to prepare data and explore modeling approaches. Use experiment tracking to review results, then use automated jobs to automate the entire workflow.

== Launch a Workspace

Workspaces are interactive code environments used to write and iterate on code directly in your browser. By default, Domino provides Jupyter Notebooks, JupyterLab, RStudio, and VSCode workspaces. However, you can configure workspaces to use custom IDEs too.

When you link:e6e601[launch a workspace], the configuration wizard guides you through the setup process.

Select the following settings for your workspace:

- **Compute environment** - Standard compute environments ensure consistency for repeatable results. To get started, use the link:0d73c6[Domino Standard Environment]. Eventually, you may want to link:e46d54[customize your environment] to use additional libraries.
+
NOTE: Your environment can be a single machine or a link:dd6c5d#enable-and-configure-cluster-deployments[cluster of machines for distributed computing].

- *Hardware tier* - Specify the CPU, memory, and GPU for the pods that host Domino. Your link:3acaee[Domino administrator defines hardware tiers] for you.

- **IDE** - By default, Domino comes with popular IDEs like Jupyter Notebooks, JupyterLab, link:fbb1db[VS Code], and RStudio. However, you can link:03e062[configure workspaces to use custom IDEs] too.

== Develop your model or solution

After you launch a Workspace, write the code to develop your solution. For examples, see the Jupyter notebooks in the 
link:123ae8[AI Hub templates].

Each Project is different, and one of Domino's greatest strengths is its flexibility to accommodate the tools that you want to use while also providing platform tools like link:fbb41f[data source connectors] and link:0a8d11[Dataset management] to save you time and ensure consistency.

TIP: Commit your code to Git frequently while you work so that you can easily revert to a previous version or share specific versions of your code with collaborators.
See link:0d2247[Use Git in your workspace] for details.

== Schedule a training Job

While you develop your model in an interactive workspace, you can train it on a small dataset.
To train it on a larger dataset, schedule a training job.
Training jobs run outside of your development workspace, on a single machine or a distributed computing cluster, depending on the size of your data and the complexity of your model.

Jobs are also useful for re-training your model on a regular basis to ensure ongoing accuracy.
For example, you can schedule a daily job that trains your model on the latest data from the previous 24 hours.

See link:5dce1f[Schedule jobs] for the steps to schedule a one-time or recurring job.

//image::/images/4.x/jobs-stats.png[alt="List of Jobs with stats", width=1200]
// This screenshot is out of date.  Need a deployment with domino stats set up so that the cool timeline visualization is included.

== Manage experiments and collaborate

Use link:da707d[built-in MLflow tracking] to track experiment runs. Use Domino's link:76dfbc[collaboration tools] to evaluate results and approve models for production.

== Best practices for model reproducibility

Versioning is the key to reproducibility.
Follow these best practices to save time and ensure consistency across similar projects and jobs:

* Create custom environments to manage dependencies, instead of installing dependencies at the command line.
* Before modifying a dataset, create a snapshot so that you can always point to a previous version of the data.
See link:dbdbff[Create a snapshot of a dataset] for instructions.
* Commit your code to Git before running any executions (even in your workspace), so that each execution is associated with a specific commit.
* For complex analysis or operations on big data, use jobs instead of workspaces.

== Next steps

After you've developed a model or analytics solution, Domino can help you operationalize your work for deployment or sharing.

If you're training a model, you may want to learn how to:


- link:08a85b[Deploy your model]
- link:5dce1f[Schedule recurring training jobs]
- link:dd6c5d[Scale-out training for larger datasets]

If you're performing data analytics or data engineering, you may want to learn how to:

- link:71635d[Publish apps]
- link:0a8d11[Save datasets]

----- user_guide/develop-models/distributed-training.txt -----
:page-version: 6.1
:page-permalink: dd6c5d
:page-title: Scale distributed workloads
:page-order: 50

Use Domino to create on-demand Spark, Dask, Ray, or MPI compute clusters to speed up computationally-intensive jobs. Execute your jobs in any cloud or on-prem cluster to preserve data locality and optimize spend.

This article contains an overview and examples for compute clusters in Domino. Learn how to do the following:

- Enable clusters in your Domino deployment.
- Use Domino to orchestrate distributed and parallel training workloads.

[[enable-and-configure-cluster-deployments]]
== Enable and configure cluster deployments

Before you use on-demand clusters, enable them in your workspace and create a base cluster image:

- link:1962f3[Configure Spark clusters] for your Domino deployment.
- link:190175[Configure Ray clusters] for your Domino deployment.
- link:3e137b[Configure Dask clusters] for your Domino deployment.
- link:402c65[Configure MPI clusters] on your Domino deployment.

[[distributed-and-parallel-training]]
== Distributed and parallel training

Generally, there are two ways you can use compute clusters to train models in Domino:

- As the compute environment for interactive workspace such as Jupyter Notebooks (or any other IDE) running on top of the cluster.
- As a job-based compute cluster that executes a training script or job you define.

Typically, interactive workspaces are used to explore datasets and training approaches. In contrast, use the job-based method after you've developed a training approach and want to repeat it.

Select the cluster type to learn more. For more information on choosing a cluster type, see our blog post https://www.dominodatalab.com/blog/spark-dask-ray-choosing-the-right-framework[Spark, Dask, and Ray: Choosing the right framework].

[#Spark2]
.Spark
--

https://spark.apache.org/[Spark] provides a simple way to parallelize compute-heavy workloads such as distributed training. Spark benefits iterative training algorithms or multi-threaded tasks over large data sets.

Domino supports fully containerized executions of Spark workloads on the Domino Kubernetes cluster. You can interact with Spark through Domino in the following ways:

- Use Spark in an link:f11f6a[interactive workspace].
- Use Spark in link:f11f6a#tr2[batch mode through a Domino job].
- Directly with https://spark.apache.org/docs/latest/submitting-applications.html[spark-submit].

When you start a workspace or a job that uses an on-demand cluster, Domino orchestrates a cluster in standalone mode. The master and workers are newly deployed containers, and the driver is your Domino workspace or job.

See the https://github.com/dominodatalab/spark-quickstart-winequality[Spark quickstart project] to walk through environment setup, project creation, and model training.


Domino also provides access to GPU-accelerated backend compute for the Spark workers. Combined with the https://rapids.ai/[RAPIDS Accelerator for Spark^], you can enable GPU-accelerated processing on the Spark worker nodes. For more information, see the https://github.com/dominodatalab/webinar-gpu-accelerated-spark-and-rapids[Webinar for GPU-accelerated Spark and RAPIDS^].

--

[#Ray2]
.Ray
--

https://www.ray.io/[Ray] is an extensive distributed computing library of tools that reduces training time for parallel and distributed applications. You can interact with Ray through a Domino in the following ways:

- In an link:c50248[interactive workspace].
- In link:c50248#tr2[batch mode through a Domino job].

Ray provides:

- Simple primitives for building and running distributed applications.
- Tools that allow users to parallelize single machine code with little to zero code changes.
- Applications, libraries, and tools for deep learning.

See the https://github.com/dominodatalab/domino-quickstart-ray[Ray quickstart project] to walk through environment setup and multiple types of training algorithms.

--

[#Dask2]
.Dask
--
https://www.dask.org/[Dask^] provides a drop-in replacement to train parallelizable scikit-learn models at scale. You can run Dask in single-node and distributed modes to enable parallelization and distribution. You can interact with Dask through a Domino in the following ways:

- Use Dask in an link:aaa2c1[interactive workspace].
- Use Dask in link:aaa2c1#tr2[batch mode through a Domino job].

See the https://github.com/dominodatalab/domino-quickstart-dask[Dask quickstart project] to walk through environment setup and multiple types of training algorithms.

See the https://github.com/dominodatalab/reference-project-dask[Dask reference project] for an example of using Dask in Domino to train a model. This project covers environment setup, data pipelines, and model training. Focus on https://github.com/dominodatalab/reference-project-dask/blob/main/dask_ml.ipynb[`dask_ml.ipynb`].

--

[#MPI2]
.MPI
--
// Pulled from user_guide/d60880/on-demand-open-mpi/ - create includes with conditional ifevals where needed.

https://www.mpi-forum.org/[MPI^] provides a communication protocol for distributed parallel computing. Open MPI is ideal for distributed multi-GPU and multi-CPU training for Tensorflow, PyTorch, Keras, or MXNet models. MPI clusters have lower overhead than other distributed computing systems and are highly customizable.

You can interact with Dask through a Domino in the following ways:

- Use MPI in an link:594e3c#_create_an_mpi_cluster_with_workspaces[interactive workspace].
- Use MPI in link:594e3c#_create_an_mpi_cluster_with_jobs[batch mode through a Domino job].

--

== Next steps

Now that you know the concepts behind using Spark, Dask, and Ray to configure clusters for jobs, see how to link:874b46[Tune Models with Ray Tune].

----- user_guide/develop-models/index.txt -----
:page-version: 6.1
:page-permalink: 3afed0
:page-title: Develop models
:page-order: 250

This article introduces training machine learning models in Domino. See the links below to learn about Domino's benefits and model development concepts.

== Train with your favorite libraries and IDEs

Domino is a flexible platform that's designed to work with your favorite tools:

* link:786d3c[Develop and train models] with open-source libraries and your favorite IDEs.
* Implement MLflow to link:da707d[track and monitor experiments] in R and Python.
* link:dd6c5d[Scale distributed workloads] with Dask, Ray, MPI, and Spark.
* link:874b46[Tune hyperparameters at scale with Ray Tune] and other open source libraries.

== Manage experiments and collaborate with others

Domino helps you manage experiments and models across your organization:

* link:da707d[Track and monitor experiments] to see logs, outputs, and more.
* link:5dce1f[Schedule jobs] for recurring processes.
* link:910370[Develop projects with Git] version control.

== Next steps

After you understand how to train models in Domino, learn how to link:08a85b[Deploy Models and Apps].

----- user_guide/develop-models/ray-tune.txt -----
:page-version: 6.1
:page-permalink: 874b46
:page-title: Tune hyperparameters with Ray Tune
:page-order: 60

Use Ray Tune and Domino to expedite hyperparameter tuning by distributing it at scale. While you have the freedom to use any library in Domino, this guide uses https://docs.ray.io/en/latest/tune/index.html[Ray Tune^] and https://mlflow.org/[MLflow^] to tune models and evaluate results.

[[set-up-your-environment]]
== Prerequisites

Domino helps you manage Ray cluster infrastructure with on-demand clusters, but before you use Ray clusters, you must first link:190175[setup Ray clusters in Domino].

[[run-and-explore-tuning-script]]
== Import and run the training script

In this example, you tune a PyTorch Lightning classifier on the MNIST dataset. You'll use Ray Tune to run permutations on layer size, learning rate, and batch size. Import the https://github.com/ddl-jwu/ray-tune[RayTune GitHub reference project^] into Domino:

. In the Domino navigation pane, click *Projects* > *New Project*.
. In the Create new Project window, enter a name like "Ray Tune example".
. Under *Hosted By*, select *Git Service Provider* > *GitHub*
. For *Git Repository URL*, enter "https://github.com/ddl-jwu/ray-tune".
. In the project, link:c50248#_create_a_cluster_with_workspaces[launch a Ray-enabled VSCode Workspace] and load *train.py* to explore the training script.
. In VSCode, open a terminal and type `python train.py` to run the script.

The following code snippet from train.py configures the RayTune parameter grid and gets the MLflow tracking URI that comes pre-configured for Domino projects:

[source, Python]
----
def tune_mnist(num_samples=4, num_epochs=5, gpus_per_trial=0, data_dir="~/data"):
    config = {
        "layer_1_size": tune.choice([16, 32, 64]),
        "layer_2_size": tune.choice([32, 64, 128]),
        "lr": tune.loguniform(1e-4, 1e-1),
        "batch_size": tune.choice([32, 64]),
        "mlflow": {
            "experiment_name": "ray-tuning",
            "tracking_uri": mlflow.get_tracking_uri()
        }
    }
----

Each permutation of the parameters produces a single run, and all runs are grouped under the same experiment. One epoch covers an entire data pass through the neural network during training. Logging is done at the epoch level. For more information on experiment tracking, see link:da707d[Track and monitor experiments with MLflow].

[[run-tuning-experiments-in-a-job]]
== Run the training script as a job

You can also run the training script as a job to remove the workspace creation steps. Jobs are useful automation mechanisms in Domino.

. In the project you imported from GitHub in the previous section, click *Jobs*.
. Configure a run with *train.py*.

This job runs the *train.py* script. An information icon on the right side under *Results* shows the experiments associated with the job. Click the link to explore them.

[[visualize-and-compare]]
== Visualize and compare results

To view an individual trial, follow these steps:

. Click the *Experiments* link to see the trials logged by a workspace or the information icon to see the trials logged by a job.
. Click an individual trial and the *Metrics* tab to see the full-time-series metrics.
. Look for the general trend of the metrics over time, especially with deep-learning scenarios where compute costs drive decisions. If your accuracy shows slight or no improvement, there's no point in using more resources to go down this path.

To compare trial results:

. Select at least two trials from the main *Experiments* page and click the compare icon.
. Use the side-by-side comparison to discover how different parameters impact accuracy and run time.
+
NOTE: When you change the *Chart* type to *Line*, you can also examine the time series charts for the metrics side-by-side.
+
. Click on a specific trial chart line.
. Analyze differences captured in metadata: different compute environments, users, start times, and workspaces.

== Share the experiment view

You can export the experiment view to share with others or generate reports.

From the Compare screen, export the experiment view as a PDF. This report also has a deep link you can share for direct access to the live report in Domino.

[[troubleshooting]]
== Troubleshooting

PyTorch and Ray version 2.0.0 show compatibility issues related to auto-logging. As a workaround, use manual logging or a Ray version beyond 2.0.0.

== Next steps

Now that you know how to perform hyperparameter tuning in Domino, explore link:da707d[Track and Monitor Experiments] or link:08a85b[model deployment].

----- user_guide/develop-models/track-monitor.txt -----
:page-version: 6.1
:page-permalink: da707d
:page-title: Track and monitor experiments
:page-order: 40

Domino's Experiment Manager uses link:https://mlflow.org/docs/latest/tracking/[MLflow Tracking^] to log experiment details, such as parameters, metrics, and artifacts. Domino stores all your experiment metadata, data, and results within the project, making it easy to reproduce your work. This allows you to:

* Monitor experiments with both automatic and manual logging.
* Compare and review experiment runs effectively.
* Collaborate securely with colleagues.

image::/images/develop-models/experiment-manager-dashboard.png[alt="Experiment Manager dashboard"]

MLflow runs as a service in your Domino cluster and works seamlessly with your workspace and jobs. You can use your current MLflow experiments without changing any code.

Domino provides better security than standalone MLflow by using role-based access control (RBAC). Users with view access to the project can see all materials, data, results, and logs for every experiment within the project.

== Before you start: How MLFlow experiments work in Domino
When you click Experiments in the Domino left navigation, you’ll see an empty dashboard unless at least one run has been logged. Domino doesn’t create experiments through the UI.

*Instead*:

* You create an MLFlow experiment by calling `mlflow.set_experiment()` in code.
* The experiment appears in the UI only after you log at least one run.

For example, to set a unique experiment name:

[source,python]
----
# create a new experiment
import mlflow
import os
starting_domino_user = os.environ["DOMINO_STARTING_USERNAME"]
experiment_name = f"Domino_Experiment_{starting_domino_user}"
# Replace <your_experiment_name> with the name of your experiment
mlflow.set_experiment(experiment_name="<your_experiment_name>")
----

*Tip*: Experiment names need to be unique across your Domino Deployment. A good practice is to use a unique identifier (such as `username`) as part of your experiment name.

== Step 1: Create and log an experiment run
You can log experiments in Domino using MLflow’s auto-logging or manual logging features. Both methods record parameters, metrics, and artifacts in your Domino project. After you run either option, the experiment and its runs will appear in the *Experiments* tab for review.

. From your project, click *Workspaces* in the left navigation. 
. Launch a workspace using any environment based on the Domino Standard Environment. These environments already have the `mlflow` package installed.
. Open a Python script or notebook to run your experiment code.

*Tip*: You can also run the same code in a job for reproducible, version-controlled experiments.

=== Option 1: Use auto-logging
Auto-logging records parameters, metrics, and artifacts automatically for supported libraries. 
See link:https://mlflow.org/docs/latest/tracking.html#automatic-logging[MLflow Tracking^] for a list of supported libraries.

This snippet shows you how to auto-log a link:https://scikit-learn.org/stable/api/sklearn.html[scikit-learn^] experiment.

[source,python]
----
# import MLflow library
import mlflow
import os

from sklearn.model_selection import train_test_split
from sklearn.datasets import load_diabetes
from sklearn.ensemble import RandomForestRegressor

# create and set a new experiment
starting_domino_user = os.environ["DOMINO_STARTING_USERNAME"]
experiment_name = f"Domino_Experiment_{starting_domino_user}" 
mlflow.set_experiment(experiment_name=experiment_name)


# enable auto-logging
mlflow.autolog()

# start the run
with mlflow.start_run():
    db = load_diabetes()
    X_train, X_test, y_train, y_test = train_test_split(db.data, db.target)
    rf = RandomForestRegressor(n_estimators = 100, max_depth = 6, max_features = 3)
    rf.fit(X_train, y_train)
    rf.score(X_test, y_test)

# end the run
mlflow.end_run()
----

=== Option 2: Set up manual logging
Manual logging gives you full control over what parameters, metrics, and artifacts are recorded. link:https://mlflow.org/docs/latest/tracking.html#automatic-logging[MLflow Tracking^] offers more details and examples.

[source,python]
----
# import MLflow library
import mlflow
import os

# create a new experiment
starting_domino_user = os.environ["DOMINO_STARTING_USERNAME"]
experiment_name = f"Domino_Experiment_{starting_domino_user}" 
mlflow.set_experiment(experiment_name=experiment_name)
# start a run and log parameter,metric, and artifact
with mlflow.start_run():
    mlflow.log_param("batch_size", 32)
    mlflow.log_metric("accuracy", 0.75)
    with open("example.txt", "w") as f:
        f.write("This is a sample artifact.")

    mlflow.log_artifact("example.txt")

# end the run
mlflow.end_run()
----

Once your workspace is running, you’re ready to write and execute code that will create and log an experiment.

=== Step 2: Monitor and evaluate experiments

Once you log your runs in an experiment, use Domino to view and assess the results.

. Click *Experiments* in Domino's left navigation pane.
. Find and click the experiment to evaluate and show its associated runs.
. Click a run to analyze the results in detail and find reproducibility information.
. Compare runs:
.. Select up to four runs from the table view.
.. Click *Compare*.
+
image::/images/develop-models/compare-experiments.png[alt="Compare experiments"]
+
You can compare runs to see how parameters affect important metrics like model accuracy or training speed. Save your visualizations and artifacts in the root folder for visual comparison of charts.
+
image::/images/develop-models/compare-runs.png[alt="Compare runs"]

=== Step 3: Export experiment results

You can export your single experiment results to a CSV or compare experiments and download to a PDF.

To download single experiment results:

. Click *Experiments* in Domino's left navigation pane.
. Find and click the experiment with the results that you want to export.
. Select the three dots in the upper right of the experiment.
. Choose *Download CSV*.

To download a report of compared experiment results:

. Click *Experiments* in Domino's left navigation pane.
. Find and click the experiment with the results that you want to export.
. Select up to four runs to compare.
. Click *Compare*.
. Select the three dots in the upper right of the experiment.
. Choose *Export as PDF*.

=== Step 4: Share your projects and experiments

You can set permissions for project assets, including *MLflow logs*, on a project level. Use these methods on your projects to control access:

* link:71afc6[Choose the visibility] for your project. This will help you control who can see your project.
** *Searchable*: Discoverable by other Domino users.
** *Private*: Only viewable or discoverable by your project collaborators.
* link:4b6411[Invite collaborators] and set their link:7876f1[permissions] based on project roles. This gives you detailed control over what they can access.

== Troubleshooting and limitations

* Experiment names must be unique across all projects within the same Domino instance.
* Child runs aren't deleted when you delete a run that has children. Delete the child runs separately.
* You can't stop a run from the UI. To stop a run, execute `mlflow.end_run()` from your workspace or job.
* When you trigger an experiment from a workspace, the manager shows the file name. But if you are running that and editing it, it doesn't rename the experiment automatically. After completing an experiment in a workspace, trigger a job to manage version control.
* Best practice is to give your runs a name - otherwise it automatically creates one like `gifted-slug-123`.

== Advanced functionality

You can upload large artifact files directly to blob storage without going through the MLflow proxy server.

This experimental link:https://github.com/cerebrotech/sagemaker-deployment-demo/blob/main/models/custom-pyfunc-advanced-llm-vllm.ipynb[feature must be enabled inside the user notebook^] code by setting the environment variable `MLFLOW_ENABLE_PROXY_MULTIPART_UPLOAD` to `true`.

[source,python]
----
import os
os.environ['MLFLOW_ENABLE_PROXY_MULTIPART_UPLOAD'] = "true"
----

This is helpful for both `log_artifact` calls and registering large language models. It is currently supported only in AWS and GCP environments. There are two additional settings available for configuration:

* `MLFLOW_MULTIPART_UPLOAD_MINIMUM_FILE_SIZE` - the minimum file size required to initiate multipart uploads.
* `MLFLOW_MULTIPART_UPLOAD_CHUNK_SIZE` - the size of each chunk of the multipart upload. Note that a file may be divided into a maximum of 1000 chunks.

link:https://mlflow.org/docs/latest/tracking/artifacts-stores.html#multipart-upload-for-proxied-artifact-access[Multipart upload for proxied artifact access^] in the MLflow documentation has more information on using this feature. link:https://github.com/cerebrotech/sagemaker-deployment-demo/blob/main/models/custom-pyfunc-advanced-llm-vllm.ipynb[Registering Hugging Face LLMs with MLflow^] has directions specific to Domino.

== Next steps

* link:08a636[Get started] with a detailed tutorial on workspaces, jobs, and deployment.
* link:5dce1f[Schedule recurring training jobs].
* link:dd6c5d[Scale-out training] for larger datasets.
----- user_guide/domino-governance/define-policy.txt -----
:page-permalink: cd4f06
:page-version: 6.1
:page-iversion: 610
:page-title: Define Domino Governance policies
:page-sidebar: Define policies
:page-order: 30

The *`GovernanceAdmin`* role provides permissions to create, edit, and publish governance policies in Domino. This role also enables access to the Governance Dashboard, audit trail, compliance views, and governance APIs.

`SysAdmins` automatically have `GovernanceAdmin` permissions. All other users must be explicitly assigned the role.

== Prerequisite: Assign `GovernanceAdmin` role

To enable a user to manage governance policies:

. In the *Admin* console, go to *Manage Resources* > *Users*.

. Select the user to assign the role to.

. Click *Edit*.

. Select *GovernanceAdmin (Admin role for Governance)* from the list of roles.

. Click *Save*.

== Define your first policy

A governance policy defines the review and approval process for a scientific output, such as deploying a model, releasing a statistical report, or launching an AI system. Policies can reflect corporate standards, regulatory frameworks, or internal risk controls.

Policies are written in YAML and must be published before use. link:f30aab[Build Domino governance policies] has more building blocks for advanced customization.

Approvers enter information into an `evidence set` during review. Policy rules use that input to control visibility by dynamically showing or hiding follow-up questions or stages.

== Option 1: Create a new governance policy from scratch

. Go to *Govern* > *Policies* and select *Create Policy*.

. From the *Policy Template* menu, use *None (start from scratch)*.

. Name your policy and provide a short description.

. Select *Create*.

. In the *Code editor*, paste your policy code.

. To visualize the workflow, toggle the *Graph* view.

. Modify the policy as needed, such as by updating stage names, changing approvers, and adding or editing rules.

. Click *Save*, then *Publish*.

== Option 2: Create a new governance policy from a template

Domino Governance includes pre-built policy templates.
You can use these templates to customize policies for your organization quickly.

. Go to *Govern* > *Policies* and select *Create Policy*.

. From the *Policy Template* menu, select a template.

. Name your policy and provide a short description.

. Select *Create*.

. Verify that the policy workflow looks correct by toggling *Graph*.

. Toggle back to the *Code editor*.

. Update the template by making a few changes, such as by updating `stages` names, changing approvers, and adding or editing rules.

. Click *Save*, then *Publish*.

image::/images/6.0/governance/create-policy-toggle-graph.png[alt="Toggle Graph to review the policy workflow", width=800, role=noshadow]

== Archive Governance policies
You can now archive policies from the Policies screen. Archived policies are only visible when the Archived filter is applied in the policy table. These policies can’t be restored, and their names aren’t linked to policy details. An empty view appears if there are no archived policies. To archive a policy:

. Go to *Govern* > *Policies* and locate the relevant policy.
. Click * … * for the policy and select *Archive*.
. If no active bundles use the policy, a confirmation modal appears. 
. Click *Confirm* to archive it.
. If any active bundles use the policy, you’ll see a modal that explains why it can’t be archived.

== Next steps

* link:f30aab[Build Domino Governance Policies.]
* link:d56edd[Create governed bundles.]
//* link:TBD[Migrate model approval to Domino Governance.]

----- user_guide/domino-governance/governance-lifecycle/add-findings-to-bundle.txt -----
:page-permalink: 9eef3e
:page-version: 6.1
:page-iversion: 610
:page-title: Add findings to governed bundles
:page-sidebar: Add findings
:page-order: 30

Use *Findings* to track and manage issues throughout the governance lifecycle of a bundle. Each finding records a concern, review note, or follow-up action, and assigns it to someone for resolution within a defined timeframe.  Contributors can add findings at any stage. Approvers can add findings to stages they haven’t approved yet. 

Each finding is tied to a specific approval stage and can include supporting evidence. Multiple findings can be added to the same bundle.  Findings help teams:

* *Identify risks* such as bias, security gaps, or regulatory non-compliance
* *Evaluate compliance* with governance standards and ethical guidelines 
* *Suggest improvements* to enhance performance, fairness, or accountability 

Assigned tasks help contributors respond to findings and keep the review process moving forward. Once all findings are resolved, the approver marks the stage as *Stages Complete* to indicate final approval. Review status options are:

* *Approved*: The request meets all acceptance criteria.
* *Rejected*: The request does not meet requirements for this stage.

== Add findings
Access the governed bundle directly to add or view findings:

. Go to *Projects* and open the relevant project.

. From the sidebar, select *Govern* > *Bundles*.

. Choose the bundle where you want to add or view findings.

image::/images/6.1/user-guide/sidebar-govern-bundle.png[alt="Find bundle list", width=600, role=noshadow]

You can add findings from two tabs: 

* *Evidence*: Scroll down and click *Add Finding*.
* *Findings*: Click an existing finding to view, or click *Add Finding* to add a new one.

=== Add finding details
Fill out the required fields, including the finding name, approval level, severity, assignee, and approver. 

image::/images/6.1/user-guide/add-new-finding.png[alt="Add new finding", width=600, role=noshadow]

You can also add an optional description and due date. Repeat as needed for additional findings.

== Address findings from tasks
If you’re assigned a task related to a finding:

. Go to *Projects* and open the relevant project.

. Select *Govern* > *Tasks*. 

. Click the task to open it.

. From the task description, click the finding link to view details.

. On the *Findings* tab, check the *Status* menu to see the current stage.

. Leave a comment to provide information or context. You can use Markdown and MathJax in your comment, or @mention people.

. Click *Send* when finished.

Findings track issues or decisions during project reviews. Approvers manage findings for any stages they haven’t approved, and tasks help contributors respond and keep the review process on track.

== Next steps
* link:d56edd[Create governed bundles] - get steps on creating and using our governed bundles.
* link:cd4f06[Define Domino Governance policies] - learn how to create policies from a template.
* link:f30aab[Domino Governance policy components] - contains an overview and examples for crafting your own policies.






----- user_guide/domino-governance/governance-lifecycle/create-bundles.txt -----
:page-permalink: d56edd
:page-version: 6.1
:page-title: Create governed bundles
:page-sidebar: Create bundles
:page-order: 10

As a practitioner, you can create governed bundles within your own projects. After you create a bundle, you can select the policy you wish to apply, collect evidence, and attach files and artifacts to each bundle.

Depending on how the policy is written, you can involve an individual or an organization when you want approval. If you choose an individual, a specific Domino Task will be created for them. If you select an organization, the organization’s owner will be promptly notified to assign the task internally for efficient allocation.

== Step 1: Create a new governed bundle

Create a new bundle:

. Open the project on which you want to create a managed bundle.

. Go to *Governance* > *Create Bundle*.

. Enter a *Bundle Name*.

. Choose a policy from the drop-down list.

. Click *Create*.

[[add-files]]
== Step 2: Add evidence to an existing bundle

There will be times when you'll need to add a file, artifact, or job output to an existing governed bundle:

. Select the file, artifact, or job output to add to an existing bundle:
.. If you are in a Domino File System project, click the code link in the right navigation.
.. If you are in a Git-based project, click the artifact link in the right navigation.
. To govern a job's output, navigate to the job and then go to the *Output* tab.
. Select *Add to an existing Bundle*.
. Choose a bundle from the menu.
. Verify that the file was added to the bundle by clicking *Attachments*.

image::/images/6.0/governance/add-file.png[alt="Attachment that has never been approved", width=1200, role=noshadow]

== Step 3: Add a model to an existing bundle

To add a model to an existing governed bundle:

. Open the Project that contains the artifact you want to add to a managed bundle.

. Choose the model from the list of available models.

. On the *Model Details* tab, select *Add to Bundles*.

. Choose *Add to an existing Bundle*.

. Select an *Existing Bundle* from the menu and click *Add*.

== Next steps

* link:1ce615[Request reviews of governed bundles.]

----- user_guide/domino-governance/governance-lifecycle/index.txt -----
:page-permalink: 744107
:page-version: 6.1
:page-iversion: 610
:page-title: Governance lifecycle
:page-order: 20

The governance lifecycle defines how governed bundles progress from creation to production readiness. Each bundle moves through predefined approval stages where evidence is collected, reviewed, and approved. Stages may include policy checks, risk assessments, and ethical evaluations. Approvers review submissions, document findings, and determine whether the bundle can proceed.

image::/images/6.1/user-guide/governance-lifecycle.png[alt="High-level diagram of Governance lifecycle", width=1000, role=noshadow]

This lifecycle enforces consistency and accountability. It ensures that decisions are based on documented criteria and that risks are identified and resolved before release. Findings and approval transitions are recorded to support audit requirements and enable continuous improvement.


== Next steps

* link:d56edd[Create governed bundles] - get steps on creating and using our governed bundles.
* link:1ce615[Send governed bundles for review] - submit your bundles for review and get them published.
* Use link:9eef3e[Findings] to track and manage issues throughout the governance lifecycle of a bundle.


----- user_guide/domino-governance/governance-lifecycle/review-bundles.txt -----
:page-permalink: 1ce615
:page-version: 6.1
:page-iversion: 610
:page-title: Send governed bundles for review
:page-sidebar: Review bundles
:page-order: 20

Governed bundle reviews ensure that stakeholder input is captured at key points in the development process. 
Practitioners can submit bundles for review at any stage, and approvers are notified when their input is needed. 

Approval flows support per-bundle customization, allowing different bundles to have distinct approvers, even under the same policy. This enables flexible and targeted stakeholder involvement throughout the review process.

Each approval stage is defined by the Governance Admin and may vary by policy type.

== How approvals work

During a bundle review, approvers can document findings to capture and track issues that block compliance. Each finding includes:

* Severity
* Ownership
* Descriptions
* Recommended actions
* Deadlines for fixing compliance gaps

These attributes help drive progress toward production readiness. 

An example approval flow might look something like this:

image::/images/6.0/governance/review-flow.png[alt="Example of a review flow", width=300, role=noshadow]

link:9eef3e[Findings] are tied to approval stages and can include supporting evidence. They help:

* *Identify Risks* such as bias, security gaps, and regulatory non-compliance.
* *Evaluate Compliance* with governance standards and ethical guidelines.
* *Suggest Improvements* to enhance performance, fairness, or accountability.

Once all findings are resolved, the approver marks the stage as *Stages Complete* to finalize the review.

== Step 1: Submit a governed bundle for review

After link:d56edd[creating a governed bundle] or editing an existing one, you can submit it for stakeholder review. This is typically done after completing requirements for each stage.

image::/images/6.0/governance/start-review.png[alt="Start an evidence review", width=600, role=noshadow]

. Go to *Project* and open the relevant project.

. From the sidebar, select *Govern* > *Bundles*.

. Select the bundle that you want to send for review.

. On *Stage 1*, fill in the required information and proceed through each stage.

. When you reach a stage with *Approval required*, click *Start an evidence review*.

. Complete all required fields for that review stage and click *Request Review*.

. To confirm your submission, go to the *Domino Welcome* page and verify that a review task was created.

== Step 2: Transition bundle to Stages Complete

Once all review stages are approved and findings resolved, transition the bundle to complete the approval process:

. Open your Evidence notebook awaiting approval.

. Click *Transition Stage*.

. Select *Stages Complete* from the menu.

. Click *Transition*.

== Next steps

* link:cd4f06[Define a Domino Governance policy.]
* Use link:f30aab[Domino Governance Policies] as building blocks for more robust policies.
----- user_guide/domino-governance/index.txt -----
:page-permalink: 7f8a63
:page-version: 6.1
:page-title: Domino Governance
:page-order: 320
:page-separator: true
:page-section: Govern

Domino Governance refers to the frameworks, policies, and guidelines that oversee the development, deployment, and use of technologies.
Domino Governance makes sure that these are aligned with corporate policies and societal values.

This involves a multidisciplinary approach: combining technical, legal, and ethical perspectives to address issues like bias, privacy, accountability, and transparency.

== Why do you need Domino Governance?

Domino Governance simplifies, organizes, and automates the process of gathering, tracking, and reviewing all assets to help enforce internal or external policies.

It provides a single interface closely connected and integrated within a data scientist's environment, ensuring smooth governance, version control, and compliance for all stakeholders involved.

image::/images/6.0/governance/governance-overview-cloud.png[alt="High-level diagram of Domino Governance", width=1000, role=noshadow]

== Terms you should know

[cols="<1,<2",options="header"]
|===
|Term |Definition

|*Approvals* |Actions added to a policy to request approval from an organization or an individual.
Anyone with project access can request approval, but only a `GovernanceApprover` can approve it.
|*Attachments* | Any files from a project that needs to be governed by a policy in a specific context.
|*Classification* | The top-level variable for a policy that is meant to carry the overall classification of the governed bundle.
It could be used in a tiered approach, such as low, medium, or high risk.
|*Classification rules* | Classification supports complex operations using the link:https://go.dev/[Go programming language^].
It uses the values results from one or multiple evidence answers and expects an output of a string or a float.
|*Evidence* | Information that is relevant to a policy.
In most cases, evidence is a question with an answer and can include input, metric checks, and scripted checks.
|*Evidence sets* | Groups of evidence that can be reused within a policy or across policies. Evidence sets are meant to promote consistency of governance.
|*Findings* | These serve as a necessary step for capturing and addressing issues that will prevent full compliance, ensuring a path to production readiness.
|*Governed bundle* | A governed bundle can be a model, an application, a report, or any other asset developed within the context of a project. It stores all evidence related to the policy it governs and keeps the lineage to the relevant attachments. 
|*Lifecycle* | A set of stages describing each step of building and maintaining a governed bundle.
|*Metrics Checks* | Automatically attach metrics to a governed bundle, eliminating the need for manual entry.
|*Policy* | Definition of all stages, evidence, and approvals that need to be met for compliance. 
|*Scripted Checks*| These checks use centralized scripts to evaluate information on a Governed Bundle. Scripted checks can also be used to measure bias consistently in a dataset.
|*Stages* | Key milestones or phases of a policy.
For example, in a model risk management policy, the stages could be: develop business case and define requirements; model development and testing; model validation and deployment; and model monitoring.
|===

== Next steps

* link:f8e3f8[Domino Governance workflows] - learn more about the people, processes, and systems that make up governance workflows.
* link:744107[Governance lifecycle] - move your bundles through their lifecycle, from creation to review to publication.
* link:d56edd[Create governed bundles] - get steps on creating and using our governed bundles.
* link:1ce615[Send governed bundles for review] - submit your bundles for review and get them published.
* link:cd4f06[Define Domino Governance policies] - learn how to create policies from a template.
* link:f30aab[Domino Governance policy components] - contains an overview and examples for crafting your own policies.
* link:85fbb1[Domino Unified Audit Trail] - captures broader compliance events to provide better transparency and accountability.

----- user_guide/domino-governance/policy-components.txt -----
:page-permalink: f30aab
:page-version: 6.1
:page-title: Domino Governance policy components
:page-sidebar: Policy components
:page-order: 40

Policies can be managed in the Governance Console.

Once a policy is published, it becomes immutable, ensuring the lineage between a Governed Bundle and the approved policy can be maintained.

To complete these tasks, you'll need to be assigned the link:2611b7[GovernanceAdmin] role.
`SysAdmins` already have permissions associated with Domino Governance.

== Define Policies with YAML

This document explains the YAML configuration structure that defines various input fields, policy scripted checks, metadata, and guidance elements.  

=== Structure overview

The YAML configuration consists of a list of items, each representing a different element in the user interface or system configuration. The `details` section contains specific properties for each artifact type.

=== Stages

A stage contains a group of evidence and approvals. Direct evidence is grouped as an evidence set, and approvals are grouped as approvals. Each approval includes a name, a list of approvers, and optionally, evidence.

[source,yaml]
----
yaml
stages:
  - name: stage1
  - name: stage2
----

=== Evidence and evidence sets

Evidence includes inputs, approvals, and checks that should be gathered as part of the governance process within a given stage. Evidence sets represent logical groupings of evidence and can also be reused in other policies if needed. Local evidence should have the full definition included when appearing for the first time in the policy. It can later be referenced by id when using it in the YAML file.

[source,yaml]
----
yaml
evidenceSet:
  - id: Local.sample
    name: sample local evidence
    description: Describe the sample local evidence
    definition: Define the evidence
----

=== Metrics checks

Model Metrics are sets of policy-defined metrics used for automated pre-approval checks. Each set follows a shared baseline structure and can be extended with additional metrics. Fields include aliases for detection, threshold operators (e.g., >, <), and expected values—helping reduce manual review for governance administrators.

[source,yaml]
----
yaml
metrics:
- id: Local.model-quality
  name: Model Quality
  description: Describe the model quality
  definition:
    - artifactType: metadata
      details:
      type: modelmetric
      metrics:
        - name: Acc
          label: Accuracy
          aliases:
            - acc
            - Correct Classification Rate
            - Percentage Correct
          threshold:
            operator: '>='
            value: 0.8
----

=== Scripted checks

Scripted Checks use centralized, policy-defined scripts to evaluate Governed Bundles, for example, to measure dataset bias. Defined in the policy YAML, these scripts run in a specified environment and attach output files to the evidence notebook. This supports standardized, auditable validation across projects.

[source,yaml]
----
yaml
    - artifactType: policyScriptedCheck
      details:
        name: Ethic and Fairness Evaluation
        label: Ethic and Fairness Evaluation
        command: evaluate_model.py create --model-hub ${model_hub} --model-name ${model_name}
        parameters:
          - name: model_hub
            type: text
            default: openai
          - name: model_name
            type: text
            default: gpt-4
            outputTypes:
              - txt
              - png
            environmentId: 674f04e2191e8f19a5d12552   # 6.0 default environment on se-demo
              hardwareTierId: small-k8s
              volumeSizeGiB: 4
    - artifactType: metadata
      details:
        label: Upload model validation report.
        type: file
----

=== Input Artifacts

Input artifacts represent various form elements for user input.

==== Radio buttons
Defines a set of radio buttons with a list of choices, each with a label (displayed text) and value (submitted data).

[source,yaml]
----
yaml
- artifactType: input
  details:
    type: radio
    label: "How would you rate the model risk?"
    options:
      - label: "High"
        value: "High"
      - label: "Medium"
        value: "Medium"
      - label: "Low"
        value: "Low"
    tooltip: "Guidance text"
----

==== Text input
Defines a text field for user input, used to collect written responses or descriptions.
[source,yaml]
----
yaml
- artifactType: input
  details:
    type: textinput
    label: "What are the expected business benefits?"
    placeholder: "Explain the benefit"
    helpText: "The text under the input box to help the user"
----

==== Text area
Defines a multi-line text input field.

[source,yaml]
----
yaml
- artifactType: input
  details:
    type: textarea
    label: "What are the expected business benefits?"
    height: 10
    placeholder: "Explain the benefit"
    helpText: "The text under the input box to help the user"
----

==== Select dropdown
Defines a dropdown selection field.

[source,yaml]
----
yaml
- artifactType: input
  details:
    type: select
    label: "Please select the base model template."
    options:
      - label: "base model1"
        value: "baseModel1"
      - label: "base model2"
        value: "baseModel2"
----

==== Multi-select
Defines a multi-select dropdown field and allows selection of multiple options.
[source,yaml]
----
yaml
- artifactType: input
  details:
    type: multiSelect
    label: "Please select the data sets used in the model."
    options:
      - label: "data set1"
        value: "dataset1"
      - label: "data set2"
        value: "dataset2"
      - label: "data set3"
        value: "dataset3"
----

==== Checkbox group
Defines a group of checkboxes and allows selection of multiple options.

[source,yaml]
----
yaml
- artifactType: input
  details:
    type: checkbox
    label: "Please select the departments that will use the model?"
    options:
      - label: "Sales"
        value: "DEPT001"
      - label: "Customer Success"
        value: "DEPT002"
----

==== Date input
Defines a date field with a start date and customizable format (e.g., YYYYMMDD, ISO8601).

[source,yaml]
----
yaml
- artifactType: input
  details:
    type: date
    label: "What is the scheduled release date?"
    startDate: 20240612
    format: ISO8601
----

==== Numeric input
Defines a numeric field with optional minimum and maximum values to constrain the input range.

[source,yaml]
----
yaml
- artifactType: input
  details:
    type: numeric
    label: "What is the allowed F score for the model to be deployed?"
    min: 0
    max: 1
----

=== Guidance artifacts
Provide users with informational content using textblock type, which displays Markdown-formatted text.

[source,yaml]
----
yaml
- artifactType: guidance
  details:
    type: textblock
    text: >-
      [Map 1.4](https://ournistpolicyreferenceurl.com) The business value or
      context of business use has been clearly defined or - in the case of
      assessing existing AI systems - re-evaluated
----

Display prominent text banners to provide important notices or key information to users.

[source,yaml]
----
yaml
- artifactType: guidance
  details:
    type: banner
    text: >-
      [Map 1.4](https://ournistpolicyreferenceurl.com) The business value or
      context of business use has been clearly defined or - in the case of
      assessing existing AI systems - re-evaluated
----

=== Approvals
Approvals are defined under stages. Each approval is defined with a name, a group of specified approvers, and evidence. Approvers must be Domino users or organizations and are specified by the user’s or organization’s name.

[source,yaml]
----
yaml
- name: 'Stage 4: validation sign off'
  approvers:
    - model-gov-org
  evidence:
    id: Local.validation-approval-body
    name: Sign-off
    description: The checklist for approvals
    definition:
      - artifactType: input
        details:
          label: "Have you read the model validation reports?"
          type: radio
          options:
            - Yes
            - No
----

=== Classification
A top-level policy variable used to assign risk tiers (e.g., low, medium, high) to a Governed Bundle, with support for tooltips, rules, and artifact-based references.

[source,yaml]
----
yaml
classification:
  rule:
  artifacts:
    - model-risk

stages:
  - name: classificationExample
      - id: Local.model-risk
        name: Model Risk
        description: Describe the risk of the model
        definition:
          - artifactType: input
            aliasForClassification: model-risk
            details:
              label: "How would you rate the model risk?"
              type: radio
              options:
                - High
                - Low
              tooltip: guidance on how to rate the model risk within the organization
----

==== Classification rule
Classification rules allow the construction of complex operations based on the outcome of multiple pieces of evidence.

[source,python]
----
func() string {
    var sum float64
    for _, value := range inputs {
        sum += value
    }
    if sum >= 1 {
        return "High"
    }
    return "Low"
}()
----

==== Visibility Rules
Control whether specific evidence sets are shown, based on conditions like risk classification (e.g., only display for high-risk bundles).

[source,yaml]
----
yaml
evidenceSet:
- id: Global.type-of-development
    visibilityRule: inputs=="High"
----


----- user_guide/domino-governance/workflows.txt -----
:page-permalink: f8e3f8
:page-version: 6.1
:page-title: Domino Governance workflows
:page-sidebar: Governance workflows
:page-order: 10

Domino governance workflows refer to the people, processes, and systems implemented to manage and regulate the use of the Domino data science platform within an organization.

There are a few distinct workflows for Domino Governance, depending on whether you are a policy approver, a data science practitioner, a SysAdmin, or a Governance admin.

image::/images/6.0/governance/governance-workflow.png[alt="Workflow for Domino Governance", width=1000, role=noshadow]

[cols="1a,4a"]
|===
|*Practitioners*
|As a data science practitioner, you answer questions, provide information related to policies, and offer evidence.
Practitioners cannot create policies or request approvals; they can only adhere to existing policies and contribute to governed bundles.
If you are a practitioner, link:d56edd[Create governed bundles] has steps for creating bundles, sending them for approval, and transitioning them to publishing.

|*Approvers*
|Once listed in a policy or organization, you become an approver.
You can access projects with a created bundle and gain a consumer role.
When a review is requested, you'll receive a task assignment.
If you are an approver, link:1ce615[Send governed bundles for reviews] has information on how to approve bundles.

|*Administrators*
|These roles can include `GovernanceAdmins` and `SysAdmins`. *Governance admins* have special permissions to create, edit, and publish policies, which require approval before publication.
They can also access the governance dashboard, audit trail, compliance monitoring, and governance APIs.
If you are an administrator, link:cd4f06[Create policies for Domino Governance] has steps on how to work with policies.
|===

== Next steps

* link:d56edd[Create governed bundles.]
* link:cd4f06[Define policies for Domino Governance.]

----- user_guide/external-data-volumes/index.txt -----
:page-version: 6.1
:page-title: External Data Volumes (EDVs)
:page-sidebar: External Data Volumes (EDVs)
:page-permalink: f12554
:page-order: 120

External Data Volumes (EDVs) are volumes from network-attached storage systems that are mounted to the Domino system.
Domino can register EDVs and attach them to projects.
When an EDV is attached to a project, Domino mounts its file system when running code in that project (such as in a workspace or a job).

Once an EDV is link:9c3564[registered by a Domino admin], Domino automatically mounts it and you can link:ee8d01[add it to your project].

image::/images/5.7/edv.png[EDV mounted in Domino, role=noshadow, width=500]

== Supported EDV types

Domino supports the following data volumes:

* Network File Systems
 (generic NFS and AWS EFS)
* Windows Share (SMB)

== EDV support in Domino executions

External Data Volumes are supported in the following Domino executions:

* link:942549[Jobs] (including link:673577[Scheduled Jobs])
* link:867b72[Workspaces]
* link:71635d[Apps]
* link:f4e1e3[Launchers]
* link:8b4418[On-demand clusters]

== Limitations

* Domino endpoints do not support External Data Volumes.
* External Data Volume actions are not exposed by the Domino Platform API or Domino CLI.
* SMB is disabled by default.
See link:4cdae9[Set up Kubernetes PV and PVC] for more details.

== Next steps

link:ee8d01[Add EDVs to Projects.]

----- user_guide/external-data-volumes/use-a-mounted-volume.txt -----
:page-version: 6.1
:page-title: Add EDVs to Projects
:page-permalink: ee8d01
:page-order: 10

If a Domino admin has registered an external data volume (EDV) and shared it with you, you can add it to any of your projects.
When you remove an EDV from a project, it remains mounted and available to other projects in Domino.

== Add an EDV to a Project
//tag::add-edv[]
. In your project, go to *Data* and click *External data volumes*.
+
image::/images/6.0/navigate-to-edv.png[alt="Navigate to External data volumes", width=500, role=noshadow]

. Click *Add External Volume*.

. Select an EDV from the list.

. Click *Add*.
//end::add-edv[]

If you cannot view, search for, or select External data volumes, they are not registered in your deployment or you do not have access privileges.
Contact your Domino administrator for assistance.

If your volume is successfully mounted, it is listed in your project at *Data* > *External data volumes*.
The table indicates which link:e6e601[data planes] have access to this EDV:

image::/images/6.0/mounted-edv-list.png[alt="List of the mounted external data volumes", width=800, role=noshadow]

== Remove an EDV from a Project

. In your project, go to *Data* > *External data volumes*.

. Click the three vertical dots at the end of the row for the volume in the table, then click *Remove*.
+
image::/images/6.0/remove-edv.png[alt="Where to click to remove an external data volume", width=800, role=noshadow]

== Use an EDV in a Project

By default, EDVs that are mounted to your project (and that you have access to) are also automatically mounted in supported executions.
To access a volume in an execution, reference the mount path of the volumes.

[[mount-specific-edvs]]
Choose to mount specific EDVs for your execution. The following example shows specific EDVs being selected in the Workspace startup modal.

image::/images/6.0/selectable-edv.png[alt="Selectable external data volumes", width=700, role=noshadow]

TIP: Each EDV used by an execution must be successfully bound to the execution during startup. If any binding issues occur, the execution will not start. Additionally, each EDV may introduce latency to the startup process. To optimize startup time and avoid unnecessary startup failures, select only the necessary EDVs for your task.

=== View all mounted volumes in a Project

In your project, go to *Data* > *External data volumes*.

Each volume's properties are shown in the table.

* *Name* – An alias for the volume, set by your Domino administrator.
* *Type* – The type of volume.
Domino supports
NFS, AWS EFS, and Windows Share (SMB).
* *Description* – A description of the volume, set by your Domino administrator.
* *Mount Path* – The mount path of the volume: `/domino/edv/name-of-volume`.
Use this mount path when using the volume in a link:ee8d01[Job, Workspace, or other supported Domino execution].
* *Data Plane* - In link:c65074[Domino Nexus deployments], the link:95520d[data plane] where the volume is mounted.

[IMPORTANT]
====
If a volume is greyed out, you do not have privileges to use it.
Your project can also contain volumes that are mounted, but not listed.
You will see a message in the application for both these situations.
If you need access to these volumes, contact your Domino administrator.
====

== Next steps

* Learn about link:7ec608[EDV security and sharing].

----- user_guide/external-tools/automate-apache-airflow.txt -----
:page-version: 6.1
:page-title: Orchestrate Jobs with Apache Airflow
:page-sidebar: Apache Airflow Job orchestration
:page-permalink: e4f67f
:page-order: 10

Data science projects often require multiple steps to go from raw data to useful data products.
These steps tend to be sequential, and involve things like:

* Sourcing data
* Cleaning data
* Processing data
* Training models

After you understand the steps necessary to deliver results from your work, it's useful to automate them as a repeatable pipeline.
Domino can link:5dce1f[schedule Jobs], but for more complex pipelines you can pair Domino with an external scheduling system like https://airflow.apache.org/[Apache Airflow].

This topic describes how to integrate Airflow with Domino by using the https://github.com/dominodatalab/python-domino[python-domino] package.

== Get started with Airflow

Airflow is an open-source platform to author, schedule, and monitor pipelines of programmatic tasks.
You can define pipelines with code and configure the Airflow scheduler to execute the underlying tasks.
You can use the Airflow application to visualize, monitor, and troubleshoot pipelines.

If you are new to Airflow, read the https://airflow.apache.org/start.html[Airflow QuickStart^] to set up your own Airflow server.

There are many options for configuring your Airflow server, and for pipelines that can run parallel tasks, you must use Airflow's
https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/executor/local.html[LocalExecutor mode^].
In this mode, you can run tasks in parallel and execute multiple dependencies at the same time.
Airflow uses a database to keep records of all the tasks it executes and schedules, so you must install and configure a SQL database for LocalExecutor mode.

Read http://stlong0521.github.io/20161023%20-%20Airflow.html[A Guide On How To Build An Airflow Server/Cluster^] to learn more about setting up LocalExecutor mode.

For more information about scheduling and triggers, notifications, and pipeline monitoring, read the https://airflow.apache.org/index.html[Airflow documentation^].

== Install `python-domino` on your Airflow workers

To create Airflow tasks that work with Domino, you must install https://github.com/dominodatalab/python-domino[python-domino^] on your Airflow workers.
Use this library to add tasks in your pipeline code that interact with the link:8c929e[Domino Platform API] to start Jobs.

Connect to your Airflow workers, and follow these steps to install and configure `python-domino`:

. Install from pip
+
`pip install dominodatalab[airflow]`

. Set up an https://airflow.apache.org/docs/apache-airflow/stable/concepts.html#variables[Airflow variable^] to point to the Domino host.
This is the URL where you load the Domino application in your browser.
+
[verse]
--
*Key*: `DOMINO_API_HOST`
*Value*: `<your-domino-url>`
--
. Set up an https://airflow.apache.org/docs/apache-airflow/stable/concepts.html#variables[Airflow variable^] to store the link:40b91f[user API key] you want to use with Airflow.
This is the user Airflow with authentication to Domino to start Jobs.
+
[verse]
--
*Key*: `DOMINO_API_KEY`
*Value*: `<your-api-key>`
--

== How Airflow tasks map to Domino Jobs

Airflow pipelines are defined with Python code.
This fits in well with Domino's code-first philosophy.
You can use https://github.com/dominodatalab/python-domino[python-domino^] in your pipeline definitions to create tasks that start Jobs in Domino.

Architecturally, Airflow has its own server and worker nodes, and Airflow will operate as an independent service that sits outside of your Domino deployment.
Airflow will need network connectivity to Domino so its workers can access the link:8c929e[Domino Platform API] to start Jobs in your Domino Project.
All the code that performs the actual work in each step of the pipeline -- code that fetches data, cleans data, and trains data science models -- is maintained and versioned in your Domino Project.
This way you have Domino's Reproducibility Engine working together with Airflow's scheduler.

image::/images/4.x/airflow-pipeline.png[alt="Airflow pipeline", width=800]

== Next steps

Learn how to link:5dce1f[schedule Jobs] and link:e1c37a[view Job results].

----- user_guide/external-tools/automate-kubeflow.txt -----
:page-version: 6.1
:page-permalink: 866fae
:page-title: Orchestrate Jobs with Kubeflow
:page-sidebar: Kubeflow Job orchestration
:page-order: 20

Data science projects often require multiple steps to go from raw data to useful data products. These steps tend to be sequential, and involve things like:

* Sourcing data
* Cleaning data
* Processing data
* Training models

After you understand the steps necessary to deliver results from your work, it's useful to automate them as a repeatable pipeline. Domino can link:5dce1f[schedule Jobs], but for more complex pipelines you can pair Domino with an external scheduling system like link:https://www.kubeflow.org/docs/started/introduction/[Kubeflow^].

This topic describes how to integrate Kubeflow with Domino by using the link:https://github.com/dominodatalab/python-domino[python-domino package^].

== Get started with Kubeflow

Kubeflow is an open-source platform designed for machine learning workflows on Kubernetes. It facilitates the orchestration of ML workflows, allowing you to define and manage machine learning pipelines.

If you're new to Kubeflow, explore the Kubeflow documentation for link:https://www.kubeflow.org/docs/started/installing-kubeflow/[setting up and configuring your Kubeflow environment^].

Configuring your Kubeflow environment might involve setting up a Kubernetes cluster, establishing the necessary networking, and defining resources for your pipeline execution.

For more detailed information on setting up Kubeflow and managing your machine learning workflows, refer to the link:https://www.kubeflow.org/docs/started/introduction/[Kubeflow documentation^].

== Create pipelines

To create Kubeflow pipelines that interact with Domino, you'll need to install the python-domino package on your Kubeflow cluster.

To install the required package, follow these steps:

. Access your Kubeflow cluster.
. Install the domino-kubeflow package using the following command:
+
[source,bash]
----
pip install dominodatalab
----
+
Configure the connection between Kubeflow and Domino by setting up environment variables that point to the Domino host and store your link:d982cc[Domino API key].
+
. Set an environment variable to point to the Domino host:
+
[source,bash]
----
export DOMINO_API_HOST=<your-domino-url>
----
+
. Store the user API key you want to use to authenticate into Domino:
+
[source,bash]
----
export DOMINO_API_KEY=<your-api-key>
----

== Map Kubeflow pipelines to Domino Jobs

Domino's code-first approach aligns well with defining tasks within Kubeflow pipelines to initiate Jobs in Domino.

Architecturally, Kubeflow pipelines are defined, managed, and operated within a Kubernetes cluster, separate from your Domino Environment. Kubeflow will require network connectivity to access the Domino API for executing Jobs in your Domino Projects. The code for each step within the pipeline, such as data retrieval, cleaning, and model training, is stored and versioned in your Domino Project. This integration ensures collaboration between Kubeflow's orchestration capabilities and Domino's reproducibility features.

== Next steps

Learn how to link:5dce1f[schedule Jobs] and link:e1c37a[view Job results].

----- user_guide/external-tools/index.txt -----
:page-permalink: 752b35
:page-version: 6.1
:page-title: Orchestrate with external tools
:page-order: 278

link:e4f67f[Orchestrate Jobs with Apache Airflow]::
Pair Domino with an external scheduling system like Apache Airflow to automate complex pipelines as a repeatable pipeline.

link:866fae[Orchestrate Jobs with Kubeflow]::
Pair Domino with an external scheduling system like Kubeflow to automate complex pipelines as a repeatable pipeline.

----- user_guide/feedback.txt -----
:page-version: 6.1
:page-permalink: fa5356
:page-title: Send feedback
:page-order: 380

Your input helps us prioritize development work that makes a difference to our customers.
You can send feedback to Domino Data Lab directly from the Domino UI.

You can suggest new features or improvements, tell us about your experience using the product, and include screenshots to illustrate your feedback.
Optionally, you can consent to being contacted by Domino for further discussion.

Click *Account > Feedback* in the top navigation to send your feedback to the Domino team.

NOTE: The feedback feature does not open a support ticket.
For help with technical issues, link:80328c[contact our Technical Support team].

----- user_guide/flows/advanced-flows.txt -----
:page-version: 6.1
:page-permalink: c9731d
:page-title: Advanced Flows
:page-sidebar: Advanced Flows
:page-order: 80

There are advanced capabilities that you can use in Domino Flows.

It is recommended that you familiarize yourself with some of the core functionality before using the following advanced capabilities.

[[caching]]
== Caching

Caching allows you to avoid redundant computations by reusing cached results from previous executions. You can enable caching at the task level by configuring parameters in the task definition.

Whether a task execution uses the cache is determined by the state of the cache and the task execution's task metadata, cache version, and cache key.

For a task execution to use the cache instead of re-computing the result, caching must be turned on for the task (more on that later) and there must be a result in the cache for the exact same:

. Domino Project.
. Task name. This is the `name` kwarg to the `DominoJobTask()`.
. Task cache version. The cache version is formed by combining:
+
* The "Flyte task cache version".
** If the `cache` kwarg to the `DominoJobTask()` is a boolean, then the Flyte task cache version is the `cache_version` kwarg to the `DominoJobTask()`.
** If instead the `cache` kwarg is a `Cache` object, then the Flyte task cache version is the result of calling the `Cache` object's `get_version()` method.
* The "domino job config task cache version", which comes from arguments to the task's `DominoJobConfig()`. All args affect the version except:
** `Title`
** `VolumeSizeGiB`
** `ComputeClusterProperties.WorkerHardwareTierId`
** `HardwareTierId`
** `ComputeClusterProperties.WorkerCount`
** `ComputeClusterProperties.WorkerStorageGiB`
** `ComputeClusterProperties.MaxWorkerCount`
** `ComputeClusterProperties.MasterHardwareTierId`
* The task's input and output names and types, which come from the `inputs` and `outputs` kwargs to the `DominoJobTask()`.
+
. Task cache key. The cache key is formed from the task execution's runtime input values.

If any of the above parameters change between task executions such that the new combination does not have a result in the cache, then the subsequent execution will not use the cache.
Conversely, if caching is enabled for a task and there is a result in the cache with the exact same above parameters, then that result will be reused instead of executing the task again.
Note that "workflow" is not mentioned above. So, two separate tasks in two different (or even the same) workflows can re-use the same result.

By default, caching is disabled for tasks. There are two ways to enable caching for a task.

The first way is to set the `cache` parameter to the boolean value `True` and specify a `cache_version` to use:

[source, python]
----
@workflow
def cache_bool_workflow(a: int, b: int) -> float:

    # Create first task w/ caching enabled via the boolean True
    add_task = DominoJobTask(
        name='Add numbers',
        domino_job_config=DominoJobConfig(Command="python add.py"),
        inputs={'first_value': int, 'second_value': int},
        outputs={'sum': int},
        cache=True, # This boolean enables caching
        cache_version="1.0", # Bumping the version is akin to invalidating the cache
        use_latest=True
    )
    sum = add_task(first_value=a, second_value=b)

    # Create second task w/ caching disabled
    sqrt_task = DominoJobTask(
        name='Square root',
        domino_job_config=DominoJobConfig(Command="python sqrt.py"),
        inputs={'value': int},
        outputs={'sqrt': float},
        use_latest=True
    )
    sqrt = sqrt_task(value=sum)

    return sqrt
----

When running the example above with arbitrary workflow inputs `a=X` and `b=Y`, both tasks will execute initially. Then, subsequent reruns of the flow with `a=X` and `b=Y` will use the cache for the first task instead of re-computing the result. This reduces overall compute cost and flow run time.

In the above example, the `cache_version` parameter forms part of the version key for a task's cached results. Changing the `cache_version` value is a method to force a rerun of the task, even if no other task parameters have changed.

With Flyte 1.15 onward, there is a second way to turn on caching for a task: by setting the `cache` parameter to a `Cache` object:

[source, python]
----
from flytekit.core.cache import Cache, CachePolicy, VersionParameters

# create a class that implements the CachePolicy interface
class SimpleCachePolicy(CachePolicy):
    def get_version(self, salt: str, params: VersionParameters) -> str:
        return "2.0"

@workflow
def cache_obj_workflow(a: int, b: int) -> float:

    # create a Cache object that uses your CachePolicy
    cache_obj = Cache(policies=SimpleCachePolicy())

    # Create first task w/ caching enabled via the Cache object
    add_task = DominoJobTask(
        name='Add numbers',
        domino_job_config=DominoJobConfig(Command="python add.py"),
        inputs={'first_value': int, 'second_value': int},
        outputs={'sum': int},
        cache=cache_obj, # This kwarg enables caching
        # the cache_version kwarg is not used when the cache kwarg is a Cache object
        use_latest=True
    )
    sum = add_task(first_value=a, second_value=b)

    # Create second task w/ caching disabled
    sqrt_task = DominoJobTask(
        name='Square root',
        domino_job_config=DominoJobConfig(Command="python sqrt.py"),
        inputs={'value': int},
        outputs={'sqrt': float},
        use_latest=True
    )
    sqrt = sqrt_task(value=sum)

    return sqrt
----

Under the hood, the `Cache` object's `get_version()` method is called to get the "Flyte task cache version" discussed above.
In the above example, calling `get_version()` on the `Cache` object calls the cache policy's `get_version()` method, which returns "2.0".
So, in this simple example, bumping the string returned by `SimpleCachePolicy.get_version()` is akin to bumping `cache_version` when the `cache` kwarg is a boolean instead of a `Cache` object.

You can choose to have the system ignore some parameters when calculating the cache version and cache key.

* To ignore specific task inputs when the `cache` kwarg is a boolean, use the `cache_ignore_input_vars` kwarg to `DominoJobTask()`.
** `cache_ignore_input_vars` takes a tuple of strings corresponding to variable names. Any task input whose name appears in the list is excluded from the cache key calculation.
** In the first code example above, providing `cache_ignore_input_vars=("second_value",)` means that a task execution could use a cached result even if the runtime value of `second_value` is different from the runtime value of the execution that originally wrote the result to the cache.
* To ignore specific task inputs when the `cache` kwarg is a `Cache` object, use the `ignored_inputs` kwarg to `Cache()`.
** `ignored_inputs` functions the same way as `cache_ignore_input_vars`.
* To ignore specific args to the `DominoJobConfig()`, use the `cache_ignore_vars` kwarg to `DominoJobConfig()`.
** `cache_ignore_vars` takes a tuple of strings corresponding to `DominoJobConfig()` args (`"CommitId"`, `"EnvironmentId"`, etc.). Args in the tuple will be excluded from the cache version calculation.
** So, providing `cache_ignore_vars=("EnvironmentId",)` means that a task execution could use a cached result even if its `DominoJobConfig.EnvironmentId` is different from the execution that originally wrote the result to the cache.

You can see which flow tasks have or have not been recomputed by the caching symbol that appears in the flow graph view.

image::/images/6.0/flows/flow-caching.png[alt="Cache", width=1000, role=noshadow]

NOTE: Caching is different than recovering from failures. While recovery retrieves the actual outputs that were written to the blob storage, caching uses the task definition to compose a unique key that is used to retrieve results from an actual cache. This means that cached results can be used across multiple flows if the task definitions are the same.

== Subflows

Subflows allow you to create nested flows by packaging a whole flow into a node. Subflows are useful when you want to abstract away complex flows into modular components that can be easily inserted into a broader flow. This also enables collaboration via assigning responsibilities at a subflow level.

To use subflows:

. Start by defining your subflow like you would define any other flow. As an example, we will use the flow created in the link:5b5259[getting started] guide as our subflow.

+
[source, python]
----
@workflow
def math_subflow(a: int, b: int) -> float:

    # Create first task
    add_task = DominoJobTask(
        name='Add numbers',
        domino_job_config=DominoJobConfig(Command="python add.py"),
        inputs={'first_value': int, 'second_value': int},
        outputs={'sum': int},
        use_latest=True
    )
    sum = add_task(first_value=a, second_value=b)

    # Create second task
    sqrt_task = DominoJobTask(
        name='Square root',
        domino_job_config=DominoJobConfig(Command="python sqrt.py"),
        inputs={'value': int},
        outputs={'sqrt': float},
        use_latest=True
    )
    sqrt = sqrt_task(value=sum)

    return sqrt
----

. In the same file, create your main flow that contains the subflow as a node.

+
[source, python]
----
@workflow
def math_subflow(a: int, b: int) -> float:

    # Create first task
    add_task = DominoJobTask(
        name='Add numbers',
        domino_job_config=DominoJobConfig(Command="python add.py"),
        inputs={'first_value': int, 'second_value': int},
        outputs={'sum': int},
        use_latest=True
    )
    sum = add_task(first_value=a, second_value=b)

    # Create second task
    sqrt_task = DominoJobTask(
        name='Square root',
        domino_job_config=DominoJobConfig(Command="python sqrt.py"),
        inputs={'value': int},
        outputs={'sqrt': float},
        use_latest=True
    )
    sqrt = sqrt_task(value=sum)

    return sqrt


@workflow
def simple_math_flow(a: int, b: int):

    # Call subflow
    sqrt_of_sum = math_subflow(a=a, b=b)

    # Create first task
    random_task = DominoJobTask(
        name='Do something else',
        domino_job_config=DominoJobConfig(Command="sleep 10"),
        inputs={'subflow_output': float},
        use_latest=True
    )
    random_result = random_task(subflow_output=sqrt_of_sum)

    return
----

. Trigger an execution of the main flow in the same way you would trigger any other flow.

+
[source, bash]
----
pyflyte run --remote workflow.py simple_math_workflow --a 4 --b 5
----

. Monitor the results. Notice how the subflow is shown as its own node in the broader flow.

+
image::/images/5.11/flows/subflows.png[alt="Domino Subflow Graph", width=1200, role=noshadow]


== Approval flow

Flows allow you to add tasks in your flow where it will pause and wait for explicit human approval before proceeding. A use case where this can be useful is when model training has finished and you want someone to review the results before proceeding to downstream tasks that prepare it for deployment.

To use approvals in a flow:

. Use the `approve` class provided by the Flyte SDK to add a required approval on the output of a task. As an example, we will use the flow created in the link:5b5259[getting started] guide.

+
[source, python]
----
from flytekit import approve
from datetime import timedelta

@workflow
def approval_flow(a: int, b: int) -> float:

    # Create first task
    add_task = DominoJobTask(
        name='Add numbers',
        domino_job_config=DominoJobConfig(Command="python add.py"),
        inputs={'first_value': int, 'second_value': int},
        outputs={'sum': int},
        use_latest=True
    )

    # Approval is added here
    sum = approve(add_task(first_value=a, second_value=b), "Approval", timeout=timedelta(hours=2))

    # Create second task
    sqrt_task = DominoJobTask(
        name='Square root',
        domino_job_config=DominoJobConfig(Command="python sqrt.py"),
        inputs={'value': int},
        outputs={'sqrt': float},
        use_latest=True
    )
    sqrt = sqrt_task(value=sum)

    return sqrt
----

. Trigger an execution of the main flow in the same way you would trigger any other flow.

+
[source, bash]
----
pyflyte run --remote workflow.py simple_math_workflow --a 4 --b 5
----

. Monitor the results. Notice how the flow execution will pause after the first task and wait for approval. Click on the *Resume* button.

+
image::/images/5.11/flows/approval.png[alt="Approval Flow Paused", width=1000, role=noshadow]

. Review the result and approve it to continue execution, or reject it to stop the execution.

+
image::/images/5.11/flows/resume-execution.png[alt="Resume Modal", width=600, role=noshadow]

----- user_guide/flows/define-flow-artifacts.txt -----
:page-version: 6.1
:page-permalink: 6a57f5
:page-title: Define Flow Artifacts
:page-sidebar: Define Flow Artifacts
:page-order: 30

A complex flow may execute many steps, producing hundreds of outputs, the majority of which are typically intermediate computations and not final results. It therefore becomes useful to elevate outputs of flow executions to easily discover, reuse, and inspect them, including the lineage of how they were produced. These task or workflow outputs are called Flow Artifacts.

image::/images/5.11/flows/artifacts-inspect.png[alt="Inspect artifacts produced by a run", width=1000, role=noshadow]

Flow Artifacts are explicitly defined in code by flow definition authors.

The `Artifact` method is used to generate a named artifact. The named artifact can have individual files added to it _anywhere_ in the workflow definition. Typically an `Artifact` is declared at the beginning of a workflow like this:

[source,python]
----
PickleArtifact = Artifact(name="Pytorch Model", type=MODEL)
----

In any part of a workflow where an output might be used, the `.File(name="file.ext")` method can be called to declare that the output is an `Artifact`. For example:

[source,python]
----
@workflow
def single_model() -> PickleArtifact.File(name="model.pkl"):
----

There are 3 types of Flow Artifacts:

* `DATA` artifact: Files that may be added later as a new dataset snapshot.
* `MODEL` artifact: Files that may later be registered as a model.
* `REPORT` artifact: Files that may be part of a collection that make up a report.

In the following example, `DataArtifact.File(name="data.csv")` adds a `FlyteFile` output from the `data_prep_job` to the Artifact declared as `Artifact(name="My Data", type=DATA)`. Unlike the use of `FlyteFile[TypeVar("csv")]` from previous examples, artifact files automatically infer their type from the given file extension.

[source,python]
----
from flytekit import workflow
from flytekit.types.file import FlyteFile
from flytekitplugins.domino.task import DominoJobConfig, DominoJobTask
from flytekitplugins.domino.artifact import Artifact, DATA, MODEL, REPORT

# define the artifact name and type which may be REPORT, DATA or MODEL
DataArtifact = Artifact(name="My Data", type=DATA)
ModelArtifact = Artifact(name="My Model", type=MODEL)

@workflow
def training_workflow(data_path: str) -> ModelArtifact.File(name="model.pt"):

    data_prep_job_config = DominoJobConfig(Command="python prep-data.py")
    data_prep_job = DominoJobTask(
        name='Prep data',
        domino_job_config=data_prep_job_config,
        inputs={'data_path': str},
        outputs={'processed_data': DataArtifact.File(name="data.csv")},
        use_latest=True
    )
    data_prep_results = data_prep_job(data_path=data_path)

    training_job_config = DominoJobConfig(Command="python train-model.py")
    training_job = DominoJobTask(
        name='Train model',
        domino_job_config=training_job_config,
        inputs={'processed_data': FlyteFile[TypeVar("csv")]},
        outputs={'model': FlyteFile[TypeVar("pt")]},
        use_latest=True
    )
    training_results = training_job(processed_data=data_prep_results["processed_data"])

    return training_results["model"] # Final output is returned here
----


There are no limits to the number of artifacts that can be defined, the number of files that can be added to each artifact, or where the artifact files are created. However, an individual artifact cannot include the same filename more than once. The following is not permitted because `DataArtifact.File(name="data.csv")` is used more than once:

[source,python]
----
DataArtifact = Artifact(name="My Data", type=DATA)

@workflow
def training_workflow(data_path: str) -> DataArtifact.File(name="data.csv"):

    data_prep_job_config = DominoJobConfig(Command="python prep-data.py")
    data_prep_job = DominoJobTask(
        outputs={
            # error - DataArtifact already has another data.csv
            'raw_data': DataArtifact.File(name="data.csv"),
            'processed_data': FlyteFile[TypeVar("csv")],
        }
    )
    data_prep_results = data_prep_job(data_path=data_path)

    return data_prep_results["processed_data"] # Final output is returned here
----

[[programmatically-export]]
== Programmatically Exporting to Domino Datasets

Flow Artifacts can be programmatically exported to Domino Datasets. This is useful for enabling continuous, automated movement of new output data into Datasets.

In the following example, `DataArtifact.File(name="data.csv")` is programmatically exported to the Domino Dataset with ID `681d3030ae3a706ef9c7f08b`, using the helper method `run_launch_export_artifacts_task`.

[source,python]
----
from flytekit import workflow
from flytekit.types.file import FlyteFile
from flytekitplugins.domino.task import DominoJobConfig, DominoJobTask
from flytekitplugins.domino.artifact import Artifact, DATA, ExportArtifactToDatasetsSpec, MODEL, REPORT, run_launch_export_artifacts_task

# define the artifact name and type which may be REPORT, DATA or MODEL
DataArtifact = Artifact(name="My Data", type=DATA)
ModelArtifact = Artifact(name="My Model", type=MODEL)

@workflow
def training_workflow(data_path: str) -> ModelArtifact.File(name="model.pt"):

    data_prep_job_config = DominoJobConfig(Command="python prep-data.py")
    data_prep_job = DominoJobTask(
        name='Prep data',
        domino_job_config=data_prep_job_config,
        inputs={'data_path': str},
        outputs={'processed_data': DataArtifact.File(name="data.csv")},
        use_latest=True
    )
    data_prep_results = data_prep_job(data_path=data_path)

    training_job_config = DominoJobConfig(Command="python train-model.py")
    training_job = DominoJobTask(
        name='Train model',
        domino_job_config=training_job_config,
        inputs={'processed_data': FlyteFile[TypeVar("csv")]},
        outputs={'model': FlyteFile[TypeVar("pt")]},
        use_latest=True
    )
    training_results = training_job(processed_data=data_prep_results["processed_data"])

    # Programmatic export is enabled here
    run_launch_export_artifacts_task(
        spec_list=[
            ExportArtifactToDatasetsSpec(
                artifact=DataArtifact,
                dataset_id="681d3030ae3a706ef9c7f08b",
            ),
            # ... More exports can be defined in this list, if needed.
        ],
        environment_name="Domino Standard Environment",
        hardware_tier_id="small-k8s",
        use_project_defaults_for_omitted=True,
    )

    return training_results["model"] # Final output is returned here
----

The helper method `run_launch_export_artifacts_task` performs the programmatic export.

It is required to specify this task to use the link:0d73c6[Domino Standard Environment (DSE)] from 6.0 onwards, or a custom environment that is built on top of the DSE >= 6.0, as these contain the required Flyte Python and jq dependencies.

The full list of available parameters to the method `run_launch_export_artifacts_task` include:

[cols="1a,1a,3a",options="header"]
|===
| Parameter| Type| Description and Example

| spec_list
| List[ExportArtifactToDatasetsSpec]
| The list of specifications used to determine the Flows artifacts to be exported to the Domino Datasets. +
_Example:_ +
`[ExportArtifactToDatasetsSpec(artifact=DataArtifact, dataset_id="681d3030ae3a706ef9c7f08b")]`

| use_project_defaults_for_omitted
| bool
| If set to `True`, this will use the latest project defaults for parameters that were not explicitly provided. For better reproducibility, it is recommended to set this to `False` and explicitly define the necessary parameters. +
_Example:_ +
`False`

| environment_name
| Optional[str]
| Name of the Environment to use in the Domino Job. +
_Example:_ +
`Domino Standard Environment`

| environment_id
| Optional[str]
| ID of the Environment to use in the Domino Job. This is recommended over using `environment_name` to prevent breaking reproducibility when environment names get changed. +
_Example:_ +
`6646530dcbd`

| environment_revision_id
| Optional[str]
| A specific revisionId of the environment to use. +
_Example:_ +
`6659daf5fc8`

| hardware_tier_name
| Optional[str]
| Name of the hardware tier to use in the Domino Job. It is recommended to use a minimal-resource hardware tier. +
_Example:_ +
`Large`

| hardware_tier_id
| Optional[str]
| ID of the hardware tier to use in the Domino Job. This is recommended over using `hardware_tier_name` to prevent breaking reproducibility when hardware tier names get changed. It is recommended to use a minimal-resource hardware tier. +
_Example:_ +
`small-k8s`

| retries
| int
| Number of times to retry this task during a workflow execution. +
_Example:_ +
`0`

| timeout
| Union[timedelta, int] = timedelta(hours=3)
| The maximum amount of time for which one execution of this task should be executed. The execution will be terminated if the runtime exceeds the given timeout. It is recommended to set this timeout duration to be greater than the workflow duration. +
_Example:_ +
`timedelta(hours=3)`

|===

== Additional Examples

* *Data artifact with a single file:*
+
_Example scenario:_ A flow produces a single ADaM dataset with the file name `adae.sas7bdat` and the user wants to track the single file under its own data entity called `adae`.
+
.Expand for example code.
[%collapsible]
====
[source,python]
----
from typing import TypeVar
from flytekitplugins.domino.helpers import Output, run_domino_job_task
from flytekitplugins.domino.artifact import Artifact, DATA
from flytekit import workflow
from flytekit.types.file import FlyteFile

DataArtifact = Artifact("adae", DATA)

@workflow
def single_adam() -> DataArtifact.File(name="adae.sas7bdat"):
    return run_domino_job_task(
        flyte_task_name="Produce single ADaM Dataset",
        command="single_adam.py",
        output_specs=[
            Output(name="adae", type=FlyteFile[TypeVar("sas7bdat")]),
        ],
        use_project_defaults_for_omitted=True,
    )
----
====

* *Data artifact with multiple files:*
+
_Example scenario:_ A flow produces multiple ADaM datasets (`adae.sas7bdat`, `advs.sas7bdat`, and `adsl.sas7bdat`) and the user wants to track the collection of files under a single data entity called `adam`.
+
.Expand for example code.
[%collapsible]
====
[source,python]
----
from typing import Tuple, TypeVar
from flytekitplugins.domino.helpers import Output, run_domino_job_task
from flytekitplugins.domino.artifact import Artifact, DATA
from flytekit import workflow
from flytekit.types.file import FlyteFile

DataArtifact = Artifact("adam", DATA)

@workflow
def multiple_adam() -> Tuple[
    DataArtifact.File(name="adae.sas7bdat"),
    DataArtifact.File(name="advs.sas7bdat"),
    # if name does not include the extension, then provide the type kwarg
    DataArtifact.File(name="adsl dataset", type="sas7bdat"),
]:
    # files in an Artifact can be produced by different Flows tasks
    # in this example, one task produces two of the files, and another task produces the third
    adae_dataset, advs_dataset = run_domino_job_task(
        flyte_task_name="Produce adae and advs Datasets",
        command="produce_adae_and_advs.py",
        output_specs=[
            Output(name="adae", type=FlyteFile[TypeVar("sas7bdat")]),
            Output(name="advs", type=FlyteFile[TypeVar("sas7bdat")]),
        ],
        use_project_defaults_for_omitted=True,
    )
    adsl_dataset = run_domino_job_task(
        flyte_task_name="Produce adsl Dataset",
        command="produce_adsl.py",
        output_specs=[
            Output(name="adsl", type=FlyteFile[TypeVar("sas7bdat")]),
        ],
        use_project_defaults_for_omitted=True,
    )
    return adae_dataset, advs_dataset, adsl_dataset
----
====

* *Model artifact with a single file:*
+
_Example scenario:_ A flow produces a single model file with the name `model.pkl` and the user wants to track the single file as its own model entity.
+
.Expand for example code.
[%collapsible]
====
[source,python]
----
from typing import TypeVar
from flytekitplugins.domino.helpers import Output, run_domino_job_task
from flytekitplugins.domino.artifact import Artifact, MODEL
from flytekit import workflow
from flytekit.types.file import FlyteFile

ModelArtifact = Artifact("My Model", MODEL)

@workflow
def single_model() -> ModelArtifact.File(name="model.pkl"):
    return run_domino_job_task(
        flyte_task_name="Produce model",
        command="produce_model.py",
        output_specs=[
            # name of the Output can differ from the name of the ArtifactFile
            Output(name="my_model", type=FlyteFile[TypeVar("pkl")]),
        ],
        use_project_defaults_for_omitted=True,
    )
----
====

* *Model artifact with multiple files:*
+
_Example scenario:_ A flow produces multiple files relating to a model (`model.pkl`, `classes.txt`) and the user wants to track the collection of files as a single model entity.
+
.Expand for example code.
[%collapsible]
====
[source,python]
----
from typing import Tuple, TypeVar
from flytekitplugins.domino.helpers import Output, run_domino_job_task
from flytekitplugins.domino.artifact import Artifact, MODEL
from flytekit import workflow
from flytekit.types.file import FlyteFile

ModelArtifact = Artifact("My Model", MODEL)

@workflow
def multiple_model() -> Tuple[
    ModelArtifact.File(name="model.pkl"),
    # if name does not include the extension, then provide the type kwarg
    ModelArtifact.File(name="classes", type="txt"),
]:
    return run_domino_job_task(
        flyte_task_name="Produce model with classes",
        command="produce_model_with_classes.py",
        output_specs=[
            # name of the Output can differ from the name of the ArtifactFile
            Output(name="my_model", type=FlyteFile[TypeVar("pkl")]),
            Output(name="my_classes", type=FlyteFile[TypeVar("txt")]),
        ],
        use_project_defaults_for_omitted=True,
    )
----
====

* *Report artifact with a single file:*
+
_Example scenario:_ A flow produces a single TFL report with the file name `t_vscat.pdf` and the user wants to track the single file as its own report entity.
+
.Expand for example code.
[%collapsible]
====
[source,python]
----
from typing import TypeVar
from flytekitplugins.domino.helpers import Output, run_domino_job_task
from flytekitplugins.domino.artifact import Artifact, REPORT
from flytekit import workflow
from flytekit.types.file import FlyteFile

ReportArtifact = Artifact("TFL Report", REPORT)

@workflow
def single_report() -> ReportArtifact.File(name="t_vscat.pdf"):
    return run_domino_job_task(
        flyte_task_name="Produce PDF",
        command="produce_t_vscat.py",
        output_specs=[
            Output(name="t_vscat", type=FlyteFile[TypeVar("pdf")]),
        ],
        use_project_defaults_for_omitted=True,
    )
----
====

* *Report artifact with multiple files:*
+
_Example scenario:_ A flow produces multiple TFL reports (`t_vscat.pdf`, `t_ae_rel.pdf`) at different steps in the workflow and the user wants to track the collection of files as a single report entity.
+
.Expand for example code.
[%collapsible]
====
[source,python]
----
from typing import Tuple, TypeVar
from flytekitplugins.domino.helpers import Output, run_domino_job_task
from flytekitplugins.domino.artifact import Artifact, REPORT
from flytekit import workflow
from flytekit.types.file import FlyteFile

ReportArtifact = Artifact("TFL Reports", REPORT)

@workflow
def multiple_report():
    # files in an Artifact can be produced by different Flows tasks
    # in this example, one task produces one file, and another task produces the other
    vscat_pdf = run_domino_job_task(
        flyte_task_name="Produce vscat PDF",
        command="produce_t_vscat.py",
        output_specs=[
            Output(name="t_vscat", type=ReportArtifact.File(name="t_vscat.pdf")),
        ],
        use_project_defaults_for_omitted=True,
    )
    ae_rel_pdf = run_domino_job_task(
        flyte_task_name="Produce ae_rel PDF",
        command="produce_t_ae_rel.py",
        output_specs=[
            # if name does not include the extension, then provide the type kwarg
            Output(name="t_ae_rel", type=ReportArtifact.File(name="t_ae_rel tfl report", type="pdf")),
        ],
        use_project_defaults_for_omitted=True,
    )
----
====

Find out more about how to inspect, bookmark, and declare artifacts in link:3db42c[Examine Flow artifacts].

== Next steps

Once you have properly defined the flow and artifacts, learn how to:

* link:113c96[Launch an execution of the flow.]

----- user_guide/flows/define-flows.txt -----
:page-version: 6.1
:page-permalink: e09156
:page-title: Define Flows
:page-sidebar: Define Flows
:page-order: 20

Flow definitions are defined via a code-first approach using Flyte's Python SDK.

== Definition file and class imports

Flows are defined inside a Python file. In this document, we will call the file `workflow.py`, but the name can be anything.

We recommend that you import a few relevant classes after you create the file. This will be important later for defining flows.

[source,python]
----
from flytekit import workflow
from flytekit.types.file import FlyteFile
from typing import TypeVar, NamedTuple
from flytekitplugins.domino.helpers import Input, Output, run_domino_job_task
from flytekitplugins.domino.task import DominoJobConfig, DominoJobTask, GitRef, EnvironmentRevisionSpecification, EnvironmentRevisionType, DatasetSnapshot
from flytekitplugins.domino.artifact import Artifact, DATA, MODEL, REPORT
----

== Define the flow interface

You'll need to define the interface for your flow after you've created a definition file and imported relevant classes.

Defining the interface entails creating a Python method with appropriate decorators and strongly typed inputs/outputs.

NOTE: Explicit, strongly typed inputs and outputs ensure that tasks pass data to each other correctly. For instance, a data processing task may emit a Pandas dataframe as output, while a training task accepts a Pandas dataframe as input. Flows will analyze workflow submissions to prevent user error before execution, providing actionable feedback for incorrectly specified parameters that don't satisfy task contracts.

[source, python]
----
@workflow
def training_workflow(data_path: str) -> NamedTuple("final_outputs", model=FlyteFile[TypeVar("pt")]):

     # Tasks will be defined in here

    return # Outputs will be returned here
----

The following key components are of interest:

- *Flow decorator*: The `@workflow` decorator marks the method as a flow definition.
- *Flow name*: The name of the method (`training_workflow`) is also the name that will be given to the flow when it is registered.
- *Launch inputs*: The parameters to the method are the initial inputs of the flow. They are later passed in as inputs to individual tasks. In the example above, there is one launch input called `data_path` of type `str`.
- *Flow outputs*: The return type to the method are the final outputs of the flow. Outputs from the final tasks are typically returned as the results to the flow. It is recommended to use a `NamedTuple` so that a name will be assigned to each output. This keeps the results more organized and easier to consume in downstream tasks. In the example above, there is one output called `model` of type `FlyteFile[TypeVar("pt")]`. The `pt` value in this case represents the file extension of the output (i.e., the extension of a PyTorch model).

NOTE: Explicitly defining input and output types is also used for caching, data lineage tracking, and previews in the UI.

The following types are supported in Domino Flows:

- Python primitives: `str`, `bool`, `int`, `float`
- Python non-primitives: `list`, `dict`, `datetime`
- Data science types: `np.ndarray`, `pandas.DataFrame`, `pyspark.DataFrame`, `torch.Tensor / torch.nn.Module`, `sklearn.base.BaseEstimator`, `tf.keras.Model`
- Generic `FlyteFile` type: To enable file rendering in the UI, this type must either:
    * Be combined with a `TypeVar` that defines the file extension type, for example, `FlyteFile[TypeVar("csv")]` and `FlyteFile[TypeVar("pdf")]`.
    * Be a link:6a57f5[Flow Artifact] like `Artifact(name="My Data", type=DATA).File(name="data.csv")`

The example above only consists of a single launch input and Flow output, but you can define as many as you want. An example with multiple inputs or outputs might look like this:

[source, python]
----
final_outputs = NamedTuple(
    "final_outputs",
    model=FlyteFile[TypeVar("pt")],
    report=FlyteFile[TypeVar("pdf")],
    accuracy=float
)

@workflow
def training_workflow(data_file: FlyteFile[TypeVar("csv")], batch_size: int, learning_rate: float) ->  final_outputs:

     # Tasks will be defined in here

    return # Outputs will be returned here
----

== Define tasks

Tasks are the core building blocks within a flow and are isolated within their own container during an execution. A task maps to a single Domino Job.

=== Flow-generated vs standalone Domino Jobs

While all tasks trigger a unique Domino Job, there are some differences between jobs launched by a flow and standalone jobs. More specifically, for jobs launched by a flow:

- Only Domino Dataset snapshots can be mounted, unlike standalone Domino Jobs which mount snapshots and read-write Datasets. For data to be used in a flow, it must be part of a versioned Dataset snapshot (`version 0` of a dataset i.e. the read-write directory is NOT considered a snapshot).

- Only one snapshot of a Dataset may be mounted at a time. When leveraging the `use_latest` flag, the latest snapshot will be mounted.

- Dataset snapshots are `read-only` and cannot be modified during a job. Any processed data that needs to be persisted should be defined and written as a task output and therefore written to the Flow blob storage.

- Dataset snapshots are mounted to a standard location that doesn't include a snapshot ID in the path (the same location used for the `latest` version of a dataset in a workspace).
+
For DFS projects, the path is:
+
`/domino/datasets/local/{name}` for local dataset snapshots.
+
`/domino/datasets/{name}` for shared dataset snapshots.
+
For Git-based projects, the path is:
+
`/mnt/data/{name}` for local dataset snapshots.
+
`/mnt/imported/data/{name}` for shared dataset snapshots.

- Snapshots of the project code and artifacts (results) are not taken at the end of the job. Any results that need to be persisted should be defined and written as a task output and therefore written to the Flow blob storage.

- There are two additional directories: `/workflow/inputs` and `/workflow/outputs`. These are where the task inputs/outputs of the flow will be stored. A job status is considered failed if the expected task outputs are not produced by the end of execution. See the link:#writing-task-code[Writing task code section] for more details on how to write your code accordingly.

- Stopping a job orchestrated by a Flow in the Jobs UI will stop the entire flow, including other jobs that are running as part of it.

- Completed jobs cannot be re-run through the Domino Jobs UI. They must be relaunched by re-running the task from the Flows UI.

- Additional job metadata is captured and displayed in the Job Details to reference the flow and task that launched it.

These differences help to guarantee the reproducibility of flow executions by ensuring the triggered jobs adhere to a strict contract and remain side-effect free.

=== Add a task to the flow

Tasks for a flow can be defined in one of the following ways:

- Use the base `DominoJobConfig` and `DominoJobTask` classes. These provide more flexibility and direct control of the exact parameters to use.
- Use the `run_domino_job_task` helper method. This offers a more user-friendly abstraction that enables definition and execution of a task in the same line of code.

In both cases, tasks will trigger a Domino Job with the specified settings and return the results.

[[use-base-classes]]
==== Use base classes

The following example defines a task using the base classes:

[source, python]
----
@workflow
def training_workflow(data_path: str) -> FlyteFile[TypeVar("pt")]:

    # First task using base classes
    data_prep_job_config = DominoJobConfig(Command="python prep-data.py")
    data_prep_job = DominoJobTask(
        name='Prepare Data',
        domino_job_config=data_prep_job_config,
        inputs={'data_path': str},
        outputs={'processed_data': FlyteFile[TypeVar("csv")]},
        use_latest=True
    )
    data_prep_results = data_prep_job(data_path=data_path)

    # Output from the task above will be used in the next step

    return # Outputs will be returned here
----

- The `DominoJobConfig` defines the configuration for the Domino Job that will be triggered by the task. The only required parameter is the `Command` configured in the example above. The full list of available parameters include:
+
[cols="1a,1a,3a",options="header"]
|===
^| Parameter (* Required) ^| Type ^| Description and Example

| Title
| String
| The title that will be given to the Domino Job. Ignored when calculating the config's cache version. +
_Example:_ +
`My training job`

| Command (*)
| String
| The command that will be executed in the Domino Job. +
_Example:_ +
`python train.py`

| CommitId
| String
| For projects hosted in the Domino File System, this refers to the commit ID of the code. For Git-based projects, this refers to the commit ID of Artifacts. +
_Example:_ +
`953f66f3153b71658d`

| MainRepoGitRef
| GitRef
| Reference a specific branch, commit, or tag (for Git-based projects only). See the link:8c929e#GitRefV1[API guide] for more details. +
_Example:_ +
`GitRef(Type="commitId", Value="2f1cb9bf696921f0858")`

| HardwareTierId
| String
| The ID of the Domino Hardware Tier. Note that this is different than the name of the hardware tier. Ignored when calculating the config's cache version. +
_Example:_ +
`small-k8s`

| EnvironmentId
| String
| The ID of the Domino Environment. Note that this is different than the name or revisionId of the Environment. +
_Example:_ +
`6646530dcbd87f1a3dec0050`

| EnvironmentRevisionSpec
| EnvironmentRevisionSpecification
| The revision of the specified Domino Environment. +
_Example:_ +
[role="small-code", source]
----
EnvironmentRevisionSpecification(
    EnvironmentRevisionType=EnvironmentRevisionType.SomeRevision,
    EnvironmentRevisionId="6659daf5fc8de"
)
----

| ComputeClusterProperties
| Optional[ClusterProperties]
| Compute cluster properties for the job. +
_Example:_ +
[role="small-code", source]
----
ClusterProperties(
    ClusterType=ComputeClusterType.Dask,
    ComputeEnvironmentId="computeEnvId",
    WorkerHardwareTierId="workerHardwareTierId",
    WorkerCount=3,
    WorkerStorageGiB=100.0,
    MaxWorkerCount=10,
    ComputeEnvironmentRevisionSpec=EnvironmentRevisionSpecification(
        EnvironmentRevisionType.SomeRevision,
        "revisionId"
    ),
    MasterHardwareTierId="masterHardwareTierId",
    ExtraConfigs={"foo": "bar"},
)
----

| VolumeSizeGiB
| Float
| The amount of disk space (in GiB) to allocate. Ignored when calculating the config's cache version. +
_Example:_ +
`10.0`

| DatasetSnapshots
| List[DatasetSnapshot]
| List of the dataset snapshots to include in the job. Note that `version 0` of a dataset cannot be used, since it is mutable. You must take a snapshot first before using a dataset in a flow. +
_Example:_ +
`[DatasetSnapshot(Id="6615af2820a4", Version=1)]`

| ExternalVolumeMountIds
| List[String]
| List of the external data volume mounts (referenced by ID) to include in the Job. +
_Example:_ +
`["9625af24kida4dc035aa881b7"]`

| cache_ignore_vars
| Optional[Tuple[str, ...]] = None
| Optional tuple of names of arguments to the `DominoJobConfig` to ignore when calculating the config's cache version. +
_Example:_ +
`("EnvironmentId",)`

|===

- The `DominoJobTask` defines the actual task itself. Each of the available parameters can be described as follows:
+
[cols="1a,1a,3a",options="header"]
|===
^| Parameter (* Required) ^| Type ^| Description and Example

| name (*)
| String
| The name that will be given to the task. +
_Example:_ +
`My training task`

| domino_job_config (*)
| DominoJobConfig
| The job configuration, as defined above. +
_Example:_ +
`DominoJobConfig(Command="python prep-data.py")`

| inputs
| Dict[String, Type]
| Inputs that are required by the task. See above for different input types that are supported. Inputs may be specified from the workflow OR may be outputs from other tasks.* +
_Example:_ +
`{'data_path': str}`

| outputs
| Dict[String, Type]
| Outputs that will be produced by the task. See above for different output types that are supported. +
_Example:_ +
`{'processed_data': FlyteFile[TypeVar("csv")]}`

| use_latest
| Boolean
| If set to `True`, this will use the latest project defaults for parameters that were not explicitly provided, like the compute environment version and hardware tier. For better reproducibility, it is recommended to set this to `False` and explicitly define the necessary parameters. +
_Example:_ +
`False`

| cache
| Union[bool, Cache]
| Indicates if caching should be enabled. Can be a `bool` or a Flyte `Cache` object. Defaults to `false`. Setting `cache=true` is a simple way to enable caching for a task, but using a `Cache` object may be more flexible depending on your use case. See the link:c9731d#caching[caching documentation] for a full explanation of how caching works. +
_Example:_ +
`True`

| cache_version
| String
| When `cache=true`, the cache version to use. Changes to the task signature will automatically trigger a cache miss, but you can always manually update this field as well to force a cache miss. You should also manually bump this version if the function body/business logic has changed, but the signature hasn't. +
_Example:_ +
`"1.0"`

| cache_ignore_input_vars
| Tuple[str, ...]
| When `cache=true`, variable names of `inputs` to the `DominoJobTask` that should not be included when calculating the hash used for caching. If not provided, all input variables will be included when calculating the hash. +
_Example:_ +
`(batch_size,)`

| retries
| Integer
| Number of times to retry this task during a workflow execution. This can be used to help automatically mitigate intermittent failures. +
_Example:_ +
`0`

| timeout
| Union[timedelta, int]
| The maximum amount of time for which one execution of this task should be executed. The execution will be terminated if the runtime exceeds the given timeout. +
_Example:_ +
`timedelta(hours=3)`

|===

- Calling the Domino Job task with the relevant inputs (`data_prep_job(data_path=data_path)`) will run the Domino Job and return the results as a `Promise`, which can be used as an input to downstream tasks.

==== Use helper methods

Helper methods reduce the amount of code necessary to invoke a task. Instead of separately defining a `DominoJobConfig` and passing it to a `DominoJobTask` in the examples above, use `run_domino_job_task` to define the task contract and execute it immediately:

[source, python]
----
@workflow
def training_workflow(data_path: str) -> FlyteFile[TypeVar("pt")]:

    # First task using helper method
    data_prep_results = run_domino_job_task(
        flyte_task_name="Prepare data",
        command="python prep-data.py",
        inputs=[Input(name="data_path", type=str, value=data_path)],
        output_spec=[Output(name="processed_data", type=FlyteFile[TypeVar("csv")])],
        use_project_defaults_for_omitted=True
    )

    # Output from the task above will be used in the next step

    return # Outputs will be returned here
----

The above method will run the Domino Jobs and return the results in the same function. The full list of available parameters include:

[cols="1a,1a,3a",options="header"]
|===
| Parameter| Type| Description and Example

| flyte_task_name
| String
| The title that will be given to the task. +
_Example:_ +
`My training task`

| job_title
| String
| The title that will be given to the Domino Job. +
_Example:_ +
`My training job`

| use_project_defaults_for_omitted
| Boolean
| If set to `True`, this will use the latest project defaults for parameters that were not explicitly provided. For better reproducibility, it is recommended to set this to `False` and explicitly define the necessary parameters. +
_Example:_ +
`False`

| dfs_repo_commit_id
| String
| For projects hosted in the Domino File System, this refers to the commit ID of the code. For Git-based projects, this refers to the commit ID of the artifacts. +
_Example:_ +
`953f66f3153b71658`

| main_git_repo_ref
| GitRef
| Reference a specific branch, commit, or tag (for Git-based projects only). See the link:8c929e#GitRefV1[API guide] for more details. +
_Example:_ +
`GitRef(Type="commitId", Value="2f1cb9bf696921f0")`

| environment_name
| String
| Name of the Environment to use in the Domino Job. +
_Example:_ +
`Domino Standard Environment`

| environment_id
| String
| ID of the Environment to use in the Domino Job. This is recommended over using `environment_name` to prevent breaking reproducibility when environment names get changed. +
_Example:_ +
`6646530dcbd`

| environment_revision_id
| String
| A specific revisionId of the environment to use. +
_Example:_ +
`6659daf5fc8`

| hardware_tier_name
| String
| Name of the hardware tier to use in the Domino Job. +
_Example:_ +
`Large`

| hardware_tier_id
| String
| ID of the hardware tier to use in the Domino Job. This is recommended over using `hardware_tier_name` to prevent breaking reproducibility when hardware tier names get changed. +
_Example:_ +
`small-k8s`

| inputs
| List[Input]
| Inputs that are required by the task. See above for different input types that are supported. +
_Example:_ +
`[Input(name="data_path", type=str, value=data_path)]`

| output_specs
| List[Output]
| Outputs that will be produced by the task. See above for different output types that are supported. +
_Example:_ +
`[Output(name="processed_data", type=FlyteFile[TypeVar("csv")])]`

| volume_size_gib
| Integer
| The amount of disk space (in GiB) to allocate. +
_Example:_ +
`10`

| dataset_snapshots
| List[DatasetSnapshot]
| List of the dataset snapshots to include in the Job. Note that `version 0` of a dataset cannot be used, since it is mutable. You must first take a snapshot before using a dataset in a flow. +
_Example:_ +
`[DatasetSnapshot(Id="6615af2820a4", Version=1)]`

| external_data_volume_ids
| List[str]
| List of the external data volume mounts (referenced by ID) to include in the Job. +
_Example:_ +
`[9625af24kida4dc035aa881b7]`

| cache
| Union[bool, Cache]
| Indicates if caching should be enabled. Can be a `bool` or a Flyte `Cache` object. Defaults to `false`. Setting `cache=true` is a simple way to enable caching for a task, but using a `Cache` object may be more flexible depending on your use case. See the link:c9731d#caching[caching documentation] for a full explanation of how caching works. +
_Example:_ +
`False`

| cache_version
| String
| When `cache=true`, the cache version to use. Changes to the task signature will automatically trigger a cache miss, but you can always manually update this field as well to force a cache miss. You should also manually bump this version if the function body/business logic has changed, but the signature hasn't. +
_Example:_ +
`"1.0"`

| cache_ignore_input_vars
| Tuple[str, ...]
| When `cache=true`, variable names of `inputs` to the `DominoJobTask` that should not be included when calculating the hash used for caching. If not provided, all input variables will be included when calculating the hash. +
_Example:_ +
`(batch_size,)`

| retries
| Integer
| Number of times to retry this task during a workflow execution. +
_Example:_ +
`0`

| timeout
| Union[timedelta, int]
| The maximum amount of time for which one execution of this task should be executed. The execution will be terminated if the runtime exceeds the given timeout. +
_Example:_ +
`timedelta(hours=3)`

|===

=== Add a dependent task to the flow

It's common for one task to *depend* on another task; that is, one task accepts an input that is produced by another task as an output. This ensures that the dependent task will not start execution until outputs from the other task are produced first.

To create dependent tasks, you can use either the base classes or helper methods to define them. In the example below, note how the second task uses the output from the first task by calling `data_prep_results[“processed_data]”`.

==== Use base classes

[source, python]
----
@workflow
def training_workflow(data_path: str) -> FlyteFile[TypeVar("pt")]:

    # First task using base classes
    data_prep_job_config = DominoJobConfig(Command="python prep-data.py")
    data_prep_job = DominoJobTask(
        name='Prep data',
        domino_job_config=data_prep_job_config,
        inputs={'data_path': str},
        outputs={'processed_data': FlyteFile[TypeVar("csv")]}, # First task produces an output named 'processed_data'
        use_latest=True
    )
    data_prep_results = data_prep_job(data_path=data_path)

    # Second task using base classes
    training_job_config = DominoJobConfig(Command="python train-model.py")
    training_job = DominoJobTask(
        name='Train model',
        domino_job_config=training_job_config,
        inputs={'processed_data': FlyteFile[TypeVar("csv")]}, # Second task consumes the output named 'processed_data' from the first task as an input
        outputs={'model': FlyteFile[TypeVar("pt")]},
        use_latest=True
    )
    training_results = training_job(processed_data=data_prep_results["processed_data"])

    return # Outputs will be returned here
----

==== Use helper methods

[source, python]
----
@workflow
def training_workflow(data_path: str) -> FlyteFile[TypeVar("pt")]:

    # First task using helper methods
    data_prep_results = run_domino_job_task(
        flyte_task_name="Prepare data",
        command="python prep-data.py",
        inputs=[Input(name="data_path", type=str, value=data_path)],
        output_spec=[Output(name="processed_data", type=FlyteFile[TypeVar("csv")])], # First task produces an output named 'processed_data'
        use_project_defaults_for_omitted=True
    )

    # Second task using helper methods
    training_results = run_domino_job_task(
        name="Train model",
        command="python train-model.py",
        inputs=[
            Input(name="data", type=FlyteFile[TypeVar("csv")], value=data_prep_results["processed_data"]), # Second task consumes the output named 'processed_data' from the first task as an input
        ],
        outputs=[
            Output(name="model", type=FlyteFile[TypeVar("pt")])
        ]
    )

    return # Outputs will be returned here
----

=== Return the final output

You can set the output of the flow by returning it in the method. Note that defining an overall Flow output is not required and does not elevate this particular output in the UI. Please see link:6a57f5[Define Flow Artifacts] for elevating important task outputs.

[source, python]
----
@workflow
def training_workflow(data_path: str) -> FlyteFile[TypeVar("pt")]:

    # First task using helper methods
    data_prep_results = run_domino_job_task(
        flyte_task_name="Prepare data",
        command="python prep-data.py",
        inputs=[Input(name="data_path", type=str, value=data_path)],
        output_spec=[Output(name="processed_data", type=FlyteFile[TypeVar("csv")])],
        use_project_defaults_for_omitted=True
    )

    # Second task using helper methods
    training_results = run_domino_job_task(
        name="Train model",
        command="python train-model.py",
        inputs=[
            Input(name="processed_data", type=FlyteFile[TypeVar("csv")], value=data_prep_results["processed_data"]),
        ],
        outputs=[
            Output(name="model", type=FlyteFile[TypeVar("pt")])
        ]
    )

    return training_results["model"] # Final output is returned here
----

[[writing-task-code]]
== Write task code

Writing code for jobs that were generated by flows is slightly different than writing code for a standalone Domino Job. Flow-generated jobs have inputs that come in from the task and additional logic needs to be added to read those inputs. Once results are produced, they also need to be explicitly written as an output to the assigned output location.

=== Read inputs

For each input that is defined for a task, a unique blob is created and accessible within a Job at `/workflow/inputs/<NAME OF INPUT>`.

For file type inputs, the blob is the actual file that was inputted to the task. Example usage:

[source, python]
----
named_input = "processed_data"
data_path = "/workflow/inputs/{}".format(named_input)
df = pd.read_csv(data_path)
----

For Python non-file types (`str`, `bool`, `int`, `list`, `dict`, etc.), the blob contents contain the input value. Example usage:

[source, python]
----
input_name = "data_path"
input_location = f"/workflow/inputs/{input_name}"
with open(input_location, "r") as file:
    input_value = file.read()
----

Inputs for real Flows tasks are handled transparently by the system. However, if you want to run your task
code locally for testing or experimentation, you must set up the inputs expected by the task code.

You can do this manually or by using helpers from the Domino Python library:

* To set up inputs manually, create each `/workflow/inputs/<NAME OF INPUT>` file that the task code expects.
+
For example, if your task code expects a CSV input file named `my_data` and you have a file in your current working directory called
`test_data.csv` that you want to use as that input, then run the terminal command `cp test_data.csv /workflow/inputs/my_data`.
+
For primitive inputs like strings and integers, write the data as a string to the file `/workflow/inputs/<NAME OF INPUT>`.

* Alternatively, the Domino Python helper library has functions that facilitate setting up input data. These functions allow you to use
inputs or outputs from past Flyte executions.
+
Here is an example that shows setting up local input data for a task that expects five inputs:
+
. `my_csv_one`, using the output of a previous Flow execution.
. `my_csv_two`, using the input of a previous Flow execution.
. `my_csv_three`, using the local file `test_data.csv`.
. `my_str`, using the string `"my string input data"`.
. `my_int`, using the integer `42`.
+
[source,python]
----
import shutil
from flytekitplugins.domino.helpers import BlobDataLocation, PrimitiveDataLocation, setup_workflow_data

blobs = [
    BlobDataLocation(
        # you can find blob URLs by inspecting node executions in the Domino Flyte UI
        "s3://flyte-data-bucket/past/execution1/output/some_csv",
        # the input name will incorrectly be inferred as "some_csv" from the blob url,
        # so the local_filename kwarg must be provided and match the input name expected by the task code
        local_filename="my_csv_one",
    ),
    BlobDataLocation("s3://flyte-data-bucket/past/execution2/input/my_csv_two"),
]
primitives = [
    PrimitiveDataLocation(
        "my string input data",
        "my_str",
    ),
    PrimitiveDataLocation(
        42,
        "my_int",
    )
]
setup_workflow_data(blobs, primitives)
shutil.copyfile("test_data.csv", "/workflow/inputs/my_csv_three")
# now, the task code can be run locally
----

=== Write outputs

Outputs defined for a task must be written to `/workflow/outputs/<NAME OF OUTPUT>`. For example:

[source, python]
----
named_output = "processed_data"
df.to_csv("/workflow/outputs/{}".format(named_output))
----

NOTE: Writing outputs to the correct location is necessary for them to persist and be usable in dependent tasks. If the defined outputs do not exist at the end of a Domino Job, the task will fail.

NOTE: Jobs that are submitted through a flow will not make any automatic commits or dataset snapshots. Results should always be written as outputs.

=== Best Practice - Dynamic Pathing

If you want your task code to seamlessly run across workspaces/jobs and flows then it is recommended that you use the `DOMINO_IS_WORKFLOW_JOB` default environment variable. This variable is `true` when your code is running as part of a flow and `false` when it is running in a workspace or standalone job. You can therefore reference this variable and set pathing for code inputs/outputs depending on where it is running.

== Next steps

Once you have properly defined the flow, learn how to:

* link:6a57f5[Define Flow Artifacts.]
* link:113c96[Launch an execution of the flow].

----- user_guide/flows/examine-flow-artifacts.txt -----
:page-version: 6.1
:page-permalink: 3db42c
:page-title: Examine Flow Artifacts
:page-sidebar: Examine Flow Artifacts
:page-order: 60


After you have defined Flow Artifacts, you can inspect artifacts produced by executions and discover bookmarked artifacts within a project.

== Inspect Flow Artifacts produced by executions

Flow Artifacts are surfaced in a dedicated section in the run view, where they can be easily discovered, inspected and downloaded.

Navigate to *Flows* > *Flow name* > *Run name* > *Artifacts* in order to inspect the artifacts produced by that particular run.

image::/images/5.11/flows/artifacts-inspect.png[alt="Inspect artifacts produced by a run", width=1000, role=noshadow]

=== Bookmark artifacts

As many flows are executed, specific flow executions may generate outputs that are especially noteworthy to the project. These Flow Artifacts can be bookmarked by clicking on the *Bookmark artifact* icon on the right panel:

image::/images/5.11/flows/artifacts-bookmark-icon.png[alt="Bookmark an artifact with the icon", width=400, role=noshadow]

Flow Artifacts can also be bookmarked in bulk by selecting multiple artifacts on the left panel, and clicking on the *Bookmark* button above:

image::/images/5.11/flows/artifacts-bookmark-bulk.png[alt="Bookmark artifacts in bulk", width=400, role=noshadow]

=== Access artifacts

Artifacts that have smaller file sizes may be viewed directly within Domino. The right panel provides two additional means of accessing artifacts, either by downloading them or by accessing them via code.

* The *Download* button will download all the files that are part of the selected artifact.
+
In this example, all five files (`model.pkl`, `python_env.yaml`, `requirements.txt`, `conda.yaml`, and `MLmodel`) are downloaded at once:
+
image::/images/5.11/flows/artifacts-download.png[alt="Download files of a selected artifact", width=600, role=noshadow]

* The *Code snippet* button generates Python code to directly download stored artifacts to a local workspace. Copy the code using the *Copy* button and run it inside a workspace:
+
image::/images/5.11/flows/artifacts-code-snippet.png[alt="Access artifacts through code snippets", width=1000, role=noshadow]
+
* The default syntax in the code snippet will copy all files within the Flow Artifact to the temporary `/workflow/inputs/` directory within your workspace.
+
To persist your Flow Artifacts outside of Flows so they are consumable for downstream processes, such as web apps or a batch job, change the `local_dir` parameter in the code snippet to write your files to a Domino Dataset or the DFS Artifacts file system.

== Discover bookmarked Flow Artifacts

Artifacts that have already been bookmarked are displayed at the top level view for Flows to allow for easier discoverability and reuse.

* Navigate to *Flows* > *Bookmarked Artifacts*, and view all the artifacts that were bookmarked on the current project, across flows and runs.
This view includes the artifact name, type, and version, as well as additional metadata.
+
image::/images/5.11/flows/artifacts-bookmarked-view.png[alt="View bookmarked artifacts", width=1000, role=noshadow]

* Clicking on the *Source* column will navigate to the run that created the artifact, while clicking on the *Artifact* column will display a view of the artifact that includes all of its files.
+
image::/images/5.11/flows/artifacts-view-files.png[alt="Display files of an artifact", width=1000, role=noshadow]

* This paginated view allows filtering files by name, as well as downloading one or more selected files at once.
Clicking on a file name will display a larger file preview, alongside an option to download the file.
+
image::/images/5.11/flows/artifacts-paginated-view.png[alt="Display a larger file preview", width=1000, role=noshadow]

== Assign Flow Artifacts to Datasets

Once you have created a Flow Artifact, you can assign it to a dataset. You can choose an existing dataset or create a new one. There is also an option to automatically take a dataset snapshot after registration. The data’s lineage is automatically maintained between the Flow Artifact and the Dataset Snapshot.

image::/images/6.0/add-to-dataset.png[alt="Add artifact to Dataset", width=1000, role=noshadow]

After the artifact is added successfully to the dataset, you’ll see a confirmation message.

== Next steps

* Maximize the link:338152[reproducibility] of Flows.

----- user_guide/flows/get-started-with-flows.txt -----
:page-version: 6.1
:page-permalink: 5b5259
:page-title: Get started with Flows
:page-sidebar: Get started with Flows
:page-order: 10

Make sure you have a good understanding of the link:78acf5#concepts[key concepts] before you get started with Domino Flows.

== Basic flow

This section demonstrates a basic example that:

* Takes two integers as an input to a flow.
* The first task adds the integers together and passes the result as an input to the second task.
* The second task takes the square root of the input and returns the result as the final output of the flow.

This example flow can be visualized as follows:

image::/images/6.0/flows/simple-math-flow.png[alt="Simple Math Flow Graph", width=900, role=noshadow]

To create a Domino Flow:

. Create a workspace using the link:0d73c6[Domino Standard Environment (DSE)] from 6.0 onwards, or a custom environment that is built on top of the DSE >= 6.0, as these contain the required Flyte Python libraries.

. Create a file named `add.py` in the root directory. Add the following code to the file to add two integer inputs together:
+
[source,python]
----
from pathlib import Path

# Read inputs
a = Path("/workflow/inputs/first_value").read_text()
b = Path("/workflow/inputs/second_value").read_text()

# Calculate sum
sum = int(a) + int(b)
print(f"The sum of {a} + {b} is {sum}")

# Write output
Path("/workflow/outputs/sum").write_text(str(sum))
----

. Create a file named `sqrt.py` in the root directory. Add the following code to the file to calculate the square root of the input:
+
[source,python]
----
from pathlib import Path

# Read input
value = Path("/workflow/inputs/value").read_text()

# Calculate square root
sqrt = int(value) ** 0.5
print(f"The square root of {value} is {sqrt}")

# Write output
Path("/workflow/outputs/sqrt").write_text(str(sqrt))
----

. Create a file named `workflow.py` in the root directory (see the https://github.com/dominodatalab/flytekit[Flytekit Python library^] documentation on GitHub). Add the following code to the file to define the flow:
+
[source,python]
----
from flytekit import workflow
from flytekitplugins.domino.task import DominoJobConfig, DominoJobTask

@workflow
def simple_math_workflow(a: int, b: int) -> float:

    # Create first task
    add_task = DominoJobTask(
        name='Add numbers',
        domino_job_config=DominoJobConfig(Command="python add.py"),
        inputs={'first_value': int, 'second_value': int},
        outputs={'sum': int},
        use_latest=True
    )
    sum = add_task(first_value=a, second_value=b)

    # Create second task
    sqrt_task = DominoJobTask(
        name='Square root',
        domino_job_config=DominoJobConfig(Command="python sqrt.py"),
        inputs={'value': int},
        outputs={'sqrt': float},
        use_latest=True
    )
    sqrt = sqrt_task(value=sum)

    return sqrt
----

. *Commit the code* and run the following command in the Workspace terminal to register and run the flow:

+
[source, bash]
----
pyflyte run --remote workflow.py simple_math_workflow --a 10 --b 6
----

. Once you run the command above, navigate to *Flows* > *Flow name* > *Run Name* in the Domino UI to monitor the results and view the outputs that were produced by the execution.
+
image::/images/6.0/flows/simple-flow.png[alt="Monitor simple flow", width=1200, role=noshadow]

. To visualize the full execution flow, click on the *Graph* pivot.
+
image::/images/6.0/flows/simple-flow-graph.png[alt="Simple Flow Graph", width=1000, role=noshadow]

== AI Hub Flow

Rather than beginning from scratch, you can start from a pre-built ecosystem template from Domino's AI Hub. This section uses a template that demonstrates a basic training flow example with the following steps:

. Data is loaded in from two different sources and a snapshot of the data is taken.
. The data is merged together as a single dataset.
. Basic preprocessing is done on the dataset.
. A model is trained using the cleaned dataset.

The training flow can be visualized as follows:

image::/images/6.0/flows/ai-flow.png[alt="Training Flow Graph", width=800, role=noshadow]

To create the training flow:

. Make a fork of the template link:https://github.com/dominodatalab/domino-ai-flows[GitHub repository].

. Create a Workspace using the link:0d73c6[Domino Standard Environment (DSE)] from 6.0 onwards, or a custom environment that is built on top of the DSE >= 6.0.
+
image::/images/5.11/flows/workspace.png[alt="Workspace", width=700, role=noshadow]

. Inspect the `mlops_flow.py` file for the definition of the flow. Note how a helper method, called `run_domino_job_task`, is used here instead of the `DominoJobConfig` and `DominoJobTask` in the basic example above.
+
[source, python]
----
task1 = run_domino_job_task(
    flyte_task_name='Load Data A',
    command='python /mnt/code/scripts/load-data-A.py',
    inputs=[Input(name='data_path', type=str, value=data_path_a)],
    output_specs=[Output(name='datasetA', type=FlyteFile[TypeVar('csv')])],
    use_project_defaults_for_omitted=True,
    environment_name=environment_name,
    hardware_tier_name="Small",
    cache=cache,
    cache_version="1.0"
)

task2 = run_domino_job_task(
    flyte_task_name='Load Data B',
    command='python /mnt/code/scripts/load-data-B.py',
    inputs=[Input(name='data_path', type=str, value=data_path_b)],
    output_specs=[Output(name='datasetB', type=FlyteFile[TypeVar('csv')])],
    use_project_defaults_for_omitted=True,
    environment_name=environment_name,
    hardware_tier_name="Small",
    cache=cache,
    cache_version="1.0"
)

# Additional tasks
----

. Run the following command in the Workspace terminal to register and run the flow:
+
[source, bash]
----
pyflyte run --remote mlops_flow.py model_training --data_path_a /mnt/code/data/datasetA.csv --data_path_b /mnt/code/data/datasetB.csv
----

. Navigate to *Flows* > *Flow name* > *Run name* to monitor the results and view the outputs that were produced by the execution.
+
image::/images/6.0/flows/ai-flows-run.png[alt="Training Flow Run", width=1200, role=noshadow]

== Next steps

* link:e09156[Define Flows]
* link:6a57f5[Define Flow Artifacts]
* link:113c96[Launch Flows]
* link:dfe1ee[Monitor Flows]
* link:3db42c[Examine Flow Artifacts]
* link:338152[Flows Reproducibility]
* link:c9731d[Advanced Flows]

----- user_guide/flows/index.txt -----
:page-version: 6.1
:page-permalink: 78acf5
:page-title: Orchestrate with Flows
:page-order: 240

image::/images/5.11/flows/flow-icon.png[alt="Domino Flows Icon", width=500, role=noshadow]

Domino Flows enables efficient orchestration and monitoring of complex, interconnected multi-step processes while ensuring full lineage and reliable reproducibility. The processes, implemented as Domino jobs, are *tasks* and the complete structure of connections between tasks is a *workflow*. Tasks produce outputs that become inputs to other tasks, forming the basis for the connections. A Flow definition constructs a DAG (directed acyclic graph).

NOTE: To support reproducibility, each task must be side-effect free by reading versioned inputs, and writing defined outputs.

[[usecases]]
== When to use Flows

Flows is flexible enough to declaratively model arbitrarily complex processes. Dependency relationships between tasks determine the order in which they run and whether they can be parallelized. Scenarios spanning machine learning, data engineering, and data analytics benefit from this level of control and reproducibility.

For instance, Flows would be an ideal choice for scenarios like:

* Executing a data processing workflow in Dask prior to a training workflow in XGBoost
* Running a clinical study pipeline by loading SDTM datasets to produce ADaM datasets and TFL reports
* Collecting image metadata from S3 with Spark and performing model inference with PyTorch
* Loading financial data from Snowflake, processing it for use in a Ray training job that registers a model in MLflow
* Processing a local protein database to search for a nucleotide sequence and generating a scatterplot

Flows may not be the most appropriate choice to use when modeling a process that accesses a single dataset and performs many small computations in a homogenous environment. Tasks that write to mutable shared state (like read-write datasets) cannot be used in Flows, but can be made compatible with modifications.

[[tasks]]
== Flow tasks vs standalone Domino Jobs

Flows extends the Domino Job system with key new functionality including:

* *Programmatic Python based authoring* of versioned, reusable, repeatable, immutable workflows
* *Strongly typed definitions* of inputs being consumed and outputs being produced for each task
* *Automatic lineage and versioning* of all task and workflow inputs and outputs
* *Heterogeneous, isolated environment* support for any task
* *Stronger reproducibility* requirements and guarantees
* *Visualization of the workflow execution graph* and the ability to inspect and monitor each task, its inputs and outputs
* *Parallel execution* of tasks at scale
* *Configurable caching* and task result reuse anywhere within the workflow
* *link:6a57f5[Flow Artifacts]* for discovery, inspection and reuse of specially annotated outputs within a project
* *Automatic recovery* from intermittent failures and manual recovery of partial executions

Read more about the link:e09156[differences between Flows-generated tasks that run Domino jobs vs standalone Domino Jobs].

Flows is built on the open-source framework link:https://flyte.org/[Flyte^].

[[concepts]]
== Important terms

Some key terms to understand before getting started with Flows include:

[cols="1,3",options="header"]
|===
|Term |Definition

|Task
|Tasks are the core building blocks within a flow and are isolated within their own container during an execution. A task maps to a single Domino Job.

|Flow
|A flow is a composition of multiple tasks or other flows (called subflows). Flows can be triggered through a single command and are tracked as a single, fully reproducible entity.

|Node
|A node represents a unit of execution or work within a flow (they show up as individual blocks in the graph views). A node can contain either a single task or a whole flow (called subflows).

|Task inputs
|Task inputs are strongly typed parameters that can be defined on individual tasks. Inputs allow tasks to be rerun with different settings through the UI, without the need to modify the code itself. Inputs can be read and used within executions.

|Task outputs
|Task outputs are strongly typed parameters that define the results that are produced by a task. Outputs are tracked and stored in discrete blob storage, so that they can be used as input to another task.

|Flow inputs/outputs
|Flow inputs/outputs are similar to the task inputs/outputs but are defined at the flow level. Inputs defined for a flow can be passed into relevant tasks, and outputs from tasks can be returned as the overall output for a flow.

|===

== Next steps

* See link:5b5259[Get started with Flows] to understand the key concepts before you get started with Domino Flows.
* link:e09156[Define Flows] via a code-first approach using Flyte’s Python SDK.
* Explicitly link:6a57f5[define Flow Artifacts] in your code.
* Once Flows are defined, you can link:113c96[register and launch] them.
* Use the comprehensive Domino Flows user experience to link:dfe1ee[monitor Flows].
* After you have defined Flow Artifacts, you can link:3db42c[examine] them.
* Find out how every flow, task, and execution are uniquely versioned in Domino Flows to guarantee link:338152[reproducibility].
* Learn more about the link:c9731d[advanced capabilities] that you can use in Domino Flows.

----- user_guide/flows/launch-flows.txt -----
:page-version: 6.1
:page-permalink: 113c96
:page-title: Launch Flows
:page-sidebar: Launch Flows
:page-order: 40

Once flows are defined, they can be registered and launched using the Flyte CLI through a Workspace or Job. Flows can also be relaunched from the UI once they have been registered into Domino.

== Launch a new flow

A new flow must be launched using the Flyte CLI via the `pyflyte run` command.

In the example below, we assume that:

* A flow is defined inside a `workflow.py` file.
* The flow is defined using a method called `test_workflow`.
* The flow takes two input arguments called `inputA` (int) and `inputB` (string).

Therefore, the following command can be used to run the flow:

[source, bash]
----
pyflyte run --remote workflow.py test_workflow --inputA 5 --inputB "Hello"
----

The above command can be executed inside a Workspace terminal or as the start command for a Job. Once it is executed, the flow is automatically registered and execution should start immediately.

=== Set a name

A name is automatically generated for each run, but you can explicitly set the name as well:

[source, bash]
----
pyflyte run --remote --name MY_CUSTOM_NAME workflow.py test_workflow --inputA 5 --inputB "Hello"
----

=== Add tags

You can also add tags to a run execution:

[source, bash]
----
pyflyte run --remote --tag MY_TAG=MY_VALUE workflow.py test_workflow --inputA 5 --inputB "Hello"
----

== Flow versioning

Flows are automatically versioned inside Domino. Runs of each flow version are grouped together and the runs of the latest flow version will be shown by default. You can view and select versions in the Runs view of your flow.

image::/images/6.0/flows/flow_versions.png[alt="Flow Versions", width=600, role=noshadow]

Any changes made to the `workflow.py` definition file will cause a new version to be serialized and registered to Domino when that flow is launched from the CLI. This flexibility allows for rapid iteration — new workflow definitions can be written, serialized, and tested in flows without requiring an immediate commit to Git or DFS. This speeds up development and debugging as workflows can be quickly adjusted and re-registered.

However, since the registered flows are stored in a serialized format, it is important to ensure that the workflow code is eventually committed to Git or DFS. Without this, there is a risk that only the compiled workflow in the Domino UI remains available and that you'll lose the syntax that built it. A good practice is to leverage this iterative flexibility while maintaining awareness of when a workflow has reached a stable state that should be preserved in version control. Regularly syncing workflow code with Git ensures that development remains traceable and reproducible.

== Relaunch an existing flow

After a flow has been triggered for the first time, it can be relaunched directly through the UI. To relaunch an existing flow:

. Navigate to *Flows* > *Flow name* > *Flow Version* > *Run name* and click *Re-run* from the top-right corner of the page.
+
image::/images/6.0/flows/rerun-button.png[alt="Rerun Flow", width=600, role=noshadow]

. Provide a name for the new run and choose whether to overwrite caching. For more details on caching, see the link:c9731d[Advanced Flows] section.
+
image::/images/6.0/flows/rerun-modal.png[alt="Rerun Modal Step 1", width=800, role=noshadow]

. Choose whether to reuse the latest inputs or enter new input parameters, then click on *Launch Run* to trigger the relaunch.
+
image:/images/6.0/flows/re-launch-flow-execution-inputs.png[alt="Rerun Modal Step 2", width=800, role=noshadow]

=== Recover from failure

If a task fails due to unexpected transient issues (job timed out, out-of-capacity issues, spot instance pre-emption, internal server error, etc.), it is possible to continue execution from the point of failure rather than restarting the entire flow execution from the beginning.

When this happens, you can click the *Recover* button of the execution view in the Domino Flows UI. This will resume the execution from the point of failure by re-using the outputs that have already been computed.

image::/images/6.0/flows/recover.png[alt="Recover From Failure", width=800, role=noshadow]

NOTE: This capability is meant for transient issues only. If an execution has failed due to a user error in the code, you must relaunch the flow from the CLI after making your changes so that a new version of the code can be registered.

=== Rerun a single node

During experimentation, you may be focused on specific parts of the flow and want to execute just a single node with different input parameters. To do this in Flows:

. Navigate to the relevant node in the execution view of the Domino UI, and click *Rerun Node*.
+
image::/images/6.0/flows/rerun-node.png[alt="Node Rerun Button", width=700, role=noshadow]

. Provide a name for the new run and choose whether to overwrite caching. For more details on caching, see the link:c9731d[Advanced Flows, role=noshadow] section.
+
image::/images/6.0/flows/rerun-node-modal.png[alt="Node Rerun Modal Step 1", width=800, role=noshadow]

. Choose whether to reuse the latest inputs or enter new input parameters. Then click on *Launch Run* to trigger the relaunch of the node.
+
image::/images/6.0/flows/rerun-node-modal-parameters.png[alt="Node Rerun Modal Step 2", width=800, role=noshadow]

NOTE: Relaunching flows through the UI only works in scenarios where the flow definition has not changed. If changes are being made to the original flow definition, they must be launched through the CLI so that a new version can be registered.

== Next steps

Once you have properly launched the flow, learn how to link:dfe1ee[monitor flow results].

----- user_guide/flows/monitor-flows.txt -----
:page-version: 6.1
:page-permalink: dfe1ee
:page-title: Monitor Flows
:page-sidebar: Monitor Flows
:page-order: 50

Domino Flows comes with a comprehensive user experience to monitor flows. This includes an integrated Domino UI, along with access to the native Flyte UI.

This document goes through some of the available monitoring experiences.

== Domino UI

The Domino Flows UI can be accessed by navigating to the *Flows* page within your Project view. This section documents some of the key experiences that are available on this page.

=== Flow and task list

Upon landing in the Flows UI, you will have access to a list of all the flows that have been registered into the Project. In this view, you can:

- Keep track of and monitor all the flows in your Project.
- Click on any of the flows to navigate to the details view for the flow.
- View the lineage of Flow artifacts attached to a Dataset.
- Filter between active/archived flows or search for flows by name.
- Change the number of entries and columns to display in the table.

image::/images/6.0/flows/flow-list.png[alt="Flow List", width=1000, role=noshadow]

Similarly, clicking on the `Flow Tasks` pivot will navigate you to a similar view but at the task level. This view is an aggregate list of tasks from across all flows registered in this project.

image::/images/6.0/flows/task-list.png[alt="Task List", width=1000, role=noshadow]

=== Flow and task details

Clicking onto a flow will navigate you to its details view. In this view, you can:

- Run the flow.
- Visualize the flow structure by clicking on the *Graph* pivot.
- Keep track of and monitor the status of all the runs for that flow.
- Filter between runs of a specific version or view runs of all versions.
- Click on any of the runs to navigate to a run details page.
- Rerun or archive a run by clicking on the overflow button (right side of a row).
- Filter between active/archived runs or search for runs by name.
- Change the number of entries and columns to display in the table.
- Archive the flow (overflow button on the top right corner).

image::/images/6.0/flows/run-list.png[alt="Run List", width=1000, role=noshadow]

Clicking on a task will navigate you to a similar view but at the task level.

image::/images/6.0/flows/task-details.png[alt="Run List", width=1000, role=noshadow]

=== Run details

To see more details about a run, click on the relevant run name to access the *Run details* page. In this view, you can:

- View a list of all the nodes along with their execution status.
- Click into a node to view more details about the results.
- Toggle to a graph view of the run details by clicking on the *Graph* pivot.
- Rerun or archive the run by clicking on the overflow button (top right).

image::/images/6.0/flows/node-list.png[alt="Node List", width=1000, role=noshadow]

=== Node details

To see more details about a node, click on the relevant node name, either via the *Nodes* or *Graph* pivot, to access the *Node details* panel. In this view, you can:

- Visualize all the versioned inputs that went into the node.

+
image::/images/6.0/flows/inputs.png[alt="Node Inputs", width=1200, role=noshadow]

- Visualize all the versioned outputs that came out of the node.

+
image::/images/6.0/flows/outputs.png[alt="Node Outputs", width=1200, role=noshadow]

- Access execution details about the node, including a link to the corresponding Domino Job.

+
image::/images/6.0/flows/details.png[alt="Node Details", width=1200, role=noshadow]

- Access execution logs.

+
image::/images/6.0/flows/logs.png[alt="Node Logs", width=1200, role=noshadow]

NOTE: To enable file rendering, you must explicitly specify the file type when you define the input or output. See link:e09156[defining flows] for more details.

=== Lineage details

Once you register a Flow Artifact to a Dataset, you can view its lineage through the *Details* pane. You can trace the lineage path throughout the Dataset by clicking the links at the bottom.

image::/images/flows/flow-lineage-6.1.png[alt="View Data Lineage", width=500, role=noshadow]

=== Graph view

Click into the *Graph* pivot to access a graph view of the execution. This helps validate that the execution flow and dependencies are defined correctly, in addition to narrowing the root cause of errors.

image::/images/6.0/flows/ai-flows-graph.png[alt="Graph View", width=1000, role=noshadow]

=== Launch Inputs view

Click into the *Launch Inputs* pivot to view the list of the flow run's launch parameters. Clicking into any individual launch input will display the value that was set for that run.

image::/images/6.0/flows/launch-inputs-flow-run-pivot.png[alt="Launch Inputs View", width=1000, role=noshadow]

== Flyte UI

The Flyte UI can be accessed directly at `\https://{DOMINO-URL}/flows/console/select-project`. Within the Flyte UI, you can search for a specific Project or directly click into any of the other pages on the left. There are also several entry points from the Domino UI that will take you directly to specific pages in the Flyte experience.

NOTE: There is a 1:1 mapping between Projects in Flyte and Domino.

image::/images/5.11/flows/flyte-landing.png[alt="Flyte Landing Page", width=1200, role=noshadow]

The Flyte UI offers many of the same capabilities as the Domino UI but also includes some additional, useful features. This section documents some of the key additional views that can help to monitor flows.

NOTE: Not all native Flyte capabilities are fully supported, so there may be some actions in the Flyte UI that don't currently work. Additional capabilities will be enabled during future releases.

=== Timeline view

Click into any execution in the Flyte UI to access a timeline view of the flow. This helps identify execution bottlenecks and to know what the critical execution path is.

image::/images/5.11/flows/timeline.png[alt="Timeline View", width=1000, role=noshadow]

Hovering over tasks in the timeline view will allow you to see an even more granular breakdown.

image::/images/5.11/flows/timeline-hover.png[alt="Timeline View - Hover", width=1000, role=noshadow]

== Flyte Output Blob URI's

The Domino UI renders output files for each task and allows you to download them to your local machine. If you need to access the Flyte blob directly, you can find the URI for each blob in the Flyte UI.

image::/images/6.0/flows/flyte-blob-uri.png[alt="Flyte Output Blob URI", width=1000, role=noshadow]

== Next steps

* link:3db42c[Examine Flow Artifacts.]
* Maximize the link:338152[reproducibility] of Flows.

----- user_guide/flows/reproducibility-flows.txt -----
:page-version: 6.1
:page-permalink: 338152
:page-title: Flows reproducibility
:page-sidebar: Flows reproducibility
:page-order: 70

Every flow, task, and execution are uniquely versioned in Domino Flows to guarantee reproducibility.

Each time a flow is executed through the CLI, the exact definition is serialized and uploaded to be stored in the Flyte blob storage. This includes:

* The structure of the flow and tasks as defined in the function wrapped by the `@workflow` decorator at the time of execution. Implicitly, this automatically captures the exact code definition for the flow at the time of execution, without the need to make a manual commit.

* The Domino Job config that is defined as part of each task and includes a precise code commit, environment version, hardware tier, dataset snapshots, and all other properties that were defined as part of a job.

* The input and output parameters that can be configured or modified for each execution.

Whenever the serialized definition of the flow/tasks do not match a previously registered version, a new version of the entity is created and is tied to every execution that is instantiated from it. This is what ensures that all re-executions of a flow with the same parameters will always produce consistent results.

== Best practices

While the automatic versioning of entities in flow helps to maximize reproducibility, there are additional practices that are recommended in order to guarantee it. More specifically:

* When the `use_latest` parameter is set when defining tasks, Domino will use project defaults or the latest versions of Domino entities for parameters that have not been explicitly defined at the time of registration. While this is useful for quick prototyping, it is recommended to explicitly define all parameters before putting a flow into production. This ensures that if you make changes to your flow definition in the future, you will not accidentally pick up changed project default settings when registering your new flow version.

Note that `use_latest` in this example is useful for development and iteration:

[source,python]
----
DominoJobTask(
    name='Add numbers',
    domino_job_config=DominoJobConfig(Command="python add.py"),
    inputs={'first_value': int, 'second_value': int},
    outputs={'sum': int},
    use_latest=True
)
----

A better definition for production use omits `use_latest` in favor of explicitly defining all parameters:

[source,python]
----
DominoJobTask(
    name='Add numbers',
    domino_job_config=DominoJobConfig(
        Command="python add.py",
        CommitId="4e6f2ee71e3bd64eaa90ce826c7d523b29f179cd",
        MainRepoGitRef=None,
        HardwareTierId="large-k8s",
        EnvironmentId="66b5324c495c6c124cfb0a28",
        EnvironmentRevisionSpec=EnvironmentRevisionSpecification(EnvironmentRevisionType.SomeRevision, "4"),
        ComputeClusterProperties=None,
        VolumeSizeGiB=10,
        DatasetSnapshots=[
            DatasetSnapshot(Name="quick-start", Version=1)
        ],
        ExternalVolumeMountIds=[],
    ),
    inputs={'first_value': int, 'second_value': int},
    outputs={'sum': int}
)
----

* When loading data from external data sources, it is recommended to have an initial task for making a snapshot of the data (i.e., a task that loads the data into Domino and copies it directly as an output). This ensures that changes in external data sources won't affect the ability to reproduce a result.

* Always write results as explicit outputs for the task, rather than to an external data location.

== Next steps

- link:c9731d[Advanced Flows]

----- user_guide/generative-ai/generative-ai-overview.txt -----
:page-version: 6.1
:page-permalink: d2ba79
:page-title: Generative AI overview
:page-order: 10

Generative AI represents a branch of artificial intelligence that enables computers to create content such as images, text, code, and synthetic data. These applications are underpinned by foundational technologies including large language models (LLMs).

* *LLMs* are deep learning models trained on extensive datasets to perform language processing tasks. They generate new text that mimics human language based on their training data.
* *Foundation models* are pre-trained machine learning (ML) models designed for further fine-tuning to achieve specific tasks in language understanding and generation. These models identify patterns in input data that form the basis for generating statistically probable outputs when prompted.

After these models have completed their learning processes, you can employ them to accomplish various tasks, including:

* *Speech and natural language tasks* such as chatbots, transcription, translation, question and answer generation, and interpretation of the intent or meaning of text.
* *Image generation* based on existing images or utilizing the style of one image to modify or create a new one.

Domino provides several generative AI design patterns that have business impact in production, including:

* *Prompt engineering*: Develop specialized prompts to direct LLM behavior.
* *link:420f0c[Retrieval Augmented Generation (RAG)]*: Integrate an LLM with external knowledge sources.

== Generative AI and LLMs developed on Domino

Domino Data Lab facilitates the entire AI lifecycle from data collection and preparation, through model development and operations, to deployment and monitoring. 

You can use the following capabilities to develop generative AI applications in Domino:


[cols="2,4",options="header"]
|===
| Capability | Description
| link:https://domino.ai/platform/ai-hub/templates[AI Hub generative AI templates] | Accelerate project development by providing turnkey sample Projects directly in Domino, showcasing best practices for generative AI and classical ML.
| link:4c6469[Vector database connectors] for link:5c64ef[Pinecone] 
and link:c2364c[Qdrant]
| Enable managed access to high-dimension vectorized data for generative AI algorithms like RAG.
| link:c9ac47[AI Gateway] | Provides users a safe and streamlined way to access external LLMs hosted by service providers like OpenAI, Anthropic, and more.
| link:5384dd[Coding assistants] | Give you on-demand code generation, data analysis, notebook generation, and more to help you analyze and develop code more efficiently.
|===

== Core platform capabilities that support generative AI

In addition to generative AI-specific features, Domino's core platform manages the complex challenges of generative AI:

[cols="2,4",options="header"]
|===
| link:16d9c1[Data access layer] | Access vast quantities of disparate data required for generative AI from a central interface.
| link:19df62[Model governance (Sentry)] | Register models and customize the model review and validation process with complete audit records and reproducibility to ensure responsible practices.
| link:8b4418[Compute scaling] | Train and deploy highly compute-intensive generative AI models.
| link:95520d[Hybrid and multi-cloud support] | Run AI workloads in any cloud or on-premise environment to reduce costs, simplify scaling, and protect data.
| link:08a85b[Deployment and monitoring] | Deploy and monitor GenAI LLMs and apps, on-premise or in the cloud.
| link:d4b465[FinOps] | Monitor and reduce AI costs through budgeting, alerts, and efficient cost allocation.
|===

== Examples of generative AI models and apps

Domino's open and interoperable design handles any generative AI scenario. The following are examples of generative AI projects from the link:https://domino.ai/platform/ai-hub/templates/[AI Hub] and the Domino User Guide:

- link:https://domino.ai/platform/ai-hub/templates/build-enterprise-question-answer-application[Enterprise Q&A applications] with RAG using a Pinecone database.
- link:https://domino.ai/platform/ai-hub/templates/summarize-product-feedback-respond[Summarize product feedback and respond with emails] with LangChain and AWS Bedrock.
- Fine-tune open source models like link:https://domino.ai/platform/ai-hub/templates/build-chatbot-fine-tuning-llama2[Llama 2] and link:https://domino.ai/platform/ai-hub/templates/fine-tune-llm-natural-language-apps[falcon].
- link:2c3a33[Build LLM chatbots] with Streamlit and OpenAI.

== Next steps

Explore more generative AI projects in the link:https://domino.ai/platform/ai-hub/templates/[AI Hub].

----- user_guide/generative-ai/index.txt -----
:page-version: 6.1
:page-permalink: 1140fb
:page-title: Generative AI
:page-order: 60
:page-separator: true
:page-section: Foundational concepts

Use Domino to rapidly deploy generative AI workloads from pilot to production. Domino's open and interoperable design lets you meet the evolving demands of generative AI.

[cols="2,4",options="header"]
|===
| Article | Description
| link:d2ba79[Generative AI overview] | Learn about generative AI and the capabilities Domino provides to enable generative AI design patterns.
| link:94beaa[RAG vs. fine-tuning] | Compare Retrieval Augmented Generation (RAG) models and fine-tuned Foundation Models to understand the use cases that apply to these technologies. 
| link:420f0c[Retrieval Augmented Generation (RAG)] | Understand the Retrieval Augmented Generation (RAG) design pattern and the RAG application architecture. 
| link:4c6469[Vector databases] |  Learn how Domino supports vector databases that represent data in high-dimensional space.
|===

----- user_guide/generative-ai/rag-vs-fine-tuning.txt -----
:page-version: 6.1
:page-permalink: 94beaa
:page-title: RAG vs. fine-tuning
:page-order: 30

In the realm of generative AI, various techniques enhance the capabilities and performance of models in specific applications. Two such techniques are Retrieval Augmented Generation (RAG) and fine-tuning Foundation Models. While both methods improve the output of large language models (LLMs), they differ significantly in their approach and use cases.

== Retrieval Augmented Generation (RAG)

RAG is a method that enhances a generative model's responses by integrating external information retrieval into the generation process. This technique is particularly useful in scenarios where the model needs to provide accurate and up-to-date information that is not contained within its original training data.

=== How RAG works

RAG operates by combining the capabilities of a traditional language model with a retrieval system. The process involves the following steps:

. Query processing: When a query is received, RAG first processes the query to understand its context and intent.
. Information retrieval: It then uses this processed query to retrieve relevant information from an external database or knowledge base.
. Response generation: The retrieved information is fed back into the language model, which synthesizes the external data with its pre-trained knowledge to generate a coherent and contextually enriched response.

=== Use cases for RAG

* Question-answering systems: Enhance accuracy by providing the most current data.
* Content creation: Generate detailed and informed content that requires external citations or up-to-date statistics.

== Fine-tuning Foundation Models

Fine-tuning involves adjusting a pre-trained foundation model on a specific dataset to adapt its responses more closely to the needs of a particular application or domain. This method is critical for tailoring generalist models to perform well on specialized tasks.

=== How fine-tuning works

Fine-tuning adjusts the weights of a pre-trained model through continued training on a new, often smaller, dataset that contains examples more representative of the target task. The steps include:

* Dataset preparation: Compile a dataset that reflects the specific nuances and requirements of the target domain.
* Model adjustment: Train the model on this new dataset, allowing it to learn from these new examples and adjust its parameters accordingly.
* Evaluation and iteration: Regularly evaluate the model's performance on validation data, and iterate on the training process to optimize accuracy and relevance.

=== Use Cases for fine-tuning

* Domain-specific applications: Adapt models to specialized fields like legal, medical, or technical domains.
* Personalization: Customize responses based on user data or preferences.

== Key differences

[cols="1,2,2",options="header"]
|===
| Feature | RAG	| Fine-tuning
| Primary goal |  To enhance responses with external data. | To adapt model behavior to specific domains.
| Data dependency | Depends on external databases for real-time data. | Relies on a specific training set relevant to the task.
| Model adaptability | 	Combines retrieval with generation; adaptable to various data sources. | Tailors the model to specific content or user data.
| Implementation complexity | Involves integrating retrieval mechanisms like vector databases with generative models. | Primarily requires training infrastructure like Domino Hardware Tiers and Jobs for model adaptation, whose progress can be tracked via Domino's Experiment Management.
|===

== Next steps

Learn more about link:420f0c[Retrieval Augmented Generation (RAG)].

----- user_guide/generative-ai/retrieval-augmented-generation.txt -----
:page-version: 6.1
:page-permalink: 420f0c
:page-title: Retrieval Augmented Generation (RAG)
:page-order: 40

Retrieval Augmented Generation (RAG) is a link:1140fb[design pattern in generative AI] that enhances a large language model (LLM) with external knowledge retrieval. This method connects real-time data to generative AI applications, enhancing the accuracy and quality of outputs by providing relevant data context to the LLM during inference.

Domino supports a comprehensive set of tools that enable effective implementation of RAG in various scenarios.

== Types of RAG and use cases

[cols="1,2,2",options="header"]
|===
|Type of RAG | Description | Example use case

|Unstructured data
|Utilizes documents such as PDFs, wikis, website content, and office documents.
|A chatbot that synthesizes enterprise documentation for question and answer.

|Structured data
|Employs tabular data such as data from Domino's link:16d9c1[data access layer] or from existing application APIs.
|A chatbot designed to check the status of an order.

|Tools & function calling
|Integrates calls to third-party or internal APIs to execute specific tasks or update statuses, such as performing calculations or initiating business workflows.
|A chatbot that facilitates order placements.

|Agents
|Uses an LLM to dynamically determine responses to user queries by selecting a sequence of actions.
|A chatbot that functions as a customer service agent.
|===

== RAG application architecture

The architecture of a RAG application includes essential components like a pipeline for data ingestion and indexing, and a chain for data retrieval and response generation. Below is an outline of the critical components and their functions:

* *Indexing*: A pipeline that ingests data from various sources, structuring it for easy access and retrieval. This data can be structured or unstructured. You can use link:942549[Domino Jobs] to schedule the indexing process to load data into a vector database.
* *Retrieval and generation*: This is the core of the RAG process. It involves taking a user query, retrieving related data from the index, and passing both the data and the query to an LLM for response generation. You can use Domino to build and host the apps that serve as the frontend web UI.

== Next steps

Learn how to use link:4c6469[vector databases].

----- user_guide/generative-ai/vector-databases.txt -----
:page-version: 6.1
:page-permalink: 4c6469
:page-title: Vector databases
:page-order: 50

Vector databases are specialized storage systems designed to handle vector embeddings that represent data in high-dimensional space. Vector databases are integral to modern artificial intelligence (AI) applications, offering the ability to efficiently manage and query high-dimensional data. By leveraging Domino's platform capabilities, users can implement sophisticated AI solutions that require fast and accurate similarity searches, enhancing both the performance and scalability of their applications.

Domino supports a wide array of vector databases, essential for powering advanced AI applications such as retrieval augmented generation (RAG), recommender systems, and media recognition tasks. This documentation outlines the process of efficiently indexing, loading, and retrieving data from vector databases using the robust capabilities of the Domino platform.

Vector databases provide:

* Efficient similarity searches: Quickly find the most similar items in large datasets, which is essential for applications like link:420f0c[RAG] and personalized recommendation systems.
* Scalable infrastructure: Handle large volumes of data and complex query patterns without sacrificing performance.
* Enhanced data handling: Store and manage high-dimensional data effectively, optimizing storage and retrieval operations.

== Set up vector databases

Set up vector databases in Domino with the following steps:

=== Step 1: Choose a vector database

Select a vector database that best fits your application needs. Domino supports popular vector databases like Pinecone, QDrant, and others, offering a range of features tailored to different requirements.

=== Step 2: Index data

* Prepare your data: Organize your data into a suitable format for vectorization. This might involve preprocessing steps like normalization or tokenization, depending on the nature of your data.
* Generate embeddings: Use a machine learning model to convert your prepared data into vector embeddings. Domino can facilitate this process through its scalable compute resources and integration with machine learning frameworks.
* Load data into the database: Upload the generated embeddings into your chosen vector database. Domino's job scheduler can automate this process, ensuring data is indexed efficiently and regularly updated as needed.

=== Step 3: Integrate a vector database

* Configure access: Set up connectivity between Domino and the vector database, ensuring secure and reliable data transfer.
* Implement API calls: Use Domino's APIs to query the vector database directly from your applications or during analytical workflows.

=== Step 4: Retrieve data

* Execute queries: Perform queries to retrieve data based on similarity or other criteria, integrating these operations within your Domino Workspaces or Runs.
* Utilize results: Use the retrieved data to enhance your AI applications, whether it's generating responses in a RAG setup, recommending products, or recognizing images or videos.

== Use cases

* RAG: Enhance chatbots and other generative AI applications by providing contextually relevant data at runtime.
* Recommender systems: Improve recommendation accuracy by matching user profiles with products or content based on similarity in vector space.
* Media recognition: Quickly identify and classify media files by comparing embeddings generated from audio or video content.

== Next steps

Find out how to connect to link:5c64ef[Pinecone] and link:c2364c[Qdrant].

----- user_guide/get-help/additional-resources.txt -----
:page-version: 6.1
:page-title: Additional resources
:page-permalink: e3c6d4
:page-order: 10

For more information about Domino, use these resources:

https://www.dominodatalab.com/[Domino Data Lab website^]:: Explore the website.
You will find https://www.dominodatalab.com/solutions[solutions^], including use cases, https://www.dominodatalab.com/resources/[resources^] such as white papers and reports, information about our https://www.dominodatalab.com/partners[partners^], and more.

https://tickets.dominodatalab.com/[Knowledge Base^]::
Search articles written by our support team.

https://blog.dominodatalab.com/[Data Science blog^]::
You can read data science tips and tutorials from leading Data Scientists.
Subscribe to have the content sent directly to your inbox.

https://learn.dominodatalab.com/[Training^]::
Use Domino Academy to learn about Domino.

NOTE: You must get an access code from your Customer Service Manager to use the Domino Academy.

If you still need assistance, contact link:80328c[Domino Technical Support].

//Would be nice to tell them how to submit doc suggestions!

----- user_guide/get-help/browser-support.txt -----
:page-version: 6.1
:page-title: Browser support
:page-permalink: 48f473
:page-order: 50

As part of our effort to make data science teams more productive by
providing a secure, central system of record, Domino strives to deliver a consistent, equal user experience across whichever platforms you prefer.

Domino supports the following browsers on Windows and MacOS desktop:

* Google Chrome (latest 3 major versions)
* Firefox (latest 3 major versions)
* Safari (latest 3 major versions)
* Microsoft Edge (latest 3 major versions)

The following browsers are officially not supported:

* Internet Explorer
* Mobile browsers
* Beta, preview, or other pre-release versions of desktop browsers

----- user_guide/get-help/contact-support.txt -----
:page-version: 6.1
:page-title: Contact technical support
:page-permalink: 80328c
:page-order: 30

You can submit a ticket to the Domino technical support team in the following ways:

. Submit a ticket through our ticketing portal at https://tickets.dominodatalab.com[tickets.dominodatalab.com^].
. Click Help in your Domino application.
. Email support@dominodatalab.com.

For critical issues, always submit a ticket through the web form or Help in the Domino application.

== Guidelines

* Provide as much information as you can.
* Provide steps to reproduce your problem.
* For issues with a specific Run, include a link to the run and a screenshot of the error.
* For issues with the CLI, include the contents of `domino.log`  from the folder in which you ran the command.
* For issues with projects, include a link to the project.
* For issues with environments, include a link to the environment.

----- user_guide/get-help/domino-version.txt -----
:page-version: 6.1
:page-title: Get Domino version
:page-permalink: 0a8782
:page-order: 20

To see the Domino version that you are running, go to `<your domino url>/version`.

For example, users on `try.domino.tech` can see the version at `try.domino.tech/version`.

----- user_guide/get-help/index.txt -----
:page-version: 6.1
:page-title: Get help
:page-permalink: 88cefb
:page-order: 370

link:e3c6d4[Additional resources]::
Refers you to more information about Domino.

link:0a8782[Get Domino version]::
Describes how to identify the version of Domino.

link:80328c[Contact Technical Support]::
How to create a support ticket.

link:055b74[Support bundles]::
How to retrieve a ZIP file with logs and reports to share with Domino support.

link:48f473[Browser support]::
Lists the browsers that Domino supports.

----- user_guide/get-help/support-bundle.txt -----
:page-version: 6.1
:page-title: Support bundles
:page-permalink: 055b74
:page-order: 40


When working with the Domino support team, you might be asked to retrieve a support bundle ZIP file that contains logs and reports from Domino components with information about a Domino execution.

Support bundles are available for Jobs, Workspaces, and Apps.

== Download Job support bundle from the UI

For Jobs, you can simply click the download link by going to: *Jobs detail* > *Logs* > *Support bundle* in the Domino UI.

== Download the support bundle programmatically using execution ID

You can retrieve a support bundle with an execution ID.
To find the execution ID, look in the execution assignment messages in the *Setup Output* in the logs panel for the execution.

The execution assignment message uses the following template:

[source,shell]
----
Successfully assigned <node-pool>/run-<execution-id>-<pod-id> to <node-name>
----

After you have the execution ID, go to the following URL to retrieve that execution's support bundle:

[source,shell]
----
<domino-url>/v4/admin/supportbundle/<execution-id>
----


For example, if you have an execution assignment message like `Successfully assigned aws-staging-compute/run--5e17a24d74904f0007099b9b-tq582 to ip-...`, the execution ID is 
`-5e17a24d74904f0007099b9b`.

To retrieve the support bundle, go to:


[source,shell]
----
<domino-url>/v4/admin/supportbundle/<5e17a24d74904f0007099b9b>
----

----- user_guide/get-started/get-started-ai-hub/ai-hub-templates.txt -----
:page-permalink: 55497f
:page-version: 6.1
:page-title: Domino Data Lab AI Hub templates
:page-sidebar: AI Hub templates
:page-order: 10

The AI Hub is a valuable resource for quickly deploying AI solutions and developing models to drive innovation within your organization.
It is designed to speed up AI development for various business tasks and industries, providing tools, best practices, and resources for projects ranging from traditional predictive models to cutting-edge Generative AI.

These templates focus on efficiency and accessibility, empowering organizations to create, share, and customize AI solutions tailored to their specific needs while gaining valuable insights into Domino's features.

This document provides a summary of the AI Hub templates available in Domino:

* link:#predictive-modeling[Predictive modeling for wind turbine output using SCADA data]
* link:#enterprise-q-a[Enterprise Q&A over your docs]
* link:#sentiment-analysis[Retail investor sentiment analysis]
* link:#fine-tuned-llm[Summarize text using a Falcon-7b fine-tuned LLM]
* link:#text-summarization[Fine-tune a Falcon-40B LLM for text summarization]
* link:#summarize-feedback[Summarize product feedback and respond]
* link:#computer-vision[Anomaly detection using computer vision]
* link:#biorag[BioRAG in partnership with BIP]
* link:#anomaly-detection[Anomaly detection in streaming data]
* link:#ai-training[AI Training Flow]
* link:#finops[Lower AI costs with Domino FinOps]
* link:#llama[Build a chatbot by using Llama 3.1]

[[predictive-modeling]]
== Predictive modeling for wind turbine output using SCADA data

You can find this project on link:https://github.com/dominodatalab/reference-project-wind-turbine/tree/8053115e8fc7287c8be4c3133ccef9d3b7db7840[GitHub^].

This project trains a predictive model on a link:https://www.kaggle.com/datasets/berkerisen/wind-turbine-scada-dataset[Supervisory Control and Data Acquisition (SCADA) dataset^] collected from a physical wind turbine.
SCADA systems are used to control, monitor, and analyze industrial devices and processes.

Our repository provides a step-by-step notebook for training a machine learning model using an link:https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html[Extra Trees Regressor^] and the freely available SCADA dataset from Kaggle.

To explore the project in more detail, visit link:https://domino.ai/platform/ai-hub/templates/predict-turbine-performance-iot-data[the Domino website^].

[[enterprise-q-a]]
== Enterprise Q&A over your docs

You can find this project on link:https://github.com/dominodatalab/reference-project-customqa/tree/89da02cd53c7e5f389152986e21dc2edeb5bc54b[GitHub^].

This template provides a guide on leveraging the open-source LangChain framework, OpenAI's language models, and either Facebook AI Similarity Search (FAISS) or Pinecone to build an interactive search engine capable of performing Q&A on information not included in OpenAI's training data.

It utilizes a Retrieval-Augmented Generation (RAG) framework integrated within a Streamlit web application.

Prerequisites:

* `OPENAI_API_KEY`, required
* `PINECONE_API_KEY`, optional

To explore the project in more detail, visit link:https://domino.ai/platform/ai-hub/templates/build-enterprise-question-answer-application[the Domino website^].

[[sentiment-analysis]]
== Retail investor sentiment analysis

You can find this project on link:https://github.com/dominodatalab/aihub-project-sentiment-analysis/[GitHub^].

This template demonstrates how to fine-tune link:https://huggingface.co/distilbert/distilbert-base-uncased[DistilBERT^] -- a lighter, faster variant of HuggingFace's BERT -- using the link:https://huggingface.co/datasets/fancyzhx/amazon_polarity[Amazon Polarity dataset^] and interacting with a model that determines sentiment on product reviews.

With this template code, your team can learn to import libraries, check NVIDIA GPU acceleration availability, load DistilBERT, read a CSV dataset, prepare training, test, and validation subsets, fine-tune a model, and output an F1 metric.

For a more detailed exploration of the project, visit link:https://domino.ai/platform/ai-hub/templates/retail-investor-sentiment-analysis[the Domino website^].

[[fine-tuned-llm]]
== Summarize text using a Falcon-7b fine-tuned LLM

You can find this project on link:https://github.com/dominodatalab/reference-project-llm-inference[GitHub^].

This template uses different inference frameworks to generate text output from Falcon-7b, a fine-tuned Large Language Model (LLM).
Additionally, this template guides the deployment of the fine-tuned LLM as a Domino endpoint and a Streamlit app in Domino.

This project utilizes a ctranslate2 model, which offers optimized implementations for various hardware, including CPUs and GPUs, which makes it faster and more resource-efficient than many other inference engines.

For a more detailed exploration of the project, visit link:https://domino.ai/platform/ai-hub/templates/summarize-text-using-fine-tuned-llm[the Domino website^].

[[text-summarization]]
== Fine-tune a Falcon-40B LLM for text summarization

You can find this project on link:https://github.com/dominodatalab/reference-project-finetune-falcon40b[GitHub^].

This reference project shows how to fine-tune the Falcon-40b parameter Large Language Model (LLM) on a link:https://huggingface.co/datasets/samsum[dataset^] to summarize conversations using the Hugging face Trainer.
Falcon-40B can create a wide range of contextually accurate content to generate high-quality natural language outputs like blogs, emails, and text translations.

In this project, we will use the model's 4-bit and 8-bit quantized version and train a LoRA adapter.

For a more detailed exploration of the project, visit link:https://domino.ai/platform/ai-hub/templates/fine-tune-llm-text-summarization[the Domino website^].

[[summarize-feedback]]
== Summarize product feedback and respond

You can find this project on link:https://github.com/dominodatalab/aihub-aws-text-generation-template[GitHub^].

In this template, we generate an email response to a customer who has provided negative feedback on services received from a customer support engineer.

The three notebooks demonstrate how to provide feedback using three different techniques:

* First, the Amazon (AWS) Titan, a large language model, and Bedrock API utilize a zero-shot prompt without context as an instruction for the model.
* Second, Anthropic's Claude text model uses the Langchain framework integration with Bedrock and uses a zero-shot prompt without context.
* The third notebook provides additional context to the prompt, including the original customer email from LangChain.

Prerequisites:

* `AWS_DEFAULT_REGION`, required
* `AWS_PROFILE`, required
* `BEDROCK_ASSUME_ROLE`, required

For a more detailed exploration of the project, visit link:https://domino.ai/platform/ai-hub/templates/summarize-product-feedback-respond[the Domino website^].

[[computer-vision]]
== Anomaly detection using computer vision

You can find this project on link:https://github.com/dominodatalab/reference-project-anomaly-detection[GitHub^].

In this project, we apply anomaly detection for quality control and defect detection.

We fit a link:https://arxiv.org/abs/2011.08785[PaDIM^] model against the link:https://openaccess.thecvf.com/content_CVPR_2019/papers/Bergmann_MVTec_AD_\--_A_Comprehensive_Real-World_Dataset_for_Unsupervised_Anomaly_CVPR_2019_paper.pdf[MVTecAD^] dataset using link:https://github.com/openvinotoolkit/anomalib/tree/main[Anomalib^], a comprehensive deep-learning library designed to serve as a hub for state-of-the-art anomaly detection algorithms.
The Domino platform can expose the persisted model as a Domino endpoint.
The score function accepts an image path as an argument and returns a boolean prediction (anomalous or not) and a confidence score of the prediction.

To learn more, visit link:https://domino.ai/platform/ai-hub/templates/defect-detection-computer-vision[the Domino website^].

[[biorag]]
== BioRAG in partnership with BIP 

You can find this project on link:https://github.com/dominodatalab/BioRAG-AI-Template-Bip[GitHub^].

Managing and analyzing large volumes of clinical information efficiently and securely is critical for innovation and regulatory compliance.
While GenAI is promising, traditional information management methods often fall short, leading to inefficiencies, increased costs, and potential risks to patient safety.

Enter BioRAG (RAG - Retrieval Augmented Generation), an advanced AI solution in link:https://domino.ai/platform/ai-hub[Domino's AI Hub^], designed to transform how clinical data is handled and utilized.
This template utilizes LangChain, Qdrant, a high-performance vector database, and Azure Blob Storage for document storage to allow users to interact with their documents via a streamlit application.

Prerequisites:

* `QDRANT_URL`, `QDRANT_KEY`, required
* `AZURE_EMBEDDINGS_DEPLOYMENT_NAME`, `AZURE_EMBEDDINGS_API_KEY`, `AZURE_EMBEDDINGS_MODEL_NAME`, `AZURE_EMBEDDINGS_ENDPOINT`, required
* `OPENAI_API_VERSION`, `OPEN_AI_TYPE`, required
* `AZURE_CHAT_ENDPOINT`, `AZURE_CHAT_DEPLOYMENT_NAME`, `AZURE_CHAT_API_KEY`, `AZURE_CHAT_MODEL`, `AZURE_BLOB_CONTAINER_NAME`, `AZURE_BLOB_CONNECTION_STRING`, required

To learn more, visit link:https://domino.ai/accelerate_clinical_trials_with_biorag[the Domino website^].

[[anomaly-detection]]
== Anomaly detection in streaming data

You can find this project on link:https://github.com/dominodatalab/aihub-project-anomaly-detection[GitHub^].

This template presents three unsupervised algorithms to detect anomalies in CPU utilization streaming data using the link:https://www.kaggle.com/datasets/boltzmannbrain/nab[Numenta Anomaly Benchmark dataset^].
This open-source dataset comprises over 50 labeled real-world and artificial time series data files, plus a novel scoring mechanism designed for real-time applications.

The three algorithms presented by the templates are moving average, exponential moving average, and isolation forest.

For a more detailed exploration of the project, visit link:https://domino.ai/platform/ai-hub/templates/anomaly-detection-streaming-data[the Domino website^].

[[ai-training]]
== AI Training Flow

You can find this project on link:https://github.com/dominodatalab/domino-ai-flows/tree/main[GitHub^].

This project demonstrates a sample AI training script using Domino Flows.
A sample data set is provided so users can run a flow by executing a provided command and utilizing the Flyte console.

The sample flow contains two tasks - one for data preparation and one for model training.
Each task ultimately triggers a Domino Job and returns the outputs.

Prerequisites:

* Domino version 5.11.0.

For a more detailed video explanation of the project, visit link:https://domino.ai/ai_training_flow[the Domino website^].

[[finops]]
== Lower AI costs with Domino FinOps

You can find this project on link:https://github.com/dominodatalab/costs-dashboard[GitHub^].

Track and control AI infrastructure spending and save without manually tagging all infrastructure assets or reconciling cloud bills.
Obtain critical insights into computing and storage spending by users, projects, organizations, computing clusters, and any dimension through clear and accurate cost breakdowns within the Domino platform.

Prerequisites:

* Admin permissions in the Domino platform.
* Storage Configuration Information (i.e., bucket, endpoint, region, etc.).

NOTE: Disclaimer - Domino Reference Projects are starter kits built by Domino researchers.
They are not officially supported by Domino.
Once loaded, they are yours to use or modify as you see fit.
We hope they will be a beneficial tool on your journey!

[[llama]]
== Build a chatbot by using Llama 3.1

You can find this project on link:https://github.com/dominodatalab/reference-project-llama2-sft-chatbot[GitHub^].

This template guides you through building a chatbot by fine-tuning Meta's state-of-the-art Llama3 model using Supervised Fine Tuning (SFT).

The model undergoes domain-specific adaptive fine-tuning, enhancing its focus and alignment by training on the link:https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k[guanaco-llama2-1K dataset^].
You can interact with the resulting model via a Streamlit application or an API.

== Next steps

You might also find the following Domino products and topics helpful:

* link:140b48[Domino Data API]
* link:f35c19[API Guide]
* link:1140fb[Generative AI]
* link:94beaa[Retrieval Augmented Generation (RAG)]
* link:e2bac4[Machine learning operations (MLOps)]

----- user_guide/get-started/get-started-ai-hub/index.txt -----
:page-version: 6.1
:page-permalink: 123ae8
:page-order: 40
:page-title: Get started with AI Hub templates
:page-sidebar: Get started with AI Hub

The https://domino.ai/solutions/ai-hub/templates[AI Hub^] is a set of curated Git repositories that customers can add to their own project template lists if they choose.
The AI Hub lets you and your team discover and reuse templates for several common ML use cases and industry-specific patterns while providing access to best practices and “art of the possible” inspiration with Domino.

== Best practices and "art of the possible"

Domino's AI Hub features cutting-edge link:55497f[Enterprise Templates] from Domino and our partners, providing access to the latest data science innovations.

You also have the freedom to design *Project Templates* tailored to your organization's best practices. These bespoke templates remain private and aren't uploaded to Domino.

Templates provide the following benefits:

* Increase productivity with ready-made templates.
* Stay updated with the latest innovations in data science.
* Establish and maintain internal standards.

== Next steps

* See the available link:55497f[Domino AI Hub templates].
* Create a link:5fed45[Project template] for yourself, your team, or your organization.
* Browse templates on our website https://domino.ai/solutions/ai-hub/templates[Enterprise Templates in the AI Hub^].

----- user_guide/get-started/get-started-matlab/2-create-project.txt -----
:page-version: 6.1
:page-title: Create a MATLAB Project
:page-sidebar: Create a Project
:page-permalink: d00413
:page-order: 10

Projects contain data, code, and environment settings. The entire
project is tracked and automatically revisioned. A commit occurs when
the system saves changes made to a project's files by a user or code.

You can create your own projects or invite others to collaborate on your
projects. You can also export project data or results for consumption by
other projects.

== Create a Project:

. If necessary, in the navigation pane, click *Projects*.
. Click *New Project*.
. In the New Project window, give your project an informative name like
`weather-prediction`.
. Leave the link:71afc6[Visibility] set to `Private`.
. Click *Create*.

----- user_guide/get-started/get-started-matlab/3-configure-project.txt -----
:page-version: 6.1
:page-title: Configure your MATLAB Project
:page-sidebar: Configure your Project
:page-permalink: 38fd0e
:page-order: 20

Every project has its own settings. Consider the following when
configuring a new project:

* link:dba65c#hardware_tier[Hardware Tier]
* link:f51038[Environment]
* link:d7731d[Collaborators]

== Step 1: Select your hardware tier

A link:dba65c#hardware_tier[Hardware Tier]
represents the compute resources that will be available for your run.
You can specify memory, CPU cores, and GPUs with hardware tiers.


. In the navigation pane, click *Settings*.
. From the *Hardware tier* list, select the compute resource on which
you will execute your code. For this tutorial, select the smallest
hardware tier.
+
By default, the selected hardware tier will be used for all subsequent
executions of code in the project. It can also be changed at any time.
+
Contact your Domino administrator for additional hardware tiers.


== Step 2: Configure your Environment

To define your link:f51038[Environment], configure the software, packages, libraries, and drivers. Depending on
your corporate Domino setup, Domino might include one or more options
for MATLAB. Environments typically map to MATLAB releases (for example,
R2021a) or to combinations of releases and toolboxes (for example,
MATLAB and Statistics and Machine Learning Toolbox or MATLAB and
Simulink).

=== Access the MATLAB environment:

. Go to the *Compute Environment* menu.
. Select the MATLAB version to use.

[NOTE]
====
Your compute environment menu will likely have different options. To
learn how to add more packages and customize or create your own
Environment, see link:f51038[Environments]. See link:release_notes/5-0-0[new features in Domino 5.0 and up] for information about
the 5.0 Domino Standard Environment.
====


== Step 3: Configure the Project permissions

As the owner of the project, you can set different access levels for
collaborators and colleagues. 

Invite a colleague to be a Contributor to your project:

. Click the *Access & Sharing* tab.
. Type the user's email or the username.
. Type a welcome message.

With the `Contributor` role, the user can read, write, and execute code in this project.
See link:7876f1[Collaborator Permissions] for more information about permissions for each collaborator role.

----- user_guide/get-started/get-started-matlab/4-start-workspace.txt -----
:page-version: 6.1
:page-title: Start a MATLAB Workspace
:page-sidebar: Start a Workspace
:page-permalink: bfda3d
:page-order: 30

Workspaces are containerized sessions created on a machine referred to
as the executor, in the required hardware tier.

Use Workspaces to interact with code in MATLAB stored in Domino.

== Start a Workspace

. In the navigation pane, click *Workspaces*.
. Click *Create New Workspace*.
. In the Launch New Workspace window, type a name for the workspace.
. Confirm the workspace environment is set to MATLAB.
. Click *Launch Now*.

The browser automatically opens the workspace.
You'll see a MATLAB desktop when your workspace becomes available.
If you're new to MATLAB, see the https://www.mathworks.com/help/matlab/index.html[MATLAB documentation^] and https://www.mathworks.com/learn/tutorials/matlab-onramp.html[on-ramp tutorials^].

image::/images/5.0/matlab-workspace.png[alt="The MATLAB Workspace"]

----- user_guide/get-started/get-started-matlab/5-get-data.txt -----
:page-version: 6.1
:page-title: Fetch and save your data in MATLAB
:page-sidebar: Fetch and save your data
:page-permalink: 45a2e6
:page-order: 40

You can upload local data into Domino or access large datasets stored in a database or data service.

== Work with data in Domino:

. You can copy your data into Domino. If you are working with data on
your local machine or in a shared server, you might want to
link:7a0fee[upload your data into Domino].
. You can query your data from Domino. If you have a large dataset
stored in a database or data service, you might have to
query the database or the API for the data service.



== Step 1: Copy data to Domino project

In this step, you'll use the terminal to copy your data into the
project. The starting file path in your workspace is `/mnt`. By default,
this is considered the root of your Domino project. If you add or modify
files in `/mnt`, you can save them back to your project when you stop or
sync the workspace.


. Copy and paste the following command to fetch data from the U.S. NOAA
Climatology service.
+
[source,matlab]
----
!curl -o tegel.csv https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access/GME00121150.csv
----

. Go to the *Current Folder* to see the downloaded file.
+
image::/images/5.0/matlab-file-download.png[alt="The downloaded file"]

[NOTE]
====
The temperature data is in tenths of Celsius degrees. Learn more here:
https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt[README File
for Daily Global Historical Climatology Network].
====

== Step 2: Save data to Domino project

. In the navigation bar, click the *File Changes* icon.
+
image::/images/5.0/file-changes-sync.png[alt="Sync file changes", width=500]

. Click *Sync All Changes*.


== Step 3: Review your changes

. Click *Domino*.
+
image::/images/5.0/domino-icon.png[alt="The Domino icon", width=500]

. In the navigation bar, click
*Code*
to see the file that you
downloaded in the previous steps. Notice that a new revision has been
saved.
+
image::/images/5.0/downloaded-file.png[alt="New revision of the downloaded file", width=1000]

See the documentation for other methods to copy data into Domino (https://curl.se/[curl^] or https://www.gnu.org/software/wget/[Wget^]) and link:fbb41f[query data from Domino].

----- user_guide/get-started/get-started-matlab/6-develop-your-model.txt -----
:page-version: 6.1
:page-title: Develop your MATLAB model
:page-sidebar: Develop your model
:page-permalink: 04e21f
:page-order: 50

When you are developing your model, you can use Workspaces to quickly
execute code, see outputs, and make iterative improvements.

You previously learned link:bfda3d[how to start a Workspace]
and explored basic options.

In this topic, you will use your workspace to load, explore, and
transform data. After the data has been prepared, you will train a
model.

== Step 1: Load and explore the dataset


. Click the browser tab to return to the Workspace.
. Click *New* > *Live Script* to create a MATLAB Live Script.
. Go to *Save > Save As...* to save it as `ber_hot_weather.mlx`.
. Copy and paste the following command:
+
[source,matlab]
----
opts = detectImportOptions('tegel.csv');
----
+
. Click *Run*.
. Copy and paste the following commands to load `tegel.csv` into MATLAB.
Then, click *Run*. This loads the data using the `readtable()` function,
giving it the import options object as a second argument.
+
[source,matlab]
----
opts.SelectedVariableNames = {'DATE', 'PRCP', 'TMIN', 'TMAX'};
opts = setvartype(opts, {'DATE','PRCP','TMIN','TMAX'},{'datetime','double', 'double', 'double'});
berWeatherTbl = readtable("tegel.csv", opts);
head(berWeatherTbl)
----
+
These commands loaded the following columns and assigned data types to
them.
+
* *Date* – date the temperature was read
* *PRCP* – total precipitation for the day
* *TMIN* – lowest temperature measured that day
* *TMAX* – highest temperature measured that day
+
The result will look similar to the following:
+
image::/images/5.0/matlab-table.png[alt="The MATLAB table"]
. Click *Section Break* to create a new section in your script.
+
[NOTE]
====
Your cursor must be at the end of the previous section. You might have
to go to the Insert tab to find *Section Break*.
====

.. Copy and paste the following command to format the dates into a
Year/Month/Day format and store each field as a table variable. This
helps you examine the data.
+
[source,matlab]
----
[berWeatherTbl.year, berWeatherTbl.month, berWeatherTbl.day] = ymd(berWeatherTbl.DATE);
----
.. Copy and paste the following command to limit the dataset to
temperatures between January 2000 and December 2019, inclusive, by
removing rows with data outside this range. This speeds data processing.
+

[source,matlab]
----
berWeatherTbl = berWeatherTbl(berWeatherTbl.year > 1999 & berWeatherTbl.year < max(berWeatherTbl.year) , :);
----


.. Copy and paste the following commands to divide all temperature data
by 10 to get the temperatures in full Celsius degrees. You might have
noticed that temperatures in the *TMAX* or *TMIN* columns look a bit
odd. This is because NOAA uses a temperature format consisting of a
tenth-of-a-degree in the Celsius scale.
+

[source,matlab]
----
berWeatherTbl.TMAX = berWeatherTbl.TMAX/10;
berWeatherTbl.TMIN = berWeatherTbl.TMIN/10;
----

.. Copy and paste the following command to complete missing data with
interpolated information.
+

[source,matlab]
----
berWeatherTbl = fillmissing(berWeatherTbl, 'linear');
----

.. Copy and paste the following `head()` function to preview the start
of the table.
+

[source,matlab]
----
head(berWeatherTbl)
----

.. Click *Run*. The result looks like the following:
+
image::/images/5.0/matlab-table-2.png[alt="The updated MATLAB table"]
+
[IMPORTANT]
====
Click *Save* occasionally to save your script.
====
. Click *Section Break* to create another section in your Live Script.
In this section, you'll calculate how many hot days have occurred in
Berlin since the year 2000. To calculate this, copy and paste the
following command to define a hot day as 29 degrees Celsius for the
baseline threshold.
+
[source,matlab]
----
hotDayThreshold = 29;
----
. Copy and paste the following command to calculate how many hot days
have occurred since (and including) the year 2000. This command creates
a table column indexing the days with maximum temperatures (*TMAX*) that
meet or exceed the hot day threshold.
+
[source,matlab]
----
berWeatherTbl.HotDayFlag = berWeatherTbl.TMAX >= hotDayThreshold;
----
.. Copy and paste the following command to use ``groupsummary()`` to count how many hot days were flagged:
+
[source,matlab]
----
      numHotDaysPerYear = groupsummary(berWeatherTbl, 'year', 'sum', 'HotDayFlag');
----
.. Copy and paste the following command to repeat the same approach to find the highest temperature of each year:
+
[source,matlab]
----
     maxTempOfYear = groupsummary(berWeatherTbl, 'year', 'max', 'TMAX');
----
.. Copy and paste the following command to combine the variables to create a table named ``annualMaxTbl``:
+
[source,matlab]
----
    annualMaxTbl = join(numHotDaysPerYear, maxTempOfYear);
     annualMaxTbl.Properties.VariableNames = {'Year', 'daysInYear', 'hotDayCount', 'maxTemp'};
     annualMaxTbl
----
.. Click **Run Section**. The table looks like the following:
+
image::/images/5.0/matlab-table-3.png[alt="Table showing the summarized data"]
+
. Click *Section Break* to create another section in your Live Script.
In this section, you'll visualize the weather data using a chart with
that combines a bar graph and line graph. The chart will use two y-axes.
+
The bar graph will represent the hot day count (for a given year), and
the line graph will represent the highest annual temperature (in
Celsius, for a given year). The y-axis on the left side of the chart
will correspond to the hot day count, and the y-axis on the right side
of the chart will correspond to the highest annual temperature.

** Copy and paste the following to create a hot day count bar graph.
+
[source,matlab]
----
figure
hold on
yyaxis left
bar(annualMaxTbl.Year,  annualMaxTbl.hotDayCount, 'FaceColor', 'b');
----
** Copy and paste the following to add a title and labels to the x-axis
and left side y-axis.
+
[source,matlab]
----
titleText = sprintf("%s%d%s%d%s%d", "Number of hot days (over ", hotDayThreshold,"\circC) - ", min(annualMaxTbl.Year), "-", max(annualMaxTbl.Year));
title(titleText)
ylabel("Hot days per year")
xlabel("Year")
----

** Copy and paste the following to draw the line plot for the highest
temperature each year.
+
[source,matlab]
----
yyaxis right
ylabel("Highest Annual Temperature in \circC")

plot(annualMaxTbl.Year, annualMaxTbl.maxTemp, 'Color', 'r', "Marker","*")
hold off
----
** Click *Run Section*. Your chart should look something like the
following:
+
image::/images/5.0/matlab-table-4.png[alt="Plot showing the summarized data"]

== Step 2: MATLAB - Generate predictions from data

In this section, you will use an interactive machine-learning MATLAB
application called *Regression Learner* to develop a model that can
predict the weather for the next 20 days.

=== 1: Partition the data

You must partition the data that will be used with Regression
Learner into the following sets:

* Data to train the model
* Data to test the model

. Click *Section Break* in the Live Script and copy and paste the
following code to remove the `HotDayFlag` column.
+
[source,matlab]
----
berWeatherTbl.HotDayFlag = [];
----
. Copy and paste the following to partition the data.
+
[source,matlab]
----
cv = cvpartition(berWeatherTbl.year, 'Holdout', 0.3);
dataTrain = berWeatherTbl(cv.training, :);
dataTest = berWeatherTbl(cv.test, :);
----
. Click *Run Section*.

=== 2: Train the model

. Click the *APPS* tab and click Regression Learner app. If you do not
see the Regression Learner app, click the arrow to expand the full app
list.
+
image::/images/5.0/regression-learner-icon.png[alt="The regressions learner icon"]
+
[IMPORTANT]
====
If Regression Learner is not in the apps list, contact your IT team or
your MathWorks account manager for assistance.
====
+
The application opens in a new window.

. Click *New Session* and select *From Workspace*.
. Use the New Session window to specify the input variables for
predictions in your model, as well as the outputs (or responses) you
want to predict. In this tutorial, the output is the maximum
temperature.

.. For the input variable, from the *Workspace Variable* list, select
*dataTrain*.
.. For the output, in the Response section, select *TMAX* (maximum
temperature).
.. Select *PRCP*, *TMIN*, *year*, *month*, and *day* in the Predictors
section.
+
image::/images/5.0/matlab-variables.png[alt="The MATLAB variables"]

.. Click *Start Session*. The Regression Learner window refreshes and
shows the original data set and the values of *TMAX*.
+
image::/images/5.0/original-train.png[alt="The original train output"]

. Select the type of model to be used for model training. For this
tutorial, select *Coarse Tree*.
+
.. Regression Learner runs best on a container with multiple cores
because it can run in parallel and produce models rapidly. If you are
using a single-core container, click *Use Parallel* in Regression
Learner to turn off parallel processing.
.. Click *Train* to start the model training process. The Domino
container spins up a _parallel pool_ which is a method to optimize the
model training.

. Select *Fine Gaussian SVM* to compare the results to *Coarse Tree*.
You can select additional models or even select all models and compare
the results to identify the best fit for your data.
.. Click the arrow to access the model types.
+
image::/images/5.0/access-model-types.png[alt="Arrow to access the model types"]
+
image::/images/5.0/model-selection.png[alt="The model selection"]
.. Click *Train*. The model list automatically selects the model that
best fits the data. Several visualizations are shown to demonstrate
this.
+
image::/images/5.0/best-performing-model.png[alt="Best performing model and visualizations"]
.. Click *Predicted vs. Actual Plot* open a chart that shows how many
predictions the model made that fit correct values in the data. The
closer the predictions are to the diagonal, the better the predictions.
+
image::/images/5.0/predicted-vs-actual.png[alt="Predicted vs. actual plot"]
. Click *Generate Function* to use Regression Learner to create a
function that will be used to deploy the model with Domino. MATLAB
generates the function in an M-file. Click *Save* to save the file as
`trainRegressionModel.m`.
+
image::/images/5.0/m-file.png[alt="Save the file"]

=== 3: Export the model

. Click *Export Model* to export the model to your Domino workspace so
you can use it for predictions.
+
[NOTE]
====
If you cannot find the *Export* button, you might have to switch back to
the Regression Learner. See the following images for guidance.

image::/images/5.0/find-reg-learner.png[alt="Navigate back to the Regression Learner"]
====
+
image::/images/5.0/export-model-button.png[alt="Export the model"]

. Type a name for the model, such as *weatherModel*, and click *OK*.
. Close the Regression Learner app and you can see the trained model in
your workspace.
+
image::/images/5.0/matlab-model-available.png[alt="The trained model is available in your Workspace"]
+
Notice that the Command Window shows information about how to use the
model to make predictions with the following line of code:
+
[source,matlab]
----
yFit  = weatherModel.predictFcn(T);
----
+
If you input a table of data, this line of code will output a prediction
(as a table). The input table must include data organized like the data
you used in `berWeatherTbl` – date, precipitation, minimum temperature,
month, day and year. It must not include *TMAX*, as that value will be
predicted. The model will predict the *TMAX* value and include it in
`yFit`.

=== 4: Test the model

. To test the model with the data you partitioned earlier, create a
*Section Break* in your Live Script (`ber_hot_weather.mlx`). Copy and
paste the following to use the model with the test data and the function
call that was listed in the Command Window.
+
[source,matlab]
----
yFit  = weatherModel.predictFcn(dataTest);
----
. Copy and paste the following to compare the results column to the
actual values in the test data set.
+
[source,matlab]
----
err = yFit - dataTest.TMAX;
----
. Copy and paste the following to draw a histogram to visualize the
results.
+
[source,matlab]
----
figure;
histogram(err)
xlim([-15 15])
ylabel('Number of predictions');
xlabel('Gap with actual test data')
----
. Click *Run Section*. The result looks like the following:
+
image::/images/get_started_matlab/step-5.2-17.png[alt="Histogram output"]
. To save the working model to be used later, copy and paste the
following in the Command Window and then press Enter.
+
[source,matlab]
----
save weatherModel weatherModel
----
+
image::/images/get_started_matlab/step-5.2-18.png[alt="Use the command window to save the model"]

=== 5: Make predictions

You can use the model to predict the weather for next year. You'll
generate a table with next year's dates and add randomly selected,
historical precipitation and minimum temperature data to the table for
those dates. This information helps the model make proper predictions.

. Create a new *Section Break* in your Live Script.
. Copy and paste the following to create a table with date and
temperature input data.
+
[source,matlab]
----
todayDate = datetime('today');
daysIntoFuture = 365;
endDate = todayDate + days(daysIntoFuture);
predictedMaxTemps = table('Size', [daysIntoFuture+1 7], 'VariableTypes', {'datetime', 'double', 'double', 'double', 'double', 'double', 'double'}, 'VariableNames', berWeatherTbl.Properties.VariableNames);
x=1;
----

. Copy and paste the following to loop through the next 20 days and
populate the table.
+
[source,matlab]
----
for i=todayDate:endDate
        [y, m, d] = ymd(i);
        minTemps = berWeatherTbl.TMIN(berWeatherTbl.month == m & berWeatherTbl.day == d);
        prcps = berWeatherTbl.PRCP(berWeatherTbl.month == m & berWeatherTbl.day == d);
    curMinTemp = NaN;
    [historicalRowCount z] = size(minTemps);
    randomRow = randi([1 historicalRowCount]);
    curMinTemp = minTemps(randomRow);
    predictedMaxTemps.TMIN(x) = curMinTemp;
    randomRow = randi([1 historicalRowCount]);
    predictedMaxTemps.PRCP(x) = prcps(randomRow);
    predictedMaxTemps.DATE(x) = i;
    predictedMaxTemps.year(x) = y;
    predictedMaxTemps.month(x) = m;
    predictedMaxTemps.day(x) = d;
    predictedMaxTemps.TMAX(x) = 0;
    x = x+1;
end

head(predictedMaxTemps)
----

. Click *Run Section*.
+
The result is a preview of the table with historical weather data that
you can use for weather predictions. The predictions will be listed in
the *TMAX* column of the table after the table is run through the model.
+
image::/images/5.0/preview-table.png[alt="A preview of the table"]
+
. To run the model, copy and paste the following into a new Section
Break and run the Section.
+
[source,matlab]
----
yFit = weatherModel.predictFcn(predictedMaxTemps);
result = table(predictedMaxTemps.DATE, yFit, 'VariableNames', {'Date', 'Predicted TMAX'})
----
+
The following is an AI-driven weather prediction.
+
image::/images/5.0/prediction.png[alt="Table showing the AI-driven prediction data"]
. Copy and paste the following code, and then *Run Section* to draw this
in another plot and count how many hot days will be forecasted:
+
[source,matlab]
----
figure
plot(result.Date, result.("Predicted TMAX"))
titleText = sprintf("%s%d%s", "Weather forecast for the next ", daysIntoFuture, " days in Berlin, Germany (\circC)");
title(titleText)
ylabel('Forecasted Daily High Temperature')
----
+
image::/images/5.0/plot-graph.png[alt="Plot showing the prediction data"]
. Copy and paste the following code, and then *Run Section* to predict
how many hot days will happen during the next year.
+
[source,matlab]
----
hotWeatherDaysIdx = result(result.("Predicted TMAX") > hotDayThreshold, :);
height(hotWeatherDaysIdx)
----
+
The result on January 24, 2022 was a prediction of 0 hot days between
January 2022 and February 2022. The results will vary based on the
dates, data, and model used.
. To export your model, in the Command Window, type the following to
save it into a MAT file:
+
[source,matlab]
----
save weatherModel weatherModel
----
+
Anyone in your Domino project can load it later with the following
command:
+
[source,matlab]
----
load weathermodel.mat
----
----- user_guide/get-started/get-started-matlab/7-clean-up-workspace.txt -----
:page-version: 6.1
:page-title: Clean up your MATLAB Workspace
:page-sidebar: Clean up your Workspace
:page-permalink: b1d21e
:page-order: 60

Stop any workspace sessions that you started as a part of this tutorial.
This prevents you from incurring unnecessary charges if your Domino is
deployed in the cloud. If your Domino is deployed on premises, this
frees up your compute resources for others to use.

To stop your session in Domino, save your files and click *Stop*. Don't
forget to *Sync All Changes*.

image::/images/5.0/save-and-stop.png[alt="Save your files and stop the session"]

[NOTE]
====
You can also go to *Workspaces* and stop the corresponding workspace
session.
====

The new files you created when you link:04e21f[developed your model] are visible in the
*Code*
section.

----- user_guide/get-started/get-started-matlab/9-working-with-domino-datasets.txt -----
:page-version: 6.1
:page-title: Working with Datasets in MATLAB
:page-sidebar: Working with Datasets
:page-permalink: c3d953
:page-order: 80

If your Domino project uses a large number of files (for example, more
than 10,000), or a single file larger than 8GB, consider using a
link:305721[Domino dataset].

The following summarizes the lifecycle of a dataset:

* Datasets are defined in a *.yaml* file, along with input folders and
output folders.
* A newly defined dataset is stored in the input folder specified in the
*.yaml* file. By default, the dataset in the input folder is read-only,
while files in the output folder are writable.
* If you do not write anything to the output folder, the dataset remains
unchanged.
* You must copy any files that you'd like to persist from the dataset in
the input folder to the output folder.
* If you write to the output folder, the dataset files will be
overwritten. However, datasets are saved as snapshots so you can roll
back to a previous snapshot of the dataset if needed.

This topic describes how to use a dataset with the weather project.


== Step 1: Add a dataset to your Project

. In the navigation pane, click *Data*.
. Click *Create New Dataset*.
. Type a *Name* (such as get-started-MATLAB-dataset) and description for
your dataset, then click *Create Dataset*.
+
image::/images/5.0/new-workspace-datasets.png[alt="Create a new dataset", width=600]

. To take an initial snapshot to create the initial version of your
dataset, in the navigation pane, click *Workspaces*. Click *Create New
Workspace* and give it a name.
. Select MATLAB as your workspace IDE. Click *Launch Now*. Your MATLAB
workspace launches with a new folder used to store the data that is part
of your dataset.
. To locate the new folder, click the "/" in the file path of your
MATLAB workspace. Next, go to the dataset folder that Domino created for
you: `/domino/datasets/local/get-started-MATLAB-dataset`.
+
image::/images/5.0/dataset-path.png[alt="The dataset path"]

. To populate the dataset, download weather station files from the same
NOAA repository that you used earlier in the project. Use the back arrow
to return to your work directory (*/mnt*), and create script named
*downloadToDatasetDir.m*.
. Copy and paste the following to create a function to download the NOAA
data:
+
[source,matlab]
----
function downloadToDatasetDir()
% NOAA data URL
baseUrlString = "https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access/";

% Prefix shared by weather stations in Argentina
baseWeatherStationId = 'AR0000000';

% the location to save the files – the dataset output directory
datasetFolder = "//domino/datasets/local/get-started-MATLAB-dataset/";
% There are 16 weather station files. We will iterate and download each one
for counter=1:16
    if counter<10
        weatherStationId = sprintf('%s%s%d', baseWeatherStationId, '0', counter);
    else
        weatherStationId = sprintf('%s%d', baseWeatherStationId, counter )
    end

    urlString = sprintf("%s%s%s", baseUrlString, weatherStationId, ".csv");
    savedFileName = sprintf("%s%s%s", datasetFolder, weatherStationId, ".csv");
websave(savedFileName, urlString);

end
end
----

. Save the file, then type *downloadToDatasetDir* to run it from the
Command Window in your MATLAB workspace. Click the / in the navigation
bar and go to */domino/datasets/local/get-started-MATLAB-dataset* to see
the output.
+
image::/images/5.0/run-command-and-see-output.png[alt="Run the command and see the output"]

. To save the files to Domino, in the navigation pane, click *Files
Changes*. Click *Sync All Changes*.
. In the navigation pane, click the Domino logo. Then, click *Data* and
you can see that the dataset is listed.
+
image::/images/5.0/datasets.png[alt="Listed datasets"]

. Click the dataset to open a list of the files that you downloaded.

== Step 2: Create a snapshot

When you are ready to version the contents of a dataset, you can create
a Snapshot.

. From the navigation pane, click *Data*.
+
image::/images/5.0/datasets.png[alt="Listed datasets"]

. Double-click the dataset for which you want to create a snapshot.
. Click *Take Snapshot > Include all files*.
+
image::/images/5.0/take-snapshot-menu.png[alt="The Take Snapshot menu", width=400]

. In the Confirm Dataset Snapshot? window, type a tag such as "weather."
You can use this tag to mount the snapshot with a friendly name in
subsequent executions. Click *Confirm*.
+
image::/images/5.0/confirm-data-snapshot.png[alt="The Confirm Dataset Snapshot window", width=600]
+
When the snapshot is done, you can see it from the Snapshots list.
+
image::/images/5.0/snapshot-complete.png[alt="Completed snapshot in the list of snapshots"]

----- user_guide/get-started/get-started-matlab/deploy-your-model/index.txt -----
:page-version: 6.1
:page-title: Deploy your MATLAB Model
:page-sidebar: Deploy your Model
:page-permalink: a8b4c4
:page-order: 70

After you developed your model and found it to be useful, you must
deploy it. There is no deployment method that is best for all models.
Therefore, Domino offers the following deployment options.

* Scheduled Reports
* Launchers
* Web Applications
* Domino endpoints

For this tutorial, you will use Scheduled Reports or Launchers to deploy
your MATLAB-generated model. The sections in this step are not dependent
on each other. For example, you do not have to complete the *Scheduled
Jobs* section to understand and complete the *Launchers* section.

* link:673577[Scheduled Jobs]
* link:ac5c29[Launchers]

----- user_guide/get-started/get-started-matlab/deploy-your-model/launchers.txt -----
:page-version: 6.1
:page-title: Launchers with MATLAB
:page-sidebar: Launchers
:page-permalink: ac5c29
:page-order: 20

Launchers are simple web forms that you can use to run templatized
scripts. They are especially useful if your script has command line
arguments that dynamically change the way the script executes. For
heavily customized scripts, those command line arguments can quickly get
complicated. Use Launchers to expose all that as a simple web form.

To do this with MATLAB, you will refactor the .m file used in your
scheduled job as a function, and give Launcher users the ability to
specify the following parameters:

* The NOAA station IDs are listed here: https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt[GHCND Stations^]
* A "hot day" temperature threshold


== Step 1: Update the scheduled Job code

. Start a new MATLAB Workspace.
. Open the file you previously created, `predictWeatherReport.m`, and
save it as `predictWeatherReportLauncher.m`.
. Copy and paste the following to the top of the script. This wraps the
previous code with a `function` statement.
+
[NOTE]
====
The `function` statement requires an `end` statement. You will add this
in the last step of this procedure.
====
+
[source,matlab]
----
function result = predictWeatherReport(weatherStationId, hotDayThreshold)
----

. Copy and paste the following over lines in the *Initial setup* section
in the existing script to refactor the code:
+
[source,matlab]
----
result = struct;

%% Download data file
baseUrlString = "https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access/";

% compose the URL for the NOAA station ID specified as an input parameter
urlString = sprintf("%s%s%s", baseUrlString, weatherStationId, ".csv");
% save the data into the /data subfolder
savedFileName = sprintf("%s%s%s%s", "data", filesep, weatherStationId, ".csv");
websave(savedFileName, urlString);
----
+
[NOTE]
====
The next section of code remains unchanged. For reference, the code is as follows:

----
%% Read the downloaded file
opts = detectImportOptions(savedFileName);
opts.SelectedVariableNames = {'DATE', 'PRCP', 'TMIN', 'TMAX'};
opts = setvartype(opts, {'DATE','PRCP','TMIN','TMAX'},{'datetime','double', 'double', 'double'});
stationWeatherTbl = readtable(savedFileName, opts);

%%
[stationWeatherTbl.year, stationWeatherTbl.month, stationWeatherTbl.day] = ymd(stationWeatherTbl.DATE);
% MATLAB strength
stationWeatherTbl = stationWeatherTbl(stationWeatherTbl.year > 1999 & stationWeatherTbl.year < max(stationWeatherTbl.year), :);
stationWeatherTbl.TMAX = stationWeatherTbl.TMAX/10;
stationWeatherTbl.TMIN = stationWeatherTbl.TMIN/10;
stationWeatherTbl = fillmissing(stationWeatherTbl, 'linear');

%% check if there is enough data for prediction
dataRows = size(stationWeatherTbl, 1);
if dataRows < 1000
  disp('Not enough data for prediction')
  result.error = 'Not enough data for prediction';
  return;
end
----
====

. Because those using Launcher will be able to request predictions for
_any_ weather station, copy and paste the following code to verify
whether a model for the station exists.
+
[NOTE]
====
If the training model (`trainRegressionModel.m`) does not exist in your
*Current Folder*, return to the previous step where you link:04e21f[trained the model].
====
+
[source,matlab]
----
%% Check if we have a model for this weather station
modelFileName = sprintf("%s%s%s%s", "models", filesep, ...
    weatherStationId, ".mat");

% make sure we have a folder for the models
if ~isfolder('models')
    mkdir('models')
end

if ~isfile(modelFileName)
  disp('Training model for weather station...')
  cv = cvpartition(stationWeatherTbl.year, 'Holdout', 0.3);
  dataTrain = stationWeatherTbl(cv.training, :);

  [weatherModel, validationRMSE] = trainRegressionModel(dataTrain);

  % display prediction precision
  doneMessage = sprintf('%s%d', "Done. Model RMSE:", validationRMSE);
  disp(doneMessage);
  save(modelFileName, 'weatherModel');
else
  load(modelFileName, 'weatherModel');
end
----
+
[NOTE]
====
The following code already exists to create the data table to drive the
prediction model.

----
%% Create table for future date prediction

todayDate = datetime('today');
daysIntoFuture = 365;
endDate = todayDate + days(daysIntoFuture);
predictedMaxTemps = table('Size', [daysIntoFuture+1 7], 'VariableTypes', ...
    {'datetime', 'double', 'double', 'double', 'double', 'double', 'double'}, ...
    'VariableNames', stationWeatherTbl.Properties.VariableNames);
x=1;


for i=todayDate:endDate
    % get the average perception and minimum temps on this date
    [y, m, d] = ymd(i);

    minTemps = stationWeatherTbl.TMIN(stationWeatherTbl.month == m & stationWeatherTbl.day == d);
    prcps = stationWeatherTbl.PRCP(stationWeatherTbl.month == m & stationWeatherTbl.day == d);

    curMinTemp = NaN;
    [historicalRowCount z] = size(minTemps);
    randomRow = randi([1 historicalRowCount]);
    curMinTemp = minTemps(randomRow);
    predictedMaxTemps.TMIN(x) = curMinTemp;
    randomRow = randi([1 historicalRowCount]);
    predictedMaxTemps.PRCP(x) = prcps(randomRow);
    predictedMaxTemps.DATE(x) = i;
    predictedMaxTemps.year(x) = y;
    predictedMaxTemps.month(x) = m;
    predictedMaxTemps.day(x) = d;
    predictedMaxTemps.TMAX(x) = 0;
    x = x+1;
end
----

====
+

[NOTE]
====
Leave the rest of the file as-is. The following is the code for
reference:
----
%% run model with future data
yFit = weatherModel.predictFcn(predictedMaxTemps);
predResult = table(predictedMaxTemps.DATE, yFit, 'VariableNames', {'Date', 'Predicted TMAX'});
result.predictedTemps = predResult;
hotWeatherDaysIdx = predResult(predResult.("Predicted TMAX") > hotDayThreshold, :);
result.hotDayCountPrediction = height(hotWeatherDaysIdx);

%% save data to file
dominoRunId = getenv('DOMINO_RUN_NUMBER');
outputFileName = sprintf('%s%s%s', 'results', filesep, 'predictData_', string(dominoRunId));
save(outputFileName, 'result');

%% publish to report
pub_options.format = 'pdf';
pub_options.showCode = false;
pub_options.outputDir = sprintf('%s%s%s', 'results', filesep, dominoRunId);
doc = publish('predictWeatherReportTemplate.m', pub_options);
----

====

. Add the `end` statement for the function.
+
[source,matlab]
----
%% Function end
end % This is the end of the function
----

. Click the *Save* and then *Sync All Changes* to save your work. Stop
the workspace.

== Step 2: Set up the shell script for the Launcher

. Close the workspace tab in your browser.
. In the navigation pane, go to *Files* to create a shell script that
will tell the Launcher which MATLAB script to run. The shell script will
also identify the parameters to pass to the script from the launcher.
+
image::/images/5.0/files-nav-pane.png[alt="The files navigation pane", width=300]

. Click the new file icon to create a new file. Name the file
*weather_launcher.sh*.
+
image::/images/5.0/new-file-icon.png[alt="Click the new file icon", width=300]
. Add the following code starting on line 1 of the file:
+
[source,matlab]
----
matlab -nodisplay -nodesktop -nosplash -r " predictWeatherReportLauncher('$1', $2)"
----
+
This line of code instructs Domino to run MATLAB from the command line
and execute the *predictWeatherReportLauncher* script with two
arguments. MATLAB will look for the function in an .m file with the same
name as the function.

. Click *Save*.


== Step 3: Create the Launcher

. From the navigation pane, click *Launchers*, then click *New
Launcher*.
. Type a descriptive title (for example, Weather Predictor), a
description, and select a hardware tier. In the *Command to run*
section, enter `weather_launcher.sh`.
+
image::/images/5.0/new-launcher.png[alt="Create a new launcher", width=500]

. Click *Add Parameter*. The parameter options open.
. In the "Command to run" field, replace `parameter1` with `station_id`.
You'll notice the parameter's name updates in the form. Confirm that the
parameter name remains enclosed within `${}`, such that the parameter is
formatted as such: `${name_of_the_parameter}`.
+
image::/images/5.0/station-id-in-form.png[alt="Update the parameter name in the form", width=500]

. Give the parameter a default value of `MXM00076680` (for Mexico City)
for the station ID. The default value will be used if the user doesn't
enter a value in the Launcher form. In the description, enter "The ID of
the station for which to predict weather".
+
image::/images/5.0/station-id-var.png[alt="Give the parameter a default value", width=500]

. Click *Add Parameter*.
. In the *Command to run* field, enter `hot_temp` as the parameter's
name between the curly braces {}. This parameter represents the hot
temperature threshold.
. Click `hot_temp` in the table. Leave the parameter's type as Text and
enter a default value (for example, `30`) and a description for the
parameter.
+
image::/images/5.0/hot-temp.png[alt="Configure the variable", width=500]

. Click *Save Launcher*, then click *Back to Launchers*. The new
launcher is listed.
+
image::/images/5.0/complete-launcher.png[alt="The complete Launcher", width=500]

. Click *Run*. The launcher form opens. Type a title for your run (for
example, "First launcher run") and click *Run*.
+
image::/images/5.0/first-launcher-run.png[alt="The first Launcher run", width=500]

. The Jobs view opens while your launcher executes the job. The job
number is listed in the No. column. Double-click the row to see the
results as Domino runs the job.
+
image::/images/5.0/launcher-job.png[alt="The Launcher job", width=600]

. The job produces a PDF report. To access the PDF report:

* Go to the navigation pane and click *Files*.
* Open the *results* folder and open the folder created by your job (in
this example, it is folder *36/*).
+
image::/images/5.0/access-pdf.png[alt="Access the PDF", width=600]

. Click the PDF file to open it.
+
image::/images/5.0/pdf-output.png[alt="Open the PDF output", width=600]

----- user_guide/get-started/get-started-matlab/deploy-your-model/scheduled-jobs.txt -----
:page-version: 6.1
:page-title: Scheduled Jobs
:page-permalink: 673577
:page-order: 10

Use the link:5dce1f[Scheduled Jobs] feature in Domino
to run a script regularly.

You can create reports in MATLAB using the `publish()` function and the
robust https://www.mathworks.com/products/matlab-report-generator.html[MATLAB
Report Generator^]. To simplify things, in this tutorial, you will use
the `publish()` function, which uses a `MATLAB m` file as a document
template.

Imagine that you receive data daily about Berlin's weather. You want to
generate a scheduled email visualizing this data, as well as the
forecasted number of hot days in the next 365 days.

To do that, create the following files:

* An `m.` file, based on your Live Script, that will:
** Load Berlin weather data from a URL.
** Prepare the data.
** Generate predictions using the model you create with the data.
** Call the `publish()` function.
* An `m.` file that will be your report template that shows:
** A weather prediction plot.
** The number of predicted hot days.

== Step 1: Publish the weather prediction report

. Go to Workspaces and click *Open Last Workspace*.
. Go to *New* > *Script* to create a new file.
. Click *Save* > *Save As* to name it `predictWeatherReport.m`.
. Copy and paste the following code to your file. This code initializes
a `struct` to hold the results, defines a hot day temperature threshold
(in degrees Celsius), downloads the current data for Berlin weather, and
saves it to the workspace. It will read the downloaded file into a table
format.
+
[source,matlab]
----
%% Initial setup
result = struct;
hotDayThreshold = 30;
%% Download data file
urlString = "https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access/GME00121150.csv";
if ~isfolder("data")
  mkdir('data');
end
savedFileName = sprintf("%s%s%s", "data", filesep, "berlin.csv");
websave(savedFileName, urlString);

%% Read the downloaded file
opts = detectImportOptions(savedFileName);
opts.SelectedVariableNames = {'DATE', 'PRCP', 'TMIN', 'TMAX'};
opts = setvartype(opts, {'DATE','PRCP','TMIN','TMAX'},{'datetime','double', 'double', 'double'});
stationWeatherTbl = readtable(savedFileName, opts);
----

. To prepare the data, copy and paste the following code that will start
with data from the year 1999, adjust the temperature data to full
degrees, and complete missing data. If there are less than 1000 rows of
data, the data will stop processing.
+
[source,matlab]
----
%%
[stationWeatherTbl.year, stationWeatherTbl.month, stationWeatherTbl.day] = ymd(stationWeatherTbl.DATE);
% MATLAB strength
stationWeatherTbl = stationWeatherTbl(stationWeatherTbl.year > 1999 & stationWeatherTbl.year < max(stationWeatherTbl.year), :);
stationWeatherTbl.TMAX = stationWeatherTbl.TMAX/10;
stationWeatherTbl.TMIN = stationWeatherTbl.TMIN/10;
stationWeatherTbl = fillmissing(stationWeatherTbl, 'linear');

%% check if there is enough data for prediction
dataRows = size(stationWeatherTbl, 1);
if dataRows < 1000
    disp('Not enough data for prediction');
    result.error = 'Not enough data for prediction';
    return;
end
----

. Copy and paste the following code which will load the model that you
created previously and save it into a `.mat` file. Then, it will create
the table to use as input with the updated data that was read from the
URL previously.
+
[source,matlab]
----
%% Check if we have a model for this weather station
modelFileName = sprintf("%s%s%s%s", "models", filesep, "weatherStationId", ".mat");

% make sure we have a folder for the models
if ~isfolder('models')
  mkdir('models')
end

if ~isfile(modelFileName)
  disp('Training model for weather station...')
  cv = cvpartition(stationWeatherTbl.year, 'Holdout', 0.3);
  dataTrain = stationWeatherTbl(cv.training, :);

  [weatherModel, validationRMSE] = trainRegressionModel(dataTrain);

  % display prediction precision
  doneMessage = sprintf('%s%d', "Done. Model RMSE:", validationRMSE);
  disp(doneMessage);
  save(modelFileName, 'weatherModel');
else
 load(modelFileName, 'weatherModel');
end
----
+
[source,matlab]
----
%% Create table for future date prediction
todayDate = datetime('today');
daysIntoFuture = 365;
endDate = todayDate + days(daysIntoFuture);
predictedMaxTemps = table('Size', [daysIntoFuture+1 7], 'VariableTypes',  {'datetime', 'double', 'double', 'double', 'double', 'double', 'double'}, 'VariableNames', stationWeatherTbl.Properties.VariableNames);
x=1;

for i=todayDate:endDate
  % get the average perception and minimum temps on this date
  [y, m, d] = ymd(i);

  minTemps = stationWeatherTbl.TMIN(stationWeatherTbl.month == m & stationWeatherTbl.day == d);
  prcps = stationWeatherTbl.PRCP(stationWeatherTbl.month == m & stationWeatherTbl.day == d);

  curMinTemp = NaN;
  [historicalRowCount z] = size(minTemps);
  randomRow = randi([1 historicalRowCount]);
  curMinTemp = minTemps(randomRow);
  predictedMaxTemps.TMIN(x) = curMinTemp;
  randomRow = randi([1 historicalRowCount]);
  predictedMaxTemps.PRCP(x) = prcps(randomRow);
  predictedMaxTemps.DATE(x) = i;
  predictedMaxTemps.year(x) = y;
  predictedMaxTemps.month(x) = m;
  predictedMaxTemps.day(x) = d;
  predictedMaxTemps.TMAX(x) = 0;
  x = x+1;
end
----

. Copy and paste the following code that will run the model and load the
`result` struct with the prediction.
+
[source,matlab]
----
%%
yFit = weatherModel.predictFcn(predictedMaxTemps);
predResult = table(predictedMaxTemps.DATE, yFit, 'VariableNames', {'Date', 'Predicted TMAX'});
result.predictedTemps = predResult;
hotWeatherDaysIdx = predResult(predResult.("Predicted TMAX") > hotDayThreshold, :);
result.hotDayCountPrediction = height(hotWeatherDaysIdx);
----

. Copy and paste the following code to share the prediction result with
the template in the `.mst` file. You must include this because the
`publish()` function runs in isolation from the workspace. To ensure the
data file has a unique name for each run of this script, this code uses
the Domino environment variable for the run number.
+
[source,matlab]
----
%% save data to file
dominoRunId = getenv('DOMINO_RUN_NUMBER');
outputFileName = sprintf('%s%s%s', 'results', filesep, 'predictData_', string(dominoRunId));
save(outputFileName, 'result');
----

. Copy and paste the following code to call the `publish()` function.
The report will be published to a subfolder of the `results/` folder,
along with the number of the current run in the filename.
+
[source,matlab]
----
%% Publish the report

% options for the report
pub_options.format = 'pdf';

% hide the report code
pub_options.showCode = false;
pub_options.outputDir = sprintf('%s%s%s', 'results', filesep, dominoRunId);
doc = publish('predictWeatherReportTemplate.m', pub_options);
----

. Click *Save*.



== Step 2: Create the report template


. To create the report template, create a script named
`predictWeatherReportTemplate.m`. Copy and paste the following code to
load the data. This tutorial uses the following format for the filename
when loading the data: `results/predictData_<Domino Run Number>`. The
data is stored in a variable called `result`.
+
[source,matlab]
----
dominoRunId = getenv('DOMINO_RUN_NUMBER');
inputFileName = sprintf('%s%s%s', 'results', filesep, 'predictData_', string(dominoRunId));
load(inputFileName, 'result');
----
+
. Copy and paste the following code to add a title to the template. In
this MATLAB template, comments will be rendered as markup. For more
information, see MATLAB's documentation about
https://www.mathworks.com/help/matlab/matlab_prog/marking-up-matlab-comments-for-publishing.html[markup comments for publishing^].
+
[source,matlab]
----
%% Predicted Weather
----

. Copy and paste the following to add the plot with the data that you
loaded previously and show the hot day predictions as output.
+
[source,matlab]
----
predResult = result.predictedTemps;

plot(predResult.Date, predResult.("Predicted TMAX"));
titleText = "Weather forecast for the next 365 days (\circC)";
title(titleText);
ylabel ('Forecasted Daily High Temperature')

%%
countPredictionText = sprintf("%s%d%s", "There will be ", ...
    result.hotDayCountPrediction, " hot days in the next 365 days");

disp(countPredictionText);
----

. Click *Save*.
. Click *File Changes* in the navigation bar. Then, click *Sync All
Changes* to save and commit your changes.
. To test the `predictWeatherReport.m` file, type the following in the
Command Window to call the file and press Enter:
+
[source,matlab]
----
predictWeatherReport
----
+
The following graph shows the weather forecast:
+
image::/images/5.0/job-executed.png[alt="Executed job creates a graph to forecast the weather"]
+
A new folder and new file are created in the `results/` folder.
+
image::/images/get_started_matlab/current-folder.png[alt="A new folder and new file are created"]
+
. Save and sync your changes.
. Stop your MATLAB session. Click the Domino icon and then, in the
navigation pane, click *Jobs* > *Schedules*.
. Click *Schedule a Job*.
.. Type a name for the job and type `predictWeatherReport.m` as the file
to run for this job.
.. Select a hardware tier.
.. Confirm that your environment is the same as the one in which you
developed your model.
.. Click *Next*.
. On the Attach Compute Cluster page, select *none* and click *Next*
. Schedule the job to run every weekday. Leave the *Run sequentially*
option checked. Click *Next*.
. Type the email addresses for those to notify when the job is complete.
Click *Create*.

You have scheduled a job that will use your MATLAB code to generate a
report. You can also go to the Jobs page to run the job on an ad-hoc
basis.

To learn how to customize the resulting email, see link:5b84c5[Set Custom Execution Notifications].
----- user_guide/get-started/get-started-matlab/index.txt -----
:page-version: 6.1
:page-title: Get started with MATLAB
:page-permalink: d003a0
:page-order: 30

This tutorial guides you through a common model lifecycle in Domino.

You will develop a model to help a friend decide whether to purchase an
air conditioner. This is uncommon in Germany. To do this, you will look
at historical weather data for Berlin, Germany to predict its future
weather.

Domino will help you answer the following questions:

* How many hot days occur in a year?
* How many hot days will occur in the next 20 days?

You'll also decide what temperature determines a hot day.

The following content is meant to be followed in sequence.

* link:d00413[Create a Project]
* link:38fd0e[Configure your Project]
* link:bfda3d[Start a Workspace]
* link:45a2e6[Fetch and save your data]
* link:04e21f[Develop your model]
* link:b1d21e[Clean up your Workspace]
* link:a8b4c4[Deploy your model]
* link:c3d953[Working with Datasets]

----- user_guide/get-started/get-started-python/1-create-project.txt -----
:page-version: 6.1
:page-title: Create a Python Project
:page-sidebar: Create a Project
:page-permalink: d24d82
:page-order: 10

Work in Domino happens in _projects_. Projects contain data, code, and
environment settings, and the entire project is tracked and revisioned
automatically. A new commit is written to a project each time its files
are changed by user action, or by the execution of code in the project.
Users in Domino can create their own new projects, invite other users to
collaborate on them, and export data or results for consumption by other
projects.

== Create a Project:

. In the top navigation bar, click *Develop > Projects*.

. Click *Create New Project*.

. Give your project an informative name (like `power-generation`)

. Set its link:71afc6[Visibility] to `Private`.

. Click **Create Project**.

----- user_guide/get-started/get-started-python/2-configure-project.txt -----
:page-version: 6.1
:page-title: Configure your Python Project
:page-sidebar: Configure your Project
:page-permalink: 68ddd0
:page-order: 20

Every project has its own settings. The following options are important
to consider when configuring a new project:

* link:dba65c#hardware_tier[Hardware Tier]
* link:f51038[Environment]
* link:d7731d[Collaborators]


== Step 1: Select your hardware tier

A link:dba65c#hardware_tier[Hardware Tier]
represents the compute resources that will be available for your run.
You can specify memory, CPU cores, and GPUs with hardware tiers.

The hardware tier dropdown menu lists your available options. The
selected hardware tier will be used by default for all subsequent
executions of code in the project. It can also be changed at any point
in the future.

. In the Project menu, click *Settings*.
. Click the Hardware tier menu to select the compute resource from which to execute your code. Choose the smallest or default hardware tier for this tutorial. Your options might look different from the following image.
This list of available hardware tiers is customizable by your Domino
administrators. If you want additional resources, contact your
Domino administrator.


== Step 2: Configure your Environment

An link:f51038[Environment] is a Domino abstraction on top of a Docker image that provides additional flexibility and versioning. You can configure the software, packages, libraries, and drivers that you need in your environment.

Domino comes with a default environment called the 
link:0d73c6#dse[Domino Standard Environment (DSE)],
which includes Python, R, Jupyter, RStudio, and key data science related
packages and libraries.

. Click the *Compute Environment* menu to select the default project Environment.


[NOTE]
====
To take advantage of link:release_notes/5-0-0[new features in Domino 5.0 and up], select the *5.0 Domino Standard Environment*.
====

Your compute environment dropdown will likely have different options. If
you're interested in learning how to add more packages and customize or
create your own Environment, see link:f51038[Environments].

== Step 3: Configure the Project permissions

As the owner of the project, you can set different access levels for collaborators and colleagues. Invite a colleague to be a Contributor to your project.

. Click the *Access & Sharing* tab.

. (Optional) Enter the email or username.
. (Optional) Enter a welcome message to be sent to your collaborator.
. Click *Invite*.

A user with the `Contributor` role can read, write, and execute code in this project.
execute code in this project.

See link:7876f1[Collaborator Permissions] for more information on the permissions for each collaborator role.

----- user_guide/get-started/get-started-python/3-start-workspace.txt -----
:page-version: 6.1
:page-title: Start a Jupyter Workspace
:page-sidebar: Start a Workspace
:page-permalink: 93aef2
:page-order: 30

Workspace sessions are interactive sessions hosted by a Domino executor
where you can interact with code notebooks like
https://jupyter.org/[Jupyter^] and https://www.rstudio.com/[RStudio^]. The
software tools and associated configurations available in your session
are called Workspaces.

For this tutorial, we will start a Jupyter Workspace.

. Click *Workspaces* from the project menu.
. Click *Jupyter*.
. Click *Launch Now*.
+
When you launch a workspace, a new containerized session is created on a
machine (also known as an executor) in the required hardware tier. The
workspace tool you requested is launched in that container, and your
browser is automatically redirected to the workspace's interface when it's
ready.
+
After your workspace is up and running, you will see a fresh Jupyter interface. 
After your workspace is running, a Jupyter interface opens. 
If you are new to Jupyter, see https://jupyter-notebook.readthedocs.io/en/stable/notebook.html#notebook-user-interface[Notebook user interface^] and https://jupyterlab.readthedocs.io/en/stable/user/interface.html[The JupyterLab Interface^].

See link:03e062[Add Workspace IDEs] if you're interested in adding additional Workspaces for tools that are available by default.

----- user_guide/get-started/get-started-python/4-get-data.txt -----
:page-version: 6.1
:page-title: Get your files and data into a Jupyter Workspace
:page-sidebar: Get your files and data
:page-permalink: 0ed610
:page-order: 40

There are two strategies to working with data in Domino:

* You can copy your data into Domino
+
If you are working with data that is on your local machine or in a
shared server, you might want to link:7a0fee[upload your data into Domino].
* You can query your data from Domino
+
If you have a large dataset stored in a database or data service, you
might just have to query the database or the API for the data service.

In this step, you'll copy your data into the project using the Jupyter
terminal. If you have not done so yet, first link:93aef2[start a Jupyter workspace].

The starting file path in your Jupyter workspace is `/mnt`. By default,
this is considered the root of your Domino project. If you add or modify
files in `/mnt`, you can save them back to your project when you stop or
sync the workspace.

[NOTE]
====
Your outbound internet access may be blocked in Trial or POC instances. If you have access to a hardware tier with egress, use that as the hardware tier and follow the normal steps. If you do not have access to a hardware tier with egress, follow these steps.

. Copy and paste the following URL into your browser to download the data: https://data.elexon.co.uk/bmrs/api/v1/datasets/FUELINST?format=csv&PublishDateTimeFrom=2019-09-29T20:30:00.000Z&PublishDateTimeTo=2019-10-02T20:30:00.000Z.
. Rename the file to `data.csv`.
. From the navigation pane, in your project, click
*Code*.
. Click the upload file to upload the file.
. Skip to the last item in the normal steps.
====

. Go to *New* > *Terminal* to open a Jupyter terminal.

. In the new terminal, run the following command to fetch some data from
the BMRS:
+
[source,bash]
----
curl -o data.csv "https://data.elexon.co.uk/bmrs/api/v1/datasets/FUELINST?format=csv&PublishDateTimeFrom=2019-09-29T20:30:00.000Z&PublishDateTimeTo=2019-10-02T20:30:00.000Z"
----
. In the *File Changes* tab of your workspace, type a commit message in the text box.
. Click *Sync All Changes* to synchronize the information and save `data.csv` to your Domino project.
. Stop the workspace.

. Click the Domino logo to return to your project in Domino.
In the navigation pane, click
*Code*.
The raw data is saved in the latest revision.



See the documentation for other methods to copy data into Domino (with https://curl.se/[curl^] or https://www.gnu.org/software/wget/[Wget^]) and link:fbb41f[query data from Domino].

----- user_guide/get-started/get-started-python/5-develop-model.txt -----
:page-version: 6.1
:page-title: Develop your Python model
:page-sidebar: Develop your model
:page-permalink: 288e42
:page-order: 50

When you are developing your model, you want to be able to quickly
execute code, see outputs, and make iterative improvements. Domino
enables this with Workspaces. We previously covered
link:93aef2[how to start a Workspace] and explored Workspace options like VS Code,
RStudio, and Jupyter.

In this section, we will use Jupyter to load, explore, and transform
some data. After the data has been prepared, we will train a model.

== Step 1: Load and explore the dataset

. From the project menu, click *Workspaces*.
. Click *Open Last Workspace* or *Open* in the link:93aef2[workspace created earlier]. In `/mnt`, you can see `data.csv`.
If not, link:0ed610[download the dataset].

. Use the *New* menu to create a Python notebook.

. Starting in the first cell, enter these lines to import some packages. After each line, press `Shift+Enter` to execute:
+
[source,python]
----
%matplotlib inline
import pandas as pd
import datetime
----
. Next, read the file you downloaded into a pandas dataframe:
+
[source,python]
----
df = pd.read_csv('data.csv', skiprows=1, skipfooter=1, header=None, engine='python')
----
. Rename the columns according to information on the column headers at https://www.bmreports.com/bmrs/?q=generation/fueltype/current[Generation by Fuel Type^] and display the first five rows of the dataset using `df.head()`.

+
[source,python]
----
df.columns = ['HDF', 'date', 'half_hour_increment',
              'CCGT', 'OIL', 'COAL', 'NUCLEAR',
              'WIND', 'PS', 'NPSHYD', 'OCGT',
              'OTHER', 'INTFR', 'INTIRL', 'INTNED',
               'INTEW', 'BIOMASS', 'INTEM','INTEL',
               'INTIFA2', 'INTNSL']
df.head()
----
+
We can see that this is a time series dataset. Each row is a successive
half hour increment during the day that details the amount of energy
generated by fuel type. Time is specified by the `date` and
`half_hour_increment` columns.
. Create a new column named `datetime` that represents the starting
datetime of the measured increment. For example, a `20190930` date and
`2` half hour increment means that the time period specified is
September 19, 2019 from 12:30am to 12:59am.
+
[source,python]
----
df['datetime'] = pd.to_datetime(df['date'], format="%Y%m%d")
df['datetime'] = df.apply(lambda x:x['datetime']+ datetime.timedelta(minutes=30*(int(x['half_hour_increment'])-1)), axis = 1)
----
. Visualize the data to see how each fuel type is used during the day by
plotting the data.
+
[source,python]
----
df.drop(
    ['HDF', 'date', 'half_hour_increment'], axis = 1
    ).set_index('datetime').plot(figsize=(15,8))
----
+
image::/images/4.x/fuel_types_graph_cell.png[alt="Graph showing how each fuel type is used", width=500]
+
The CCGT column, representing combined-cycle gas turbines, generates a lot of energy and is volatile.
+
We will concentrate on this column and try to predict the power
generation from this fuel source.



== Step 2: Train a model

Data scientists have access to many libraries and packages that help
with model development. Some of the most common for Python are XGBoost,
Keras and scikit-learn. These packages are already installed in the
link:0d73c6#dse[Domino Standard Environment]
. However, there might be times when you want to
experiment with a package that is not installed in the
environment.

We will build a model with the https://facebook.github.io/prophet/[Facebook Prophet^] package, which is not installed into the default environment. You will see that you can
quickly get started with new packages and algorithms just as fast as
they are released into the open source community.


. In the next Jupyter cell, install Facebook Prophet and its
dependencies, including PyStan (a slightly older version of
Plotly).
PyStan requires 4 GB of RAM to be installed. Make sure your
workspace is set to use a large enough hardware tier:
+
[source,shell]
----
!pip install -q --user "holidays==0.10.3" "lunarcalendar==0.0.9" "pystan==2.19.1.1"
!pip install -q --user "fbprophet==0.7.1"
----

. For Facebook Prophet, the time series data needs to be in a DataFrame
with 2 columns named `ds` and `y`:
+
[source,python]
----
df_for_prophet = df[['datetime', 'CCGT']].rename(columns = {'datetime':'ds', 'CCGT':'y'})
----
. Split the dataset into train and test sets:
+
[source,python]
----
X = df_for_prophet.copy()
y = df_for_prophet['y']
proportion_in_training = 0.8
split_index = int(proportion_in_training*len(y))
X_train, y_train = X.iloc[:split_index], y.iloc[:split_index]
X_test, y_test = X.iloc[split_index:], y.iloc[split_index:]
----
. Import Facebook Prophet and fit a model:
+
[source,python]
----
from fbprophet import Prophet
m = Prophet()
m.fit(X_train)
----
+
If you encounter an error when running this cell, you may need to
downgrade your pandas version first. In that case you would run
+
[source,shell]
----
!pip install -Iq --user "pandas==1.3.4"
----
+
and then
+
[source,python]
----
from fbprophet import Prophet
m = Prophet()
m.fit(X_train)
----
+
After running the code above, you may encounter a warning about code
deprecation. The warning can be ignored for the purposes of this
walkthrough.
. Make a DataFrame to hold prediction and predict future values of CCGT
power generation:
+
[source,python]
----
future = m.make_future_dataframe(periods=int(len(y_test)/2), freq='H')
forecast = m.predict(future)
# forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail() #uncomment to inspect the DataFrame
----
. Plot the fitted line with the training and test data:
+
[source,python]
----
import matplotlib.pyplot as plt
plt.gcf()
fig = m.plot(forecast)
plt.plot(X_test['ds'].dt.to_pydatetime(), X_test['y'], 'r', linewidth = 1, linestyle = '--', label = 'real')
plt.legend()
----
. Rename the notebook to be `Forecast_Power_Generation`

. Save the notebook.




== Step 3: Export the model

Trained models are meant to be used. There is no reason to re-train the
model each time you use the model. Export or serialize the model to a
file to load and reuse the model later. In Python, the `pickle` module
implements protocols for serializing and de-serializing objects. In R,
you can commonly use the `serialize` command to create RDS files.


. Export the trained model as a pickle file for later use:
+
[source,python]
----
import pickle
# m.stan_backend.logger = None    #uncomment if using Python 3.6 and fbprophet==0.6
with open("model.pkl", "wb") as f:
      pickle.dump(m, f)
----

We will use the serialized model later when we link:9f10c9[create an API from the model].

----- user_guide/get-started/get-started-python/6-clean-up.txt -----
:page-version: 6.1
:page-title: Clean up Jupyter Workspaces
:page-sidebar: Clean up Workspaces
:page-permalink: fb9e32
:page-order: 60

To avoid spending unnecessary compute resources, make sure to stop any
Workspace sessions that you started as a part of this tutorial. If your
Domino is deployed in the cloud, this will prevent you from incurring
unnecessary charges. If your Domino is deployed on premises, this will
free up your compute resources for others to use.

Prior to stopping your workspace, go to *File Changes* to add a commit message, then click *Sync All Changes* to save your work. The files created previously when you link:288e42[developed your model] are now visible on the Files page.

There are two places in the Domino application where you can stop your sessions.

== Option 1: Stop your Workspace session from inside the workspace

. Above your Jupyter notebook in the blue menu bar, click *Stop*.

. Enter a descriptive commit message in the text box. Now click
*Sync All Changes*.

== Option 2: Stop your Workspace session from the Workspaces page

. Go the to the Workspaces page in Domino. Since the Jupyter workspace
opened in a new tab, you might have to select the previous Domino tab.
. Click *Stop* for your workspace session.

. Click
*Stop My Workspace*.
----- user_guide/get-started/get-started-python/7-deploy.txt -----
:page-version: 6.1
:page-title: Deploy your Python model
:page-sidebar: Deploy your model
:page-permalink: 9f10c9
:page-order: 70

After you have developed your model and deemed it good enough to be
useful, you will want to deploy it. There is no single deployment method
that is best for all models. Therefore, Domino offers four different
deployment options. One may fit your needs better than the others
depending on your use case.

The available deployment methods are:

* Scheduled reports
* Launchers
* Web applications
* Domino endpoints

The remaining sections of this tutorial are not dependent on each other.
For example, you will not need to complete the *Scheduled report*
section to understand and complete the *Web application* section.


== Package setup

A prerequisite to the following sections is to install a few packages.
To do this, you create a `requirements.txt` file in the project, which installs the Python packages listed in the file prior to every job or workspace session.

. Go to the Files page of your project.
. Click *New File*.
+
. Name it `requirements.txt`, copy and paste the following contents, and
*Save*:
+
//[source,python]
+
----
convertdate
pyqt5<5.12
jupyter-client>6.0.0
nbformat>5.0
papermill<2.0.0
pystan==2.17.1.0
plotly<4.0.0
dash
requests
nbconvert >= 5.4
----

If you want to install these libraries permanently into a custom
environment, find out more in the link:#NewComputeEnvironment[Domino endpoint]
tutorial.


== Scheduled reports

The link:5dce1f[Scheduled Jobs] feature in Domino
allows you to run a script on a regular basis. In Domino, you can also
schedule a notebook to run from top to bottom and export the resulting
notebook as an HTML file. Since notebooks can be formatted with plain
text and embedded graphics, you can use the scheduling feature to create
regularly scheduled, automated reports for your stakeholders.

In our case, we can imagine that each day we receive new data on power
usage. To make sure our predictions are as accurate as possible, we can
schedule our notebook to re-train our model with the latest data and
update the visualization accordingly.


. link:93aef2[Start] a new Jupyter session.
. Select the Jupyter notebook you created when you
link:288e42[developed your Python model].
. Go to *File > Make a copy* to create a copy of the notebook.
. Add some dynamically generated text to the upcoming report. We want to
pull the last 30 days of data.
.. Insert a new cell before the first cell by selecting the first cell
and selecting *Insert Cell Above*.

.. Copy and paste the following code into the new cell:
+
//[source,python]
+
----
import datetime
today = datetime.datetime.today().strftime('%Y-%m-%d')
one_month = (datetime.datetime.today() - datetime.timedelta(30)).strftime('%Y-%m-%d')
!curl -o data.csv "https://www.bmreports.com/bmrs/?q=ajax/filter_csv_download/FUELHH/csv/FromDate%3D{one_month}%26ToDate%3D{today}/&filename=GenerationbyFuelType_20191002_1657" 2>/dev/null
----

. Since this is a report, you will want to add some commentary to guide
the reader. For this exercise, we will just add a header to the report
at the top. To add a Markdown cell:
.. Insert a new cell before the first cell again by selecting the first
cell and selecting *Insert Cell Above*.
.. Change the cell type to Markdown.
.. Enter the following in the new Markdown cell:
+
//[source,python]
+
----
# New Predictions for Combined Cycle Gas Turbine Generations
----

. Save the notebook.
. *Sync All Changes* in 
the workspace session.
. Test the notebook.
.. Go the Files page.
.. Click the link for the new copy of the notebook.
.. Click *Run*.
.. Click *Start* on the modal.
.. Wait for the run to complete. While running, the Status icon will
appear blue. Click a job to view its logs.
.. After the job has completed successfully, you'll see the Status icon
turn green. You can then browse the Results tab.
. At this point, you can schedule the notebook to run every day. Go to
the Scheduled Jobs page.

. Start a new scheduled job and enter the name of the file that you want
to schedule to run. This will be the name of your Jupyter notebook.
. Select how often and when to run the file.
. Enter emails of people to send the resulting file(s) to.

. Click *Schedule*.

To learn how to customize the resulting email, see link:5b84c5[Set Custom Execution Notifications].


== Launchers

Launchers are web forms that allow users to run templatized scripts.
They are especially useful if your script has command line arguments
that dynamically change the way the script executes. For heavily
customized scripts, those command line arguments can quickly get
complicated. Launcher allows you to expose all of that as a simple web
form.

Typically, we parameterize script files (i.e. files that end in `.py`,
`.R`, or `.sh`). Since we have been working with Jupyter notebooks until
now, we will parameterize a copy of the Jupyter notebook that we created
link:288e42[when we developed the Python model].

To do so, we will insert a few new lines of code into a copy of the
Jupyter notebook, create a wrapper file to execute, and configure a
Launcher.


. Parameterize the notebook with a Papermill tag and a few edits:
.. link:93aef2[Start] a Jupyter session. Make sure you are
using a Jupyter workspace, not a Jupyterlab workspace. We recently added
the `requirements.txt` file, so the session will take longer to start.
.. Create a copy of the notebook that you created
link:288e42[when you developed your Python model]. Rename it to
`Forecast_Power_Generation_for_Launcher`.
.. In the Jupyter menu bar, select *View/Cell Toolbar/Tags*.

.. Create a new cell at the top of the notebook and enter the
following into the cell:
+
//[source,python]
+
----
!pip install fbprophet==0.6
----
.. Create another new cell.

.. Add a `parameters` tag to the top cell.

.. Enter the following into the cell to create default parameters:
+
//[source,python]
+
----
start_date_str = 'Tue Oct 06 2020 00:00:00 GMT-0700 (Pacific Daylight Time)'
fuel_type = 'CCGT'
----

.. Insert another cell.
.. Launcher parameters get passed to the notebook as strings. The
notebook will need the date parameters to be in a differently formatted
string.
+
//[source,python]
+
----
import datetime
today = datetime.datetime.today().strftime('%Y-%m-%d')
start_date = datetime.datetime.strptime(start_date_str.split(' (')[0], '%a %b %d %Y 00:00:00 GMT%z').strftime('%Y-%m-%d')
----

.. Insert another new cell with the following code:
+
//[source,python]
+
----
!curl -o data.csv "https://www.bmreports.com/bmrs/?q=ajax/filter_csv_download/FUELHH/csv/FromDate%3D{start_date}%26ToDate%3D{today}/&filename=GenerationbyFuelType_20191002_1657" 2>/dev/null
----
+
The top of your notebook should look like this:
+
image::/images/4.x/top_of_launcher_notebook.png[alt="Top of Launcher notebook", width=900]

.. In the cell where `df_for_prophet` is defined, replace `CCGT` with `fuel_type`:
+
//[source,python]
+
----
df_for_prophet = df[['datetime', fuel_type]].rename(columns = {'datetime':'ds', fuel_type:'y'})
----
+
image::/images/4.x/replace_ccgt.png[alt="Rename a column", width=700]
.. Save the notebook.

.. *Stop and Commit* the workspace session.

. Create a wrapper file to execute.
.. Go back to the Files page.
.. Create a new file called `forecast_launcher.sh`.
.. Copy and paste the following code for the file and save it:
+
//[source,python]
+
----
papermill Forecast_Power_Generation_for_Launcher.ipynb forecast.ipynb -p start_date "$1" -p fuel_type $2
----
+
The command breaks down as follows:
+
`papermill <input ipynb file> <output ipynb file> -p <parameter name> <parameter value>`
+
We will pass in our values as command line arguments to the shell script
`forecast_launcher.sh`, which is why we have `$1` and `$2` as our
parameter values.

. Configure the Launcher.
.. Go to the Launcher page, found under the Publish menu in the sidebar.
.. Click *New Launcher*.
.. Name the launcher "Power Generation Forecast Trainer"
.. Copy and paste the following into the field "Command to run":
+
//[source,python]
+
----
forecast_launcher.sh ${start_date} ${fuel_type}
----

.. Select the *start_date* parameter and change the type to *Date*.

.. Select the *fuel_type* parameter and change the type to
*Select (Drop-down menu)*.
.. Copy and paste the following into the *Allowed Values* field:
+
//[source,python]
+
----
CCGT, OIL, COAL, NUCLEAR, WIND, PS, NPSHYD, OCGT, OTHER, INTFR, INTIRL, INTNED, INTEW, BIOMASS, INTEM ,INTEL,INTIFA2,INTNSL
----

.. Click *Save Launcher*.

. Try out the Launcher.
.. Go back to the main Launcher page.
.. Click *Run* for the "Power Generation Forecast Trainer" launcher.
.. Select a start date for the training data.
.. Select a fuel type from the dropdown.
.. Click *Run*

This will execute the parameterized notebook with the parameters that
you selected. In this particular launcher, a new dataset was downloaded
and the model was re-trained. Graphs in the resulting notebook represent
the new dataset. You can see them in the Results tab.

When the run has been completed, an email will be sent to you and others
that you optionally specified in the launcher with the resulting files.
To learn how to customize the resulting email, see link:5b84c5[Set Custom Execution Notifications].

== Domino endpoints

If you want your model to serve another application, you will want to
serve it in the form of an API endpoint.
link:8dbc91[Domino endpoints] are scalable REST
APIs that can create an endpoint from any function in a Python or R
script. The Domino endpoints are commonly used when you need an API to query your model in near real-time.

=== Create an endpoint

For example, we created a model to forecast power generation of combined cycle gas turbines in the UK.

In this section, we will deploy an API that uses the link:288e42[model that we
trained] to predict the generated power given a date in the future. To do so, we will create a new
link:f51038[compute environment] to install
necessary packages, create a new file with the function we want to
expose as an API, and finally deploy the API.

[[NewComputeEnvironment]]

. Create a new compute environment.

.. Go to the Environments page in Domino.

.. Click *Create Environment*.

.. Name the environment and enter a description for the new environment.
.. Click *Create Environment*.
.. Click *Edit Definition*.

.. In the *Dockerfile Instructions* section, enter the following:
+
[source,shell]
----
RUN pip install "pystan==2.17.1.0" "plotly<4.0.0" "papermill<2.0.0" requests dash && pip install fbprophet==0.6
----
.. Scroll to the bottom of the page and click *Build*.
+
This will start the creation of your new compute environment. These
added packages will now be permanently installed into your environment
and be ready whenever you start a job or workspace session with this
environment selected.
Note that PyStan needs 4 GB of RAM to install;
reach out to your admin if you see errors so they can ensure that
builds have the appropriate memory allocation.

.. Navigate back to your project page and go to the Settings
page.
.. Select your newly created environment from the *Compute Environments* dropdown menu.


. Create a new file with the function we want to expose as an API

.. From the Files page of your project, click *New File*.

.. Name your file `forecast_predictor.py`.
.. Enter the following contents:
+
[source,shell]
----
import pickle
import datetime
import pandas as pd

with open('model.pkl', 'rb') as f:
    m = pickle.load(f)

def predict(year, month, day):
    '''
    Input:
    year - integer
    month - integer
    day - integer

    Output:
    predicted generation in MW
    '''
    ds = pd.DataFrame({'ds': [datetime.datetime(year,month,day)]})
    return m.predict(ds)['yhat'].values[0]
----
.. Click *Save*.

. Deploy the API.
.. Go to the *Deployments > Endpoints* page in your project.
.. Click *New Endpoint*.

.. Name your model, provide a description, and click Next.

.. Enter the name of the file that you created in the previous step.
.. Enter the name of the function that you want to expose as an API.
.. Click *Create Endpoint*.

. Test the API.
.. Wait for the Domino endpoint status to turn to Running. This may take a
few minutes.
.. Click the *Overview* tab.
.. Enter the following into the Request box in the tester:
+
[source,json]
----
{
  "data": {
    "year": 2019,
    "month": 10,
    "day": 15
  }
}
----
.. Click *Send*. If successful, you will see the response in the pane.

As a REST API, any other common programming language will be able to
call it. Code snippets from some popular languages are listed in the
other tabs.

Domino endpoints are built as Docker images and deployed on Domino. You can
export the endpoint images to your external container registry and deploy
them in any other hosting environment outside of Domino using your
custom CI/CD pipeline.
The link:8c929e[Domino Platform API] enables you to programmatically build new model images on Domino and export them to your external container registry.

=== Stop an endpoint

To stop an endpoint:

. Click the appropriate Project.
. From the navigation, click *Deployments* > *Endpoints* > *Versions*.
. Under Actions, click the three vertical dots, then click *Stop Version*.

image::/images/6.0/stop-endpoint.png[alt="Stop an endpoint", width=1000, role=noshadow]

== Web applications

When experiments in Domino yield results that you want to share with your colleagues, you can easily do so with a link:71635d[Domino App].
Domino can host Apps built with many popular frameworks, including Flask, Shiny, and Dash.

While Apps can be significantly more sophisticated and provide far more functionality than a Launcher, they also require significantly more code and knowledge in at least one framework.
In this section, we will convert some code that we developed link:288e42[when we trained a Python model] and create a https://plotly.com/dash/[Dash^] app.

. Add the `app.py` file, which will describe the app in Dash, to the
project:
+
//[source,python]
+
----
# -*- coding: utf-8 -*-
import dash
import dash_core_components as dcc
import dash_html_components as html
from datetime import datetime as dt
from dash.dependencies import Input, Output
import requests
import datetime
import os

import pandas as pd
import datetime
import matplotlib.pyplot as plt
from fbprophet import Prophet
import plotly.graph_objs as go

external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']

app = dash.Dash(__name__, external_stylesheets=external_stylesheets)

app.config.update({'requests_pathname_prefix': '/{}/{}/r/notebookSession/{}/'.format(
    os.environ.get("DOMINO_PROJECT_OWNER"),
    os.environ.get("DOMINO_PROJECT_NAME"),
    os.environ.get("DOMINO_RUN_ID"))})

colors = {
    'background': '#111111',
    'text': '#7FDBFF'
}

# Plot configs
prediction_color = '#0072B2'
error_color = 'rgba(0, 114, 178, 0.2)'  # '#0072B2' with 0.2 opacity
actual_color = 'black'
cap_color = 'black'
trend_color = '#B23B00'
line_width = 2
marker_size = 4
uncertainty=True
plot_cap=True
trend=False
changepoints=False
changepoints_threshold=0.01
xlabel='ds'
ylabel='y'

app.layout = html.Div(style={'paddingLeft': '40px', 'paddingRight': '40px'}, children=[
    html.H1(children='Predictor for Power Generation in UK'),
    html.Div(children='''
        This is a web app developed in Dash and published in Domino.
        You can add more description here to describe the app.
    '''),
     html.Div([
        html.P('Select a Fuel Type:', className='fuel_type', id='fuel_type_paragraph'),
        dcc.Dropdown(
            options=[
                {'label': 'Combined Cycle Gas Turbine', 'value': 'CCGT'},
                {'label': 'Oil', 'value': 'OIL'},
                {'label': 'Coal', 'value': 'COAL'},
                {'label': 'Nuclear', 'value': 'NUCLEAR'},
                {'label': 'Wind', 'value': 'WIND'},
                {'label': 'Pumped Storage', 'value': 'PS'},
                {'label': 'Hydro (Non Pumped Storage', 'value': 'NPSHYD'},
                {'label': 'Open Cycle Gas Turbine', 'value': 'OCGT'},
                {'label': 'Other', 'value': 'OTHER'},
                {'label': 'France (IFA)', 'value': 'INTFR'},
                {'label': 'Northern Ireland (Moyle)', 'value': 'INTIRL'},
                {'label': 'Netherlands (BritNed)', 'value': 'INTNED'},
                {'label': 'Ireland (East-West)', 'value': 'INTEW'},
                {'label': 'Biomass', 'value': 'BIOMASS'},
                {'label': 'Belgium (Nemolink)', 'value': 'INTEM'},
            {'label': 'France (Eleclink)', 'value': 'INTEL'},
            {'label': 'France (IFA2)', 'value': 'INTIFA2'},
           {'label': 'Norway 2 (North Sea Link)', 'value': 'INTNSL'}
            ],
            value='CCGT',
            id='fuel_type',
            style = {'width':'auto', 'min-width': '300px'}
        )
    ], style={'marginTop': 25}),
    html.Div([
        html.Div('Training data will end today.'),
        html.Div('Select the starting date for the training data:'),
        dcc.DatePickerSingle(
            id='date-picker',
            date=dt(2020, 9, 10)
        )
    ], style={'marginTop': 25}),
    html.Div([
        dcc.Loading(
            id="loading",
            children=[dcc.Graph(id='prediction_graph',)],
            type="circle",
            ),
        ], style={'marginTop': 25})
])

@app.callback(
    # Output('loading', 'chhildren'),
    Output('prediction_graph', 'figure'),
    [Input('fuel_type', 'value'),
     Input('date-picker', 'date')])
def update_output(fuel_type, start_date):
    today = datetime.datetime.today().strftime('%Y-%m-%d')
    start_date_reformatted = start_date.split('T')[0]
    url = 'https://www.bmreports.com/bmrs/?q=ajax/filter_csv_download/FUELHH/csv/FromDate%3D{start_date}%26ToDate%3D{today}/&filename=GenerationbyFuelType_20191002_1657'.format(start_date = start_date_reformatted, today = today)
    r = requests.get(url, allow_redirects=True)
    open('data.csv', 'wb').write(r.content)
    df = pd.read_csv('data.csv', skiprows=1, skipfooter=1, header=None, engine='python')
    df.columns = ['HDF', 'date', 'half_hour_increment',
                'CCGT', 'OIL', 'COAL', 'NUCLEAR',
                'WIND', 'PS', 'NPSHYD', 'OCGT',
                'OTHER', 'INTFR', 'INTIRL', 'INTNED', 'INTEW', 'BIOMASS', 'INTEM',
                'INTEL','INTIFA2', 'INTNSL']
    df['datetime'] = pd.to_datetime(df['date'], format="%Y%m%d")
    df['datetime'] = df.apply(lambda x:
                          x['datetime']+ datetime.timedelta(
                              minutes=30*(int(x['half_hour_increment'])-1))
                          , axis = 1)
    df_for_prophet = df[['datetime', fuel_type]].rename(columns = {'datetime':'ds', fuel_type:'y'})
    m = Prophet()
    m.fit(df_for_prophet)
    future = m.make_future_dataframe(periods=72, freq='H')
    fcst = m.predict(future)
    # from https://github.com/facebook/prophet/blob/master/python/fbprophet/plot.py
    data = []
    # Add actual
    data.append(go.Scatter(
        name='Actual',
        x=m.history['ds'],
        y=m.history['y'],
        marker=dict(color=actual_color, size=marker_size),
        mode='markers'
    ))
    # Add lower bound
    if uncertainty and m.uncertainty_samples:
        data.append(go.Scatter(
            x=fcst['ds'],
            y=fcst['yhat_lower'],
            mode='lines',
            line=dict(width=0),
            hoverinfo='skip'
        ))
    # Add prediction
    data.append(go.Scatter(
        name='Predicted',
        x=fcst['ds'],
        y=fcst['yhat'],
        mode='lines',
        line=dict(color=prediction_color, width=line_width),
        fillcolor=error_color,
        fill='tonexty' if uncertainty and m.uncertainty_samples else 'none'
    ))
    # Add upper bound
    if uncertainty and m.uncertainty_samples:
        data.append(go.Scatter(
            x=fcst['ds'],
            y=fcst['yhat_upper'],
            mode='lines',
            line=dict(width=0),
            fillcolor=error_color,
            fill='tonexty',
            hoverinfo='skip'
        ))
    # Add caps
    if 'cap' in fcst and plot_cap:
        data.append(go.Scatter(
            name='Cap',
            x=fcst['ds'],
            y=fcst['cap'],
            mode='lines',
            line=dict(color=cap_color, dash='dash', width=line_width),
        ))
    if m.logistic_floor and 'floor' in fcst and plot_cap:
        data.append(go.Scatter(
            name='Floor',
            x=fcst['ds'],
            y=fcst['floor'],
            mode='lines',
            line=dict(color=cap_color, dash='dash', width=line_width),
        ))
    # Add trend
    if trend:
        data.append(go.Scatter(
            name='Trend',
            x=fcst['ds'],
            y=fcst['trend'],
            mode='lines',
            line=dict(color=trend_color, width=line_width),
        ))
    # Add changepoints
    if changepoints:
        signif_changepoints = m.changepoints[
            np.abs(np.nanmean(m.params['delta'], axis=0)) >= changepoints_threshold
        ]
        data.append(go.Scatter(
            x=signif_changepoints,
            y=fcst.loc[fcst['ds'].isin(signif_changepoints), 'trend'],
            marker=dict(size=50, symbol='line-ns-open', color=trend_color,
                        line=dict(width=line_width)),
            mode='markers',
            hoverinfo='skip'
        ))

    layout = dict(
        showlegend=False,
        yaxis=dict(
            title=ylabel
        ),
        xaxis=dict(
            title=xlabel,
            type='date',
            rangeselector=dict(
                buttons=list([
                    dict(count=7,
                         label='1w',
                         step='day',
                         stepmode='backward'),
                    dict(count=1,
                         label='1m',
                         step='month',
                         stepmode='backward'),
                    dict(count=6,
                         label='6m',
                         step='month',
                         stepmode='backward'),
                    dict(count=1,
                         label='1y',
                         step='year',
                         stepmode='backward'),
                    dict(step='all')
                ])
            ),
            rangeslider=dict(
                visible=True
            ),
        ),
    )
    return {
        'data': data,
        'layout': layout
    }

if __name__ == '__main__':
    app.run_server(port=8888, host='0.0.0.0', debug=True)
----
. Add an `app.sh` file to the project, which provides the commands to
instantiate the app:
+
//[source,python]
+
----
python app.py
----
. Publish the App.
.. Go to the App page under the Publish menu of your project.
.. Enter a title and a description for your app.

.. Click *Publish*.
.. After the app status appears as *Running* (which might take a few
minutes), you can click *View App* to open it.

. Share your app with your colleagues.
.. Go to the Publish/App page and select the *Permissions* tab.
.. Invite your colleagues by username or email.
.. Or, toggle the Access Permissions level to make it publicly
available.

See link:71635d[Domino Apps] for more information.

----- user_guide/get-started/get-started-python/index.txt -----
:page-version: 6.1
:page-title: Get started with Python
:page-permalink: 9a69d9
:page-order: 10


This tutorial will guide you through a common model lifecycle in Domino.
You will start by working with data from the https://www.bmreports.com/bmrs/?q=help/about-us[Balancing Mechanism Reporting Service^] in the UK. We will be exploring the https://www.bmreports.com/bmrs/?q=generation/fueltype[Electricity
Generation by Fuel Type] and predicting the electricity generation in the future. You'll see examples of http://jupyter.org/[Jupyter^], https://dash.plot.ly/[Dash^], https://pandas.pydata.org/[pandas^], and https://facebook.github.io/prophet/[Prophet^] used in Domino.

The following content is meant to be followed in sequence.

* link:d24d82[Create a Project]
* link:68ddd0[Configure your Project]
* link:93aef2[Start a Workspace]
* link:0ed610[Get your files and data]
* link:288e42[Develop your model]
* link:fb9e32[Clean up Workspaces]
* link:9f10c9[Deploy your model]

----- user_guide/get-started/get-started-r/1-create-project.txt -----
:page-version: 6.1
:page-title: Create an R Project
:page-sidebar: Create a Project
:page-permalink: ccba8f
:page-order: 10

Work in Domino happens in _projects_. Projects contain data, code, and
environment settings, and the entire project is tracked and revisioned
automatically. A new commit is written to a project each time its files
are changed by user action, or by the execution of code in the project.
Users in Domino can create their own new projects, invite other users to
collaborate on them, and export data or results for consumption by other
projects.

== Create a Project:

. In the top navigation, click **Develop > Projects**.

. Click **Create New Project**.

. Give your project an informative name (like `power-generation`)

. Set its link:71afc6[Visibility] to `Private`.

. Click **Create Project**.

----- user_guide/get-started/get-started-r/2-configure-project.txt -----
:page-version: 6.1
:page-title: Configure your R Project
:page-sidebar: Configure your Project
:page-permalink: 9b7b29
:page-order: 20

Every project has its own settings. The following options are important
to consider when configuring a new project:

* link:dba65c#hardware_tier[Hardware Tier]
* link:f51038[Environment]
* link:d7731d[Collaborators]


== Step 1: Select your hardware tier

A link:dba65c#hardware_tier[Hardware Tier]
represents the compute resources that will be available for your run.
You can specify memory, CPU cores, and GPUs with hardware tiers.

The hardware tier dropdown menu lists your available options. The
selected hardware tier will be used by default for all subsequent
executions of code in the project. It can also be changed at any point
in the future.

. In the Project menu, click **Settings**.
. Click the Hardware tier menu to select the compute resource from which to execute your code. Choose the smallest or default hardware tier for this tutorial. Your options might look different from the following image.
+
This list of available hardware tiers is customizable by your Domino
administrators. If you want additional resources, contact your
Domino administrator.


== Step 2: Configure your Environment

An link:f51038[Environment] is a Domino abstraction on top of a Docker image that provides additional flexibility and versioning. You can configure the software, packages, libraries, and drivers that you need in your environment.

Domino comes with a default environment called the 
link:0d73c6[Domino Standard Environment], 
which includes Python, R, Jupyter, RStudio, and key data science related
packages and libraries.

. Click the *Compute Environment* menu to select the default project Environment.

[NOTE]
====
To take advantage of link:release_notes/5-0-0[new features in Domino 5.0 and up],
be sure to select the *5.0 Domino Standard Environment*.
====

Your compute environment dropdown will likely have different options. If
you're interested in learning how to add more packages and customize or
create your own Environment, see the help article on
link:f51038[Domino Environments].

== Step 3: Configure the Project permissions

As the owner of the project, you can set different access levels for collaborators and colleagues. Feel free to invite a colleague to be a Contributor to your project.

. Click  the *Access & Sharing* tab.

. (Optional) Enter the email or the username of the user that you would
like to invite.
. (Optional) Enter a welcome message to be sent to your collaborator.

The `Contributor` role allows the invited user to read, write, and
execute code in this project.

See link:7876f1[Collaborator Permissions] for more information about permissions for each collaborator role.

----- user_guide/get-started/get-started-r/3-start-workspace.txt -----
:page-version: 6.1
:page-title: Start an R Workspace
:page-sidebar: Start a Workspace
:page-permalink: 064d93
:page-order: 30

Workspace sessions are interactive sessions hosted by a Domino executor
where you can interact with code notebooks like
https://jupyter.org/[Jupyter^] and https://www.rstudio.com/[RStudio^]. The
software tools and associated configurations available to you are called
Workspaces.

For this tutorial, we will start an Rstudio Workspace.



. Click *Workspaces*.
. Click *Rstudio*.
. Click *Launch Rstudio Workspace*.

When you launch a workspace, a new containerized session is created on a
machine (also known as an executor) in the required hardware tier. The
workspace tool you requested is launched in that container, and your
browser is automatically redirected to the workspace's interface when it's
ready.


After your workspace is up and running, you will see a fresh RStudio interface.
If you're brand new to Rstudio, you might find the https://dss.princeton.edu/training/RStudio101.pdf[RStudio intro^] and https://github.com/rstudio/cheatsheets/blob/main/rstudio-ide.pdf[cheatsheet^] helpful.

See link:03e062[Add Workspace IDEs] if you're interested in adding additional Workspaces for tools that are available by default.

----- user_guide/get-started/get-started-r/4-get-data.txt -----
:page-version: 6.1
:page-title: Get your files and data into R
:page-sidebar: Get your files and data
:page-permalink: a177dd
:page-order: 40


There are two strategies to working with data in Domino:

* You can copy your data into Domino
+
If you are working with data that is on your local machine or in a
shared server, you might want to link:7a0fee[upload your data into Domino].
* You can query your data from Domino
+
If you have a large dataset stored in a database or data service, you
might just have to query the database or the API for the data service.

In this tutorial, we will use the terminal in RStudio to copy data into
the project.

. If you have not done so yet, first link:064d93[start an RStudio workspace].
Your starting file path is `/mnt`. By default, this is considered the
root of your Domino project. If you add or modify files in `/mnt`, you
can save them back to your project when you stop or sync the workspace.
+
[NOTE]
====
Your outbound internet access may be blocked in Trial or POC instances. If you have access to a hardware tier with egress, use that as the hardware tier and follow the normal steps. If you do not have access to a hardware tier with egress, follow these steps.

. Copy and paste the following URL into your browser to download the data: https://data.elexon.co.uk/bmrs/api/v1/datasets/FUELINST?format=csv&PublishDateTimeFrom=2019-09-29T20:30:00.000Z&PublishDateTimeTo=2019-10-02T20:30:00.000Z.
. Rename the file to `data.csv`.
. From the navigation pane, in your project, click
*Code*.
. Click the upload file to upload the file.
. Skip to the last item in the normal steps.

====
+
. Use the *Tools > Terminal > New Terminal* menu to open a RStudio
terminal.
. In the new terminal, run the following command to fetch some data from
the BMRS:
+
[source,r]
----
curl -o data.csv "https://data.elexon.co.uk/bmrs/api/v1/datasets/FUELINST?format=csv&PublishDateTimeFrom=2019-09-29T20:30:00.000Z&PublishDateTimeTo=2019-10-02T20:30:00.000Z"
----

. In the *File Changes* tab of your workspace, type a commit message in the text box.
. *Sync All Changes*
to save the new file `data.csv` to Domino.

+
This saves any changes that were made in your workspace
session back to your project.

. Stop your workspace.

. Click the Domino logo to return to your project.
In the navigation pane, go to the
*Code*
section of Domino. Notice that the raw
data has been saved in the latest revision.


See the documentation for other methods to copy data into Domino (https://curl.se/[curl^] or https://www.gnu.org/software/wget/[Wget^]) and
link:fbb41f[query data from Domino].

----- user_guide/get-started/get-started-r/5-develop-model.txt -----
:page-version: 6.1
:page-title: Develop your R model
:page-sidebar: Develop your model
:page-permalink: d8138c
:page-order: 50

When you are developing your model, you want to be able to quickly
execute code, see outputs, and make iterative improvements. Domino
enables this with Workspaces. We previously covered
link:064d93[how to start a Workspace] and explored Workspace options like VSCode,
RStudio, and Jupyter.

In this section, we will use Rstudio to load, explore, and transform
some data. After the data has been prepared, we will train a model.

== Step 1: Load and explore the dataset

. If you have not done so yet, first link:a177dd[download the dataset].
You should see `data.csv` in `/mnt` in files pane.


. Use the *New* menu to create a R script and save is as `power.R`.

. Enter these lines to import some packages:
+
[source,r]
----
library(tidyverse)
library(lubridate)
----
. Create a list of the columns according to information on the column
headers at https://www.bmreports.com/bmrs/?q=generation/fueltype/current[Generation by Fuel Type^].
+
[source,r]
----
col_names <-  c('HDF', 'date', 'half_hour_increment',
     'CCGT', 'OIL', 'COAL', 'NUCLEAR',
     'WIND', 'PS', 'NPSHYD', 'OCGT',
     'OTHER', 'INTFR', 'INTIRL', 'INTNED',
     'INTEW', 'BIOMASS', 'INTEM', 'INTEL', 'INTIFA2', 'INTNSL')
----
. Read the file you downloaded into a dataframe and set the column
names, then display the data:
+
[source,r]
----
#Load the data into a data frame
df <- read.csv('data.csv',header = FALSE,col.names = col_names,stringsAsFactors = FALSE)

#remove the first and last row
df <- df[-1,]
df <- df[-nrow(df),]

#Preview the data
View(df)
----

. Execute the script by selecting *Code* > *Source*. This opens the content of the df data frame in a new tab:

+
We can see that this is a time series dataset. Each row is a successive
half hour increment during the day that details the amount of energy
generated by fuel type. Time is specified by the `date` and
`half_hour_increment` columns.

. https://r4ds.had.co.nz/tidy-data.html[Tidy up^] that data so that
variables are in columns, observations are in rows, and values are in
cells. Switch back to the *power.R* code tab and add the following:
+
[source,r]
----
df_tidy <- df %>% gather('CCGT', 'OIL', 'COAL', 'NUCLEAR',
                'WIND', 'PS', 'NPSHYD', 'OCGT',
                'OTHER', 'INTFR', 'INTIRL', 'INTNED',
                'INTEW', 'BIOMASS', 'INTEM', key="fuel", value="megawatt" )
----

. Create a new column `datetime` that represents the starting datetime
of the measured increment. For example, a `20190930` date and `2` half
hour increment means that the time period specified is September 19,
2019 from 12:30am to 12:59am.
+
[source,r]
----
df_tidy <- df_tidy %>% mutate(datetime=as.POSIXct(as.Date(date, "%Y%m%d"))+minutes(30*(half_hour_increment-1)))
----
. Visualize the data to see how each fuel type is used during the day by
plotting the data.
+
[source,r]
----
#plot the graph
p <- ggplot(data=df_tidy, aes(x=datetime, y=megawatt, group=fuel)) +
  geom_line(aes(color=fuel))
print(p)
----
+
Execute the script again by selecting *Code > Source*.
This will update the *Plot* tab.
+
The CCGT column representing "combined-cycle gas turbines" seems to be
the most interesting. It generates a lot of energy and is very volatile.
+
We will concentrate on this column and try to predict the power
generation from this fuel source.

== Step 2: Train a model

Data scientists have access to many libraries and packages that help
with model development. Some of the most common for R are randomForest,
caret, and nnet. These packages are already installed in the
link:0d73c6#dse[Domino Standard Environment],
the default environment. However, there may be times that you want to
experiment with a package that is new and not installed in the
environment.

We will build a model with the
https://facebook.github.io/prophet/[Facebook Prophet^] package, which is
not installed into the default environment. You will see that you can
quickly get started with new packages and algorithms just as fast as
they are released into the open source community.


. In your R console, install Facebook Prophet. This might take
>5 minutes to install. If it fails, make sure that you've selected a
hardware tier with >2gb of ram:
+
[source,r]
----
# Install specific version of the prerequisite RcppParallel package as its latest version 5.0.2 fails to install
install.packages('https://cran.r-project.org/src/contrib/Archive/RcppParallel/RcppParallel_5.0.1.tar.gz', repos=NULL, type="source")
# Now install latest version of Facebook Prophet
install.packages('prophet')
----
. For Facebook Prophet, the time series data needs to be in a DataFrame
with 2 columns named `ds` and `y`. Let's rename the columns and filter
to just to fuel type "CCGT":
+
[source,r]
----
df_CCGT <- df_tidy %>% filter(fuel=="CCGT") %>% select(datetime,megawatt)

names(df_CCGT) <- c("ds","y")
----
. Split the dataset into train and test sets:
+
[source,r]
----
split_index <- round(nrow(df_CCGT)*.8)
df_CCGT_train <- df_CCGT[1:split_index,]
df_CCGT_test <- df_CCGT[(split_index+1):nrow(df_CCGT),]
----
. Import Facebook Prophet and fit a model:
+
[source,r]
----
library(prophet)
m <- prophet(df_CCGT_train)
----
. Make a DataFrame to hold prediction and predict future values of CCGT
power generation:
+
[source,r]
----
predict_ln <- round(nrow(df_CCGT_test)/2)
future <- make_future_dataframe(m, periods = predict_ln,freq = 1800 )
forecast <- predict(m, future)
----
. Plot the fitted line with the training and test data:
+
[source,r]
----
p <- dyplot.prophet(m, forecast)
print(p)
----
. Save the code.

== Step 3: Export the model

Trained models are meant to be used. There is no reason to re-train the
model each time you use the model. Export or serialize the model to a
file to load and reuse the model later. In R, you can commonly use the
`saveRDS` command to create RDS files.


. Export the trained model as an rds file for later use:
+
[source,r]
----
saveRDS(m, file = "model.rds")
----

We will use the serialized model later when we link:731fd1[create an API from the model].

----- user_guide/get-started/get-started-r/6-clean-up.txt -----
:page-version: 6.1
:page-title: Clean up R Workspaces
:page-sidebar: Clean up Workspaces
:page-permalink: 118efa
:page-order: 60

To avoid spending unnecessary compute resources, make sure to stop any
Workspace sessions that you started as a part of this tutorial. If your
Domino is deployed in the cloud, this will prevent you from incurring
unnecessary charges. If your Domino is deployed on premises, this will
free up your compute resources for others to use.

There are two places in the Domino UI that you can stop your session.

== Option 1: Stop your Workspace session from inside the workspace

. Above your Rstudio workspace in the blue menu bar, save your files and
click `Stop`.

. Enter a descriptive commit message.
. Click *Stop and Commit*.


== Option 2: Stop your Workspace session from the Workspaces page

. Go the to the Workspaces page in Domino. Since the Rstudio workspace
opened in a new tab, you may have to select the previous Domino tab.
. Click *Stop* for your workspace session.

+
. Click Stop and Commit.


In both options, you were able to select *Stop and Commit*. The *Stop and
Commit* button stops the workspace session and also saves your work back
to your project. The new files created from Step 5 should now be visible
on the Files page.

----- user_guide/get-started/get-started-r/7-deploy.txt -----
:page-version: 6.1
:page-title: Deploy your R model
:page-sidebar: Deploy your model
:page-permalink: 731fd1
:page-order: 70

After you have developed your model and deemed it good enough to be
useful, you will want to deploy it. There is no single deployment method
that is best for all models. Therefore, Domino offers four different
deployment options. One may fit your needs better than the others
depending on your use case.

The available deployment methods are:

* Scheduled reports
* Launchers
* Web applications
* Domino endpoints

The remaining sections of this tutorial are not dependent on each other.
For example, you will not need to complete the *Scheduled report*
section to understand and complete the *Web application* section.


== Compute Environments

In our previous section, Step 5, we installed the Prophet package in
Rstudio in order to train the model. In Domino, any package installed in
one work session will not persist to another. In order to avoid having to
re-install Prophet each time we need it, you can add it to a custom
compute environment.

. Create a new compute environment.

.. Go to the Environments page in Domino.
.. Click *Create Environment*.


.. Click *Create Environment*.
.. Click *Edit Definition*.


.. In the *Dockerfile Instructions* section, enter the following:
+
[source,dockerfile]
----
RUN R --no-save -e "install.packages(c('prophet'))"
----

.. Scroll to the bottom of the page and click *Build*.
+
This will start the creation of your new compute environment. These
added packages will now be permanently installed into your environment
and be ready whenever you start a job or workspace session with this
environment selected.

.. Go back to your project page and go to the Settings page.
.. Select your newly created environment from the *Compute Environments* menu.

If you want to learn more about how to customize your environment, see the link:f51038[Environments] tutorials. You can also learn more about what's included in our default environment, the
link:0d73c6#dse[Domino Standard Environment].

== Scheduled reports

The link:5dce1f[Scheduled Jobs] feature in Domino
allows you to run a script on a regular basis. In Domino, using the R
package knitr, you can blend text, code, and plots in an RMarkdown to
create attractive HTML or pdf reports automatically.

In our case, we can imagine that each day we receive new data on power
usage and want to email out a visualization of the latest data daily.


. link:064d93[Start] a new Rstudio session.
. Create a new Rmarkdown file named `power_report.Rmd` and select HTML
as our desired output.

. Rstudio automatically creates a sample Rmarkdown file for you, but you
can replace it entirely with the following which reuses code from our
`power.R` script from step 5.
+
[source,r]
----
title: "Power_Report"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)

col_names <-  c('HDF', 'date', 'half_hour_increment',
                           'CCGT', 'OIL', 'COAL', 'NUCLEAR',
                           'WIND', 'PS', 'NPSHYD', 'OCGT',
                           'OTHER', 'INTFR', 'INTIRL', 'INTNED',
                           'INTEW', 'BIOMASS', 'INTEM')
df <- read.csv('data.csv', header = FALSE, col.names = col_names, stringsAsFactors = FALSE)

#remove the first and last row
df <- df[-1,]
df <- df[-nrow(df),]

#Tidy the data
df_tidy <- df %>% gather('CCGT', 'OIL', 'COAL', 'NUCLEAR',
                       'WIND', 'PS', 'NPSHYD', 'OCGT',
                       'OTHER', 'INTFR', 'INTIRL', 'INTNED',
                       'INTEW', 'BIOMASS', 'INTEM', key="fuel", value="megawatt")
```
----
+
*R Markdown*
+
Combining R Markdown, Knitr and Domino allows you to create attractive
scheduled reports that mix text, code and plots.
+
[source,r]
----
```{r, echo=FALSE, warning=FALSE}
df_tidy <- df_tidy %>% mutate(datetime=as.POSIXct(as.Date(date, "%Y%m%d"))+minutes(30*(half_hour_increment-1)))
print(head(df_tidy))
```
----
+
*Including Plots*
+
You can also embed plots, for example:
+
[source,r]
----
```{r, echo=FALSE}
p <- ggplot(data=df_tidy, aes(x=datetime, y=megawatt, group=fuel)) +
    geom_line(aes(color=fuel))
print(p)
```
----
. With your new Rmarkdown file, you can "knit" this into an html file
and preview it directly in Domino by hitting the "Knit" button.

. To create a repeatable report, you must create a script that you
can schedule that will automatically render your Rmarkdown file to html.
Start by creating a new R script named `render.R` with the following
code:
+
[source,r]
----
rmarkdown::render("power_report.Rmd")
----
. Save your files and *Stop and Commit() your workspace.
. Go to the Scheduled Jobs page.


. Enter the file that you want to run. This will be the `render.R` script you created earlier.
. Select how often and when to run the file.
. Enter emails of people to send the resulting file(s) to.


. Click *Schedule*.

To discover more tips on how to customize the resulting email, see link:5b84c5[Set Notification Preferences] for more information.

== Launchers

Launchers are simple web forms that allow users to run templatized
scripts. They are especially useful if your script has command line
arguments that dynamically change the way the script executes. For
heavily customized script, those command line arguments can quickly get
complicated. Launcher allows you to expose all of that as a simple web
form.

Typically, we parameterize script files (that is, files that end in `.py`,
`.R`, or `.sh`). Since we have been working with an R script until now,
we will parameterize and reuse our R script that we created when we
link:d8138c[developed the R model].

To do so, we will insert a few new lines of code into a copy of the R
script, and configure a Launcher.

. Parameterize your R script by setting it to take command line
arguments:
.. link:064d93[Start] an Rstudio session.
.. Create script named `Power_for_Launcher.R` with the following:
+
[source,r]
----
library(tidyverse)
library(lubridate)

#Pass in command line arguments
args <- commandArgs(trailingOnly = TRUE)
fuel_type <- args[1]

col_names <-  c('HDF', 'date', 'half_hour_increment',
                'CCGT', 'OIL', 'COAL', 'NUCLEAR',
                'WIND', 'PS', 'NPSHYD', 'OCGT',
                'OTHER', 'INTFR', 'INTIRL', 'INTNED',
                'INTEW', 'BIOMASS', 'INTEM')
df <- read.csv('data.csv', header = FALSE, col.names = col_names, stringsAsFactors = FALSE)

#remove the first and last row
df <- df[-1,]
df <- df[-nrow(df),]

#Tidy the data
df_tidy <- df %>% gather('CCGT', 'OIL', 'COAL', 'NUCLEAR',
                         'WIND', 'PS', 'NPSHYD', 'OCGT',
                         'OTHER', 'INTFR', 'INTIRL', 'INTNED',
                         'INTEW', 'BIOMASS', 'INTEM', key="fuel", value="megawatt" )

#Create a new column datetime that represents the starting datetime of the measured increment.
df_tidy <- df_tidy %>% mutate(datetime=as.POSIXct(as.Date(date, "%Y%m%d"))+minutes(30*(half_hour_increment-1)))

#Filter the data
df_fuel_type <- df_tidy %>% filter(fuel==fuel_type) %>% select(datetime,megawatt)

#Save out data as csv
write.csv(df_fuel_type, paste(fuel_type,"_",Sys.Date(),".csv",sep=""))
----

.. Notice the lines in our script that define an object from a command
line arguments
+
[source,r]
----
args <- commandArgs(trailingOnly = TRUE)
fuel_type <- args[1]
----

.. Save the files and Stop and Commit the workspace session.

. Configure the Launcher.
.. Go to the Launcher page. It is under the Publish menu on the project page.
.. Click *New Launcher*.
.. Name the launcher "Power Generation Forecast Data".
.. Copy and paste the following into the field "Command to run":
+
[source,r]
----
Power_for_Launcher.R ${fuel}
----
+
You should see the following parameters:


.. Select the *fuel_type* parameter and change the type to *Select (Drop-down menu)*.
.. Copy and paste the following into the *Allowed Values* field:
+
[source,r]
----
CCGT, OIL, COAL, NUCLEAR, WIND, PS, NPSHYD, OCGT, OTHER, INTFR, INTIRL, INTNED, INTEW, BIOMASS, INTEM
----

.. Click *Save Launcher*.

. Try out the Launcher.
.. Go back to the main Launcher page.
.. Click *Run* for the "Power Generation Forecast Trainer" launcher.
.. Select a fuel type from the dropdown.
.. Click *Run*.
+
This will execute the parameterized R script with the parameters that
you selected. In this particular launcher, your dataset is filtered
based on your input parameter with the results returned as a csv. When
the run has been completed, an email will be sent to you and others that
you optionally specified in the launcher with the resulting files. If
you optionally specified in the launcher with the resulting files.
See link:5b84c5[Set Custom Execution Notifications] to learn how to customize the resulting email.

== Domino endpoints

If you want your model to serve another application, you will want to
serve it in the form of an API endpoint.
link:8dbc91[Domino endpoints] are scalable REST
APIs that can create an endpoint from any function in a Python or R
script. The Domino endpoints are commonly used when you need an API to
query your model in near real-time.

For example, we created a model to forecast power generation of combined
cycle gas turbines in the UK.

In this section, we will deploy an API that uses the
 link:d8138c[previously trained model] to predict the generated power
given a date in the future. To do so, we will create a new
link:f51038[compute environment] to install
necessary packages, create a new file with the function we want to
expose as an API, and finally deploy the API.

[[NewComputeEnvironment]]

. Create a new file with the function we want to expose as an API.
.. From the Files page of your project, click *Add File*.
.. Name your file `forecast_predictor.R`.
.. Enter the following contents:
+
[source,r]
----
library("prophet")
m <- readRDS(file = "model.rds")

domino_endpoint <- function(year, month, day, hour, minute) {
  date <- paste(year, "-", month, "-", day, " ", hour, ":", minute, sep="")
  date = as.POSIXct(date, format="%Y-%m-%d %H:%M")
  df_api <- data.frame(ds=date)
  df2 <- predict(m, df_api)
  return(df2["yhat"])
}
----

.. Click *Save*.


. Deploy the API.
.. Go to the Publish/Domino endpoints page in your project.
.. Click *New endpoint*.
.. Name your model, provide a description, and click *Next*.
.. Enter the name of the file that you created in the previous step.
.. Enter the name of the function that you want to expose as an API.
.. Click *Create endpoint*.

. Test the API.
.. Wait for the Domino endpoint status to turn to Running. This might take a few minutes.
.. Click the *Overview* tab.
.. Enter the following into the tester:
+
[source,json]
----
{
  "data": {
    "year": 2019,
    "month": 10,
    "day": 15,
    "hour": 8,
    "minute": 15
  }
}
----

.. Click *Send*. If successful, you will see the response on the right panel.


As a REST API, any other common programming language will be able to
call it. Code snippets from some popular languages are listed in the
other tabs.

Domino endpoints are built as docker images and deployed on Domino. You can
export the endpoint images to your external container registry and deploy
them in any other hosting environment outside of Domino using your
custom CI/CD pipeline.
The link:8c929e[Domino Platform API] enables you to programmatically build new endpoint images on Domino and
export them to your external container registry.

== Web applications

When experiments in Domino yield interesting results that you want to
share with your colleagues, you can easily do so with a
link:71635d[Domino App]. Domino supports hosting
Apps built with many popular frameworks, including Flask, Shiny, and
Dash.

While Apps can be significantly more sophisticated and provide far more
functionality than a Launcher, they also require significantly more code
and knowledge in at least one framework. In this section, we will
convert some code that we developed when we link:d8138c[developed the model]
and create a https://shiny.rstudio.com/[Shiny^] app.


. Add the `app.R` file, which will describe the app in Shiny, to the
project:
+
[source,r]
----
library(tidyverse)
library(lubridate)
library(prophet)
library(dygraphs)

col_names <-  c('HDF', 'date', 'half_hour_increment',
                'CCGT', 'OIL', 'COAL', 'NUCLEAR',
                'WIND', 'PS', 'NPSHYD', 'OCGT',
                'OTHER', 'INTFR', 'INTIRL', 'INTNED',
                'INTEW', 'BIOMASS', 'INTEM')
df <- read.csv('data.csv',header = FALSE,col.names = col_names,stringsAsFactors = FALSE)

#remove the first and last row
df <- df[-1,]
df <- df[-nrow(df),]

fuels <- c('CCGT', 'OIL', 'COAL', 'NUCLEAR',
           'WIND', 'PS', 'NPSHYD', 'OCGT',
           'OTHER', 'INTFR', 'INTIRL', 'INTNED',
          'INTEW', 'BIOMASS', 'INTEM')

predict_ln <- round((nrow(df))*.2)

#Tidy the data and split by fuel
df_tidy <- df %>%
  mutate(ds=as.POSIXct(as.Date(date, "%Y%m%d"))+minutes(30*(half_hour_increment-1))) %>%
  select(-c('HDF', 'date', 'half_hour_increment')) %>%
  gather("fuel", "y", -ds) %>%
  split(.$fuel)

#remove unused column
df_tidy <- lapply(df_tidy, function(x) { x["fuel"] <- NULL; x })

#Train the model
m_list <- map(df_tidy, prophet)

#Create dataframes of future dates
future_list <- map(m_list, make_future_dataframe, periods = predict_ln,freq = 1800 )

#Pre-Calc yhat for future dates
#forecast_list <- map2(m_list, future_list, predict) # map2 because we have two inputs



ui <- fluidPage(
    verticalLayout(
      h2(textOutput("text1")),
      selectInput(inputId = "fuel_type",
                 label = "Fuel Type",
                 choices = fuels,
                 selected = "CCGT"),
      dygraphOutput("plot1")))

server <- function(input, output) {
  output$plot1 <- renderDygraph({
    forecast <- predict(m_list[[input$fuel_type]], future_list[[input$fuel_type]])
    dyplot.prophet(m_list[[input$fuel_type]], forecast)
  })
  output$text1 <- renderText({ input$fuel_type })
}

shinyApp(ui = ui, server = server)
----
. Add an `app.sh` file to the project, which provides the commands to
instantiate the app:
+
[source,r]
----
R -e 'shiny::runApp("app.R", port=8888, host="0.0.0.0")'
----

. Publish the App.
.. Go to the App page under the Publish menu of your project.
.. Enter a title and a description for your app.
.. Click *Publish*.
.. After your app starts successfully, which might take a few minutes, you can click *View App* to open it.

. Share your app with your colleagues.
.. Back on the Publish/App page, click the *App Permissions* tab.
.. Invite your colleagues by username or email.
.. Or, toggle the Access Permissions level to make it publicly available.
+
image::/images/4.x/share_app.png[alt="Share your app", width=800]

See link:71635d[Domino Apps] for more information.

----- user_guide/get-started/get-started-r/index.txt -----
:page-version: 6.1
:page-title: Get started with R
:page-permalink: c5ce58
:page-order: 20

This tutorial will guide you through a common model lifecycle in Domino.
You will start by working with data from the
https://www.bmreports.com/bmrs/?q=help/about-us[Balancing Mechanism
Reporting Service^] in the UK. We will be exploring the
https://www.bmreports.com/bmrs/?q=generation/fueltype[Electricity
Generation by Fuel Type^] and predicting the electricity generation in
the future. 
You'll see examples of https://en.wikipedia.org/wiki/RStudio[RStudio^], https://shiny.rstudio.com/[Shiny^], https://www.tidyverse.org/[tidyverse^], and https://facebook.github.io/prophet/[Prophet^] used in Domino.

The following content is meant to be followed in sequence.

* link:ccba8f[Create a Project]
* link:9b7b29[Configure your Project]
* link:064d93[Start a Workspace]
* link:a177dd[Get your files and data]
* link:d8138c[Develop your model]
* link:118efa[Clean up Workspaces]
* link:731fd1[Deploy your model]

----- user_guide/get-started/get-started-snowflake/0-understand-the-data.txt -----
:page-version: 6.1
:page-permalink: 0927f7
:page-title: Understand the data
:page-order: 10

The US National Oceanic and Atmospheric Administration (NOAA) collects climate data across the globe. NOAA provides location and date-based climate records for thousands of weather stations around the world in the form of https://www.ncei.noaa.gov/access/search/data-search/daily-summaries[Global Historical Climatology Network (GHCN)^] data. This data includes:

* Minimum temperature,
* Maximum temperature,
* Precipitation,
* Snowfall,
* Wind,
* and https://www.ncei.noaa.gov/pub/data/ghcn/daily/readme.txt[much more^].

== Scenario
A friend in Berlin, Germany, asks you if they should buy an air conditioner.
You can use https://www.ncei.noaa.gov/pub/data/ghcn/daily/[climate data^] from https://www.noaa.gov/[NOAA^] to build a simple prediction tool that uses machine learning (ML) regression.

For this tutorial, you will use the https://www.ncei.noaa.gov/access/search/data-search/daily-summaries?bbox=55.352,6.151,46.870,14.633[Global Historical Climatology Network – Daily (GHCND)^] dataset. 

=== Option 1: Data per location (weather station)

The station data file contains a pre-set collection of data points (high/low temperature, wind speed, precipitation, etc.) for one day. This makes it easy to review and understand the data.

An initial approach would be to use historical data from a single station. Weather from every day of the last century or so was recorded in the source data.

To simplify things, you can then use data from the last couple of years and focus on Berlin.

However, you will find that this specific weather station shut down in the last two years, together with the airport (Tegel) that housed it.
There are also gaps in the data (probably due to the world wars that took place in the area) that might need special consideration.

=== Option 2: Super GHCND data (global dataset)

In this large dataset (12GB tar gzipped, ~103GB), each row contains one data point about one weather station per day, including:

* `station_id`
* `date`
* `data point`
* `value` fields 

For example, the data would read as follows: the station in Potsdam Germany, January 9, 1960, maximum temperature, 15c, followed by metadata about the observation.

While this format is more confusing, it offers a foundation to collect data in increments from this point on, as NOAA provides a daily diff file.

To maintain the data on your own server, you can download the daily updates and changes (which will also be used to demonstrate Domino's model monitoring capability later on).

[[data-files]]
The Super GHCND dataset contains a daily-updated `all-station-history-data` file called `superghcnd_full_<creation date>.csv.gz` from the link:https://www.ncei.noaa.gov/pub/data/ghcn/daily/superghcnd/[GHCND index page^], along with several metadata files:

** `ghcnd-countries` – the list of country codes.
** `ghcnd-states` – the list of states and provinces.
** `ghcnd-stations` – the weather station names, codes, and location information.
** `ghcnd-inventory` – inventory listing the availability of data points for each weather station. For example, a station may offer daily high and low temperatures from 1929 to the current day, but wind speed is only available from 1944.

== Next steps

Let's link:054d0b[prepare and load the data into Snowflake].

----- user_guide/get-started/get-started-snowflake/1-data-engineering.txt -----
:page-version: 6.1
:page-permalink: 054d0b
:page-title: Data engineering - Prepare and load the data into Snowflake
:page-sidebar: Data engineering
:page-order: 20
:source-highlighter: highlightjs

// TODO: Validate all steps against the new UI that will be implemented with the 6.0 release.

This document will guide you to engineer the NOAA data into a form Snowflake finds palatable to digest, and then load the data from text/CSV files into Snowflake tables.

== Data engineering in Domino

Before you can load the files into Snowflake, you need to download and adapt them to the format that would work for its data-loading functions.

This document will guide you on how to prepare your data through the following steps:

. Download the dataset files.
. Adapt them to a CSV or another character-delimited file (because commas don't work in all situations).
. Upload the file into Snowflake using the SnowSQL tool.
. Use SnowSQL to load the files into database tables.

=== Big data in Domino: Domino Datasets

When you want to download a large file in the hundreds of megabyte size range or larger, or have a vast number of data files to process, Domino recommends that you use your project's Dataset facility instead of files.
Besides a clean separation between your code and data, Datasets also allow Domino to optimize file storage and file sharing, and create an easy way to load data into your Environment.

Since the first file is at least 11GB, use a Domino Dataset.

. Log into Domino and create a new Project called `Snowflake-Quickstart`.

. Open a new Workspace, normally the Domino default Environment with Python, and choose *JupyterLab* as your IDE.

. Click *Launch*.
A new browser tab will open and your Workspace will start loading.
After a minute or two – depending on the availability of resources in your Domino Environment – the Workspace will open and display a *Launcher* page.

. In the *Other* section, click on the *Terminal* shortcut to start a terminal.
This will allow you to access and manipulate data files using Linux's built-in file processing tools.

. Since you are downloading large amounts of data, you need to store the data in a Domino Dataset.
Domino's Datasets reside in a special directory.
Switch to that directory for the time being:
+
[source,bash]
----
cd /domino/datasets/local/Snowflake-Quickstart/
----

. Use the `wget` command to download the link:0927f7#data-files[dataset files]:
+
[source,bash]
----
wget https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt
wget https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-countries.txt
wget https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-states.txt
wget https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-inventory.txt
wget https://www.ncei.noaa.gov/pub/data/ghcn/daily/superghcnd/superghcnd_full_<date of newest file>.csv.gz
----

. Unzip the large `superghcnd_full_*` file using the `gunzip` command:
+
[source,bash]
----
gunzip superghcnd_full_<date>.csv.gz
----
+
Replace `<date>` with the specific date shown in the filename.
Depending on the Domino instance you are using, this extraction can take a while.
You can add an ampersand (`&`) to the end of the line to run the command in the background.

=== Format the file

While the main data file is a 'proper' CSV file, these metadata files use size-delimited fields (column width as measured in the number of characters).
For example:
....
----------------------------------
Variable    Columns   Type
----------------------------------
ID          1-11      Character
LATITUDE    13-20     Real
LONGITUDE   22-30     Real
ELEMENT     32-35     Character
FIRSTYEAR   37-40     Integer
LASTYEAR    42-45     Integer
----------------------------------
....

In practice, it looks something like this:

....
ACW00011647  17.1333  -61.7833 WT16 1961 1966
FM00040990  31.5000   65.8500 TAVG 1973 2020
AG000060390  36.7167    3.2500 TMAX 1940 2022
....

It appears some data in the files contain commas, which can add 'columns' to the file unintentionally if a comma is used as a delimiter.
While it is suboptimal to use the vertical bar (`|`) as the delimiter (the vertical bar is a Unix operand), it will make do for this example Project.
Since Snowflake does not support field size-based data ingestion, you need to use regular expressions and classic Unix utilities, such as `sed` and `tr`, to modify this file to add a delimiter.
https://www.gnu.org/software/sed/[Sed^] is a stream editor, i.e. it reads chunks, normally lines, and can modify each line independent of the next.
In general, `sed` works in a nifty way when using (truly basic) regular expressions:

[source,bash]
----
sed 's/<pattern to match>/<replacement text>/<location in the line or everywhere>' > <output file>
----

While there is more than one way to do it, you can use `sed` as follows:

[source,bash]
----
sed 's/./|/12;s/./|/21;s/./|/31;s/./|/38;s/./|/41;s/./|/72;s/./|/76;s/./|/80' ghcnd-stations.txt > ghcnd-stations.csv
----

Look at the first replacement in the line to understand what's happening:

[source,bash]
----
s/./|/12
----

That would mean that, for each line of the input, count twelve characters that match the regular expression `.` (any character), then replace the twelfth character with a vertical bar (`|`).

These directives are repeated throughout the line, separated by semicolons, to place vertical bar characters to delimit the fields based on the number of characters in the line.
The outcome looks like this:

....
ACW00011647|17.1333|-61.7833|WT16|1961|1966
AFM00040990|31.5000|65.8500|TAVG|1973|2020
...
AG000060390|36.7167|3.2500|TMAX|1940|2022
....

Similarly, you can also convert the inventory, country, and state files:

[source,bash]
----
sed 's/./|/12;s/./|/21;s/./|/31;s/./|/36;s/./|/41' ghcnd-inventory.txt > ghcnd-inventory.csv
sed 's/./|/3' ghcnd-countries.txt > ghcnd-countries.csv
sed 's/./|/3' ghcnd-states.txt > ghcnd-states.csv
----

=== Collect a data subset

Western Europe has consistently collected weather data over the last 70 years or so for Germany (mostly West Germany), Italy, France, Spain, United Kingdom, Portugal, The Netherlands, Belgium, Switzerland, and Austria.
Therefore, datasets as big as 100GB are reasonable in the real world.
However, you can extract a subset of this data (with weather information for Western Europe) of around 2GB to simplify matters.

Since each entry in the dataset represents one data item per weather station, it starts with the weather station's ID.
That ID starts with the country code, so you can filter the dataset to return only the information you need for these countries.

. Use the `sed` tool to find the identifying country codes then extract the data from the big dataset you unzipped.
+
* One way is to look for one country, get the country code, and extract its data from the full dataset into a file.
+
[source,bash]
----
sed -n '/Neth.*/p'
----
+
Which will return the code for The Netherlands:
+
....
NL Netherlands
....
+
* Alternatively, you can abbreviate it to this:
+
[source,bash]
----
sed -n '/Neth.*/p;/Ita*/p;/Spa*/p;/.. United K.*/p;/Germany/p;/Switz*/p;/Portu*/p;/Belg.*/p;/.. France/p;/Austria/p' ghcnd-countries.txt
----
+
The result:
+
....
AU Austria
BE Belgium
FR France
GM Germany
IT Italy
NL Netherlands
PO Portugal
SP Spain
SZ Switzerland
UK United Kingdom
....

. Next, you can extract the data from the dataset for one country like this:
+
[source,bash]
----
% sed -n '/^NL.*/p' superghcnd_full_20220907.csv > netherlands_data.csv
----
+
To break it down:

* `-n` – Suppresses the output.
* `/^NL.*` – Returns lines starting with (`^`) NL, followed by any number of characters.
* `/p` – Prints the line.
* `> netherlands_data.csv` – Redirects the output to a new CSV file.

. You can combine all these statements for all the countries in all the lines into one command using the codes you extracted:
+
[source,bash]
----
sed -n '/^AU.*/p;/^BE.*/p;/^FR.*/p;/^GM.*/p;/^IT.*/p;/^NL.*/p;/^PO.*/p;/^SP.*/p;/^SZ.*/p;/^UK.*/p' superghcnd_full_<strong><your file date></strong>.csv > west_euro_data.csv
----

. Use an identical approach on the weather stations and inventory files:
+
[source,bash]
----
sed -n '/^AU.*/p;/^BE.*/p;/^FR.*/p;/^GM.*/p;/^IT.*/p;/^NL.*/p;/^PO.*/p;/^SP.*/p;/^SZ.*/p;/^UK.*/p' ghcnd-stations.csv > ghcnd-stations-west-eu.csv

sed -n '/^AU.*/p;/^BE.*/p;/^FR.*/p;/^GM.*/p;/^IT.*/p;/^NL.*/p;/^PO.*/p;/^SP.*/p;/^SZ.*/p;/^UK.*/p' ghcnd-inventory.csv > ghcnd-inventory-west-eu.csv
----

At this point, you are finally ready to go to Snowflake!

== Load the data into Snowflake

The data is loaded into Snowflake through _staging_, which refers to either uploading the data to a special area in your Snowflake account or having Snowflake pull your data from an AWS or Azure data store.

In this tutorial, you will follow the local file-to-stage route.
To upload the data and work with Snowflake directly, Snowflake offers a powerful command-line tool, called https://docs.snowflake.com/en/user-guide/snowsql[SnowSQL^].

=== Connect to the Environment

Domino offers a Snowflake-optimized Environment that has the tool preinstalled.
To connect to this Environment, you need your account ID, username, and password, along with the names of your warehouse, database, and schema (which is often `PUBLIC`).
Once connected, you will get an interface to your very own cloud database.

. The fastest way to connect is via command-line options such as the following:
+
[source,bash]
----
snowsql -a <account ID> -u <user name> -w <warehouse name> -d <database name> -s <schema name> -o log_file=~/.snowsql/log
----
+
Note that in Domino, you may first need to delete the `.snowsql/log` file using:
+
[source,bash]
----
rm ~/.snowsql.log
----

. Once connected, enter your password when prompted.

=== Create the schema

NOTE: Going forward, all steps are executed in SnowSQL.

Before you can upload the data, you need to define tables to hold the data.

This is how it looks in an entity-relationship diagram:

image::/images/6.0/snowflake/NOAA-weather-ERD.jpg[alt="NOAA Weather Data Entity Relationship Diagram", width=850, role=noshadow]

. Below is the corresponding SQL code to create the tables.
Run this script, command by command, in SnowSQL:
+
[source,sql]
----
CREATE OR REPLACE TABLE country (country_id CHAR(2) PRIMARY KEY, country_name VARCHAR(100) NOT NULL);

CREATE OR REPLACE TABLE state_province (state_code CHAR(2) PRIMARY KEY,
NAME VARCHAR(50) NOT NULL);

CREATE OR REPLACE TABLE weather_station(station_id CHAR(12) PRIMARY KEY,
latitude float NOT NULL,
longitude float NOT NULL,
elevation FLOAT NOT NULL,
station_state CHAR(2) FOREIGN KEY REFERENCES state_province (state_code) NULL,
station_name VARCHAR (40) NOT NULL,
gsn CHAR(5) NULL,
hcn_crn CHAR(5) NULL,
wmo_id CHAR(5) NULL);

CREATE OR REPLACE TABLE station_data_inventory (entry_id INT IDENTITY PRIMARY KEY,
station_id CHAR(12) FOREIGN KEY REFERENCES weather_station (station_id),
latitude float NOT NULL,
longitude float NOT NULL,
element VARCHAR(4) NOT NULL,
first_year INT,
last_year INT);

CREATE OR REPLACE TABLE station_data (
   entry_id INT IDENTITY PRIMARY KEY,
   station_id CHAR(12) FOREIGN KEY REFERENCES weather_station (station_id),
   data_date DATE NOT NULL,
   element CHAR(4) NOT NULL,
   element_value INT NOT NULL,
   mflag CHAR(1),
   qflag CHAR(1),
   sflag CHAR(1),
   obs_time TIME);
----

. To see the tables, run the following command:
+
[source,sql]
----
show tables;
----
+
image::/images/6.0/snowflake/snowsql-tables.png[alt="Screenshot of the SnowSQL tool following the execution of the show tables command, listing the tables that were created", width=800, role=noshadow]

=== Upload and stage the data

==== Create a file format object

The following steps are based on Snowflake's own SnowSQL https://docs.snowflake.com/en/user-guide/data-load-internal-tutorial[data loading^] tutorial.

To upload the data you wrangled in the previous steps:

. Create a file format object – this helps guide Snowflake on how to read your file and map it into your database tables.
Although the file type is still CSV (comma-separated values), remember to specify `|` as the field delimiter.
For example:
+
[source,sql]
----
create or replace file format bar_csv_format
type = 'CSV'
field_delimiter = '|'
skip_header = 1;
----

. Create another file format for the main data file, which uses commas as delimiters:
+
[source,sql]
----
CREATE OR REPLACE FILE FORMAT comma_csv_format
type = 'CSV' field_delimiter = ',';
----

==== Create the staging area

Create the stage (i.e. the holding area for uploaded files) and add the file format specification to it (the other file format will be used later):

[source,sql]
----
CREATE OR REPLACE STAGE weather_csv_stage
file_format = bar_csv_format;
----

This creates a staging area called `weather_csv_stage` to hold your uploaded data.

==== Upload the data files

. Upload the files from Domino.
Make sure you adjust the command to the correct directory for your dataset:
+
[source,bash]
----
put file:///domino/datasets/local/<project dataset directory>/ghcnd*.csv @weather_csv_stage auto_compress=true;
----
+
The result should look like this:
+
image::/images/6.0/snowflake/file-listing.png[alt="SnowSQL shows you the list of files that were uploaded", role=noshadow]

. You can also list the files you uploaded using the following command:
+
[source,sql]
----
list @weather_csv_stage;
----

. Upload your Western European data file:
+
[source,bash]
----
put file:///domino/datasets/local/<your project dataset directory>/west_euro_data.csv @weather_csv_stage auto_compress=true;
----
+
This might take a while, but eventually, the result should look something like this:
+
image::/images/6.0/snowflake/file-upload-status.png[alt="Snowflake response to data upload of large file", role=noshadow]

== Copy data files into tables

. Use the `COPY INTO` command to load the data into the Snowflake database tables.
Start with the smaller files and ensure foreign key constraints are documented, as Snowflake does not enforce referential integrity:
+
[source,sql]
----
COPY INTO STATE_PROVINCE
FROM @weather_csv_stage/ghcnd-states.csv.gz
file_format = (format_name = bar_csv_format);

COPY INTO COUNTRY
FROM @weather_csv_stage/ghcnd-countries.csv.gz
file_format = (format_name = bar_csv_format);

COPY INTO
WEATHER_STATION FROM @weather_csv_stage/ghcnd-stations-west-eu.csv.gz
file_format = (format_name = bar_csv_format);
----
+
These are relatively straight-forward and should result in something like this:
+
image::/images/6.0/snowflake/table-loading-result.png[alt="The result of the table loading operation where data from CSV files is loaded into tables", role=noshadow]

. The station data inventory is slightly more complex.
You are using an artificial identity field as the primary key, which requires that you load data from the CSV file into specific columns.
This is done with a subquery, for example:
+
[source,sql]
----
COPY INTO station_data_inventory (station_id, latitude, longitude, element, first_year, last_year)
FROM
    (select t.$1, t.$2, t.$3, t.$4, t.$5, t.$6
    from @weather_csv_stage
    (file_format => 'bar_csv_format',
    pattern => '.*ghcnd-inventory-west-eu.csv.gz') t)
ON_ERROR = CONTINUE;
----
+
The query will result in hundreds of thousands of rows being loaded.
Snowflake will show you how many rows were ready in the result.
Note the use of the `ON_ERROR = CONTINUE` behavior in the query - you can tolerate missing a small number of rows as these are very large data files that can result in errors.

. Finally, load the Western Europe dataset in a similar fashion:
+
[source,sql]
----
COPY INTO STATION_DATA (STATION_ID, DATA_DATE, ELEMENT, ELEMENT_VALUE, MFLAG, QFLAG, SFLAG, OBS_TIME)
FROM
    (SELECT
    t.$1, TO_DATE(t.$2, 'yyyymmdd'), t.$3, t.$4, t.$5, t.$6, t.$7, TO_TIME(t.$8, 'hhmm')
    FROM @weather_csv_stage
    (file_format => 'COMMA_CSV_FORMAT',
    pattern => '.*west_euro_data.csv.gz') t)
ON_ERROR = CONTINUE;
----
+
You should have approximately 120 million rows of data.

. To demonstrate the speed at which Snowflake executes queries, try running the following query to count how many unique station IDs exist in this large table:
+
[source,sql]
----
SELECT COUNT(DISTINCT STATION_ID) AS STATION_COUNT
FROM STATION_DATA;
----

== Next steps

The next step will focus on link:569d37[whittling down the data], this time in Domino, with Snowflake code.

----- user_guide/get-started/get-started-snowflake/2-snowflake-as-domino-data-source.txt -----
:page-version: 6.1
:page-permalink: 569d37
:page-title: Use Domino Data Sources to connect to Snowflake
:page-sidebar: Connect to Snowflake
:page-order: 30
:source-highlighter: highlightjs

// TODO: Validate all steps against the new UI that will be implemented with the 6.0 release.

The simplest and fastest way to connect and interact with the Snowflake database is by using a link:fbb41f[Domino Data Source].
Data Sources simplify the database setup via configuration records and are optimized for read-heavy, SQL-based usage.

== Snowflake Data Source setup

. From inside your project, click on *Data* in the Project menu bar. This will open the *Data* window.

. On the *Data Sources* page, click the *+ Add a Data Source* button.

. In the popup window, select *Snowflake* from the list.

. On the *Configure* window, enter your account and connection credentials, add an optional description, then click *Next*.

. On the next *Authenticate* window, enter your Snowflake username and password then click *Test Credentials*.

. Domino will attempt to connect to the specified database using the user credentials you entered.
If all goes well, you will see a message, confirming that the connection was successful.
Click *Next*.

. On the next *Permissions* screen, click *Finish Setup*.

You are now ready to use the Data Source.

== Use the Data Source in the Workspace

. Start a Domino Workspace.
As the workspace starts up, you will notice a red dot next to *Data* in the Project menu.
Once the Workspace is loaded, click *Data*.

. The left pane of the screen will expand to show the Data Source.
Click the *copy* icon to copy the connection code associated with the Snowflake Data Source.

. Next, on the *Launcher* page, click on the *Python* shortcut in the *Notebook* section to open a Python Notebook.

. Use *Command + V* (on Mac) or *Ctrl + V* on Windows and Linux to paste the code you copied into the first cell of the notebook.
The Data Source functionality in Domino enables users to connect to the database using these templated commands.
It will look something like this:
+
[source,python]
----
from domino.data_sources import DataSourceClient

# Instantiate a client and fetch the DataSource instance
ds = DataSourceClient().get_datasource("weather_db")

# Create a simple wrapper from the query result
res = ds.query("select * from {{table_reference}}")

# Load the result into a Pandas dataframe
df = res.to_pandas()
----

. Modify the third statement (the query) so that it matches one of your database tables:
+
[source,python]
----
res = ds.query("select * from state_province")
----
+
This will load the list of states from the database table into a result object, which is then converted into a Pandas dataframe with the following code:
+
[source,python]
----
df = res.to_pandas()
----

. Add the following code to display the first few rows of the table:
+
[source,python]
----
df.head()
----

. Run the cell to get the following results:
+
....
    STATE_CODE  NAME
----------------------------------
0   AK          ALASKA
1   AL          ALABAMA
2   AR          ARKANSAS
3   AS          AMERICAN SAMOA
4   AZ          ARIZONA
....

. As another example, run the following code, with a different query, to count how many weather stations are located in each country (note that the country code takes the first two characters in the station identifier):
+
[source,python]
----
sfQuery = """SELECT SUBSTR(STATION_ID, 1, 2) AS COUNTRY_CODE, COUNT(STATION_ID) AS STATION_COUNT
          FROM WEATHER_STATION
          GROUP BY SUBSTR(STATION_ID, 1, 2)
          ORDER BY STATION_COUNT DESC;"""

res = ds.query(sfQuery)
df = res.to_pandas()
df
----
+
The following results are returned:
+
....
    COUNTRY_CODE    STATION_COUNT
----------------------------------
0   GM              1123
1   NL              386
2   SP              207
3   UK              158
4   FR              111
5   IT              104
6   PO              26
7   AU              12
8   SZ              10
9   BE              1
....

In summary, the Data Source usage pattern entailed the following:

. Define the query in SQL.
. Use the Data Source to send the query and collect the results.
. Convert the results into a Pandas dataframe.
. Do something productive with the dataframe.

== Next steps

link:ee0e3d[Use Snowflake's Python driver] for read and write operations to create a predictive weather model.

----- user_guide/get-started/get-started-snowflake/3-snowflake-feature-exploration.txt -----
:page-version: 6.1
:page-permalink: ee0e3d
:page-title: Use Snowflake to create a predictive weather model
:page-sidebar: Create a prediction model
:page-order: 40
:source-highlighter: highlightjs

// TODO: Validate all steps against the new UI that will be implemented with the 6.0 release.

In this section, you will use Jupyter Notebook to create a predictive model for a single weather station.
You will explore the data, create a chart, and finally use the https://xgboost.readthedocs.io/en/stable/[XGBoost^] library to create the model.
This will allow you to predict the weather using the data you have stored in Snowflake.

NOTE: Each code snippet can fit inside a Jupyter Notebook cell.
The results displayed are based on data downloaded at the time of writing this tutorial and might not be identical to your results.

== Explore the data

. Import the necessary libraries and connect to the database:
+
[source,python]
----
from domino.data_sources import DataSourceClient
import pandas as pd
import xgboost as xg
import sklearn as sk
from datetime import date, timedelta, datetime
import numpy as np
import os
from pathlib import Path

# Directory in which you will store the model
model_directory = "/mnt/models"

# Hot temperature threshold in degrees Celsius
hot_day_flag = 33

# Instantiate a client and fetch the Data Source instance
ds = DataSourceClient().get_datasource("weather_db")
res = ds.query("select * from state_province")
df = res.to_pandas()
df.head()
----
+
Note the following:
+
* The `hot_day_flag` variable is created to indicate excessively warm days, which will be used in the visualization.
* A separate folder is created in which the model will be stored.
* The database initialization code allows you to connect to and fetch data from the Snowflake Data Source instance.
If the code runs successfully, a list of the states in the database is displayed:
+
....
    STATE_CODE  NAME
-------------------------------
0   AK          ALASKA
1   AL          ALABAMA
2   AR          ARKANSAS
3   AS          AMERICAN SAMOA
4   AZ          ARIZONA
....

. To whittle down your dataset, search for stations that have recorded data from the past 50 years up to the present day.
This will help you build a more reliable model; the more data you have, the better its predictive power.
You can achieve this by examining the data inventory table:
+
[source,python]
----
sfQuery = "SELECT * FROM STATION_DATA_INVENTORY LIMIT 10"
res = ds.query(sfQuery)
df = res.to_pandas()
df.head()
----
+
The result:
+
....
    ENTRY_ID  STATION_ID    LATITUDE  LONGITUDE  ELEMENT  FIRST_YEAR  LAST_YEAR
----------------------------------------------------------------------------------
0   1044481   AU000005010   48.0500   14.1331    TMIN     1876        2022
1   1044482   AU000005010   48.0500   14.1331    PRCP     1876        2022
2   1044483   AU000005010   48.0500   14.1331    SNWD     1982        2022
3   1044484   AU000005010   48.0500   14.1331    TAVG     1977        2022
4   1044485   AU000005901   48.2331   16.3500    TMAX     1855        2022
....

. Of the features tracked by NOAA, see how many stations tracked them for a fairly good period:
+
[source,python]
----
feature_list = ['TMIN','TMAX', 'PRCP', 'RHAV', 'AWND', 'ASTP']

for cur_feature in feature_list:
    sfQuery = f"SELECT COUNT(DISTINCT STATION_ID) AS ELEMENT_COUNT " \
            f"FROM STATION_DATA_INVENTORY WHERE ELEMENT = '{cur_feature}' AND FIRST_YEAR < 1951 AND LAST_YEAR >2021"
    res = ds.query(sfQuery)
    df = res.to_pandas()
    feature_station_count = df.iloc[0,0]
    print(f"The number of stations with 70 years of data for feature {cur_feature} is {feature_station_count}")
----
+
The result:
+
....
The number of stations with 70 years of data for feature TMIN is 268
The number of stations with 70 years of data for feature TMAX is 265
The number of stations with 70 years of data for feature PRCP is 584
The number of stations with 70 years of data for feature RHAV is 0
The number of stations with 70 years of data for feature AWND is 0
The number of stations with 70 years of data for feature ASTP is 0
....
+
It looks like you will need to focus on minimum (`TMIN`) and maximum (`TMAX`) temperatures, along with the precipitation (`PRCP`), as the features for your model.
You will rely on the date (i.e. the time of year) to build your predictive model for temperatures.

. Find the stations that have all three features for the 70-year period.
+
.. Define the query:
+
[source,python]
----
sfQuery = """SELECT t1.STATION_ID
            FROM
            (SELECT STATION_ID FROM STATION_DATA_INVENTORY WHERE ELEMENT = 'PRCP' AND FIRST_YEAR < 1951 AND LAST_YEAR >2021) t1,
            (SELECT STATION_ID FROM STATION_DATA_INVENTORY WHERE ELEMENT = 'TMIN' AND FIRST_YEAR < 1951 AND LAST_YEAR >2021) t2,
            (SELECT STATION_ID FROM STATION_DATA_INVENTORY WHERE ELEMENT = 'TMAX' AND FIRST_YEAR < 1951 AND LAST_YEAR >2021) t3
            WHERE t1.station_id = t2.STATION_ID AND t1.station_ID = t3.STATION_ID ORDER BY station_id ASC"""
----
+
In the query, you are asking Snowflake to find the stations that have the features of interest:
+
* Firstly, it filters the data for all stations that have the `PRCP` feature by using a subquery.
The results from this subquery are stored as `t1`.
* The second subquery, for which results are stored as `t2`, filters for stations with the `TMIN` feature.
* The third subquery, with results stored as `t3`, contains stations with the `TMAX` feature.
* The main query filters the results to only include `STATION_ID` values that are common to results from all three subqueries (by joining the three subqueries on `STATION_ID`).
+
.. Next, run the query:
+
[source,python]
----
res = ds.query(sfQuery)
df_station_with_data = res.to_pandas()
df_station_with_data.head()
----
+
The query result:
+
....
    STATION_ID
----------------
0   AU000005901
1   AU000006306
2   AU000011801
3   AU000016402
4   BE000006447
....

. Use the first station on the list:
+
[source,python]
----
station_to_check = 'AU000005901'
----

. Find where this station is located:
+
[source,python]
----
sfQuery = f"""SELECT *
            FROM WEATHER_STATION WS, COUNTRY C
            WHERE WS.STATION_ID = '{station_to_check}'
            AND C.COUNTRY_ID = SUBSTRING ('{station_to_check}', 1, 2)"""

res = ds.query(sfQuery)
station_data_df = res.to_pandas()
station_data_df.head()
----
+
You will see the following:
+
....
    STATION_ID   LATITUDE  LONGITUDE  ELEVATION  STATION_STATE  STATION_NAME  GSN  HCN_CRN  WMO_ID  COUNTRY_ID  COUNTRY_NAME
-----------------------------------------------------------------------------------------------------------------------------
0   AU000005901  48.2331   16.35      199.0                     WIEN          GSN           11035   AU          Austria
....
+
You are joining two tables: `WEATHER_STATION` and `COUNTRY`.
The `COUNTRY` table uses two characters for its `COUNTRY_ID` field, which are the first two characters in the `STATION_ID`.
You can therefore join the two tables on that field to get the full country name - `Austria` in this case.

. Have a look at the first few lines of this data:
+
[source,python]
----
sfQuery = f"""SELECT *
            FROM STATION_DATA_INVENTORY
            WHERE STATION_ID = '{station_to_check}'"""

res = ds.query(sfQuery)
df = res.to_pandas()
df.head()
----
+
The output:
+
....
    ENTRY_ID  STATION_ID    LATITUDE  LONGITUDE  ELEMENT  FIRST_YEAR  LAST_YEAR
----------------------------------------------------------------------------------
0   1044485   AU000005901   48.2331   16.35      TMAX     1855        2022
1   1044486   AU000005901   48.2331   16.35      TMIN     1855        2022
2   1044487   AU000005901   48.2331   16.35      PRCP     1901        2022
3   1044488   AU000005901   48.2331   16.35      SNWD     1916        2022
4   1044489   AU000005901   48.2331   16.35      TAVG     1952        2022
....
+
The results confirm that the features you need are indeed provided by the stations in the list within the `station_to_check_df` dataframe.

. Determine how many stations match this criteria:
+
[source,python]
----
len(df_station_with_data)
----
+
This result is:
+
....
250
....
+
Therefore, there are 250 stations in the dataset you can use to build a model.

. Finally, dive into the actual weather data in the `STATION_DATA` table.
Find how much data the 250 stations have that meet your criteria.
+
.. Define the query:
+
[source,python]
----
sfQuery = """SELECT COUNT(*)
            FROM STATION_DATA
            WHERE STATION_ID IN
            (SELECT t1.STATION_ID
                FROM
                    (SELECT STATION_ID FROM STATION_DATA_INVENTORY WHERE ELEMENT = 'PRCP' AND
                    FIRST_YEAR < 1951 AND LAST_YEAR >2021) t1,
                    (SELECT STATION_ID FROM STATION_DATA_INVENTORY WHERE ELEMENT = 'TMIN' AND
                    FIRST_YEAR < 1951 AND LAST_YEAR >2021) t2,
                    (SELECT STATION_ID FROM STATION_DATA_INVENTORY WHERE ELEMENT = 'TMAX' AND
                    FIRST_YEAR < 1951 AND LAST_YEAR >2021) t3
                WHERE t1.station_id = t2.STATION_ID AND t1.station_ID = t3.STATION_ID)
            AND DATA_DATE > to_date('1949-12-31')
            ORDER BY STATION_ID ASC, DATA_DATE ASC"""
----
+
This query uses the previous query (stations that have the features from 1950 onward) as a subquery and only loads the data since the first day of 1950.
It orders the data by station ID and the dates of each reading.
+
.. Run the query:
+
[source,python]
----
res = ds.query(sfQuery)
data_df = res.to_pandas()
data_df
----
+
This will return:
+
....
    COUNT(*)
-------------
0   27003504
....
+
The result shows that there are about 27 million data rows related to your stations alone (this number may differ in your results as NOAA adds new data over time).
That's a pretty big number to hold in memory in a dataframe!

. You don't really need all that data in memory to develop your model.
You can start with one station instead.
Look at the station data table using a station that you chose at random:
+
[source,python]
----
sfQuery = f"""SELECT *
            FROM STATION_DATA
            WHERE STATION_ID = '{station_to_check}'
            AND DATA_DATE > to_date('1949-12-31')
            ORDER BY DATA_DATE ASC
            LIMIT 10"""

res = ds.query(sfQuery)
df = res.to_pandas()
df
----
+
The table result:
+
....
    ENTRY_ID  STATION_ID   ELEMENT  ELEMENT_VALUE
-------------------------------------------------
0   755       AU000005901  SNWD       0
1   752       AU000005901  TMAX     -11
2   753       AU000005901  TMIN     -49
3   754       AU000005901  PRCP       0
4   323815    AU000005901  PRCP      85
5   323813    AU000005901  TMAX      46
6   323814    AU000005901  TMIN     -76
7   323816    AU000005901  SNWD       0
8   648137    AU000005901  SNWD       0
9   648136    AU000005901  PRCP       4
....
// TODO This must be updated to also include the DATA_DATE column.
+
From the `STATION_DATA` table, we only need the following features:
+
* `ENTRY_ID` - The sequential unique ID for the data item (on the date the data was captured).
* `STATION_ID` - The weather station's unique identifier.
* `DATA_DATE` - The date on which the data was captured.
* `ELEMENT` - The data that was measured (precipitation or temperature).
* `ELEMENT_VALUE` - The measurement value.

Note that there are multiple rows of data per day for this station.
It will be easier to train your regression model if you associate the data with each day instead.
That way, each row will have the date, precipitation, and temperatures as features, instead of one feature per row.

== Wrangle the data

. Run a single query to obtain all the data, and then reshape it into a new, date-based dataframe.
Start by getting the data you need for the station:
+
[source,python]
----
# Get all data for the current station
sfQuery = f"""SELECT DATA_DATE, ELEMENT, ELEMENT_VALUE
            FROM STATION_DATA
            WHERE STATION_ID = '{station_to_check}'
            AND DATA_DATE > to_date('1949-12-31')
            AND (ELEMENT = 'PRCP' OR ELEMENT = 'TMIN' OR ELEMENT = 'TMAX')
            ORDER BY DATA_DATE ASC"""

res = ds.query(sfQuery)
station_data_full = res.to_pandas()
station_data_full.size
----
+
The result:
+
....
84159
....
+
Therefore, there are almost 85 000 records with multiple readings per day.
(Note that this number may differ in your results as NOAA adds new data over time.)

. Remove duplicates from the data, if any exist, and see how many rows are left:
+
[source,python]
----
dupes = station_data_full[station_data_full.duplicated()]

if len(dupes) > 0:
    station_data_full = station_data_full.drop_duplicates()

# See how many rows are left after we remove the duplicates
print(len(station_data_full))
----
+
The result:
+
....
81326
....
+
We removed close to 3 000 duplicates from the data.

. Now check for missing data:
+
[source,python]
----
# Date of last record in the dataframe
latest_date = station_data_full.iloc[-1]['DATA_DATE']

# See where the data is missing
station_data_full = station_data_full.set_index('DATA_DATE')
missing_dates = pd.date_range(start='1950-1-1', end=latest_date).difference(station_data_full.index)
print(missing_dates)
----
+
The result:
+
....
DatetimeIndex(['2022-04-19', '2022-12-15', '2024-05-01'], dtype='datetime64[ns]', freq=None)
....
+
(This may differ from your results as NOAA adds new data over time.)

. Looking at the data, how many dates are missing per year?
+
[source,python]
----
len(missing_dates)
----
+
Based on the output, `3` dates are missing some data.

. Iterate over this date list and add three new rows for each date that is missing - one for each feature (`TMIN`, `TMAX`, and `PRCP`).
+
[source,python]
----
# Add missing dates to the dataframe
element_list = ['PRCP', 'TMIN', 'TMAX']

for missing_date in missing_dates:
    cur_date = pd.to_datetime(missing_date).date()
    for cur_element in element_list:
        missing_row_test=station_data_full[(station_data_full['DATA_DATE'] == cur_date) & (station_data_full['ELEMENT'] == cur_element)]
        if len(missing_row_test) == 0:
            new_row = pd.DataFrame({'DATA_DATE': cur_date, 'ELEMENT': cur_element, 'ELEMENT_VALUE': np.NaN}, index=['DATA_DATE'])
            station_data_full = pd.concat([station_data_full, new_row], ignore_index=True)
----

. Check how well you did with one of the missing dates:
+
[source,python]
----
rows_for_date = station_data_full[station_data_full['DATA_DATE'] == date(2024, 5,1)]
print(rows_for_date)
----
+
The result:
+
....
        DATA_DATE   ELEMENT  ELEMENT_VALUE
------------------------------------------
81332   2024-05-01  PRCP     NaN
81333   2024-05-01  TMIN     NaN
81334   2024-05-01  TMAX     NaN
....
+
The dates that were missing are now included in the dataframe.

. Next, create a new dataframe that will be date-oriented, where each row will consist of a `DATA_DATE`, `TMAX` value, `TMIN` value, and `PRCP` value.
+
.. Create and populate individual dataframes for each feature, starting with `TMAX`:
+
[source,python]
----
tmax_df = station_data_full[station_data_full['ELEMENT'] == 'TMAX']
tmax_df = tmax_df[["DATA_DATE", "ELEMENT_VALUE"]]
tmax_df = tmax_df.rename(columns={"ELEMENT_VALUE": "TMAX"})
tmax_df.head()
----
+
The result:
+
....
    DATA_DATE   TMAX
----------------------
2   1950-01-01  -11.0
4   1950-01-02   46.0
8   1950-01-03   46.0
11  1950-01-04   37.0
14  1950-01-05    4.0
....
+
Then for `TMIN`:
+
[source,python]
----
tmin_df = station_data_full[station_data_full['ELEMENT'] == 'TMIN']
tmin_df = tmin_df[["DATA_DATE", "ELEMENT_VALUE"]]
tmin_df = tmin_df.rename(columns={"ELEMENT_VALUE": "TMIN"})
----
+
Then for `PRCP`:
+
[source,python]
----
prcp_df = station_data_full[station_data_full['ELEMENT'] == 'PRCP']
prcp_df = prcp_df[["DATA_DATE", "ELEMENT_VALUE"]]
prcp_df = prcp_df.rename(columns={"ELEMENT_VALUE": "PRCP"})
----
+
.. Join all three dataframes on the date:
+
[source,python]
----
station_data_merged = tmax_df.merge(tmin_df, on="DATA_DATE", how="left")
station_data_merged = station_data_merged.merge(prcp_df, on="DATA_DATE", how="left")
station_data_merged.head()
----
+
This will create a dataframe that looks something like this:
+
....
    DATA_DATE   TMAX  TMIN   PRCP
----------------------------------
0  1950-01-01  -11.0  -49.0   0.0
1  1950-01-02   46.0  -76.0  85.0
2  1950-01-03   46.0   17.0   4.0
3  1950-01-04   37.0    3.0   3.0
4  1950-01-05    4.0  -18.0  63.0
....

. If you look at the number of rows in this merged dataframe, you will see that the results correspond to 70+ years of data:
+
[source,python]
----
len(station_data_merged) / 365
----
+
The result:
+
....
75.11780821917809
....

. NOAA saves temperature data in tenths of degrees.
Therefore, you need to divide the temperature columns by 10.
+
[source,python]
----
station_data_merged['TMAX'] = station_data_merged['TMAX']/10;
station_data_merged['TMIN'] = station_data_merged['TMIN']/10;
station_data_merged.head()
----
+
The dataframe now looks like this:
+
....
    DATA_DATE  TMAX  TMIN  PRCP
--------------------------------
0  1950-01-01  -1.1  -4.9   0.0
1  1950-01-02   4.6  -7.6  85.0
2  1950-01-03   4.6   1.7   4.0
3  1950-01-04   3.7   0.3   3.0
4  1950-01-05   0.4  -1.8  63.0
....

== Impute the data

Use the `interpolate()` method from Pandas to fill the missing dates.

. First, ensure that the dataframe is ordered by date and that no duplicate rows exist:
+
[source,python]
----
station_data_merged = station_data_merged.sort_values(by=['DATA_DATE'])
station_data_merged = station_data_merged.drop_duplicates()
station_data_merged['DATA_DATE'] = pd.to_datetime(station_data_merged['DATA_DATE'])
----
+
Note that in the last statement, you converted the `DATA_DATE` column to be of type `datetime`.
That is important because you are going to interpolate data that is part of a time series, and Pandas requires the date to be the index in order to enable time-based interpolation.

. Convert the index to the date column:
+
[source,python]
----
station_data_merged = station_data_merged.set_index('DATA_DATE')
----

. Finally, interpolate the missing dates in the series:
+
[source,python]
----
station_data_merged['TMAX'] = station_data_merged['TMAX'].interpolate(method='time', limit_direction='backward')
station_data_merged['TMIN'] = station_data_merged['TMIN'].interpolate(method='time', limit_direction='backward')
station_data_merged['PRCP'] = station_data_merged['PRCP'].interpolate(method='time', limit_direction='backward')
----

. Check on one of the dates that was previously missing:
+
[source,python]
----
station_data_merged.loc['2024-05-01']
----
+
The above command returns the following result:
+
....
TMAX    2.370000
TMIN    1.240000
PRCP    6.666667
Name: 2024-05-01 00:00:00, dtype: float64
....
+
The missing values were filled through the interpolation.

== Make sense of the data

How many days can be regarded as hot weather days?
A good number to use is 90° Fahrenheit (33° Celsius).
Find how many days per year were considered hot days.

. Start by flagging days as _hot_ if they exceeded a temperature of 33° Celsius:
+
[source,python]
----
hot_day_df = station_data_merged.copy()
hot_day_df['hot_day'] = hot_day_df['TMAX'] > hot_day_flag
hot_day_df.head()
----
+
The flag is now added to the dataframe:
+
....
DATA_DATE   TMAX  TMIN  PRCP  hot_day
--------------------------------------
1950-01-01  -1.1  -4.9   0.0  False
1950-01-02   4.6  -7.6  85.0  False
1950-01-03   4.6   1.7   4.0  False
1950-01-04   3.7   0.3   3.0  False
1950-01-05   0.4  -1.8  63.0  False
....

. How many hot days were experienced during these 70 years?
+
[source,python]
----
len(hot_day_df[hot_day_df['hot_day'] == True])
----
+
It looks like this station had `231` hot days.
(Note that this number may differ as NOAA adds new data over time.)

. Find out how many hot days there were every year:
+
[source,python]
----
hot_day_df['DATA_DATE'] = pd.to_datetime(hot_day_df.index.date)

# Separate the year into its own dataframe column
hot_day_df['year'] = pd.DatetimeIndex(hot_day_df['DATA_DATE']).year
hot_day_summary = hot_day_df.groupby('year')['hot_day'].apply(lambda x: (x==True).sum()).reset_index(name='count')

# Show the top 10 years with the most hot days
hot_day_summary.head(10)
----
+
Here, you extract the year, from the index into its own dataframe column, and then apply a lambda function to count how many hot days were experienced in each year.
+
....
    year   count
----------------
0   1950   3
1   1951   0
2   1952   2
3   1953   0
4   1954   0
5   1955   0
6   1956   0
7   1957   7
8   1958   0
9   1959   0
....

. Now determine the hottest temperature in each year:
+
[source,python]
----
hottest_temp_in_year = hot_day_df.groupby('year')['TMAX'].max()
hottest_temp_in_year.head(10)
----
+
This returns the following output:
+
....
year
1950    32.0
1951    32.0
1952    35.7
1953    31.9
1954    32.1
1955    31.5
1956    28.8
1957    35.5
1958    29.6
1959    36.2
Name: TMAX, dtype: float64
....

== Visualize the data

. Merge the two dataframes you just created so that you can visualize the data:
+
[source,python]
----
hot_day_summary = pd.merge(hot_day_summary, hottest_temp_in_year, on="year")

# Sort data based on the number of hot days in each year
sorted_hot_day_summary = hot_day_summary.sort_values(by = ['count'], ascending=False)
sorted_hot_day_summary.head(10)
----
+
The result:
+
....
     year   count   TMAX
-------------------------
65   2015   25      37.1
67   2017   13      38.4
72   2022   12      36.3
53   2003   12      37.6
42   1992   12      36.4
63   2013   11      38.5
69   2019   11      37.0
62   2012   10      36.3
68   2018    9      35.2
73   2023    8      36.2
....

. Now visualize the data in a histogram.
You may notice extreme deviations in years where too much data is missing and it could not be interpolated properly.
+
[source,python]
----
ax = hot_day_summary.plot(y=['TMAX'], kind='line', color='red', marker='*', figsize=(15,15))
ax2 = hot_day_summary.plot(y=['count'], ax=ax, kind='bar', color='blue', secondary_y=True)
ax.set_xticks(hot_day_summary.index, hot_day_summary.year)
ax.set_xlabel('Year')
ax.set_ylabel('Hottest Day Temperature')
ax2.set_ylabel('Number of Hot Days')
----
+
image::/images/6.0/snowflake/hot-day-histogram.png[alt="Histogram of weather data", width=850, role=noshadow]

[[train-model]]
== Create a predictive model

Now that you have the data you need for a single station, you can create a predictive model by using the https://xgboost.readthedocs.io/en/stable/[XGBoost library^] and build the model using linear regression.

. The parts of each date are features you'll use for your model.
Therefore, first split the date, which is currently the index of the dataframe, into individual columns:
+
[source,python]
----
station_data_merged['day'] = station_data_merged.index.day
station_data_merged['month'] = station_data_merged.index.month
station_data_merged['year'] = station_data_merged.index.year
station_data_merged.head()
----
+
The output:
+
....
DATA_DATE   TMAX  TMIN  PRCP  day  month  year
-----------------------------------------------
1950-01-01  -1.1  -4.9   0.0   1    1     1950
1950-01-02   4.6  -7.6  85.0   2    1     1950
1950-01-03   4.6   1.7   4.0   3    1     1950
1950-01-04   3.7   0.3   3.0   4    1     1950
1950-01-05   0.4  -1.8  63.0   5    1     1950
....

. Create the following two dataframes:

* One dataframe containing the predictive features.
* One dataframe containing the target values (`TMAX`) that you want to predict.
+
[source,python]
----
X = station_data_merged[['TMIN', 'PRCP', 'day', 'month', 'year']]
Y = station_data_merged['TMAX']
----

. Split the dataframes into training and testing dataframes using https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html[Scikit-Learn's train_test_split method^].
+
[source,python]
----
X_train, X_test, Y_train, Y_test = sk.model_selection.train_test_split(X, Y, test_size = 0.3, random_state = 101)
----

. Now you can go ahead and use XGBoost's regression capability to create the predictive model:
+
[source,python]
----
regressor = xg.XGBRegressor(max_depth=5, learning_rate = 0.3, n_estimators=100, subsample = 0.75, booster='gbtree')
tmax_model = regressor.fit(X_train, Y_train)
prediction = tmax_model.predict(X_test)
----

. To see how well your model predicts:
.. Calculate the mean squared error:
+
[source,python]
----
mse = sk.metrics.mean_squared_error(Y_test, prediction)
print("MSE: %.2f" % mse)
----
+
The output:
+
....
MSE: 9.53
....
+
..Then calculate the coefficient of determination:
+
[source,python]
----
r2score = sk.metrics.r2_score(Y_test, prediction)
print("Coefficient of determination: %.2f" % r2score)
----
+
The output:
+
....
Coefficient of determination: 0.85
....

=== Predict tomorrow's weather for this location

To predict tomorrow's maximum temperature for this location, you will need to do the following:

* Create a dataframe to submit to the prediction algorithm.
* Get tomorrow's date and insert it into the `day`, `month`, and `year` columns.
* Get the median `TMIN` and `PRCP` from all days with the same date in your data sample and set those values for `TMIN` and `PRCP` in the dataframe.
* Run the prediction and get the predicted maximum temperature.

Here are the steps to achieve the above:

. List the inputs that your predictive model requires:
+
[source,python]
----
X_test.dtypes
----
+
The result:
+
....
TMIN     float64
PRCP     float64
day        int64
month      int64
year       int64
dtype: object
....
+
In other words, the model requires a dataframe with date components (`day`, `month`, `year`), minimum temperature (`TMIN`), and precipitation (`PRCP`) as inputs.

. Create a dataframe for tomorrow, maintaining column names and data types.
(Note that your current date and weather station will affect what the data will look like.)
+
[source,python]
----
tomorrow_df = pd.DataFrame({'TMIN': pd.Series(dtype='float64'),
                            'PRCP': pd.Series(dtype='float64'),
                            'day': pd.Series(dtype='int'),
                            'month': pd.Series(dtype='int'),
                            'year': pd.Series(dtype='int')})
tomorrow_date = datetime.now() + timedelta(1)
tomorrow_df.loc[0, 'day'] = tomorrow_date.day
tomorrow_df.loc[0, 'month'] = tomorrow_date.month
tomorrow_df.loc[0, 'year'] = tomorrow_date.year
tomorrow_df
----
+
Which will return:
+
....
   TMIN  PRCP  day   month  year
-----------------------------------
0  NaN   NaN   18.0  7.0    2024.0
....

. To figure out what values to use for `TMIN` and `PRCP` for tomorrow, find the median value across history for tomorrow's date (in this case, for January 23).
+
[source,python]
----
tomorrow_historical = station_data_merged[(station_data_merged['day'] == tomorrow_date.day) & (station_data_merged['month'] == tomorrow_date.month)]
tomorrow_historical
----
+
The result:
+
....
DATA_DATE   TMAX  TMIN  PRCP  day  month  year
------------------------------------------------
1950-07-18  26.5  16.8   0.0   18    7     1950
1951-07-18  24.1  16.0   0.0   18    7     1951
1952-07-18  26.3  14.6   0.0   18    7     1952
1953-07-18  32.2  17.9   9.0   18    7     1953
1954-07-18  23.0  15.9  34.0   18    7     1954
...         ...   ...   ...    ...   ...   ...
2019-07-18  26.7  16.1   2.0   18    7     2019
2020-07-18  20.0  13.2  93.0   18    7     2020
2021-07-18  26.7  18.0  40.0   18    7     2021
2022-07-18  29.7  13.3  40.5   18    7     2022
2023-07-18  31.4  22.6   0.0   18    7     2023

74 rows x 6 columns
....

. Calculate the median `TMIN` and `PRCP` for tomorrow's date:
+
[source,python]
----
tomorrow_df.loc[0, 'TMIN'] = tomorrow_historical['TMIN'].median()
tomorrow_df.loc[0, 'PRCP'] = tomorrow_historical['PRCP'].median()
tomorrow_df
----
+
The result:
+
....
   TMIN  PRCP  day   month  year
-----------------------------------
0  16.0   0.0  18.0  7.0    2024.0
....

. Now you can go ahead and run your prediction, then format the results to look human readable:
+
[source,python]
----
# Use the dataframe in the predict function
prediction = tmax_model.predict(tomorrow_df)

# Format the result
date_str = tomorrow_date.strftime("%A, %B %-d, %Y")
country_name = station_data_df.loc[0, 'COUNTRY_NAME'].strip()
station_name = station_data_df.loc[0, 'STATION_NAME'].strip()

result_output = f"The predicted weather for {station_name}"
result_output = result_output + f", {country_name} for tomorrow, {date_str}, is: {round(prediction[0],0)}\xb0c"
print(result_output)
----
+
The result for this location and date is:
+
....
The predicted weather for WIEN, Austria for tomorrow, Thursday, July 18, 2024, is: 29.0°
....

. Since you now have a trained model, you can save it for future use and avoid having to repeat the work you just did:
+
[source,python]
----
# Check if the output directory exists
if not os.path.exists(model_directory):
    os.mkdir(model_directory)

model_file_path = f"{model_directory}/{station_to_check}.json"
tmax_model.save_model(model_file_path)
----
+
You should now see a predictive model file for the location you picked in your JupyterLab session directory.

== Next steps

Convert the predictive model you created into a Domino-hosted link:02278a[Domino endpoint].

----- user_guide/get-started/get-started-snowflake/4-snowflake-weather-prediction-api.txt -----
:page-version: 6.1
:page-permalink: 02278a
:page-title: Domino endpoint using Snowflake data for weather prediction
:page-sidebar: Create a Domino endpoint
:page-order: 50
:source-highlighter: highlightjs

// TODO: Validate all steps against the new UI that will be implemented with the 6.0 release.

A link:f4e1e3[Domino Launcher] provides data scientists with a way to create a simple form to trigger the execution of a file with basic data entry.
You can use Python, R, MATLAB, or any language that can be called from the command line.
A Launcher allows you to define the following:

* The file to run: Domino expects the file to have a default function that will trigger the execution.
* A parameter list in a variety of data types, including:
** Text
** Drop-down list of options you can define, including multiple selection
** File upload
** Date selection
** Checkbox

Domino Launchers will trigger the file execution as a link:942549[Domino Job].
Like any Domino Job, the execution can perform data operations and output results as files, if relevant.

In this tutorial, you will create a simple launcher that triggers the weather prediction for a single weather station.

== Enable Snowflake access

Since your model uses a Domino Data Source to connect to Snowflake, you first need to set up permissions for your launcher to use it.

[[launcher-api-key]]
=== Obtain the API Key

Follow the steps on the link:d982cc[Get API key] page to obtain your Domino API key.
// TODO: Confirm that this linked page is correct and that is is okay to link this info instead of providing the steps here as well.

If you do not yet have an API key that you can copy, you can generate one.

=== Store the API key as part of your project

To avoid including your API key in clear text as part of your code, store it as an environment variable in Domino.

. From your project navigation menu, click on *Settings*.

. In the *Project settings* window, find the *Environment variables* area.
This is where you will store the API key.

. In the *Name* field, provide a variable a name, such as `launcher_api_key`, and then paste the API key into the *Value* field.

. Click *Set Variable*.
Domino will store the value securely for future use.

. Save a copy of this key for later in the tutorial or return to this screen later to retrieve the key.

== Write the Domino Launcher executable

[[launcher-setup]]
=== Setup

. link:e6e601[Launch a Domino Workspace] with JupyterLab.
// TODO: Confirm that this linked page is correct and that all the info on that page is relevant.

. Once the workspace is launched, select `Python File` from the `Other` group in the Jupyterlab Launcher.

. When the file opens, save it as `predict_location.py`.

=== Write the code

The code you will write will be roughly based on the work you did before to link:ee0e3d#train-model[train your model].

. First import the necessary libraries:
+
[source,python]
----
from domino.data_sources import DataSourceClient
import pandas as pd
import xgboost as xg
import sklearn as sk
from datetime import date, timedelta, datetime
import numpy as np
import sys
import os
from pathlib import Path
----

. Next, retrieve the API key and connect to the data source:
+
[source,python]
----
custom_api_key = os.environ['launcher_api_key']

# Instantiate a client and fetch the datasource instance
ds = DataSourceClient(api_key=custom_api_key).get_datasource("NOAA_Weather")
----

. Structure the files as a collection of individual functions.
The first function, that will also act as the entry point and orchestrator, is `predict_weather`.
+
[source,python]
----
#------------------------------------------------------------------------
# Starts the function
# Arguments: The weather station ID that you want a prediction for.
#            Number of days in the future to check for (default 7).
# Returns: An array containing the weather predictions for the next week.
#------------------------------------------------------------------------
def predict_weather(station_to_check, days = 7):

    station_data_merged = prep_data(station_to_check)

    tmax_model = build_model(station_to_check, station_data_merged)
    results = predict(tmax_model, station_data_merged, days)

    # Save results
    write_results(station_to_check, results, days)

    return dict(enumerate(results))
----
+
The function takes two arguments, the station ID and number of days to forecast.
In the body of the function you prepare the data, build the model using the data, and finally return the forecast.

. The next function should look similar to the code you used previously:
+
[source,python]
----
#-----------------------------------------------------------------
# Get the data ready for analysis.
# Returns: Station data shaped with data elements in a single row.
#-----------------------------------------------------------------
def prep_data(station_to_check):
     # Get all station data
    sfQuery = f"""SELECT DATA_DATE,
                ELEMENT, ELEMENT_VALUE FROM STATION_DATA
                WHERE STATION_ID = '{station_to_check}'
                AND DATA_DATE > to_date('1949-12-31')
                AND (ELEMENT = 'PRCP' OR ELEMENT = 'TMIN' OR ELEMENT = 'TMAX')
                ORDER BY DATA_DATE ASC"""
    res = ds.query(sfQuery)
    station_data_full = res.to_pandas()

    # Remove duplicates
    station_data_full = station_data_full.drop_duplicates()

    # Check if no station was found with the requseted ID
    if station_data_full.empty:
        print("ERROR: The requested station does not exist", file=sys.stderr)
        sys.exit()

    # Add missing dates to the dataframe
    latest_date = station_data_full.iloc[-1]['DATA_DATE']

    # See where the data is missing
    station_data_full_compare = station_data_full.set_index('DATA_DATE')
    missing_dates = pd.date_range(start='1950-1-1', end=latest_date).difference(station_data_full_compare.index)
    print(missing_dates)

    # Add missing dates to the data frame
    element_list = ['PRCP', 'TMIN', 'TMAX']
    for missing_date in missing_dates:
        cur_date = pd.to_datetime(missing_date).date()
        for cur_element in element_list:
            missing_row_test=station_data_full[(station_data_full['DATA_DATE'] == cur_date) & (station_data_full['ELEMENT'] == cur_element)]
            if len(missing_row_test) == 0:
                new_row = pd.DataFrame({'DATA_DATE': cur_date, 'ELEMENT': cur_element, 'ELEMENT_VALUE': np.NaN}, index=['DATA_DATE'])
                station_data_full = pd.concat([station_data_full, new_row], ignore_index=True)

    # Reshape the data
    station_data_full = station_data_full.reset_index()
    tmax_df = station_data_full[station_data_full['ELEMENT'] == 'TMAX']
    tmax_df = tmax_df[["DATA_DATE", "ELEMENT_VALUE"]]
    tmax_df = tmax_df.rename(columns={"ELEMENT_VALUE": "TMAX"})
    tmin_df = station_data_full[station_data_full['ELEMENT'] == 'TMIN']
    tmin_df = tmin_df[["DATA_DATE", "ELEMENT_VALUE"]]
    tmin_df = tmin_df.rename(columns={"ELEMENT_VALUE": "TMIN"})

    prcp_df = station_data_full[station_data_full['ELEMENT'] == 'PRCP']
    prcp_df = prcp_df[["DATA_DATE", "ELEMENT_VALUE"]]
    prcp_df = prcp_df.rename(columns={"ELEMENT_VALUE": "PRCP"})

    station_data_merged = tmax_df.merge(tmin_df, on="DATA_DATE", how="left")
    station_data_merged = station_data_merged.merge(prcp_df, on="DATA_DATE", how="left")

    # Adjust the temperatures
    station_data_merged['TMAX'] = station_data_merged['TMAX']/10;
    station_data_merged['TMIN'] = station_data_merged['TMIN']/10;

    # Fill in the missing data
    station_data_merged["DATA_DATE"] = pd.to_datetime(station_data_merged["DATA_DATE"])
    station_data_merged = station_data_merged.sort_values(by=['DATA_DATE'])

    station_data_merged['TMAX'] = station_data_merged['TMAX'].interpolate(method='linear')
    station_data_merged['TMIN'] = station_data_merged['TMIN'].interpolate(method='linear')
    station_data_merged['PRCP'] = station_data_merged['PRCP'].interpolate(method='linear')

    # Adjust the dates for use in model
    station_data_merged['day'] = pd.DatetimeIndex(station_data_merged['DATA_DATE']).day
    station_data_merged['month'] = pd.DatetimeIndex(station_data_merged['DATA_DATE']).month
    station_data_merged['year'] = pd.DatetimeIndex(station_data_merged['DATA_DATE']).year
    return station_data_merged
----
+
The function returns a dataframe shaped for the model to be created and with missing data interpolated.

. The next function creates the predictive model:
+
[source,python]
----
#-----------------------------------------------------------------
# Build model for weather prediction.
# Arguments: Station ID, the reshaped station data.
# Returns: A trained model for the weather station.
#-----------------------------------------------------------------
def build_model(station_to_check, station_data_merged):
    print("Building model for station " + station_to_check)

    # Separate dataframes into training and testing
    X = station_data_merged[['TMIN', 'PRCP', 'day', 'month', 'year']]
    Y = station_data_merged['TMAX']
    X_train, X_test, Y_train, Y_test = sk.model_selection.train_test_split(X, Y, test_size = 0.3, random_state = 101)

    # Set up XGBoost and train model
    regressor = xg.XGBRegressor(max_depth=5, learning_rate = 0.3, n_estimators=100, subsample = 0.75, booster='gbtree')
    tmax_model = regressor.fit(X_train, Y_train)
    print(f"Model created for {station_to_check}\n");
    return tmax_model
----

. Now use the model to forecast the weather:
+
[source,python]
----
#-----------------------------------------------------------------
# Predict maximum temperature for the next week for the specified
# weather station.
# Arguments: the predictive model, the reshaped station data,
# days in the future to predict.
# Returns: Array containing prediction for the number of days.
#-----------------------------------------------------------------
def predict(tmax_model, station_data_merged, days):

    # Create dataframe to use as input to prediction model
    future_df = pd.DataFrame({'TMIN': pd.Series(dtype='float64'),
                            'PRCP': pd.Series(dtype='float64'),
                            'day': pd.Series(dtype='int'),
                            'month': pd.Series(dtype='int'),
                            'year': pd.Series(dtype='int')})
    # Populate with a week's worth of data
    for x in range (days):
        future_date = datetime.now() + timedelta(x+1)
        future_df.loc[x, 'day'] = future_date.day
        future_df.loc[x, 'month'] = future_date.month
        future_df.loc[x, 'year'] = future_date.year
        tomorrow_historical = station_data_merged[(station_data_merged['day'] == future_date.day) &
                                          (station_data_merged['month'] == future_date.month)]

        future_df.loc[x, 'TMIN'] = tomorrow_historical['TMIN'].median()
        future_df.loc[x, 'PRCP'] = tomorrow_historical['PRCP'].median()

    prediction = tmax_model.predict(future_df)

    # Convert prediction from NumPy float to regular float
    prediction2 = list()
    for x in range (days):
        prediction2.append(float(prediction[x]))

    return prediction2
----

. Finally, output the results into a file:
+
[source,python]
----
#-----------------------------------------------------------------
# Write the prediction to file.
# Argumetns: Station ID, prediction.
# Returns: Nothing.
#-----------------------------------------------------------------
def write_results(station_to_check, results, days):

    # Get the station's name
    sfQuery = f"""SELECT * FROM WEATHER_STATION WS, COUNTRY C
                   WHERE WS.STATION_ID = '{station_to_check}' AND C.COUNTRY_ID = SUBSTRING
                   ('{station_to_check}', 1, 2)"""
    res = ds.query(sfQuery)
    station_data_df = res.to_pandas()
    station_name = station_data_df['STATION_NAME'].iloc[0].strip()
    country_name = station_data_df['COUNTRY_NAME'].iloc[0].strip()

    # Current time and date
    datetime_str = datetime.today().strftime('%Y-%m-%d-%H%M%S')

    # Where to save the file
    path_to_file = f"/mnt/results/{station_to_check}-{datetime_str}.txt"

    # Compose string to write
    try:
        outfile = open(path_to_file, "w")

        outfile.write(f"Weather prediction for station {station_to_check} in {station_name}, {country_name}:\n")

        for x in range (days):
            future_date = datetime.now() + timedelta(x+1)
            cur_predict = results[x]
            date_str = future_date.strftime("%A, %B %-d, %Y")
            outfile.write(f"{date_str}: {round(cur_predict,0)}\xb0 \n")

        outfile.close()
    except:
        print("ERROR: Unable to output file", file=sys.stderr)
----
+
This will save the results to a file inside the Domino `/mnt/results` folder.
The file name will follow the format: `<station name>-<year>-<month>-<date>-<time>.txt`.

. Importantly, you need to add the block telling Python what the default function in the file is:
+
[source,python]
----
if __name__ == '__main__':
    station_to_check = sys.argv[1]
    days_to_check = int(sys.argv[2])
    predict_weather(station_to_check, days_to_check)
----

. If you want to test the file, open a command line tab in JupyterLab and enter the following:
+
[source,bash]
----
python3 -c "import predict_location; predict_location.predict_weather('GME00102396',7)"
----

You are now ready to set up the Launcher.

== Set up the Domino Launcher

As mentioned, the launcher needs to call a file in order to process user requests for a weather forecast.
The launcher form will accept two inputs:

* The station ID in a `text` format.
* The number of days to forecast the weather, as an integer between `1` and `7`.

Follow these steps to set up your launcher:

. From the project menu, click *Launchers*.

. In the screen that appears, click the *New Launcher* button.

. The configuration form will appear.
Complete the fields as follow:
* *Name* and *Description*: Give the launcher a descriptive name and describe its usage.
* *Environment* and *Hardware Tier*: Choose the same Domino Environment and hardware tier you used when you wrote and tested the launcher in your Domino Workspace.
* *Command to run*: To configure the Domino Launcher, specify the name of the Python file you created (i.e. `predict_location.py`).
Domino will search for the file in the project's root folder.
Since we have two arguments to the function, click the *Add Parameter* button twice.
You will see the form expanding and adding two new fields below.
* *parameter0*: The first parameter will be the station information.
In the *Default Value* field, add one of the station IDs you used, e.g. `AU000005901`, and in the *Description* field, add `station ID`.
* *parameter1*: Click on `parameter1`.
This parameter will be used to specify how many days you want a forecast for.
Pick `Select (drop-down menu)` as the *Type* (instead of `Text`).
The form will now change and ask you to enter a comma-separated list of allowable values.
Each value entered will become an option for the drop-down.
In the *Allowed Values* field, enter `1,2,3,4,5,6,7`.
Domino will use the literal value for each option in text form (e.g. a string `1` will be passed to the Python script as the value of the argument).
Add a *Description* for the parameter, e.g. `How many days should the launcher forecast?`.

. Finally, click the *Save and Preview* button.
Domino will save the launcher form and present you with a preview of the UI you just defined.

. If the form looks acceptable, click *Back to All Launchers*.
Alternatively, if you want to make changes to the form, click *Keep Editing*.

. The launcher screen will now show the launcher you just created.

. Click the *Run* button - the form will appear as a popup layer.

. In the popup window, you can modify the parameters if you like.
Once you are satisfied with the parameters, click *Run*.
Domino will do the following:
* Start a Kubernetes container using the hardware tier and environment you specified.
* Trigger Python to run the file you named and pass it the parameters you entered.

. Since the above is how Domino Jobs work, and since Launchers act like Domino Job triggers, the screen will change to the *Jobs* screen in your project.
You will see your job at the top of the Jobs list as the most recent execution, along with the job `STATUS`.
More information about this can be found on the link:b4cb04[Job states] page.
// TODO: Confirm that this linked page is correct.
+
[[results-view]]
. When you click on the job, a job detail overlay will appear:
.. The *Logs* tab in the overlay offers a variety of logs that will help you debug issues when they occur.
.. Now switch to the *Results* tab.
Domino will present links to the files created as an output of the Launcher's job execution.
You can also see a preview of the files if they are text-based.
Here you can see that your Launcher worked and the prediction results for tomorrow was output to a file.

. Click on the file link.
This will take you to a full-screen preview of the file that the job created.

You can now share your model with other people who have access to Domino.

== Next steps

* link:e3cca3[Use Snowflake's Python driver in Domino to build a data update service with a Domino Job].
* link:d2a397[Use a Domino endpoint to share your model with others who do not have access to Domino].

----- user_guide/get-started/get-started-snowflake/5-snowflake-weather-data-updates.txt -----
:page-version: 6.1
:page-permalink: e3cca3
:page-title: Create a Domino Job to automate weather data updates
:page-sidebar: Automate data updates
:page-order: 60
:source-highlighter: highlightjs

// TODO: Validate all steps against the new UI that will be implemented with the 6.0 release.

You already downloaded a sizeable collection of link:0927f7#data-files[historical weather data], but NOAA weather stations worldwide collect new data every day.
NOAA also reviews the data that stations collect and corrects mistakes.
Instead of downloading and storing the entire historical record every day, NOAA offers almost daily https://www.ncei.noaa.gov/pub/data/ghcn/daily/superghcnd/[diff archives^].

For example, the file `superghcnd_diff_20170123_to_20170124.tar.gz` contains updates and new data collected between January 23 and January 24, 2017.
Each `diff` file consists of three CSV files:

* `delete.csv` - records to delete from station data.
* `insert.csv` - new data collected at stations during the period associated with the file.
* `update.csv` - changes to station data.

These files contain rows of records similar to the table below where the columns are the station ID, observation date, observation type (maximum temperature, precipitation, etc.), observation value, comments, observation flags to denote additional metadata, and the observation time.

[cols="^3,^3,^2,^2,^1,^1,^2,^2"]
|===
|USC00219072 |20170120 |SNWD |152  | | |H |600
|USC00471287 |20170120 |SNWD |0    | | |H |730
|USC00241127 |20170120 |TMIN |-44  | | |H |1700
|RSM00031369 |20170120 |TMAX |-233 | | |S |1700
|===

In this part of the tutorial, you will create a script that handles this data update task.
The script will:

. Download the relevant `diff` files since it was last run.
. Update the database - deleting, updating, and inserting records.

While the script can run manually, it is much more reliable to pull the data from NOAA using an automated scheduled (`cron`) job.
Given that NOAA releases these files daily, we will schedule the script to execute daily.
Domino makes that part very simple.

== Domino Jobs

Domino Jobs execute scripts in your preferred programming language in a headless fashion, meaning that they run without a graphical user interface.
Each job runs using an individual container (or cluster of containers) that starts up, runs, and shuts down automatically.
The job will only consume resources during its execution.

You can use any of your existing link:f51038[Domino Environments] with your jobs, ensuring consistency and helping with reproducibility.
You can learn more about jobs in the link:942549[Domino documentation], but we will cover the basics here.

== Domino Data Source vs. Snowflake Python Connector

As you might imagine, the data update operation will need to perform diverse operations such as delete, update, and insert.
For this reason, you should switch to using the Snowflake Python Connector instead of the Domino Data Source you link:569d37[used previously].

While the Domino Data Source can handle virtually all operations, it's optimized for data exploration and lightweight read operations.
On the other hand, the Python Connector offers access to many of the highly optimized capabilities and workflows Snowflake offers, which you will be taking advantage of.

== Data processing with Snowflake

While Snowflake works great with SQL commands, it has especially fast data loading and staging capabilities.
Snowflake performs large-scale data operations rapidly using the following workflow:

* Uploads your CSV (or another delimited text format) data into your Snowflake staging area.
The staging area is a temporary storage space used for data prep and is part of your Snowflake cloud space.
* Loads your data from the staged CSV file into a temporary (or permanent) table using the `COPY INTO` Snowflake command.
* Performs database operations (e.g. `DELETE`, `UPDATE`, `INSERT`) using normal SQL notations.
The code below follows this pattern.

== The data update script

The script consists of a main function that orchestrates calls to individual functions responsible for specific operations.

The main function, `update_weather_data()`, calls these functions in the following order:

. `Initialize_connection` - obtains a database connection object using the credentials you stored securely in Domino.
. `Get_update_start_date` - checks the database to see what is the date of the latest weather records.
This is used to identify how many files you will need to download to stay up to date with NOAA's weather data.
Note that this is not a failsafe approach to ensuring no duplicate records exist.
. `Get_diff_file_urls` - reaches out to the NOAA website to find the files you need to download and process.
If none are found, the script will exit.
. `Perform_updates` - handles downloading the files, dispatching them to dedicated functions to delete (`do_delete`), update (`do_update`), and insert (`do_insert`) records, and finally clean up the downloads from your file system.
. A `main` block - as the job will only call the script by name and not specify a function.

=== Create the script

To get started, go ahead and create a Python file in a Domino Workspace, as link:02278a#launcher-setup[you have done for the Domino Launcher file].
Name the file `DataUpdaterJob.py` and save it into the project's root folder.

What follows is the code with highlights of notable items that you can add to the script:

==== The constants

[source,python]
----
import snowflake.connector
import os
import pandas as pd
import numpy as np
import requests
import shutil
import tarfile
import json

noaa_url = "https://www.ncei.noaa.gov/pub/data/ghcn/daily/superghcnd/"

# Make sure to point at a dataset directory as the file sizes are occasionally large
download_path = '/domino/datasets/local/Snowflake-Quickstart/downloads/'

# Save the countries you need for later operations
country_prefixes = ['AU', 'BE', 'FR', 'GM', 'IT', 'NL', 'PO', 'SP', 'SZ', 'UK']

# How many rows to process at once, to avoid running out of memory
chunksize = 10 ** 6

# The name you will use for the downloaded diff files
download_file_name = 'cur_diff.tar.gz'

# Columns used in your dataframe and temporary database table
column_names = ['station_id', 'data_date', 'element', 'element_value', 'mflag', 'qflag', 'sflag', 'obs_time', 'operation']
----

Note that the script limits the number of rows you read from the diff file to avoid memory problems.
It will perform multiple reads until the entire file is read.

==== The main function

[source,python]
----
def update_weather_data():
    try:
        conn = initialize_connection()
    except:
        print("ERROR: Failed to connect to database. Exiting.")
        return 1
    else:
        print("Connected successfully to Snowflake...")

        update_start_date = get_update_start_date(conn)
        diff_df = get_diff_file_urls(update_start_date)

        if len(diff_df) == 0:
            print("Nothing to update. Exiting.")
            return 0

        result_list = perform_updates(diff_df, conn)

        with open('dominostats.json', 'w') as f:
            f.write(json.dumps({"Deletes": result_list[0], "Updates": result_list[1], "Inserts": result_list[2]}))
    finally:
        conn.close()

    return
----

The `main` function orchestrates the script's execution.

* You try to connect to the database, call the functions that perform individual steps, and close the connection when the execution ends.
* The call to `perform_updates` returns an array containing the number of database records that were changed.
* Those numbers are used to populate Domino's job visualization capability—custom job metrics.
+
To help you visualize how your job executions performed over time, Domino provides a line chart to accompany the job execution listing view.
In this example, you control what the chart displays by saving a simple JSON file (`dominostats.json`) to the project's root folder.
You then track how many row delete, update, and insert operations the job performs in each run by saving a JSON object that maps the counts as values to the keys `Deletes`, `Updates`, and `Inserts`.
Domino then takes the file and uses the keys and values to populate the visualization for the job.
Domino collects the data over time to create the chart.

[NOTE]
====
* Besides the visualization, the Jobs results table also displays the custom information in dedicated *DELETES*, *INSERT*, and *UPDATES* columns.
* Visualization can also alert you to job malfunctions and you can use email notifications to help flag issues when they happen.
* To learn more about this Domino feature, see link:5b84c5[Customize Job result views].
====

==== Initialize the connection to Snowflake

[source,python]
----
#-----------------------------------------------------
# Returns: connection object
#-----------------------------------------------------

def initialize_connection():
    conn = snowflake.connector.connect(
        user = os.environ['username'],
        password = os.environ['password'],
        account = os.environ['accountname'],
        warehouse = os.environ['warehousename'],
        schema = os.environ['schemaname'],
        database = os.environ['dbname'])

    return conn
----

This function aims to obtain a connection to the database.
It pulls the credentials from the secure store in Domino and calls the Snowflake Python Connector's `connect` method.
It then returns the connection object for use in the rest of the script.
If the connection fails, the main function handles the exception and exits.

==== Find the latest date

[source,python]
----
#----------------------------------------------------------------
# Argument(s): The connection object
# Returns: The date of the newest data record in the database
#----------------------------------------------------------------

def get_update_start_date(conn):
    cursor = conn.cursor()
    # Get the most recent date for which we have data
    cursor.execute("SELECT DATA_DATE FROM STATION_DATA ORDER BY DATA_DATE DESC LIMIT 1;")

    df = cursor.fetch_pandas_all()
    cursor.close()
    last_update_date = df['DATA_DATE'][0]

    print(f"Most recent record in database is from {last_update_date}")

    return last_update_date
----

This function queries the database to find the date of the latest weather data available and returns it to the main function.
You need this date to perform the data update.

==== Get a list of the files to download

[source,python]
----
#------------------------------------------------------------------
# Argument(s): The date of the most recent record in the database
# Returns: Dataframe with list of files to read for the data update
#------------------------------------------------------------------

def get_diff_file_urls(last_update_date):
    print("Checking updates from NOAA...")
    page = pd.read_html(noaa_url)
    df = page[0]
    df['Last modified'] = pd.to_datetime(df['Last modified'])
    df = df.astype({'Name':'string'})
    # Keep diff files only
    diff_df = df[df['Name'].str.contains('diff')].sort_values(by=['Last modified'], ascending=False)

    # Limit to only the diff files that are relevant to you - files newer than your last update date
    diff_df=diff_df[diff_df['Last modified'].dt.date > last_update_date]

    # Sort the files so the oldest file is first
    diff_df = diff_df.sort_values('Last modified')

    # Remove files with gigabyte-sized updates (normally an NOAA error)
    diff_df = diff_df[diff_df['Size'].str.find('G') == -1]

    print(f"Need to retrieve {len(diff_df)} update files")
    return diff_df
----

Given the date of the latest data in the database, this function downloads the list of available files from the NOAA website.
It then filters down the dataframe containing the file list to the ones newer than your last update date.
It also excludes files that are larger than 1GB to avoid memory issues (due to your job hardware limitations - you can choose to provide higher-capacity hardware and storage space).

The main function will then check to see if the dataframe is empty.
If it is, the script will exit as no updates are available.

==== Perform delete, update, and insert operations

[source,python]
----
#------------------------------------------------------------------
# Argument(s):
# 1. Dataframe containing diff file URLs in oldest to newest order
# 2. Database connection object
# Returns: List containing number of updates
#------------------------------------------------------------------

def perform_updates(diff_df, conn):
    cursor = conn.cursor()

    delete_count = 0
    update_count = 0
    insert_count = 0

    # Iterate over all diff files
    for index, row in diff_df.iterrows():

        # Define the size of data chunks for us to read at a time
        chunksize = 10 ** 6
        oldest_file = getattr(row, 'Name')
        oldest_file_url = noaa_url + oldest_file

        print(f"Now dowloading {oldest_file}...")

        # Download the file
        response = requests.get(oldest_file_url, stream=True)
        if response.status_code == 200:
            with open(download_path+download_file_name, 'wb') as f:
                f.write(response.raw.read())
        else:
            print(f"Failed to download {oldest_file}. SKIPPING!")
            continue

        print("Done. Extracting...")

        # Extract the file
        file = tarfile.open(download_path+download_file_name)
        file.extractall(download_path)
        file.close()

        print("Done. Processing...")

        # Get the directory name by removing the .tar.gz
        dir_name = oldest_file.split('.', 1)[0]

        # Perform updates
        delete_count += do_delete(cursor, dir_name)
        update_count += do_update(cursor, dir_name)
        insert_count += do_insert(cursor, dir_name)

        # Clean up
        shutil.rmtree(download_path+dir_name)

    cursor.close()

    result_list = [delete_count, update_count, insert_count]

    return result_list
----

This function receives the database connection and the list of files as its arguments.
It uses the connection to obtain a Snowflake database https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-api#object-cursor[cursor object^].
The cursor will help you handle database changes as transactions.

Next, the function iterates over the file list - downloading the files, oldest to newest, one by one.
After downloading a file, it extracts it.
If the download fails, it will try to download the next file on the list.

Once extracted, it passes the file path and the cursor to each of the functions that handle delete, update, and insert operations.
Each function will return the number of rows it processed in the database.
When the operations conclude, it deletes the files and closes the cursor.

Finally, it creates an array to hold the number of processed rows and returns it to the main function.

Let's now have a look at each of the three database update execution functions that were used in the code above.
Since all three functions are similar, we will examine the first (delete) more thoroughly and then offer insights into the unique aspects of the others (update and insert).
Note that you can easily refactor these three functions - we chose to include this explicit approach for clarity.

*The `do_delete` function*

[source,python]
----
#------------------------------------------------------------------
# Argument(s):
# 1. Database connection cursor
# 2. File path
# Returns: Number of rows deleted
#------------------------------------------------------------------

def do_delete(cursor, dir_name):

    print(">>> Starting delete operation <<<")

    chunkcount = 0
    west_eu_df = pd.DataFrame(columns = column_names)

    for del_df in pd.read_csv(download_path+dir_name+'/delete.csv', names = column_names,
                         dtype={'station_id': 'str', 'data_date': 'str', 'element': 'str', 'element_value': 'int',
                                'mflag': 'str', 'qflag':'str', 'sflag':'str', 'obs_time': 'str'},
                         chunksize = chunksize):
        chunkcount = chunkcount + 1
        print(f"Reading chunk {chunkcount} of delete data")
        del_df['data_date'] = pd.to_datetime(del_df['data_date'], yearfirst=True)

        # Filter the file for Western European stations
        for cur_country in country_prefixes:
            country_df = del_df[del_df['station_id'].str.startswith(cur_country)]
            west_eu_df = pd.concat([west_eu_df, country_df], ignore_index=True)

    print(f"Read data in {chunkcount} chunks. Deleting {len(west_eu_df)} records")

    # Output file
    west_eu_df.to_csv(download_path+dir_name+'/diff-data.csv', index=False, sep="|")

    sfQuery = """CREATE OR REPLACE TEMPORARY TABLE TEMP_DIFF_DATA
                (station_id TEXT,
                data_date DATE,
                element TEXT,
                element_value INT,
                mflag char(1),
                qflag char(1),
                sflag char(1),
                obs_time TIME,
                operation char(1))"""
    cursor.execute(sfQuery)
    print("table created")
    sfQuery = 'PUT file://' + download_path+dir_name+'/diff-data.csv @%TEMP_DIFF_DATA'
    cursor.execute(sfQuery)
    print("file uploaded")
    cursor.execute("COPY INTO TEMP_DIFF_DATA file_format = (type = csv field_delimiter = '|' skip_header = 1)")
    print("data copied from file to table")

    cursor.execute("BEGIN TRANSACTION")

    sfQuery = """DELETE FROM STATION_DATA sd USING TEMP_DIFF_DATA tdd
                 WHERE sd.station_id = tdd.station_id
                 AND sd.element = tdd.element
                 AND sd.data_date = tdd.data_date""";
    try:
        print("Executing deletion...")
        cursor.execute(sfQuery)
    except:
        cursor.execute("ROLLBACK")
        print("Transaction failed")
        return 0
    else:
        cursor.execute("COMMIT")
        print("Delete transaction successful")

    os.remove(download_path+dir_name+'/diff-data.csv')

    return len(west_eu_df)
----

The `do_delete` function will read a chunk of rows from the `delete.csv` file at the specified location, dedicated to rows to delete from the database.
It will load the rows into a dataframe and remove rows with data about stations not in countries that are part of the West European region.
The resulting dataframe will be output into a CSV file.

Next, the function will create a temporary table in Snowflake and upload the CSV file you created to the Snowflake staging area.
It will then copy the data from the CSV into the temporary table.
The function will then create a transaction to contain the delete operation.
The deletion itself will be done using a SQL statement that compares files in your `station_data` table to the contents of the temporary table.
If the operation is successful, the function will commit the transaction; otherwise, it will roll it back.
The function will return the number of deleted rows.

*The `do_update` function*

[source,python]
----
#------------------------------------------------------------------
# Argument(s):
# 1. Database connection cursor
# 2. File path
# Returns: Number of rows updated
#------------------------------------------------------------------

def do_update(cursor, dir_name):

    print(">>> Starting update operation <<<")
    west_eu_df = pd.DataFrame(columns = column_names)
    chunkcount = 0

    with pd.read_csv(download_path+dir_name+'/update.csv', names = column_names,
                         dtype={'station_id': 'str', 'data_date': 'str', 'element': 'str', 'element_value': 'int',
                                'mflag': 'str', 'qflag':'str', 'sflag':'str', 'obs_time': 'str'},
                         chunksize = chunksize) as reader:

        for update_df in reader:

            chunkcount = chunkcount + 1
            update_df['data_date'] = pd.to_datetime(update_df['data_date'], yearfirst=True)

            for cur_country in country_prefixes:
                country_df = update_df[update_df['station_id'].str.startswith(cur_country)]
                west_eu_df = pd.concat([west_eu_df, country_df], ignore_index=True)


    print(f"Read data in {chunkcount} chunks. Updating {len(west_eu_df)} records")

    # Output file
    west_eu_df.to_csv(download_path+dir_name+'/diff-data.csv', index=False, sep="|")

    sfQuery = """CREATE OR REPLACE TEMPORARY TABLE TEMP_DIFF_DATA
                (station_id TEXT,
                data_date DATE,
                element TEXT,
                element_value INT,
                mflag char(1),
                qflag char(1),
                sflag char(1),
                obs_time TIME,
                operation char(1))"""
    cursor.execute(sfQuery)
    print("table created")
    sfQuery = 'PUT file://' + download_path+dir_name+'/diff-data.csv @%TEMP_DIFF_DATA'
    cursor.execute(sfQuery)
    print("file uploaded")
    cursor.execute("COPY INTO TEMP_DIFF_DATA file_format = (type = csv field_delimiter = '|' skip_header = 1)")
    print("data copied")

    cursor.execute("BEGIN TRANSACTION")

    sfQuery = """UPDATE STATION_DATA sd
                SET sd.element_value = tdd.element_value
                FROM TEMP_DIFF_DATA tdd
                 WHERE sd.station_id = tdd.station_id
                 AND sd.element = tdd.element
                 AND sd.data_date = tdd.data_date""";
    try:
        print("Executing update...")
        cursor.execute(sfQuery)
    except:
        cursor.execute("ROLLBACK")
        print("Transaction failed")
        return 0
    else:
        cursor.execute("COMMIT")
        print("Update transaction successful")

    os.remove(download_path+dir_name+'/diff-data.csv')

    return len(west_eu_df)
----

The `do_update` function is similar to the `do_delete` function, except for the second SQL query that contains a `SQL UPDATE` statement.

*The `do_insert` function*

[source,python]
----
#------------------------------------------------------------------
# Argument(s):
# 1. Database connection cursor
# 2. File path
# Returns: Number of rows inserted
#------------------------------------------------------------------

def do_insert(cursor, dir_name):

    print(">>> Starting insert operation <<<")

    west_eu_df = pd.DataFrame(columns = column_names)
    chunkcount = 0

    with pd.read_csv(download_path+dir_name+'/insert.csv', names = column_names,
                         dtype={'station_id': 'str', 'data_date': 'str', 'element': 'str', 'element_value': 'int',
                                'mflag': 'str', 'qflag':'str', 'sflag':'str', 'obs_time': 'str'},
                    chunksize = chunksize) as reader:

        for insert_df in reader:
            chunkcount = chunkcount + 1
            insert_df['data_date'] = pd.to_datetime(insert_df['data_date'], yearfirst=True)

            for cur_country in country_prefixes:
                country_df = insert_df[insert_df['station_id'].str.startswith(cur_country)]
                west_eu_df = pd.concat([west_eu_df, country_df], ignore_index=True)

    print(f"Read data in {chunkcount} chunks. Inserting {len(west_eu_df)} records")

    west_eu_df.to_csv(download_path+dir_name+'/diff-data.csv', index=False, sep="|")

    sfQuery = """CREATE OR REPLACE TEMPORARY TABLE TEMP_DIFF_DATA
                (station_id TEXT,
                data_date DATE,
                element TEXT,
                element_value INT,
                mflag char(1),
                qflag char(1),
                sflag char(1),
                obs_time TIME,
                operation char(1))"""
    cursor.execute(sfQuery)
    print("table created")
    sfQuery = 'PUT file://' + download_path+dir_name+'/diff-data.csv @%TEMP_DIFF_DATA'
    cursor.execute(sfQuery)
    print("file uploaded")
    cursor.execute("COPY INTO TEMP_DIFF_DATA file_format = (type = csv field_delimiter = '|' skip_header = 1)")
    print("data copied")

    cursor.execute("BEGIN TRANSACTION")

    sfQuery = """INSERT INTO STATION_DATA
                (station_id, data_date, element, element_value, mflag, qflag, sflag, obs_time)
                SELECT station_id, data_date, element, element_value, mflag, qflag, sflag, obs_time
                FROM TEMP_DIFF_DATA""";
    try:
        print("Executing insert...")
        cursor.execute(sfQuery)
    except:
        cursor.execute("ROLLBACK")
        print("Transaction failed")
        return 0
    else:
        cursor.execute("COMMIT")
        print("Transaction successful")

    return len(west_eu_df)
----

The `do_insert` function is similar to the `do_delete` function, except for the second SQL query that contains a `SQL INSERT` statement.

==== Call the main function

[source,python]
----
if __name__ == "__main__":
    update_weather_data()
----

The final two lines of the file ensure that the `main` function is called whenever the script is executed.

*Remember to save the script to your project's root folder and commit the changes to the Domino file system.*

=== Test the script

. To verify that the script runs successfully, open a *Terminal* from the Jupyter *Launcher*.

. In the terminal tab that will open, enter the following call:
+
[source,bash]
----
python -c "from DataUpdaterJob import *; update_weather_data()"
----

. Script execution may require up to several minutes and the messages on the screen will offer updates on its progress.

. Ideally, the script will run without an error, and you will see an output similar to that in the image below.
Note that the dates and values will differ from that in the image when you execute the script.
+
image::/images/6.0/snowflake/script-execution-terminal.png[alt="Results from executing the script in the terminal", role=noshadow]

== Domino Job setup

To set up your script as a job, follow these steps:

. Click *Jobs* in Domino's main navigation menu.

. Jobs can run once or on schedule, with automated setup and teardown.
That makes them especially valuable for periodical model training and when using high-cost hardware like GPUs.
To ensure your script successfully runs as a job, click the *Run* button.

. The job setup menu will open.
Now, fill out the form:
* Add a *Job Title* (e.g. `Weather Data Update`).
* Enter the *File Name* (`DataUpdaterJob.py`).
* Choose an execution *Environment* and *Hardware Tier*.

. Since you are not using a cluster for the job and will connect directly to the database, you can click the *Start* button.
Your job will start to execute and will appear in the list of *Active* jobs.

. Once the job execution is completed, it will appear in the *Completed* run tab.
Click on the job you have just run.

. You will see that the job was completed successfully and the number of updates, deletes and inserts that the script performed.
link:02278a#results-view[Like with the Domino Launcher], you can click on the job to see the detailed execution results view.

=== Schedule the Job

. To schedule the job to run, click the *Schedules* tab at the top of the screen.

. Click the *Schedule a Job* button.
A form will appear.
Similar to the previous job form, enter the details on the first step of the form.

. Click *Next*.
As you are not using a cluster for your job, click *Next* again.

. Now, set a schedule for the job and click *Next*.

. You can now add your email address so that Domino can notify you when the job completes successfully or fails.

. Optional: If this job re-trains a model, you can also use it to update the model used in a Domino endpoint.
You don't need that at this time.

. Click *Create*.
The job will now appear in the *Schedules* job listings where you can edit and update the job configuration in the future.

. Domino will trigger the execution at the scheduled time.
When Domino executes the job, it will send you a confirmation email.

== Next steps

link:d2a397[Use a Domino endpoint to share your model with others who do not have access to Domino].

----- user_guide/get-started/get-started-snowflake/6-domino-endpoint-share-model.txt -----
:page-version: 6.1
:page-permalink: d2a397
:page-title: Use a Domino endpoint to share your model
:page-sidebar: Share your model
:page-order: 70
:source-highlighter: highlightjs

// TODO: Validate all steps against the new UI that will be implemented with the 6.0 release.

Earlier in the tutorial, you saw how to link:ee0e3d#train-model[create a model] that uses linear regression to predict the weather for a single station.
You could then use Domino Launchers to link:02278a[share the model] with other people who have access to Domino.
However, we frequently need to share our models with other teams and applications.
Domino makes that easy through Domino endpoints.

link:8dbc91[Domino endpoints] wrap your models into a REST API.
Domino handles the hosting, access control, and even the upscaling of your model if it needs to handle large amounts of traffic.
As with all hosting, the server is persistent and will keep on running until you shut down the Domino endpoint.
Domino allows you to version and provide secure access to your APIs.
Domino will use Kubernetes containers to host the model, running it using the environment definition you control.
Domino endpoints can even run on remote Domino Data Planes.

As a REST API, your model becomes available to every programming language that supports that standard.
That includes virtually all programming languages across mobile, desktop and even embedded devices.

This document will explain how you can share your weather prediction model as an API — the best part being that you can reuse your `predict_location.py` launcher file.

== Domino endpoint setup

. To get started, click *Domino endpoints* in the Domino project menu bar then click the *Create Domino endpoint* button.

. On the screen that appears, add an *Endpoint Name* (e.g. `Weather Prediction Domino endpoint`), a *Description* and leave it associated with your current *Project*.
Click *Next*.

. On the next screen, below *The file containing the code to invoke (must be a Python or R file)*, click inside the text box.
A drop down will be displayed with the files Domino can use for the Domino endpoint.
Select `predict_location.py`.

. For *The function to invoke*, select `predict_weather`, which is the entrypoint function to the file.

. Choose the *Environment* and under *Deployment Target*, the hardware to use for the Domino endpoint.

. Click *Create Domino endpoint*.

. Domino will load the Domino endpoint overview screen, and will initially report a *Preparing to build* status, then move to a *Building* status, and finally to a *Running* status.

However, in its current state, the API will fail to launch, repeatedly switching between *Starting* and *Running*.

== Debug the Domino endpoint

To get an idea of what is not working:

. Click the *Versions* tab to switch to that view.

. On the screen that will appear, click *View All Instance Logs*.

. Now scroll down to see the log of the container that Domino started to host your model.
You will notice the following lines towards the bottom of the log (the time and server IP address will differ on your version):
+
....
Aug 2, 2024 3:02 PM -04:00  KeyError: 'launcher_api_key'  ip-10-12-48-186.us-west-2.compute.internal  model-66ad2a6a3273e40b723dc6f3-667b666ccc-xxr4b  model-66ad2a6a3273e40b723dc6f3
Aug 2, 2024 3:02 PM -04:00  unable to load app 0 (mountpoint='') (callable not found or import error)
....
+
This means that the API attempted to start looking for the API key `launcher_api` that you stored earlier as part of the project's environment variables.
Environment variables for Domino endpoints need to be set differently.

. Click the *Back* button in your web browser to return to the endpoint version overview screen.
In your Domino endpoint window, click on the *Settings* tab.

. Now, switch to the *Environment* view by clicking on it in the navigation menu.

. Click *Add Variable*.
Domino will give you the option to add a key-value pair for the environment variable.
For the *Name*, enter `launcher_api_key`.
For the *Value*, use the same API key as when you've link:02278a#launcher-api-key[set up the environment variable for your launcher].

. Finally, click *Save Variables*.

. Switch back to the *Versions* tab and then click the vertical dots in the *Actions* column.

. In the menu that will appear, click *Start Version*.
This time, the endpoint will find the missing environment variable and run, showing a *Running* status.

== Test the Domino endpoint

. Switch to the *Overview* tab.

. Scroll down to the area titled *Calling your endpoint*.

. In the box titled *Request*, enter the following text:
+
[source,js]
----
{
  "data": {
    "station_to_check": "FRE00104120",
    "days": 7
  }
}
----
+
This is a JSON snippet containing a call to your Domino endpoint.
It sends the necessary two arguments to the `predict_weather` method so that it can run.
You can either use the station ID in the code above or change it.

. Click the *Send* button below the *Request* box.
This will trigger the execution of your prediction script.

. The result will appear in the *Response* box.
For example:
+
[source,js]
----
{
  "model_time_in_ms": 2327,
  "release": {
    "harness_version": "0.1",
    "model_version": "66ad2a6a3273e40b723dc6f3",
    "model_version_number": 1
  },
  "request_id": "1M3TGHQURC2DBOFS",
  "result": {
    "0": 26.98301887512207,
    "1": 26.8093204498291,
    "2": 26.454402923583984,
    "3": 26.672496795654297,
    "4": 26.63031578063965,
    "5": 26.91615104675293,
    "6": 26.892757415771484
  },
  "timing": 2327.2523880004883
}
----
+
The actual prediction is in the `result` block of the above JSON snippet that the Domino endpoint returned.

. You can now share the Domino endpoint with your colleagues using the language-specific code snippets in the tabs next to the *Tester*.
For example, this is how a JavaScript application will call your Domino endpoint:
+
image::/images/6.0/snowflake/model-api-javascript-snippet.png[alt="JavaScript application calling the Domino endpoint", role=noshadow]

----- user_guide/get-started/get-started-snowflake/index.txt -----
:page-version: 6.1
:page-permalink: 62ad69
:page-title: Get started with Snowflake (and Python)
:page-sidebar: Get started with Snowflake
:page-order: 50

This tutorial will give you a chance to experience working with the Snowflake database and Domino.
You will follow a basic data collection, engineering, and loading workflow and then create a model in Python that uses the data in Snowflake.

Domino offers various methods to connect to Snowflake:

* Snowflake https://docs.snowflake.com/en/user-guide/snowsql[SnowSQL^].
* link:d4ef2b[Domino Data Source] using Snowflake.
* Snowflake https://docs.snowflake.com/en/developer-guide/python-connector/python-connector[Connector for Python^].
* Snowflake https://docs.snowflake.com/en/developer-guide/snowpark/python/index[Snowpark^].

== Overview

In this Get Started series, you'll learn how to work with Domino Data Stores to crush big data with the following workflow:

. Preliminaries – Data Engineering:
.. Find data.
.. Understand the data.
.. Get the data.
.. Wrangle data into a format usable for analysis.
. Analysis:
.. Look at the data – normally using a subset of the complete dataset.
.. Clean the data – deal with missing and errant data.
.. Identify the arguments that you believe matter for your prediction to work.
. Model development:
.. Try out several algorithms to determine which one produces the best results.
.. Save the training function.
. Model training:
.. Run the model training function on the complete dataset.
.. Collect the model.
.. Test again.

== Assumptions

* This tutorial is aimed at data science professionals familiar with JupyterLab, Jupyter Notebooks, and the Python language.
* The code is for illustration purposes. It is functional, tested, and offers a very basic view into the use of Domino with data in Snowflake.
* Domino offers multiple connectivity modes with Snowflake — primarily:
    ** Domino Data Sources - meant for read-oriented exploration.
    ** The Snowflake Python library - meant for full-featured database operations in Snowflake.
* Please use Domino's file sync functionality to store your file progress in the project's repository throughout the tutorial.

== Pre-requisites

* Familiarity with Domino Workspaces and link:0a8d11[Datasets].
* Access permissions (username, password, and authorization) to a Snowflake database.
* The name of your Snowflake warehouse, database, and schema.
* Domino permissions to set up a Snowflake Data Source (if applicable).
* Snowflake's SnowSQL command line tool for the data engineering and loading sections of this tutorial.
* Familiarity with the SQL language and https://pandas.pydata.org/[Pandas library^].

== Next steps

The tutorial is designed to be followed in a sequence:

. link:0927f7[Understand the data].
. link:054d0b[Data engineering - Prepare and load the data into Snowflake].
. link:569d37[Use Snowflake with a Domino Data Source - A simple connectivity example].
. link:ee0e3d[Feature exploration, data wrangling, and predictive weather model creation with the Snowflake Data Source].
. link:02278a[Create a weather prediction Launcher].
. link:e3cca3[Use Snowflake's Python driver in Domino: Build a data update service with a Domino Job].
. link:d2a397[Domino endpoint: Share your model with your organization].
. Snowflake Snowpark - Create a model in Domino and set it up as a Snowflake user-defined function https://youtu.be/xzMrtSKvRt8[(Video)^].

----- user_guide/get-started/index.txt -----
:page-version: 6.1
:page-title: Get started
:page-permalink: 08a636
:page-order: 20

link:9a69d9[Get started with Python]::
A tutorial that guides you through using the common model lifecycle in Domino in a Python environment.

link:c5ce58[Get started with R]::
A tutorial that guides you through using the common model lifecycle in Domino in an R environment.

link:d003a0[Get started with MATLAB]::
A tutorial that guides you through using the common model lifecycle in Domino in a MATLAB environment.

link:123ae8[Get started with AI Hub]::
The AI Hub lets you quickly build AI applications from curated, prebuilt, open-source solutions that are enterprise-ready. The AI Hub lets you and your team discover and reuse templates for several common ML use cases and industry-specific patterns.

link:62ad69[Get started with Snowflake]::
A tutorial that guides you through working with the Snowflake database and Domino.

----- user_guide/index.txt -----
:page-version: 6.1
:page-title: User guide
:page-permalink: 1a73c8

Domino is an open platform for data science that unifies various programming languages, integrated development environments (IDEs), data sources, and tools in one location. 

== Domino as a system of record
Domino provides a central hub for AI operations and knowledge across the enterprise, enabling best practices, cross-functional collaboration, faster innovation, and efficiency. This integration enhances the research, development, and deployment of data science and works well to:

* Unify teams, tools, data, and infrastructure; and democratize all AI. 
* Orchestrate model life cycles and industrialize AI from pilot to scale.
* Govern data, models, and processes to make AI responsible by default. 

Domino makes it easier to use your favorite tools by setting up a few basic computing environments with partner technologies. We build and test these environments and run security checks to ensure they are safe. 

== Domino delivers value for all teams

Domino integrates seamlessly with your existing stack, allowing access to various open-source and commercial tools. Domino compounds knowledge, serving as a reliable source of organizational information and enhancing team collaboration. 

* *Data Science teams*: Domino is designed and optimized for unique and complex data science workflows. It gives you self-service access to data, tools, and infrastructure. You can reuse and collaborate with other teams and the business, enforce best practices, and compound knowledge and efficiency.  
* *IT and DevOps teams*: Domino has a secure, managed infrastructure with minimal IT support burden. The smart controls and full visibility allow you to slash cloud costs, and you can run Domino on any cloud, on-prem, hybrid, or multi-cloud environment.
* *MLOps and Risk teams*: Domino allows you to track, review, and validate all models using robust processes. It also offers flexible deployment options within any environment and turnkey model monitoring with easy remediation. Automatic versioning of code, data, environments, and results and customizable templates for best practices make compliance easier to monitor.

image::/images/6.1/ecosystem-new.png[alt="Domino machine learning ecosystem", role=noshadow, width=1200]

== How do we do it?
Domino is an open system that provides self-service access to data and tools, enables the reuse of materials, and facilitates collaboration with other teams in your organization while enforcing best practices, enhancing knowledge, and improving efficiency.

* link:16d9c1[Data]: Domino connects to external data sources like databases, data warehouses, and data lakes. You can find a list of supported data sources under link:fbb41f[Data Source Connectors].
* link:d2ba79[LLMs]: Our deep learning models are trained on extensive datasets for language processing tasks. Based on their training data, they generate new text that mimics human language.
* link:a8e081[Software]: You can connect your Domino projects to link:40f92c[Jira], link:910370[GitHub], link:da707d[MLflow], or link:02ec6d[Sagemaker]. This enables seamless integration for your data science workflows and allows users to track progress on data science projects.
* link:08a636[Languages]: Domino allows data scientists to use their preferred languages and tools, such as Python, SAS, Matlab, and R.
* link:867b72[IDEs and Tools]: You can use IDEs and tools like Jupyter Notebook, JupyterLab, RStudio, VS Code, MATLAB, and SAS with Domino.
* link:bfa148[Packages and Libraries]: We support a range of packages and libraries, including open-source options such as Python, R, TensorFlow, PyTorch, and others.
* link:799193[Compute Environments]: You can add different environments to any Domino installation by visiting https://quay.io/[quay.io^]. To get started, download the image from the repository link. Our documentation has step-by-step instructions for setting up these environments.

== Domino framework

We seamlessly integrate your data, infrastructure, and frameworks by employing diverse strategies and techniques. Additionally, we focus on creating comprehensive frameworks tailored to your specific needs, ensuring that your existing IT ecosystem functions cohesively and efficiently. Through these methods, we aim to optimize your operations and streamline workflows for improved performance and productivity:

* link:f35c19[APIs]: Use Domino APIs to expand and facilitate innovation and efficiency.
* link:71635d[Applications]: Configure web applications to optimize scalability and performance for several popular frameworks.
* link:95520d[Edge]: Capture, process, store, and analyze data locally rather than fetching it from a distant server.
* link:06da1b[NetApp Volumes]: Share data more easily across projects by storing files on external NetApp-backed storage.
* link:f12554[External Data Volumes(EDVs)]: Volumes from network-attached storage systems that are mounted to the Domino system.
----- user_guide/jobs/create-and-run-jobs.txt -----
:page-version: 6.1
:page-permalink: af97b7
:page-title: Create and run Jobs
:page-sidebar: Run Jobs
:page-order: 10

Learn about the many flexible ways that you can launch Domino Jobs depending on your needs. Domino link:942549[Jobs] provide a structured approach to run link:a068c1[code files] organizing the necessary hardware, compute environment, data resources, and security protocols to create a highly reproducible execution environment.

== Run a Job in the UI

You can start a Job interactively from multiple locations in the Domino UI.

=== Jobs dashboard in a Project

To run a Job from the Job dashboard in your Project:

. In your project, navigate to the *Jobs* section.
The Jobs dashboard shows all Jobs for your Project, sorted into tabs by Job status.
+
NOTE: You can also use the Quick Action menu to create a Job. To do this, click the + next to Jobs in the navigation pane.
. Click *Run* to start a new Job.

. In the Start Run window, configure the Job.
. Enter the *File Name* of the script you want Domino to execute, followed by any arguments for the script.
. Choose the link:f51038[Compute Environment] for the Job and its specific version. By default, the Job will use the latest version of your environment.
//Does the environment have to match the one in which the code was created? I think so.
+
TIP: Domino recommends that you use the revision that was set as active by the Project owner. If you select another revision, you can see a *Not Recommended* warning.
+
NOTE: link:400957[Restricted Projects] may limit your choice of environments.
. If your administrator has enabled link:7be849[volume provisioning recommendations], you can select a volume size automatically recommended by Domino, instead of the default volume size.
+
NOTE: In a link:910370[Git-based Project], the first Job you launch always uses the volume size link:e6e601#workspace-volume-size[configured in your Project settings]. Subsequent Job launches receive volume provisioning recommendations.
. Click *Next* and select the link:9d16e5[Hardware Tier] to use for this Job.
+
In a link:c65074[Domino Nexus deployment], hardware tiers in the dropdown are grouped by link:95520d[data plane], such as `GCP` below.
The `Local` data plane corresponds to running the execution in the Domino control plane cluster.
+
image::/images/6.0/job-nexus.png[alt="Launch a Job and select a hardware tier", width=600, role=noshadow]

. If necessary, attach a compute cluster to your execution.
To learn more about clusters, see the following:
+
** link:user_guide/68faaa[Spark on Domino]
** link:user_guide/d13903[On-demand Ray]
** link:user_guide/747a51[On-demand Dask]
** link:user_guide/d60880[On-demand Open MPI]


+
. Domino shows you what data will be mounted in your execution based on the selected data plane.
+
TIP: If you want to use a data mount that is not available, note which data planes it is available in, then return to the first step in the modal to select a hardware tier in the corresponding data plane.

. You can also configure your Job to create new link:dbdbff[Dataset Snapshots] to mount upon execution completion. Created snapshots are shown under the details tab for a Job.

image::/images/6.0/job-dataset.png[alt="Data in selected data plane", width=900, role=noshadow]

[NOTE]
====
This feature requires `AllowDatasetSnapshotsOnExecutionCompletion` to be enabled. See the link:dbdbff[create a snapshot] guide for more details.
====



[[tr16]]
// Run a Job from the Files page of a Project

=== Project files page

You can start a Job directly from the Project
*Code*
page.

. Go to your Project > *Code* > Find the file you want to run. 


. Click the gear icon > *Run*.
. Go to the file you want to run and click the gear icon at the end of the row in the files list, then click *Run*.

. Use the window to configure the Run.
This window has additional options to set up recurring scheduled Jobs and select different hardware tiers.
+
Title:: In this field, enter a name for the Job.
This name will be the label for the Job in the Jobs dashboard.

Parameters::
If there are any arguments or parameters you'd like to pass to your script, enter them here.

Hardware tier::
This dropdown list lets you set the hardware tier used by the Job.

Schedule to repeat::
In this section, specify if you want the run to execute only once, or to repeat on a schedule.

Publish after complete::
Check this if you want Domino to republish a link:8dbc91[Domino endpoint] once the Job completes successfully.


=== Launcher UI

A link:f4e1e3[Launcher] is a web form that rests on top of a script that Domino can execute in a Job.
You can use a Launcher to pass arguments to your script from UI fields in a customized web form.


== Run a Job from the CLI

From your workstation, you can start Jobs through the link:30b067[Domino CLI].
You must first link:e21e55[install the Domino CLI] on your system.

After you have logged in and opened a Project, you can start Jobs with:

[source,bash]
----
`domino run <filename>`
----

Examples:

[source,bash]
----
`domino run calculateStats.r`
`domino run runModel.m model1 .05`
`domino run --wait runModel.m model1 .05`
`domino run --direct "pip freeze | grep pandas"`
----

=== Domino Python SDK
You can use the link:c5ef26[Domino Python SDK] to start and monitor Jobs in Domino either from a tool like a Jupyter Notebook or VS Code or from within another Domino Job to chain together multiple Jobs in a more complex workflow.  

. Write code to connect to Domino and start a Run using the following example:

[source,python]
----
import os

from domino import Domino

domino = Domino(
    "marks/quick-start",
    api_key = os.environ["DOMINO_USER_API_KEY"],
    host = os.environ["DOMINO_API_HOST"],
)

# Blocking: this will start the run and wait for the run to finish before returning the status of the run
domino_run = domino.runs_start_blocking(
    ["main.py", "a", "b", "9"], title = "Started from Python API example code"
)
print(domino_run)


# Non-blocking: this will start the run and print the Run ID and check the status of the run *once*
# domino_run = domino.runs_start(["main.py", "a", "b", "9"],
#                                title = "Started from Python API example code")
# print(domino_run)
# run_status = domino.runs_status(domino_run.get("runId"))
# print(run_status)
----

NOTE: When using the Python SDK from within a Domino Workspace or Job, the environment variables needed for authentication are automatically configured. If you would like to set up your workstation outside Domino, learn more about how to link:40b91f[programmatically access Domino] 

== Run a Job from the Domino API

You can use the Domino API to start a Job with a POST request.

For more information about how to send valid configuration data, see the link:8c929e#_startJob[Domino Platform API reference].

=== Python and R Domino API bindings

The Domino API is also available in language-specific wrappers.

Use the link:d9cc25[dominodatalab Python library] and link:ad1224[domino R package] to call the Domino API in your preferred language.


== Schedule a Job

Use Domino to link:5dce1f[schedule Jobs] in advance, and set them to execute on a regular schedule.
These can be useful when you have a data source that is updated regularly.

== Organize and manage 

Jobs
Data science is an experimental science - it is common for a Project to require many Jobs before you've achieved your goals. Domino allows you to tag and rename Jobs to help you and your team track and manage Project work history effectively.

=== Tag Jobs

Tagged Jobs facilitates better search and discoverability in the link:2e25bb[Jobs dashboard].

It's best practice to start tagging Jobs from the start of a Project. For your tagging strategy, think about groups of Jobs you and collaborators would like to view in the Jobs dashboard.

For example, tagging your LLM pre-training Jobs differently than the instruction fine-tuning Jobs might provide for better search and visualization.

. From the navigation pane, click *Jobs*.
. Select the Jobs you want to tag, then click the Tag icon.


=== Rename Jobs

Rename Jobs to clarify their purpose or highlight the importance of a particular Job.

. From the navigation pane, click *Jobs*.
. Click the *Edit* button next to the Job name.

=== Stop a Job

//tell why you would want to stop a Job
[[tr29]]
// Bulk Stop Jobs from the Jobs dashboard

. From the navigation pane, click *Jobs*.
. Go to the *Active* tab.
. Select the checkboxes for the Jobs to stop and click the Stop icon.


=== Re-run a Job

To re-run a Job:

. From the *Jobs* page, mark the check box of the Run that you want.
. Select either *Re-Run with Original Version* or *Re-Run with the Current Version*.
+
Original and Current are referencing all inputs to the Job _except_ the command input argument string.
See the table below that outlines the behavior of the two re-run modes in both Git-based projects (GBP) and Domino File System (DFS) projects.

==== Git-based projects

[cols="3,2,2,2,2,2",options="header"]
|===
|
|Command input arguments
|Code in primary Git repository
|Compute environment
|Artifacts in DFS
|Imported Git repositories

|Re-Run with Original Version
|Original
|Original
|Original
|Original
|Original

|Re-Run with Current Version
|Original
|Current
|Current
|Current
|Current
|===

==== Domino File System projects

[cols="3,2,2,2,2",options="header"]
|===
|
|Command input arguments
|Compute environment
|Files in DFS
|Imported Git repositories

|Re-Run with Original Version
|Original
|Original
|Original
|Original

|Re-Run with Current Version
|Original
|Current
|Current
|Current
|===

[NOTE]
====
* When working in a GBP project, re-running a Job using *Re-Run with Current Version* only works as expected on an _original Job_.
If you re-run a Job that was itself a re-run, the platform will not fetch the latest commit from the branch — it will simply use the commit ID from the re-run Job itself.
This is because re-runs are tied to a specific commit, not the branch state.

* In both DFS and GBP projects, the *Re-Run with Current Version* button will only fetch the latest Git commits for your main or imported repositories if the original Job was run from a branch (like the default or any named branch).
If the Job was originally run from a specific commit or tag, the re-run will not pull the latest Git commit.

* In link:0093e8[restricted Projects], Job re-runs use the current restricted revision of the environment. This revision may be different from the revision that was used to run the Job previously.
====

== Next steps

Learn how to link:5dce1f[schedule Jobs] and link:e1c37a[view Job results].

----- user_guide/jobs/create-custom-jobs.txt -----
:page-version: 6.1
:page-permalink: a068c1
:page-title: Create custom Job types
:page-order: 60

Learn about Domino's natively supported link:942549[Job] types and languages, as well as how to create custom Job types to support your specific needs.

Domino has native support for running a wide variety of different types of Jobs, both in a single container and using on-demand, auto-scaling clusters. Additionally, Domino lets you customize the type of programming languages and applications that can be used in Domino Jobs to achieve nearly any type of computational workload. This flexibility lets you do things like large-scale data preparation, distributed GPU deep learning model training, and more.

== Natively supported Job types
The following lists the supported file types and the commands that Domino executes when it runs the file type:

[cols="1a,^1a,3a",options="header"]
|===
|Language |Filetype/Keyword |Invocation
|Python |`py` |`python -u`
|R |`r` |`R --no-save --no-environ --no-site-file --no-restore --max-ppsize=100000`
|Spark |`--spark` |`python /spark-submit-wrapper.py`
|Markdown |`rmd`/`rhtml` |`R -e require('knitr'); knit('file_name')`
|Bash |`sh` |`bash file_name`
|Matlab |`m`/`mlx`/`mexa64` |`matlab –sd $(dirname file_name) -batch`
|Perl |`pl` |`perl file_name`
|Julia |`jl` |`julia file_name`
|Python notebook |`ipynb` |`ipython nbconvert -to notebook --execute file_name --output file_name`
|Fsharp |`fsx` |`fsharpi file_name`
|Sas |`sas` |`sas file_name`
|Stata |`do` |`stata -b do`
|===

[[tr2]]
//Run a Python (.py) file as a job
[[tr3]]
//Run a R (.r) file as a job
[[tr4]]
//Run a Spark (--spark) file as a job
[[tr5]]
//Run a Markdown (.rmd) file as a job
[[t13]]
//Run a Markdown (.rhtml) file as a job
[[tr6]]
//Run a Bash (.sh) file as a job
[[tr7]]
//Run a Matlab (.m) file as a job
[[tr8]]
//Run a Perl (.pl) file as a job
[[tr9]]
//Run a Julia (.jl) file as a job
[[tr10]]
//Run a Python notebook (.ipynb) file as a job
[[tr11]]
//Run a Fsharp (.fsx) file as a job
[[tr12]]
//Run a Sas (.sas) file as a job
[[tr14]]
//Run a Sas (.wp) file as a job




== Custom Job types
Domino supports executing nearly any type of user application code to achieve a wide variety of computational scenarios by link:5dd2c1[editing an Environment definition] to support your needs.

NOTE: This feature is disabled by default.
To use this feature, your administrator must set the link:71d6ad[Configuration records] key `com.cerebro.domino.executableTarget.enabled` to `true`.

=== What is a file handler?

When you run a Job, Domino automatically handles the execution of the file. For natively supported file types, Domino already has commands it runs based on the file extension. To execute a file extension that isn't natively supported, or to run a different command for a file extension that is in the list, you can use a file handler.

A file handler is a bash script with the name of the file extension (without the `.sh` extension), located in `/opt/domino/handlers/{extension}`.

=== File handler examples

The file handler lives in the Environment's file system, so the execution of the Job depends on the Environment in which you run the Job.

* The file handler `/opt/domino/handlers/jpeg` targets files with the `.jpeg` extension.
* The file handler `/opt/domino/handlers/py` overrides the default behavior for `.py` files.

[TIP]
====
Always include instructions to install the necessary software and dependencies to execute the file type.
For example, if you create a file handler to add support for Node.js, include a line to install `nodejs`:
[source,shell]
----
RUN apt-get -y install nodejs
----
====

=== Add a file handler through the Domino UI

To add a file handler through the Domino UI, link:5dd2c1[edit an Environment definition].

. Click *Environments* in the Domino sidebar.

. In the *Dockerfile instructions* field, create a directory for the custom file handler:
+
[source, shell]
----
RUN mkdir -p /opt/domino/bin/file-handlers
----

. Add the commands you want to execute when you invoke the file.
Save the custom commands to a file with the same name as the file extension you want to target:
+
[source, shell]
----
RUN echo "#!/bin/bash" >> /opt/domino/bin/file-handlers/<target-file-extension>
RUN echo "<Line 1 of bash script>" >> /opt/domino/bin/file-handlers/<target-file-extension>
RUN echo "<Line 2 of bash script>" >> /opt/domino/bin/file-handlers/<target-file-extension>
...
----

=== Add a file handler through the terminal

To add a file handler through the terminal, you must build a Docker image and upload it to an external repository.
You can use the Docker image to link:ed1c08[create an Environment] that contains your file handler.

. Create a directory for the custom file handler:
+
[source, shell]
----
mkdir -p /opt/domino/bin/file-handlers
----

. Add the commands you want to execute when you invoke the file.
Save the custom commands to a file with the same name as the file extension you want to target:
+
[source, shell]
----
RUN echo "#!/bin/bash" >> /opt/domino/bin/file-handlers/<target-file-extension>
RUN echo "<Line 1 of bash script>" >> /opt/domino/bin/file-handlers/<target-file-extension>
RUN echo "<Line 2 of bash script>" >> /opt/domino/bin/file-handlers/<target-file-extension>
...
----

. Save a copy of this file to a Docker image:
+
[source,shell]
----
FROM {DOCKER_BASE_IMAGE}
RUN mkdir -p /opt/domino/bin/file-handlers
COPY <target-file-extension> /opt/domino/bin/file-handlers/
----

. Build the Docker image and upload it to an external repository associated with your Project.
+
[source, shell]
----
docker build -t "repository/domino-env-with-executable-target" .
docker push "repository/domino-env-with-executable-target"
----

=== Share a custom file handler

When you add custom file handlers to an Environment, the file handlers are associated with that Environment's Docker image.
Each Environment has its own Docker image.
You can only have one Docker image per Environment.

If you must use your custom file handler in another Environment, you have several alternatives:

. If the file handler is in the Dockerfile instructions field, you can copy and paste the handler's commands into the same field in a different Environment.

. If the file handler is in a Docker image in an link:ed1c08[external repository]:
  .. Use the Docker image as a base to build Docker images for any Environment that needs your custom file handler.
  .. Upload the file handler itself to an external repository.
  You can reference this file from multiple Docker images.


== Next step

Learn about all the ways you can link:af97b7[launch a Job].

----- user_guide/jobs/customize-job-results.txt -----
:page-version: 6.1
:page-permalink: 5b84c5
:page-title: Customize Job result views
:page-sidebar: Customize results views
:page-order: 40

Domino offers a solution for capturing and visualizing Job metrics. While it's recommended to use Domino's MLflow implementation for this purpose, using Domino directly remains a popular choice. Users have the flexibility to log metrics in both places if they prefer.

. Click *Jobs* from the Project menu.
. Click the funnel icon above the table of Jobs to customize the columns in the Jobs dashboard.

== Add new columns to the Jobs dashboard with dominostats.json

To include metrics in the Jobs dashboard that Domino doesn't provide by default, you can define custom columns. This can be done by using the `dominostats.json` file, letting you tailor the dashboard to better understand and analyze your Job performance.

. Write a file named `dominostats.json` to the root of your project directory.
. Use keys in `dominostats.json` to identify the outputs you're interested in and add the corresponding values.
. If Domino detects that `dominostats.json` has been written to the Project root by a Job, it parses the values and shows them as columns on the link:942549[Jobs dashboard].

NOTE: Domino automatically deletes `dominostats.json` before each execution.

The following is sample R code that writes three key/value pairs to `dominostats.json`:

[[tr1]]
//Enable run diagnostic statistics for Jobs.

[source,r]
----
diagnostics = list("R^2" = 0.99, "p-value" = 0.05, "sse" = 10.49)
library(jsonlite)
fileConn<-file("dominostats.json")
writeLines(toJSON(diagnostics), fileConn)
close(fileConn)
----

The following is the same data written to `dominostats.json` in Python:

[source,python]
----
import json
with open('dominostats.json', 'w') as f:
    f.write(json.dumps({"R^2": 0.99, "p-value": 0.05, "sse": 10.49}))
----

The resulting `dominostats.json` from these code examples looks like this:

[source,json]
----
{
  "sse": 10.49,
  "R^2": 0.99,
  "p-value": 0.05
}
----

== Use .dominoresults and .dominoignore to render specific files in the Jobs dashboard

// Specify results files by name to be rendered in the Results UI

Use the `.dominoresults` and `.dominoignore` files to control which files Domino renders in the Jobs results view. This can be useful to limit your results if many files are changed in your execution, or to exclude unnecessary log files generated during the execution.

TIP: R language exceptions can be especially large, which can impact sync times. So you may want to exclude the R debug outputs, depending on your use cases.

=== Include or exclude single files in the results dashboard

To include or exclude only specific files, your `.dominoresults` or `.dominoignore` file must list the relative path to those, each on a new line.
The following example will exclude or include `histogram.pdf` and `output.txt` in the top-level directory of the Project.



[source,shell]
----
histogram.pdf
output.txt
----

=== Use patterns with wildcard characters

[[tr8]]

Domino can use wildcard patterns in `.dominoresults` and `.dominoignore`.
This way you can specify groups of files to show.
The following example limits the files to PDF documents in the top-level directory, all PNG files in a directory images and text files arbitrarily deep in sub-directories of the Project.


[source,shell]
----
*.pdf
results/*.png
**/*.txt
----

=== Configure Job result views

You can also configure link:71d6ad#_run_results[Run results in configuration records] to control things like the maximum number of files Domino renders for comparisons and diffs.

== View custom metrics as a time series over time

You can see how your custom keys change over time.

. Click *Jobs Timeline* to expand a line chart of `dominostats.json` values over time.
+
This chart shows all Jobs listed in the current dashboard.
To filter to a specific set of related Jobs, link:942549[tag] the Jobs to create a separate dashboard view.

The x-axis ticks on the timeline represent individual Jobs, and the y-axis represents the values for the statistics in those Jobs.

. Hold your cursor over the chart to see individual data points as tooltips.
. Click on a point to open the details for the Job that produced that value.
. Click and drag on the chart to zoom in.
. Click each stat in the legend to toggle its line on and off.

== Email notifications

By default, Domino notifies you by email about _failed_ Project executions.
You and your Project collaborators can also receive notifications about _successful_ executions.

// Run failure email is sent to project collaborators and owner

[[tr2]]
// Run success email is sent to project collaborators and owner

[[tr3]]
// Run success email is sent to project collaborators and owner with less than 10 automatically detected output files

[[tr4]]
// Run success email is sent to project collaborators and owner with more than 10 output files detected

On successful executions, you will receive an email with the last few lines of `stdout`, and up to ten results files from your execution.
Domino detects which files were added or changed during your execution, and captures those as the execution's results.

=== Edit notification preferences

. In the navigation pane, click *Projects*.
. Go to your Project and click  *Settings*.
. Click *Access and Sharing* and go to *Collaborations and permissions*.
. Set the *Notifications preference* for users.

== Send custom Emails

[[custom-notifications]]
[[tr5]]
// Run success email is sent that includes custom HTML content with images

You can create fully customizable Job email notifications with any formatting and results you want.

For link:ca786d[Domino File System (DFS) projects], create a file named `email.html` in the _root of your project folder_ as part of your run.

For link:910370[Git-based projects], create a file named `email.html` in the `/mnt/artifacts/` directory as part of your run.

The HTML in `email.html` can be used as the body and subject of the email sent on success.

=== Tips and tricks
* To include images, reference the path to the image from the root of the folder.
The image can be anywhere in your Project.
For example, to include an image in `plots/plot1.png`, write `<img src="plots/plot1.png">`.
* Put all CSS styles in inline `style` attributes.
Most email clients ignore `<style>` blocks in HTML emails.
* You can render a Jupyter Notebook to HTML and name it `emails.html` to send notebooks.
* Use tables for complex layouts.
Most email clients ignore CSS positioning.
[[tr6]]
// Run success email is sent that inlcudes customized subject
* Include the `<head>` and `<title>` tags at the start of the HTML file to customize the subject.
For example: `<head><title>Custom Subject</title></head>` creates an email with the subject "[Domino] Custom Subject".
[[tr7]]
// Run success email includes specific files
* To explicitly define the files that are sent with your success email, create a file named `.dominoresults` and write a filename per line in the `.dominoresults` file.
+
CAUTION: If you create a `.dominoresults` file, you must list `email.html` in this file to ensure that your custom email is used.
If you don't, the default results email will be generated.
* For Git-based projects, write Job results to the `/mnt/artifacts/results/` directory. This allows the files to be included as attachments to the email
and appear as results in the Job UI. See link:63ac71#save-artifacts[Save Artifacts to the Domino File System] for more info.

[#R]
--
.Example of an R script and the resulting email:
[source,r]
----
# generate and save plot
png("pressure.png")
plot(pressure)
dev.off()

# generate HTML in a string
html_string <- paste0("
<head>
  <title>",Sys.Date()," - Pressure report","</title>
</head>
<body>
  <h2>Exponential pressure growth! </h2>
  <h3>",Sys.time(),"</h3>
  <img src='pressure.png' />
  <p>Caption goes here</p>
</body>
")

# write string to file
write(html_string, "email.html")
----
--

[#Python]
--
.Example of a Python script:
[source,python]
----
import matplotlib.pyplot as plt
import numpy as np
x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8])
y = np.array([495, 620, 761.88, 899.8, 1039.93, 1270.63, 1589.04, 1851.31, 2154.92])
plt.axis([0, 8, 0, 2200])
plt.xlabel('year')
plt.ylabel('balance')
plt.plot(x, y)
plt.savefig('results/plot.png')
email_content = """
<head>
  <title>Latest projections</title>
</head>
<body>
  <h1>Latest projections</h1>
  <div>
      <img src="results/plot.png">
  </div>
</body>
"""
f = open('email.html', 'w')
f.write(email_content)
f.close()
----
--

----- user_guide/jobs/index.txt -----
:page-version: 6.1
:page-title: Work with Jobs
:page-permalink: 942549
:page-order: 230

Domino Jobs are designed to execute code logic in a fully reproducible manner. Whereas Domino link:867b72[Workspaces] are for interactive development, Jobs are for batch/headless workloads. Ideal when you need a fully contained and complete reproducibility package.

The result is a highly reproducible execution environment capable of handling a wide range of tasks including data processing, analytics, and machine learning. Jobs can handle single, simple tasks with intricate dependencies. Complex, multi-task workflows are possible, but link:78acf5[Domino Flows] is the recommended approach for such workflows, especially when they span different execution environments.

== Flexible Job run options

Domino offers numerous ways to link:af97b7[run Jobs]:

- *Launch from the UI*: Execute Jobs directly from the Jobs UI for a low-code option.

- *Job scheduler*: Use the link:5dce1f[Job scheduler] to easily run Jobs on-demand or at regular intervals.

- *Domino CLI*: Execute Jobs from the link:30b067[Domino CLI]. This is typically used for iterative local development. For example, when using an IDE that can't run in Domino, or you don't want to use Domino compute resources to launch a Job.

- *Domino API*: Run Jobs using the link:8c929e[Domino API]. Often used when integrating with an external pipeline tool to launch Jobs using triggers.

== Job reproducibility

Key information about each Job is automatically captured to ensure the reproducibility and auditability of work. When you start a Job, Domino launches a new link:f51038[Environment] for your code on the target hardware tier.

You can start multiple concurrent Jobs and each Job gets its own container environment, so you can try multiple parameters and techniques in parallel.

== Monitor Jobs and view results

link:e1c37a[Monitor Jobs] for a complete picture of your Jobs at various levels, from individual Job details to the broader view of how metrics evolve. Job monitoring supports reproducibility and auditability by capturing extensive Job context, including storage logs, hardware specifications, environment details, inputs, outputs, metadata, and custom metrics.

Quickly view your results directly in the Domino UI or link:5b84c5[customize the results views] including notifications, reports, and custom emails to display your results elsewhere.

== Jobs vs Workspaces

Depending on the specific needs and nature of your work, you may want to use link:867b72[Workspaces] instead of Jobs:

It's best to use a *Job* under the following circumstances:

* *Model training & large-scale computations* — If you need to train models on a large dataset or run any sort of long-running, computationally intensive tasks.

* *Batch processing* — If you have tasks you want to run in the background and/or in parallel.

* *Reproducibility* — If strict reproducibility is important, Jobs provide a guarantee that the same execution Environment can be re-created.

* *Automation* — If the task needs to be run on a regular schedule or is part of a retraining or automated workflow.

It's best to use a *Workspace* under the following circumstances:

* *Exploratory data analysis (EDA)* — If you are in the initial stages of a data science Project and need to explore and analyze your data interactively.

* *Rapid model iterations* — If you want to iterate rapidly on model development, hyperparameter tuning, or feature engineering and need rapid feedback with the ability to make instant adjustments.

* *Code development & debugging* — If you need an Environment to write, test, and modify code in real-time and see results immediately.

== Next steps

* Learn about the many flexible ways that you can link:af97b7[create and run Domino Jobs] depending on your needs.
* Learn how to link:5dce1f[schedule Domino Jobs] in advance and set them to execute on a regular cadence.
* Find out more about how you can link:e1c37a[monitor Jobs in Domino].
* Find out how to link:5b84c5[Customize Job result views] to capture and visualize Job metrics.
* Get more info about the link:b4cb04[states of a Domino Job] lifecycle.
* Learn how to link:a068c1[create custom Job types] to support your specific needs.
* Save money while training machine learning models using Domino hardware tiers based on link:2629d4[AWS Spot Instances].

----- user_guide/jobs/job-states.txt -----
:page-version: 6.1
:page-title: Job states
:page-permalink: b4cb04
:page-order: 50

When you start a link:942549[Job] in Domino, it moves through several states during its lifecycle.

* *Queued*:
The execution is waiting for a machine of your specified hardware tier to become available.
If one is available, it quickly leaves this state.
However, if no slots are available, it can take several minutes to start up a new machine.

* *Scheduled*:
The dispatcher requested an executor to process the execution and the executor acknowledges this request, but hasn't begun processing.
An execution only remains in this state for a few seconds.

* *Pending*:
A Run is placed in the pending state before it starts running in Kubernetes.

* *Preparing*:
Your Project files are being copied to the executor where your code will run.
The time this takes depends on the size of your data and the number of files in your Project.
Files are cached whenever possible.
Therefore, if you start an execution on a hardware tier that you used recently with the same Project, this can be quick, even for Projects with large files.

* *Building*:
If you are using a link:f51038[custom environment], you might have to wait for the Docker image to build.
This is cached whenever possible, so subsequent executions on the same hardware tier might skip this step.

* *Pulling*:
When your Docker image has been saved to a network-attached repository, this state indicates that Domino is fetching the image.

* *Running*:
Your code is executing.
You can view the console output and resource usage on the Jobs or Workspaces dashboard.

* *Finishing*:
Your execution has completed.
File changes are being copied back to the Domino file store.

* *Succeeded*:
Your execution has finished without error.

* *StopRequested*:
The system received the request to stop your execution.

* *Stopping*:
If you manually stop your execution, it enters this state while new or updated files are synced back to the Project.

* *Stopped*:
The execution has been manually stopped.

* *Failed*:
Your execution did not complete due to a problem with your code.

* *Error*:
Some problem outside your code caused the execution to terminate.
----- user_guide/jobs/monitor-jobs.txt -----
:page-version: 6.1
:page-permalink: e1c37a
:page-title: Monitor Jobs
:page-sidebar: Monitor and view results
:page-order: 30

Monitoring Jobs in Domino is comprehensive and flexible, giving you a complete picture of your Jobs at various levels, from individual Job details to the broader view of how metrics evolve over time. Domino's Job monitoring supports reproducibility and auditability by capturing extensive Job context, including storage logs, hardware specifications, environment details, inputs, outputs, metadata, and custom metrics.

The Jobs dashboard provides a way to view all Jobs in a given Project. Use it to monitor ongoing Jobs, view their results, and reproduce results.

image::/images/6.0/job-details.png[alt="Job details", width=1200, role=noshadow]


== Track and log Jobs with MLflow

It's a Domino best practice to use link:da707d[MLflow to log experiment metrics] and other key metadata.

Domino implements MLflow and hardens the MLflow experience by providing link:de48cc[role-based access controls (RBAC)] on top to limit access to sensitive information.


== View Job results and details

You can see the information about a Job and its results on the Jobs dashboard. Job results are any files created or modified during the Job run.

Domino stores versioned key Job components to ensure reproducibility.

[[tr24]]
// Jobs Dashboard shows details of a specific Job
. From the navigation pane, click *Jobs*.
Use *Filter table* entries to filter the list of Jobs.
. Select the row for a Job to open the Job details.
You can filter the table by Job title with the Search box.
+

[[tr25]]
// Jobs Dashboard shows Jobs up to 15 days old
+
[IMPORTANT]
====
Domino retains up to 15 days worth of usage details.
This applies to all executions (Jobs, Workspace sessions, and web Apps).
====
. Click the tabs to see details such as:
* *Logs*: Download the *Setup Output*, *User Output*, *Deployment Logs*, and *Support Bundle*.
Domino Support often requests these logs to review and resolve Jobs-related issues.

* *Results*: See the results for successful Jobs.
You can add comments to Job results.

== Compare Jobs

To create a link:08118a[Jobs comparison report], select two Jobs in the table, then click *Compare*.


If you are tracking link:5b84c5[execution performance], the comparison view shows the difference between your stats.

Domino snapshots the state of all the files in the Project before the Run starts (the inputs) and snapshots the Project after the Run completes (the outputs).
Any files that were added or modified between the input and outputs are considered results.

Domino highlights the lines in text files that are different. Other types of files are rendered side-by-side for visual comparison.

If Domino can't render a file, it provides simple metadata and links to download the files.

== Archive Jobs

[[tr28]]
// Bulk Archive Jobs from the Jobs dashboard

You can archive Jobs by selecting the checkbox next to the related job and clicking *Archive*.


== Next steps

- link:5b84c5[Customize Job results views and notifications].

----- user_guide/jobs/schedule-a-job.txt -----
:page-version: 6.1
:page-title: Schedule Jobs
:page-permalink: 5dce1f
:page-order: 20

You can schedule link:942549[Domino Jobs] in advance and set them to execute on a regular cadence.
These can be useful when you have a data source that is updated regularly.

[[tr1]]
// Create a new scheduled Job

. Go to a project.
. From the navigation pane, click
*Jobs* > *Schedules*.
. Click
*Schedule a Job*.
. In the Create a Scheduled Job page, define the Job:
+
--
Scheduled Job Name::
Enter the name of the Job.
The Jobs Dashboard lists each Job by this name.
Branch or Commit Id::
In a link:910370[Git-based project] you can specify a branch or commit ID when launching or editing a Job.
File Name::
Enter the name of the file to execute.
Include any optional arguments to pass to the file.
Hardware tier::
Select the hardware tier used by the Job.
Environment::
Select the compute environment used by the Job.
[[s_jobs_er]]
*Optional*: Select the Environment revision. By default, Domino uses the latest revision of your Environment when the Environment starts.

NOTE: Domino recommends using the revision set as the active one by the project owner. If you select a different revision, the *Not Recommended* warning is shown.

Data::
Click to expand the section to see the Datasets configuration used by the Job.

You can also configure your Job to create new Snapshots of Datasets it mounts upon execution completion. This feature requires `AllowDatasetSnapshotsOnExecutionCompletion` to be enabled. See the link:dbdbff[create a snapshot] guide for more details.
--
+
. Select and define the compute cluster.
+
[[tr2]]
// Create a scheduled Job with an attached compute cluster
The configuration options for each cluster type are explained here:

* link:f11f6a#spark_cluster_settings[Spark Cluster Settings]
* link:c50248#ray_cluster_settings[Ray Cluster Settings]
* link:aaa2c1#dask_cluster_settings[Dask Cluster Settings]
* link:594e3c#_create_an_mpi_cluster_with_jobs[MPI Cluster Settings]
+
. Set up the schedule.
+
[[tr3]]
//Create a scheduled Job that uses a custom Quartz CronTrigger

Use custom expression::
Enter a custom Quartz CronTrigger expression.
For example, if you want to run the Job on the 5th minute of every day, enter the following:
+
[source,bash]
----
`0 5 * ? * *`
----
+
NOTE: To learn more about these expressions, see http://www.quartz-scheduler.org/documentation/quartz-2.3.0/tutorials/crontrigger.html[CronTrigger expressions^].

Repeat every::
Set the frequency at which you want the Job to repeat.
Run sequentially::
When you set a Job to run sequentially, the scheduler always waits for the last Job it started to complete before starting the next one.
For example, if you set up a scheduled Job to run once per hour, and one of the Jobs launched by the scheduler takes 90 minutes to complete, the next hourly Job does not start until the previous one has finished.
Otherwise, multiple Jobs from this scheduler are allowed to run simultaneously.
The scheduler does not wait for the previous Job to finish if it's still running.
This mode should be used when your Job doesn't depend on output from the previous Job.
+
. Set up any additional actions:
+
Notify emails::
Enter a list of email addresses to notify when the Job is completed.
Update Domino endpoint::
If a Domino endpoint has been published from the project, the selected Domino endpoint will be republished after the Job has been completed.
Use this for retraining and updating a Domino endpoint regularly.
+
NOTE: In link:400957[restricted Projects], scheduled Runs use the current restricted revision of the Environment selected for the Job. This revision may be different for a scheduled Run from the revision used in a previously scheduled Run.

[[tr4]]
// Create a scheduled Job that runs sequentially
[[tr5]]
//Create a scheduled Job that sends email notifications when completed.
[[tr6]]
//Create a scheduled Job that will update the Domino endpoint


== Next steps

- link:e1c37a[Monitor and view Job results].
- Automate complex pipelines with link:e4f67f[Apache Airflow] or link:866fae[Kubeflow].

----- user_guide/jobs/spot-instances.txt -----
:page-version: 6.1
:page-permalink: 2629d4
:page-title: Run Jobs on Spot Instances (Preview)
:page-sidebar: Jobs on Spot Instances
:page-order: 70

Domino makes it easy to save money while training machine learning models using Domino hardware tiers based on link:https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html[AWS Spot Instances^]. Amazon Spot Instances give you access to unused AWS EC2 compute instances at a discount over their on-demand prices.

== Spot Instance interruptions

It's important to note that Spot Instances can be interrupted, causing Jobs to take longer to start or finish, or even fail during the Job if the Spot Instance is reclaimed. Spot Instances are best used for various fault-tolerant and flexible applications.

To mitigate the impact of losing progress of a model training Job, we recommend you use link:https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html[model checkpointing]. Domino automatically reloads the model checkpoints and mounts them to a local file when the Job is restarted, enabling the training Job to resume from the last checkpoint instead of restarting completely. 

If AWS interrupts a Spot Instance, the Job may fail to start or complete. If this happens, we suggest retrying the Job after some duration (more than 30 minutes). If the issue persists, the remediation is to change the hardware tier of the Job to use a non-spot node pool.

== Use model training checkpoints

Domino Jobs run in containers, so link:https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html[model checkpointing^] files are saved under a local directory of the containers (we recommend that you save checkpoints at `/mnt/artifacts/checkpoints/`). Domino synchronizes artifacts, including model checkpoints, to external storage. Existing checkpoints in S3 are mounted to the container at the start of the Job, enabling Jobs to resume from a checkpoint. Checkpoints added to the S3 folder after the Job has started are not copied to the training container. 

Use checkpoints to do the following:

- Save your model snapshots under training due to an unexpected interruption to the training Job or instance.

- Resume training the model in the future from a checkpoint.

- Analyze the model at intermediate stages of training.

- Use checkpoints with Spot Instances to save on training costs.
    
=== Checkpointing considerations

Consider the following when using checkpoints in Domino:

- To avoid overwrites in distributed training with multiple instances, you must manually configure the checkpoint file names and paths in your training script accounting for the rank of each worker node in your cluster.

- To control the checkpointing frequency, modify your training script using the framework's model save functions or checkpoint callbacks.

== Next steps

Learn how to link:3813fa[create a node pool with Spot Instances].

----- user_guide/mlops/index.txt -----
:page-version: 6.1
:page-title: MLOps
:page-permalink: e2bac4
:page-order: 80

Machine learning operations (MLOps) involve applying development operations (DevOps) principles to artificial intelligence (AI)-infused applications. It focuses on making AI systems reliable, scalable, and automated. This guide highlights how Domino uniquely supports MLOps to help organizations deliver AI solutions effectively.

== How Domino enables MLOps
MLOps is a systematic approach to integrating machine learning (ML) models into enterprise application development. It encompasses the deployment, monitoring, and maintenance of ML models in production environments. The goal of MLOps is to bridge the gap between the development of ML models and their operational deployment, enabling faster delivery and more robust solutions.
Domino's platform offers comprehensive support for MLOps through several key capabilities:

* *Seamless collaboration and version control*: Domino provides tools for seamless collaboration among data scientists, engineers, and business analysts. The platform supports version control for both code and data, ensuring experiments are reproducible and that teams can revert to or compare different versions effortlessly.

* *Scalable experimentation and model management*: Users can run multiple experiments simultaneously, leveraging Domino's scalable infrastructure. The platform allows for easy comparison of models and manages the lifecycle of each model from development to deployment.

* *Continuous monitoring and deployment*: Domino facilitates continuous monitoring of models in production, detecting issues like data drift or model degradation early on. The platform integrates with existing CI/CD pipelines, enabling continuous deployment and ensuring models are updated without downtime or loss of service quality.

* *Automated retraining and rollback*: Automated workflows in Domino can trigger retraining of models based on performance metrics or data changes, with options to roll back to previous versions if the new model underperforms. This ensures that the production environment remains stable and reliable.

* *Transparent and governable AI*: Domino's platform emphasizes transparency and governance, with features designed to explain model decisions and manage compliance with regulatory requirements. This is crucial for industries where understanding model behavior is as important as the model's predictions.

* *Customizable environments*: Users can create and manage customizable environments in Domino, which helps maintain consistency across the development and production stages. This reduces conflicts and dependencies, streamlining the transition from model development to deployment.

== MLOps best practices

Incorporate the following set of best practices to ensure efficient and effective management of MLOps:

* *Use version control for code, data, and experimentation outputs*: Ensure all components of your ML projects, including code, datasets, and models, are version-controlled. This approach guarantees reproducibility, facilitates collaboration, and saves time and resources that would otherwise be spent recreating experiments. Domino integrates robust version control systems that automatically track changes in code, data, and models. The platform provides a unified environment where all assets are versioned, helping teams to revert, compare, and build upon previous work seamlessly.

* *Use multiple environments*: Maintain separate environments for development, staging, and production to isolate changes and reduce the risk of introducing errors into production systems. Domino facilitates the creation and management of multiple Workspaces, each corresponding to different stages of the ML lifecycle. This separation enhances security and control, as access permissions can be managed per environment, ensuring that only authorized changes are deployed.

* *Manage your infrastructure and configurations as code*: Utilize Infrastructure as Code (IaC) to manage and provision computing resources, ensuring consistency across environments and reducing manual configuration errors. Domino supports IaC practices by allowing users to define and execute ML pipelines and environments programmatically. These definitions can be stored as code and versioned, enabling easy replication of environments and experiments.

* *Track and manage ML experiments*: Keep detailed records of experiments, including parameters, metrics, and outcomes, to analyze performance over time and replicate successful experiments. Domino offers comprehensive tracking features that automatically capture and store all experiment details, making it easy for teams to monitor progress and evaluate the impact of different variables on model performance.

* *Test code, validate data integrity, and ensure model quality*: Test your code regularly for data handling and feature extraction accuracy, and validate the integrity of your data to ensure high-quality model outputs. Domino integrates testing and validation tools directly into the workflow, allowing users to automate tests for data integrity and model accuracy as part of the experiment process.

* *Continuous integration and delivery for ML*: Implement continuous integration and delivery pipelines to automate testing and deployment of ML models, ensuring robustness and rapid iteration. Domino supports CI/CD for ML with automated pipelines that manage the entire lifecycle from data processing to model training and deployment. This automation ensures that only models that meet predefined quality thresholds are deployed to production.

* *Monitor services, models, and data*: It is best practice to continuously monitor deployed models and underlying data for performance, compliance, and operational health to detect and respond to issues such as data drift or model degradation. Domino provides monitoring tools that track the performance and health of both models and their data inputs in real-time. The platform can trigger alerts and automate retraining processes based on custom thresholds, helping maintain model accuracy and reliability.

== Domino best practices for MLOps

Domino supports the following best practices for MLOps:

=== People

* *Team collaboration and role flexibility*: Domino encourages the formation of project teams that leverage specialist and domain knowledge effectively. The platform facilitates role-based access controls that allow for granular permissions, ensuring that team members can fulfill multiple roles based on the project's needs.
* *Lifecycle and agile methodology*: Domino supports standardized project lifecycles, integrating agile methodologies to enhance responsiveness and flexibility. This helps teams to adapt quickly to new insights and iterate on ML models efficiently.
* *Balanced teams*: Domino's environment is conducive to supporting all stages of MLOps, including exploration, development, and operational deployment, allowing teams to function effectively across the full spectrum of tasks.

=== Process

* *Standardization of code and processes*: Domino promotes the use of standardized code templates, which accelerates project setup and ensures consistency across multiple projects. These templates can be used to quickly start new projects or to bring new team members up to speed.
* *Version control and experiment management*: The platform provides comprehensive version control for code, data, and models, ensuring reproducibility and traceability. Domino's experiment-tracking capabilities enhance collaboration and allow teams to monitor the progress and efficacy of their models over time.
* *Continuous integration and delivery (CI/CD)*: Domino integrates CI/CD practices into ML workflows, automating the testing and deployment of models. This ensures that models are robust and can be updated in production without downtime.
* *Strategic experimentation*: Domino encourages a strategic approach to managing experiments, using a champion/challenger model where new models are rigorously tested against baseline models to ensure they deliver expected improvements before full-scale deployment.

=== Technology

* *Automation and orchestration*: Domino automates various stages of the ML pipeline, from data processing to model training and deployment. This reduces manual effort and allows data scientists to focus on higher-value tasks.
* *Monitoring and retraining*: It is crucial to continuously monitor model performance in MLOps. Domino provides tools to monitor model health, performance, and drift, and can trigger retraining workflows automatically when certain thresholds are met.
* *Scalable infrastructure*: Domino offers scalable infrastructure options that can grow with your project needs. Whether deploying lightweight models or conducting massive-scale ML experiments, Domino ensures that your infrastructure can handle it efficiently.
* *Event-driven automation*: Domino supports event-based triggers for ML workflows, allowing teams to automate responses to model performance issues or operational anomalies, ensuring high availability and performance continuity.

=== Enhanced debugging and deployment

* *Testing and validation*: Domino allows for rigorous testing and integration of inference pipelines, which helps with debugging and validating model performance before deployment.
* *Staging environments*: The platform supports the use of staging environments where models can be tested in conditions that closely mimic the production environment, thereby minimizing deployment risks.

== Domino as an "AI factory" for MLOps

An AI factory refers to a system of standardized processes and artifacts designed to streamline the development and deployment of ML projects at scale. It allows an organization to manage multiple ML projects efficiently by optimizing team setups, recommending practices, architectural patterns, and reusable templates tailored to specific business requirements. The primary aim of an AI factory is to achieve high-quality, reliable, and maintainable ML solutions that can scale as the number of use cases increases, often from tens to thousands.

Domino provides a comprehensive platform that supports the establishment and operation of an AI factory by offering scalable solutions, enhanced collaboration tools, advanced monitoring and automation, and specialist support and integration. It provides project templates, version control, shared workspaces, role-based access control, automated workflows, and integration with enterprise systems to optimize ML operations.

=== Scalable and repeatable solutions

* *Project templates and architectural standards*: Domino offers a range of project templates and supports the development of custom templates that align with organizational standards. These templates ensure that new projects start with a proven baseline, reducing setup time and increasing the likelihood of success.
* *Version control and experiment management*: Domino's robust version control capabilities ensure that all aspects of an ML project, including data, code, and models, are tracked. This enables reproducibility and helps maintain a clear lineage of project artifacts, which is critical for scaling operations.

=== Enhanced collaboration tools

* *Shared workspaces and repositories*: Domino promotes collaboration by providing shared workspaces where teams can easily access and contribute to ongoing projects. This shared environment helps harness collective expertise and accelerates the iteration cycle.
* *Role-based access control*: Access controls in Domino are designed to ensure that team members have appropriate access to projects and tools, aligning with their roles and responsibilities. This helps maintain security and governance while promoting efficiency.

=== Advanced monitoring and automation

* *Automated workflows*: Domino automates key aspects of the ML workflow, including data processing, model training, and deployment. Automation not only speeds up the development process but also ensures consistency and reliability in operations.
* *Monitoring and retraining*: Continuous monitoring of deployed models is facilitated in Domino, with features that detect performance degradation or data drift. The platform can automatically trigger retraining processes, ensuring that models remain accurate and relevant over time.

=== Specialist support and integration

* *Support for ML engineering roles*: Domino is designed to support specialized roles, such as ML engineers, by providing tools that are essential to develop and maintain production-grade ML systems, including advanced pipeline management and deployment capabilities.
* *Integration with enterprise systems*: Domino seamlessly integrates with existing enterprise systems, allowing data to flow securely and efficiently between systems and enabling ML models to leverage and enrich the broader information technology ecosystem.

== Next steps

* link:20f423[MLOps vs Devops]
* link:d03252[Develop and deploy ML pipelines]
* link:1d334e[Review and approve models]
* link:67333d[Deploy to production]
* link:bd26e2[Version control and Git]
* link:2793b2[Orchestrate with CI/CD]
* link:916d88[Monitor and improve]

----- user_guide/mlops/mlops-deploy-production.txt -----
:page-version: 6.1
:page-title: Deploy to production
:page-permalink: 67333d
:page-order: 40

In the realm of machine learning (ML) and artificial intelligence (AI), deploying models to production is the final, crucial step in the lifecycle of model development. This process does not only involve the deployment itself but also rigorous testing in controlled environments and a systematic transition to production settings. This article explores the significance of deploying models to production, the methodology for testing them in a staging environment, and leveraging Domino's capabilities, including Model Sentry, to ensure these models operate effectively in production with a human-in-the-loop review and approval process.

== Bridge the gap between data science and business value

Model deployment is the phase where theoretical data science transitions into practical application, allowing businesses to start reaping the tangible benefits of their investments in AI and ML. Models, no matter how well they perform in a controlled test environment, only deliver value when they are integrated into real operational systems where they can influence decision-making and automate processes.

== Continuously monitor and improve

Deploying models to production environments allows them to interact with real, live data. It also allows for the gathering of feedback necessary for iterative improvement, ensuring the model stays relevant as market dynamics and data patterns evolve.

== Best practices to deploy models

Consider the following best practices when you deploy your models:

=== Test in a controlled environment

Before a model is fully deployed into production, it is essential to deploy it in a test or staging environment. This environment closely mirrors the production environment but does not affect the actual operational processes or workflows.

* *Validation*: In this stage, the model is tested against a set of predefined criteria to validate its accuracy, efficiency, and reliability.
* *Integration testing*: The model's integration with other systems and workflows is tested to ensure that there are no integration issues.
* *Load testing*: This tests the model's performance under various loads to ensure that it can handle expected transaction volumes.

== Transition to production with Domino

Using Domino's advanced ML lifecycle management capabilities, transitioning models from a staging to a production environment involves structured steps and checks powered by Domino Model Sentry. Model Sentry allows for the registration of models in the Model Registry, followed by a human-in-the-loop review and approval process, powered by Domino, before deployment to production. Finally, Domino Model Monitoring continuously tracks model performance to ensure they are effectively meeting your business goals.

=== Separate environments

It's important to have a controlled process to separate changes in a `Dev` or `Staging` environment before going to `Production`. This can help prevent unintended consequences. However, there are multiple ways to achieve this.

* *Separate data planes*: Domino supports a hybrid, multi-cloud architecture called Domino Nexus. Domino Nexus enables preparing data or training models in one data plane or region and deploying to another, or using many different data planes for training and deploying models. As a best practice, we recommend that you use separate data planes for `Dev`, `Staging`, and `Production` to ensure the isolation of resources and tight control over who can make changes in production for security reasons. To learn more, see link:95520d[Domino Nexus].

* *Node pool isolation*: Domino runs on Kubernetes-based infrastructure, both in the cloud and on-premise. If you'd like to have separate `Dev` and `Staging` environments, which are collocated in the same Kubernetes cluster, it is possible to create separate Kubernetes Node Pools to ensure complete isolation of the virtual machines and containers that get allocated. Therefore, you can create a node pool for `Dev` and a separate node pool for `Production`. To learn more, see link:3813fa[node pools in Domino].

== Model governance

Domino supports different aspects of model governance:

* *Model cards*: Domino uses model cards to track model lineage to create a system of records that helps you evaluate a model's accuracy, fairness, compliance, and auditability. Project owners can collaborate with multiple stakeholders on these cards, ensuring security with fine-grained access control. Use Domino's integrated model review process to ensure that production models meet organizational policies and regulations, and avoid unintended business impacts.
* *Review process*: Before a model is transitioned to production, it undergoes a review process where domain experts, data scientists, and stakeholders analyze its performance metrics and outcomes in the staging environment.
* *Approval process*: After thorough review, the model must be approved by authorized personnel. This step ensures accountability and adherence to organizational standards and regulatory requirements.
* *Gradual rollout*: Instead of a full-scale deployment, models are often rolled out gradually. This allows teams to monitor the impact of the deployment and make the necessary adjustments before full deployment.
* *Feedback loop*: Establish a feedback mechanism to continuously collect insights on the model's performance in the production environment. This data is crucial for ongoing improvement and future iterations of the model.

*Accountability*: Domino ensures clear role and responsibility assignment with its access control policy based on attributes and actions associated with model creation and management.

*Transparency and reproducibility*: Domino records the exact code, Environment, Workspace settings, datasets, and data sources for each experiment and associates them with the published model. This data is easily accessible in every model card to help you understand exactly what went into a model and how to reproduce it.

*Accuracy and monitoring*: Domino offers prebuilt and customizable environments to let users embed best-in-class explainability solutions to report model fairness and bias for every training job. Customizable model cards can present these reports, giving all collaborators visibility into critical metrics like feature importance, cohort impact, and bias evaluations. Additionally, registered models deployed to a Domino endpoint can take advantage of automatic model drift detection to ensure continuous accuracy.

== Integrated model review process

. *Initiate the review*: Project owners and collaborators can request a review from reviewers (fellow project members). The requester creates a note for the reviewers and sets the desired stage for the model, for example, `Staging` or `Production`.

. *Review notification*: Once initiated, reviewers receive a Domino notification informing them of the model review request. Reviewers can follow the link provided in the notification message that takes them to the model card details for the specific model version.
+
. *Review the model*: Reviewers examine the sections of the model card relevant to their role and responsibility. Reviewers can test the model via the embedded Domino endpoints, explore the experiment details, or even open a Workspace to dive deeper into the training context. The review workflow continues until all reviewers are satisfied and approve the model. Once all reviews are approved, the model automatically transitions to the target stage.

=== Override stage transitions

The Project owner can also choose to roll back a stage or transition to a new stage without approval. Domino logs stage transition overrides in the model card history. To override stage transitions:

. Go to the model card.
. Click on the current stage and select a new stage.


== Custom review stages

Domino supports custom stages for model versions. You can expand on the default `Staging` and `Production` states of MLflow, giving you the flexibility to mold the review process to your needs. You must be a Domino admin or a project owner to define custom stages.

For example, you could add model version stages like `Pre-Production` or `Staging-2` to add more granularity to your model reviews.


[NOTE]
====
- If you set new model version stages while a review is in progress, the current and target stages are kept intact until the review is completed.

- You can also select custom stages for manual stage transitions.
====

=== Global stages settings

Domino admins can modify global custom stages for all Projects in the organization.

To set global stages:

. Go to the *Admin Panel* > *Platform settings* > *Configuration records*.
. Add a *com.cerebro.domino.registeredmodels.stages* record with the desired set of comma-separated global custom stage values.
. Restart the services to apply the changes.

=== Project stage settings

A Project owner can override the global custom stages for a specific Project. The project-level custom stages apply to all model versions in the Project.

For example, when you start with the MLflow defaults (`Staging` and `Production`), an admin can override these system defaults with global custom stages (`Staging`, `Pre-Production`, and `Production`). A Project owner can override the global custom stages with project-level custom stages (`Staging-2`, `Pre-Production-2`, and `Production-2`).

To set Project stages:

. Go to *Project* > *Project Settings* > *Models*.
. Edit the *Custom model version stages*.

image::/images/5.8/manage-models/project-level-stages.png[width=1200, alt="Add, modify or order project-level custom stages"]

== Activity log

See a log of previous stage transitions, review requests, and review responses in the activity log. Find links to a model version's activity log in the *Reviewers* section of its model card or in the *Activity* column of the version history table.

image::/images/5.8/model-review-history.png[width=500, alt="See a model version's review activity"]

== Next steps

- link:bd26e2[Use versioning in MLOps].
- link:8dbc91[Deploy models as a REST endpoint hosted by Domino].
- link:ba503b[Export a model as a container to serve as a prediction endpoint in your own production environment].
- Export via one of our integrated solutions for link:02ec6d[SageMaker], link:5cef47[NVIDIA Fleet Command], or link:a1b168[Snowflake].

----- user_guide/mlops/mlops-develop-and-deploy.txt -----
:page-version: 6.1
:page-title: Develop and deploy ML pipelines
:page-permalink: d03252
:page-order: 20

Machine learning (ML) pipelines are crucial for streamlining the complex processes involved in building and deploying ML models. These pipelines not only enhance collaboration among data scientists, engineers, and IT professionals but also improve training efficiency and reduce costs. This article explores the significance of ML pipelines, their differences from CI/CD pipelines, and how tools like Kubeflow, Airflow, and Domino can be integrated to manage the complete data processing, model training, and deployment workflows.

== Differences between ML and CI/CD pipelines

While ML pipelines focus on the model development lifecycle, CI/CD pipelines are centered on software development and delivery processes. CI/CD pipelines automate the steps involved in integrating code changes from multiple contributors, testing them, and deploying stable versions to production. In contrast, ML pipelines manage the flow of data through various stages of processing, model training, and deployment within ML projects.

== Why are ML pipelines needed?

The core of an ML pipeline is to split a complete ML task into a multistep workflow. Each step is a manageable component that can be developed, optimized, configured, and automated individually. Steps are connected through well-defined interfaces. 

This modular approach brings two key benefits:

. Standardize the MLOps practice and support scalable team collaboration
+
Machine learning operations (MLOps) automate the process of building ML models and taking the model to production. This is a complex process that usually requires collaboration from different teams with different skills. A well-defined ML pipeline can abstract this complex process into a multistep workflow, mapping each step to a specific task such that each team can work independently.
+
For example, a typical ML project includes the steps of data collection, data preparation, model training, model evaluation, and model deployment. Data engineers usually concentrate on data steps, data scientists spend most of their time on model training and evaluation, and ML engineers focus on model deployment and automation of the entire workflow. By leveraging the ML pipeline, each team only needs to work on building their own steps. The best way to build steps is to use the Azure Machine Learning component (v2), a self-contained piece of code that does one step in an ML pipeline. All these steps, built by different users, are finally integrated into a single workflow through the pipeline definition. 
+
The pipeline is a collaboration tool for everyone in the project. The process of defining a pipeline and all its steps can be standardized by each company's preferred development operations (DevOps) practice. The pipeline can be further versioned and automated. If the ML projects are described as a pipeline, then the best MLOps practice is already applied.
+
. Training efficiency and cost reduction
+
The ML pipeline not only puts MLOps into practice but also improves the training efficiency and reduces the cost of a large model. For example, modern natural language model training requires pre-processing large amounts of data and GPU-intensive transformer model training. It takes hours to days to train a model each time. When the model is being built, the data scientist wants to test different training codes or hyperparameters and run the training many times to get the best model performance. In most cases, there are small changes from one training iteration to another. It would be a significant waste to conduct the entire training process, from data processing to model training, each time. By using an ML pipeline, unchanged step results can be calculated and outputs from previous training iterations can be reused automatically.
+
Additionally, the ML pipeline supports running each step on different computation resources. For example, the memory-heavy data processing work can run on high-memory CPU machines, while the computation-intensive training can run on expensive GPU machines. By properly choosing which step to run on which type of machine, the training cost can be significantly reduced.

== Best practices to get started

The starting point of building an ML pipeline might vary, depending on your team's maturity and familiarity with ML. There are a few typical approaches to building a pipeline.

The first approach usually applies to teams that haven't used pipelines before. In this situation, data scientists typically have developed some ML models on their local environment using their favorite tools. ML engineers need to take the data scientists' output into production. The work often involves cleaning up some unnecessary code from original notebooks or Python code, changing the training input from local data to parameterized values, splitting the training code into multiple steps as needed, performing a unit test of each step, and finally wrapping all steps into a pipeline.

The second approach applies to teams that have some familiarity with pipelines and have done the first approach a few times so that common patterns have begun to emerge. Once the teams get familiar with pipelines and want to do more ML projects using pipelines, they'll find the first approach is hard to scale. To accelerate and scale adoption, they may have a centralized team set up a few pipeline templates, each trying to solve one specific ML problem. The template predefines the pipeline structure and includes the number of steps, each step's inputs and outputs, and the connectivity of each. To start a new ML project, the team first forks one template repository. The team leader then assigns each member a specific step to work on. The data scientists and data engineers continue with their regular work and when they're happy with their results, they structure their code to fit the predefined steps. Once the structured codes are checked in, the pipeline can be executed or automated. If any changes are necessary, each member only needs to work on their piece of code without touching the rest of the pipeline code.

Once a team has built a collection of ML pipelines and reusable components, they can start to build the ML pipeline by cloning previous pipelines or tying existing reusable components together. At this stage, the team's overall productivity would have improved significantly.

== Integrate Kubeflow and Airflow with Domino

You can seamlessly integrate Kubeflow and Airflow with Domino.

=== Use Kubeflow with Domino

Kubeflow is an open-source platform designed to orchestrate ML pipelines using Kubernetes. It is particularly well-suited for managing complex workflows in ML projects that involve large datasets and require scalable processing. Domino's code-first approach aligns well with defining tasks within Kubeflow pipelines to initiate Jobs in Domino.

Architecturally, Kubeflow pipelines are defined, managed, and operated within a Kubernetes cluster, separate from your Domino Environment. Kubeflow will require network connectivity to access the Domino API for executing Jobs in your Domino Projects. The code for each step within the pipeline, such as data retrieval, cleaning, and model training, is stored and versioned in your Domino Project. This integration ensures collaboration between Kubeflow's orchestration capabilities and Domino's reproducibility features.

image::/images/jobs/kubeflow-pipeline.png[alt="Kubeflow pipeline", width=850]

To learn more about using Kubeflow with Domino, see link:866fae[Orchestrate Jobs with Kubeflow^].

=== Use Airflow with Domino

Apache Airflow is another powerful tool used to orchestrate complex computational workflows. Airflow's scheduler executes tasks on an array of workers while following specified dependencies. The example graph below was developed using Airflow and python-domino and executes all the dependencies in Domino using the Airflow scheduler. It trains a model using multiple datasets and generates a final report.

image::/images/4.x/example-pipeline.png[alt="Example Airflow pipeline", width=800]

To learn more about using Airflow with Domino, see link:e4f67f[Orchestrate Jobs with Apache Airflow^].

== Next steps

link:1d334e[Review and approve models].

----- user_guide/mlops/mlops-git-integration.txt -----
:page-version: 6.1
:page-title: Version control and Git
:page-permalink: bd26e2
:page-order: 50

Version control is a foundational practice in the world of software development, crucial for tracking changes, maintaining stability, and facilitating collaboration. In machine learning operations (MLOps), versioning extends beyond code to include data, models, and more. 

== Why versioning matters in MLOps

There are several reasons why you should use versioning in MLOps:

=== Reproducibility

Reproducibility is a cornerstone of scientific and engineering disciplines. In MLOps, the ability to reproduce results is essential for validating models and their predictions. Versioning code and data ensures that experiments are repeatable, helping teams understand which changes influenced model performance and ensuring that models can be audited or debugged.

To learn more about reproducibility in Domino, see link:0264e0[Domino Reproducibility Engine (DRE)].

=== Collaboration

Versioning facilitates smoother collaboration within teams. Data scientists often work in diverse teams where experiments are built upon and iterated by different team members. Versioning allows these teams to track who made what changes, merge contributions efficiently, and revert changes if something goes wrong.

To learn more about collaboration, see link:d82ba3[Collaborative data science in Domino].

=== Compliance

For many industries, compliance with regulatory standards requires detailed records of data and model versions used to make decisions. Versioning provides an audit trail that can be invaluable for compliance and monitoring purposes.

To learn more about compliance in Domino, see link:c91e77[Security and compliance].

== Continuous improvement via versioning

In MLOps, models are continuously updated with new data. Versioning allows for the seamless integration of new data and models into production environments, supporting continuous improvement and deployment practices. This is crucial for applications where model performance can degrade over time as patterns in data change. Domino provides robust support to version both data and code, integrating these capabilities into its platform to enhance MLOps workflows.

=== Versioning code via Git integration

Domino integrates seamlessly with Git repositories, supporting code versioning practices that are familiar to software developers. This integration allows users to track changes in their code, collaborate through pull requests, and maintain a history of modifications. This linkage ensures that all aspects of model development, from data preparation to model training and evaluation, are versioned and traceable.

To learn more, see link:910370[working with Git integration].

=== Versioning data via Domino Snapshots

Domino allows users to create snapshots of datasets at various points in their projects. These snapshots capture the state of the dataset at a specific time, providing a versioned history that can be used to train models at different stages of the project. This is particularly useful for tracking how changes in data affect model performance over time.

To learn more, see link:dbdbff[version data with Snapshots].

=== Versioning tools and libraries via Domino Environments

Domino has built-in support for versioning Environments, which are based on Docker containers that contain tools and libraries that data scientists and machine learning (ML) engineers need to train and deploy great models. It can often be challenging to install these tools and libraries, as versions of the different open-source libraries may not be compatible with each other. Domino makes it easy to add new tools or libraries to an Environment, which then creates a new version of that Environment. Other team members or auditors can then use that Environment to test changes or review the model and software for potential risks.

To learn more, see link:f51038[working with Domino Environments].

=== Unified workflow

By supporting both data and code versioning, Domino provides a unified workflow that maintains the integrity of ML projects. This approach reduces errors, simplifies debugging, and enhances the collaborative efforts of data science teams.

To learn more, see link:9a69d9[getting started with Projects in Domino].

== Next steps

link:2793b2[Orchestrate MLOps with CI/CD].

----- user_guide/mlops/mlops-github-actions.txt -----
:page-version: 6.1
:page-title: Orchestrate with CI/CD
:page-permalink: 2793b2
:page-order: 60

Continuous Integration / Continuous Delivery (CI/CD) has transformed how software engineers continuously improve software applications and deploy them. However, data scientists and machine learning (ML) engineers are often still learning how to work together to manage the end-to-end machine learning operations (MLOps) lifecycle using CI/CD pipelines. In this article, we'll explain how to use GitHub Actions together with Domino to orchestrate MLOps in Domino. GitHub Actions is a powerful tool to automate workflows, making it highly suitable for setting up an MLOps pipeline. This pipeline can handle various tasks such as data preparation, model training, model deployment, and monitoring. Note that any CI/CD system like Azure DevOps, Jenkins, or others could be used instead of GitHub.

== Prerequisites

Before setting up the MLOps pipeline, ensure you have:

* A GitHub account with a repository for your ML project.
* Git installed on your local machine.
* A basic understanding of GitHub Actions workflows.

== Connect a Domino Project to a Git repository

Domino Projects can contain one or more Workspaces. A Domino Workspace is an interactive session where you can conduct research, analyze data, train models, and more. Use workspaces to work in the development environment of your choice, like https://jupyter.org/[Jupyter^] notebooks, https://rstudio.com/[RStudio^], https://code.visualstudio.com/[VS Code^], and many other customizable environments. 

When you edit and save code and other files in a Workspace session, Domino will automatically sync these changes into your Git repository, which will execute the GitHub Actions defined above. These actions will orchestrate running the data preparation or model training jobs and even deploy the models, depending on the configuration of your GitHub action.

. In the Domino navigation pane, click *Projects* > *New Project*.

. In the *Create New Project* window, enter a name for your Project.
. Set your Project's *Visibility*.
. Click *Next*.
. Under *Hosted By*, click *Git Service Provider*.
. Select the *Git Service Provider* currently hosting the repository you want to import.
This is the target repository.
. Select the *Git Credentials* authorized to access the target repository.
. Click to *Choose a repository* or *Input Git URL*.
If you are using PAT credentials with GitHub or GitLab, you can select *Create new repository*.
. Click *Create*.

[[tr3]]

During the project creation process, you can create a new repository for GitHub and GitLab.

== Set up a GitHub repository

Start by setting up a new GitHub repository or using an existing one where your ML code resides. To prepare your repository:

* Add your ML code, data files, and any necessary configuration files to the repository.
* Organize your repository with clear directory structures and `README` files to guide other users or collaborators.
* Ideally employ Infrastructure-as-Code (IaC) techniques to ensure infrastructure requirements, like Hardware Tiers and Environments, are also automatically provisioned as part of the MLOps pipeline.

=== Step 1: Create GitHub Secrets

GitHub Secrets store sensitive data, like API keys and access tokens, which you can use in your GitHub Actions workflows. For this setup, you will need secrets for your Domino access credentials or other sensitive data necessary for your workflows.

* Navigate to your repository settings.
* Go to *Secrets* and select *Actions*.
* Create necessary secrets by selecting *New repository secret*. Common secrets include the `DOMINO_USER_API_KEY`, which are the credentials with which GitHub will connect to Domino to execute the Domino API to facilitate the steps of the workflow.


=== Step 2: Set up the GitHub Actions workflow

GitHub Actions workflows are located in the `.github/workflows/` folder for the end-to-end MLOps pipeline. The pipeline includes stages for data preparation, model training, and model deployment. There are three workflows, `Dev`, `Staging`, and `Production`, representing different stages for the MLOps pipeline. The configuration file, `cicd-e2e-mlops-env-variables.ini` in the `src/cicd` folder of the repository, provides options to configure your data preparation, training, or deployment during different stages of your MLOps pipeline.

Create a `.github/workflows` directory in your repository if it doesn't already exist. Inside this directory, you can define your workflow files.

==== Separate environments

It's important to have a controlled process to separate changes in a development or staging environment before going to production. This can help prevent unintended consequences. However, there are multiple ways to achieve this. See link:67333d[Deploy to production] for more detail on how to separate environments.

image::/images/mlops/environments.png[alt="Domino environments", width=800]

==== Environment variables

Regardless of how you separate `Dev`, `Staging`, and `Production`, you'll often need to use different environment variables, passwords, and other secrets for each. Be sure to configure them. For example, you may use a different Domino service account for `Dev` than for `Production` and you'll need to get the User API Key for each. To learn about service accounts and API authentication, see link:40b91f[Domino API authentication].

image::/images/mlops/environment_variables.png[alt="Domino environment variables", width=800]

==== Approve changes in production

Optionally, you can leverage GitHub or other external services to gate, coordinate, and record the actual approvals. In this case, using GitHub's integrated approvals will prevent the code from running in the production environment to run the jobs or deploy models to ensure that only allowed changes happen in production.

image::/images/mlops/approvers.png[alt="gated approvals", width=800]

==== Train and validate models

Create a workflow file (e.g., `ml_train_deploy.yml`) to define the steps for training and deploying your model.

```yaml
# This workflow will install Python dependencies, run tests, and lint with a single version of Python.
# For more information, see https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python.

name: Create Domino Jobs to Train & Validate Model

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]
  pull_request_target:
    branches: [ "main" ]  

permissions:
  contents: read

jobs:
  deploy-dev:
    runs-on: ubuntu-latest
    environment: 'Dev'
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python 3.10
      uses: actions/setup-python@v3
      with:
        python-version: "3.10"
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flake8 pytest dominodatalab
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
    - name: Lint with flake8
      run: |
        # stop the build if there are Python syntax errors or undefined names
        # flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
        # flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    - name: Deploy to Domino Dev
      run: |
        python src/cicd/cicd-jobs.py ${{ vars.DOMINO_ENV }} ${{ secrets.DOMINO_USER_API_KEY }}
  deploy-staging:
    runs-on: ubuntu-latest
    environment: 'Staging'
    needs: deploy-dev
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python 3.10
      uses: actions/setup-python@v3
      with:
        python-version: "3.10"
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flake8 pytest dominodatalab
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
    - name: Lint with flake8
      run: |
        # stop the build if there are Python syntax errors or undefined names
        # flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
        # flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    - name: Deploy to Domino Pre Prod
      run: |
        python src/cicd/cicd-jobs.py ${{ vars.DOMINO_ENV }} ${{ secrets.DOMINO_USER_API_KEY }}
  deploy-production:
    runs-on: ubuntu-latest
    environment: 'Prod'
    needs: deploy-staging
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python 3.10
      uses: actions/setup-python@v3
      with:
        python-version: "3.10"
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flake8 pytest dominodatalab
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
    - name: Deploy to Domino Production
      run: |
        python src/cicd/cicd-jobs.py ${{ vars.DOMINO_ENV }} ${{ secrets.DOMINO_USER_API_KEY }}
```

==== Register and deploy models 

Optionally, create a GitHub Action workflow to register and deploy your models:

```yaml
# This workflow will install Python dependencies, run tests, and lint with a single version of Python.
# For more information, see https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python.

name: Deploy Domino Model

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]
  pull_request_target:
    branches: [ "main" ]  

permissions:
  contents: read

jobs:
  deploy-dev:
    runs-on: ubuntu-latest
    environment: 'Dev'
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python 3.10
      uses: actions/setup-python@v3
      with:
        python-version: "3.10"
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flake8 pytest dominodatalab
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
    - name: Lint with flake8
      run: |
        # stop the build if there are Python syntax errors or undefined names
        # flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
        # flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    - name: Deploy to Domino Dev
      run: |
        python src/cicd/cicd-models.py ${{ vars.DOMINO_ENV }} ${{ secrets.DOMINO_USER_API_KEY }}
  deploy-staging:
    runs-on: ubuntu-latest
    environment: 'Staging'
    needs: deploy-dev
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python 3.10
      uses: actions/setup-python@v3
      with:
        python-version: "3.10"
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flake8 pytest dominodatalab
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
    - name: Lint with flake8
      run: |
        # stop the build if there are Python syntax errors or undefined names
        # flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
        # flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    - name: Deploy to Domino Pre Prod
      run: |
        python src/cicd/cicd-models.py ${{ vars.DOMINO_ENV }} ${{ secrets.DOMINO_USER_API_KEY }}
  deploy-production:
    runs-on: ubuntu-latest
    environment: 'Prod'
    needs: deploy-staging
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python 3.10
      uses: actions/setup-python@v3
      with:
        python-version: "3.10"
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flake8 pytest dominodatalab
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
    - name: Deploy to Domino Production
      run: |
        python src/cicd/cicd-models.py ${{ vars.DOMINO_ENV }} ${{ secrets.DOMINO_USER_API_KEY }}
```

=== Step 3: Configure the training and deployment scripts

Ensure your repository includes scripts referenced in the workflow, such as `train.py`, `save_model.py`, and `deploy_model.py`. These scripts should handle the following:

* Training the model: `train.py` should include code to train your ML model.
* Saving the model: `save_model.py` should handle saving the trained model to a designated storage or model management service.
* Deploying the model: `deploy_model.py` should include code to deploy the model to a production environment or a model serving platform.

== Next steps

link:916d88[Monitor and improve MLOps].

----- user_guide/mlops/mlops-monitor-and-improve.txt -----
:page-version: 6.1
:page-title: Monitor and improve
:page-permalink: 916d88
:page-order: 70

In machine learning (ML), models are often trained using historical data to predict future outcomes. However, as conditions change over time, the performance of these models can deteriorate. This degradation can arise due to shifts in the data distribution, changes in patterns, or evolving business requirements. It is vital to monitor models to detect these issues early and take corrective action, ensuring consistent and reliable performance. It helps prevent adverse business impacts, maintains trust in AI systems, and provides insights into model behavior.

Domino's Model Monitoring uses training and prediction data to track drift for the model's input features and prediction variables. If you have ground truth data for the model's predicted values, Domino can ingest it to produce model quality metrics using standard measures such as accuracy, precision, and recall. Domino can also alert you about every feature and metric that exceeds a configurable threshold.

== Understanding model drift

There are various reasons why model quality can degrade over time: 

* *Data drift*: Changes in the input data distribution that differ from the training data. For instance, a model trained on sales data may encounter new customer demographics that affect its predictions.
* *Concept drift*: Changes in the relationship between input and output variables. For example, a fraud detection model might lose accuracy if fraudsters adapt their techniques.

== Fixing model quality issues

When Domino detects model quality degradation via drift, Domino raises alerts.

* *Root cause analysis*: Identify whether data or concept drift is occurring and assess its impact on model accuracy and business outcomes.
* *Retraining*: Retrain models with recent data to reflect the new patterns or relationships. This might involve data cleaning or new feature engineering.
* *Continuous improvement*: Monitor retrained models and incorporate feedback loops to refine them iteratively.

== Next steps

link:a8e081[Get started with Projects in Domino].

----- user_guide/mlops/mlops-review-and-approve.txt -----
:page-version: 6.1
:page-title: Review and approve models
:page-permalink: 1d334e
:page-order: 30

In the rapidly evolving domain of artificial intelligence (AI) and machine learning (ML), the governance of AI models is a critical concern. As organizations increasingly depend on AI to make business decisions, it is paramount to ensure the integrity, accuracy, and fairness of these models before deployment.

This document explores the importance of reviewing and approving models as part of an AI governance strategy, particularly focusing on risk management in financial services and generative AI. Additionally, it discusses how tools like MLflow Model Registry and Model Sentry in Domino can facilitate these processes.

== The need for model review and approval

* *Mitigate risks*: AI models can pose significant risks if not properly vetted. In financial services, for example, models that predict stock prices, assess credit risk, or detect fraudulent activities must perform accurately to avoid potential losses and ensure regulatory compliance. In the realm of generative AI, which includes technologies like natural language processing and image generation, risks include generating biased or inappropriate content that could harm an organization's reputation or violate ethical standards.

* *Regulatory compliance*: Many industries, especially financial services, are subject to stringent regulatory requirements that dictate thorough testing and validation of AI models. Regulations such as the General Data Protection Regulation (GDPR) and the upcoming AI Act in Europe emphasize transparency, accountability, and fairness in AI systems, mandating organizations to implement robust governance frameworks.

* *Ensure model integrity and performance*: Reviewing and approving models helps ensure that they perform as expected on new data and in varying conditions, which is crucial for maintaining the reliability of business processes that depend on AI. This step also helps identify potential degradation in model performance over time, prompting necessary recalibrations or replacements.

* *Model governance framework*: A structured model governance framework involves several key components:

- *Model validation*: Rigorous testing of model accuracy, performance, and robustness under different scenarios.

- *Ethical review*: Examination of the model decisions for fairness, bias, and ethical implications, ensuring that they adhere to organizational ethics policies.

- *Audit trails*: Maintaining records of model development, testing, and changes to ensure traceability and accountability.

- *Stakeholder review*: Involving cross-functional teams in the review process to ensure that all business and technical requirements are met.

== Review and approve models in Domino

* *MLflow Model Registry in Domino*: MLflow Model Registry provides a centralized hub for managing the lifecycle of ML models. It allows teams to version models, track their lineage, and manage stages of model development from experimentation to production. Here's how it enables the review and approval process:

- *Model staging*: Models can be moved through staged environments such as `Staging`, `Production`, and `Archived`, allowing for controlled progression and testing at each stage.

- *Version control*: Keep track of different model versions along with corresponding performance metrics to select the best version for production.

- *Role-based permissions*: Regulate who can transition models into different stages to ensure that only authorized personnel can approve and move models into production.

- *Model Sentry in Domino*: Model Sentry is designed to monitor model performance and detect deviations in real-time. It acts as a safeguard, ensuring models continue to perform optimally in production environments.

== Next steps

link:67333d[Deploy to production].

----- user_guide/mlops/mlops-vs-devops.txt -----
:page-version: 6.1
:page-title: MLOps vs DevOps
:page-permalink: 20f423
:page-order: 10

Machine learning operations (MLOps) differ from development operations (DevOps) in several key areas, and Domino supports these differences with key capabilities that are optimized to maximize flexibility for data scientists while ensuring control and governance for leaders and information technology (IT) professionals.

== Exploration precedes development and operations

In traditional DevOps, the process typically begins with a clear definition of requirements and a direct move to development. In contrast, MLOps often starts with an exploratory phase where data scientists analyze data to validate hypotheses and understand underlying patterns before formalizing a solution. Domino provides an interactive, collaborative workspace where data scientists can easily explore and visualize data without cumbersome setup. The platform supports rapid prototyping and iterative analysis, enabling teams to quickly and effectively experiment with different models. With Domino, exploratory work is seamlessly integrated with later stages of model development and deployment, thanks to the platform's ability to manage the entire lifecycle within a unified environment.

== The data science lifecycle requires an adaptive way of working

Unlike DevOps, where processes can be linear and predictable, the data science lifecycle is inherently iterative and experimental as it requires frequent adjustments based on new findings and evolving data insights. Domino accommodates the adaptive nature of data science projects by facilitating easy iteration of models and experiments. The platform's project management tools allow for flexible adjustments and integrations with other services, ensuring that data scientists can adapt their workflows as needed without losing track of changes or performance metrics.

== Limits on data quality and availability limit progress

DevOps relies on stable, predictable software development environments, while MLOps must often contend with variable data quality and availability, which can significantly impact model performance and development timelines. Domino tackles these challenges by providing robust data management tools that enhance data quality and ensure reliable access to datasets. The platform supports versioning of data alongside code, enabling teams to maintain consistency across experiments and to easily roll back to earlier states if new data adversely affects model performance.

== A greater operational effort is required than in DevOps

MLOps requires more operational oversight due to the ongoing need to monitor and adjust machine learning (ML) models based on continuous data inputs, unlike traditional software which requires less monitoring after deployment. Domino offers comprehensive monitoring tools that track model performance in real-time, alerting teams to issues like data drift or performance degradation. The platform also automates many operational tasks, such as model retraining and redeployment, reducing the manual effort required to maintain high model accuracy and reliability.

== ML is a team sport and requires collaboration

While DevOps teams often comprise software developers and IT professionals, MLOps teams need a broader range of expertise, including data scientists, ML engineers, and domain-specific experts. Domino enhances collaboration across diverse teams by providing tools tailored to each role's needs. For example, data scientists can focus on model building and testing, while ML engineers can utilize Domino's deployment and monitoring features. The platform also facilitates knowledge sharing and best practices through integrated documentation and project management features, ensuring that all team members are aligned and the project is effectively managed.

== Next steps

link:d03252[Develop and deploy with pipelines].

----- user_guide/model-monitoring/1-monitor-workflows.txt -----
:page-version: 6.1
:page-title: Monitor workflows
:page-permalink: 85f76d
:page-order: 10

In Domino, you can monitor models deployed on Domino and elsewhere. For Domino models deployed as Domino endpoints, you can use our Domino endpoint monitoring workflow, so you don't have to maintain access to external data sources. For models deployed in other formats (such as an link:71635d[App], link:f4e1e3[Launcher], or link:942549[Job]) or external to Domino, you can use our Model Monitor workflow.

Use Domino's Model Monitor to see a single list of monitored models no matter how they are deployed.

== Domino endpoint monitoring

When a model is deployed on Domino as a Domino endpoint, Domino:

* Analyzes the training data to extract the model schema (if you register a Domino link:9c4dec[TrainingSet^]).
//Where to link for this?
* Captures predictions as Domino datasets for monitoring.
* Generates drift detection and model quality analysis on a schedule (if you share the ground truth dataset with Domino), and alerts you if any thresholds are exceeded.
* Allows you to easily link:74f8ed#reproduce-the-environment[reproduce the environment] with access to the captured predictions to diagnose and fix issues with your model.

See link:2a7c3b[Set up monitoring for Domino endpoints].

If you do not want Domino to manage the prediction data collection, use the Model Monitor to configure monitoring, even for Domino endpoints.

== Model Monitor

For models deployed as other assets on Domino (link:71635d[App], link:f4e1e3[Launcher], or link:942549[Job]) or external to Domino, you can use Domino to:

* Connect to the data source where the training, prediction, and ground truth data reside.
* Register a model's entry along with its schema.
* Set up drift detection and model quality monitoring by registering the location of every new batch of prediction or ground truth data.
* Set up a schedule for Domino to run drift and model quality checks periodically and alert you if thresholds are exceeded.

See link:679cc1[Set up Model Monitor].

Contact your Domino support person if you need more help in determining the best workflow.

----- user_guide/model-monitoring/10-troubleshoot.txt -----
:page-version: 6.1
:page-title: Troubleshoot the Model Monitor
:page-permalink: f66a1b
:page-order: 100


This topic provides solutions for common error messages you might see in the model monitoring interface.

== Data ingestion taking longer than expected

Data is ingested sequentially. When multiple users and/or models ingest data at the same time, the ingest jobs are queued and executed sequentially. This can cause ingest jobs to take longer than expected, occasionally delaying ingesting a small dataset if a larger dataset is ahead of it in the queue.

== Model not running

If your model is still building or starting, wait for its status to change to *Running*.

If your model is stopped, use the following steps to start the model:

. In the Domino endpoint, click *Versions*.
. In the *Actions* column for the version you want to start, click the three dots to open the menu.
. Click *Start Version*.
. Wait for the model status to change to *Running*.

== Monitoring is not enabled

This happens for the following reasons:

* Your model's prediction data might not be configured.
* Your model's `predict()` function might not include the `domino.log()` function needed to enable monitoring. See link:93e5c0[Set up Prediction Capture] for instructions.
* Your model's link:9c4dec[training data^] might not be configured. Training data is required for monitoring data drift. See link:c97091[Set up Drift Detection].
* Your model's ground truth data might not be configured. Ground truth data is required for monitoring model quality. See link:ce3835[Set up Model Quality Monitoring].
* The first scheduled check might not have occurred yet. Check the link:4da05b[monitoring schedule] to see when the next monitoring data check will occur.
+
[NOTE]
====
link:93e5c0[Prediction Data] is analyzed through 23:59 of the previous day. Data from the current day is not included. Domino reads the timestamps in the dataset, if they are present; if not, then it uses the ingestion timestamp.
====

== Waiting for data

The Model Monitor is waiting for sufficient data to start rendering monitoring results.

. From the Domino endpoint, click *Grafana Monitoring.*
. Click *Configure monitoring > Target Ranges*. Go to *Date Filter > Today* to view the data that has been analyzed so far.

[[tr1]]

// Validate the data in Target Ranges.

== Model quality monitoring is not enabled

Model quality is based on ground truth data. If you see this message on your model's monitoring page, then your model's ground truth data might not be configured. See link:ce3835[Set up Model Quality Monitoring].

[[tr2]]

// Validate empty state Model Quality Not enabled.

== Drift monitoring is not enabled

If you see this message on your model's monitoring page, then your model's training data might not be set up. See link:86bc1f[Set up Drift Detection].

[[tr3]]

// Validate empty state for Data drift not enabled.

== Selected training set version cannot be used

* The selected Training Set Version cannot currently be used for monitoring because it doesn't contain a schema definition.
* The Training Set version you selected might be empty, you might have selected the wrong model type, or you might need to also add Target to the Categorical feature while creating the Training set. See link:86bc1f[Set up Drift Detection].


[[tr4]]

// Selected training version cannot be used.


== Error in library(“DominoDataCapture”)

`Error in library(“DominoDataCapture”): there is no package called ‘DominoDataCapture`

Your project's environment does not include the prediction data capture library. To add it, do one of the following:

* link:dba65c#compute-environment[Change your project's compute environment to the 5.0 DSE].
* link:be1fd2[Install Packages for Model Monitoring].

== Notifications not being sent

See link:16cd89[Configure Notifications for Model Monitoring].

----- user_guide/model-monitoring/2-data-drift-and-model-quality.txt -----
:page-version: 6.1
:page-title: Data drift and quality monitoring
:page-permalink: b08113
:page-order: 20

Domino monitors data drift and model quality.

image::/images/5.0/model-monitoring-approach.png[alt="Data drift metrics compare training data and prediction data, while model quality metrics compare prediction data and ground truth data"]

== Data drift


Data drift occurs when production data diverges from the model's original training data. Data drift can happen for many reasons, including a changing business environment, evolving user behavior and interest, modifications to data from third-party data sources, data quality issues, and even issues in upstream data processing pipelines. Data drift monitoring compares live predictions with the model's training data, and then sends an alert when live predictions diverge too much from the training data. See link:f33b63[Analyze Data Drift].


== Model quality

Model quality monitoring compares the model's predicted values against the actual results (or labels for the predictions) using ground truth data to generate quality metrics. For classification models, Domino reports the following metrics:

. Accuracy
. Precision
. Recall
. F1
. AUC ROC
. Log Loss
. Gini (Normalized)

Domino also provides a confusion matrix and classification report for further quality evaluation.

For regression models, Domino reports the following metrics.

. Mean Square Error (MSE)
. Mean Absolute Error (MAE)
. Mean Absolute Percentage Error (MAPE)
. R-Squared (R2)
. Gini (Normalized)

See link:d4f9b7#metrics-for-classification-and-regression-models[Metrics for classification and regression models] for details.


See link:d4f9b7[Analyze Model Quality].


[[tr1]]

// Observe the list of metrics on classification Model.

[[tr2]]

// Observe the list of metrics on regression model.

[[tr3]]

// Validate the confusion metrics for classification models.

[[tr4]]

// Validate the classification report for classification Models.

== Remediation

When data drift or model quality issues are detected, the Cohort Analysis feature identifies underperforming cohorts of data so you can take remedial action. See link:ac36e3[Cohort Analysis].

Use Domino to link:74f8ed#reproduce-the-environment[recreate the development environment] originally used to train the model. Then, you can use this environment to access the prediction data to diagnose issues, update code, and retrain your model with the latest production data.

----- user_guide/model-monitoring/3-set-up-domino-endpoints/1-set-up-prediction-capture.txt -----
:page-version: 6.1
:page-title: Set up prediction capture
:page-sidebar: Prediction capture
:page-permalink: 93e5c0
:page-order: 10

Prediction data is a combination of the inputs to the model and the predictions that are output from the model. Inputs are the values of the features that were input as link:a94c1c[API requests] into the Domino endpoint. When you incorporate a Domino-provided data capture library in your Domino endpoint code, Domino automatically captures the prediction data.

The data ingestion client is part of the Domino Standard Environment (DSE) with the latest version of the client library. The client library records prediction data for deployed models.

[[capture-pd]]
== Capture prediction data

[[tr1]]
// Deploy and run data through a model that uses the DominoDataCapture library using R

[[tr2]]
// Deploy and run data through a model that uses the domino_data_capture library using python

To add the DominoDataCapture library details to your model, you must add the following lines so that this logic will be executed when the model is deployed. See link:8dbc91#Synchronous-reqeuests[call a Domino endpoint] for details.

. In the top navigation pane, click *Develop > Projects*.
. In the navigation pane of your project, click *Workspaces*.
. Start the appropriate workspace.
. Edit your prediction code:
+

Edit your prediction code to add the DataCaptureClient. See the following examples for Python and R:

[[tr3]]
// Code snippets from documentation work as described

[source,python]
----
import datetime
import uuid

from domino_data_capture.data_capture_client import DataCaptureClient

feature_names = ["dropperc", "mins", "consecmonths", "income", "age"]
feature_values = ["dropperc", "mins", "consecmonths", "income", "age"]

features_dict = dict(zip(feature_names, feature_values))

predict_names = ["y"]
predict_values = [100]

predict_dict = dict(zip(predict_names, predict_values))

# Record eventID and current time
event_id = uuid.uuid4()
event_time = datetime.datetime.now(datetime.timezone.utc).isoformat()

# Custom metadata I want to track for this event
metadata_names = ["cohort"]
metadata_values = ["cohort_id"]
prediction_probability = [0.1, 0.9]
sample_weight = 0.3

data_capture_client = DataCaptureClient(feature_names, predict_names, metadata_names)

data_capture_client.capturePrediction(
    feature_values,
    predict_values,
    metadata_values=metadata_values,
    event_id=event_id,
    timestamp=event_time,
    prediction_probability=prediction_probability,
    sample_weight=sample_weight,
)
----

[source,r]
----
library(“DominoDataCapture”)
data_capture_client <- DataCaptureClient(feature_names, predict_names, metadata_names)
data_capture_client.capturePrediction(feature_values, predict_values, metadata_values, event_id, timestamp, prediction_probabily, sample_weight)
----

The following table explains the parameters from the `DataCaptureClient` statement:

[cols="2a,2a,4a",options="header"]

|===
|Parameter |Type |Description

|`feature_names` | `Array[String]` |The feature names against which the user will calculate the prediction.
|`predict_names` |`Array[String]` |The prediction names collection. This value must be an array.
|`metadata_names` (Optional) |`Array[String]` |Collection of any metadata keys to pass.
|===

The following table explains the parameters from the `capturePrediction` statement:

[cols="2a,2a,4a",options="header"]
|===
|Parameter |Type |Description
|`feature_values` |`Array[float/String]` |The feature values against which the user will calculate the prediction.
|`predict_values` |`Array[float/String]`|The prediction values collection. This value must be an array.
|`metadata_values` |`Array[float/String]` |Collection of any metadata values to pass.
|`event_id` (Optional) |`String` |A unique record ID for each prediction. If not provided the client library generates one.
|`timestamp` (Optional) |`Int` |The event timestamp. If not provided the client library generates it.
|`prediction_probability` (Optional) |`Array[float]` |The collection of prediction probabilities. This value must be an array.
|`sample_weight` (Optional) |`Array[float]`|The collection of associated sample weights. This value must be an array.
|===

=== Run DataCaptureClient

[[tr4]]
// Run a model using DominoDataCapture library in developer mode

Use the `DominoDataCapture` library to capture prediction data in the Domino endpoint or in developer mode. Use developer mode to test the library calls to verify that the data capture will work, without actually capturing data. After verifying that the data capture will work, you must invoke the Domino endpoint code in a workspace (for example, an iPython notebook), where you can review the output of the library calls, validate, and debug the code.

[NOTE]
====
Domino endpoints support capturing up to 8GB of data per 24-hour period. Bursting has been tested up to five times this limit, beyond which there might be errors or warnings in the Domino endpoint log.
====

[[tr5]]
// Performance metric: DominoDataCapture library can capture 8GB of data per 24 hour period, bursting to 40GB/24 Hour rate

*Step 1: Run DataCaptureClient in developer mode*

[#python-example]
.Run an example in Python
--
. Open a Python Prediction Client workspace.
. Go to *New > Python3*.
. Add the following lines and update them for your model:
* Import the predict function: `from python_model_with_logging import *`
* Invoke the predict method with parameters: `predict_iris_variety(5.3,3,1.1,0.1, 1)`
--
[#r-example]
.Run an example in R
--
. Open R Studio.
. Go to *File > New File > R Notebook*.
. Add the following lines and update them for your model:
* The source for the model instrumented with data capture: `source("model_to_predict_iris_variety.R")`
* Invoke the predict method with parameters: `predict_iris_variety(5.3,3,1.1,0.1, 1)`
. Click *Run* and view the console.
--

*Step 2: Run DataCaptureClient in Domino endpoint*

The following are examples of models that use Domino data capture:

[source,python]
----
import datetime
import pickle
import pandas as pd
import uuid

from sklearn import metrics
from domino_data_capture.data_capture_client import DataCaptureClient

feature_names = ['sepal.length', 'sepal.width', 'petal.length', 'petal.width']
predict_names = ['variety']
pred_client = DataCaptureClient(feature_names, predict_names)

loaded_model = pickle.load(open("model.pkl", 'rb'))

def predict_iris_variety(sepal_length, sepal_width, petal_length, petal_width, event_id):
    feature_values = [sepal_length, sepal_width, petal_length, petal_width]
    predict_values = loaded_model.predict([feature_values])

    event_time = datetime.datetime.now(datetime.timezone.utc).isoformat()

    pred_client.capturePrediction(feature_values, predict_values, event_id=event_id, timestamp=event_time)

    return dict(predict_value=predict_values[0])
----

[source,R]
----
library("randomForest")
library("DominoDataCapture")

training_data <- read.csv(
 "iris.csv",
 header = TRUE
)

training_data$variety <- factor(training_data$variety)
iris_rf <- randomForest(
 variety ~ .,
 data = training_data,
 importance = TRUE,
 proximity = TRUE
)

data_capture_client <- DataCaptureClient(
 feature_names = c("sepal.length", "sepal.width", "petal.length", "petal.width"),
 predict_names = c("variety")
)

predict_iris_variety <- function(sepal_length, sepal_width, petal_length, petal_width, event_id) {
 mytestdata <- data.frame(
   "sepal.length" = sepal_length,
   "sepal.width" = sepal_width,
   "petal.length" = petal_length,
   "petal.width" = petal_width
 )
 predicted_value_factor <- predict(iris_rf, mytestdata)
 predicted_value <- as.character(predicted_value_factor)
 data_capture_client$capturePrediction(
   feature_values = c(sepal_length, sepal_width, petal_length, petal_width),
   predict_values = c(predicted_value),
   event_id = event_id
 )
 return(list(variety = predicted_value))
}
----

=== Data capture examples

See more examples of MLflow-supported models that use Domino data capture:

. https://github.com/dominodatalab/reference-project-domino-mlflow-supported-models/blob/main/domino-mlflow-model-xgboost-imm.ipynb[Registering XGBoost model with integrated model monitoring].
. https://github.com/dominodatalab/reference-project-domino-mlflow-supported-models/blob/main/domino-mlflow-model-sklearn-imm.ipynb[Registering Sklearn model with integrated model monitoring].

== (Optional) Customize Domino Environments

[[tr6]]
// Modify Domino environment to use domino data capture library

If you want to use a specific version of the client library, or enable client libraries in another environment:

. In your Environment, click *Edit Definition*.

. In the *Dockerfile Instructions*, add the following lines to enable the library:
+
[source,python]
----
USER root
RUN pip install domino-data-capture
USER ubuntu
----
+
[source,r]
----
RUN R --no-save -e "install.packages(c(DominoDataCapture))"
----

. Select *Full rebuild without cache* and click *Build*.
. From the navigation bar, click *Endpoints*.
. Click *New endpoint* and create the endpoint from the newly built image.

== Test the Domino endpoint


See link:c97091#validate-setup[Validate your Setup] to confirm your prediction data is being captured.

After you link:8dbc91#Deploy-a-model[publish your Domino endpoint] and it is running, call the Domino endpoint to capture prediction data.

. Go to the Domino endpoint to test.
. From the *Tester* tab, enter the values from your model's schema.
. Click *Send*. The *Response* field shows a prediction in the form of key-value pair.

After the logged predictions are captured and processed by Domino, you can see a preview of the drift results. See link:c97091#validate-setup[Validate Your Setup] for more information.

== Advanced configuration

See the link:71d6ad[Administration Guide] for configuration keys that tune the prediction data capture feature.

----- user_guide/model-monitoring/3-set-up-domino-endpoints/2-set-up-drift-detection.txt -----
:page-version: 6.1
:page-title: Domino endpoint drift detection
:page-sidebar: Drift detection
:page-permalink: c97091
:page-order: 20

To monitor a model's data drift you must ingest training data. Training data is the data used while constructing a model that leverages machine learning techniques. Training data records must include both model input values and a previously-known corresponding output for each input value combination from which the model can learn.


Your model's training data is used to link:f33b63[analyze data drift]. This topic describes how to register a training set for monitoring purposes.


== Prerequisites

Before you proceed, you must create a link:9c4dec[training set] that was used to build a model (`.pkl` file), and then that model was deployed into a Domino endpoint.

== Select the training set

[[tr1]]
// Select a training set for a Domino endpoint with integrated monitoring

. In the top navigation pane, click *Deploy > Endpoint*.
. Select the endpoint you would like to configure and navigate to the *Grafana Monitoring* tab.
. Click *Configure Monitoring > Data*.
. From the Configure Data window, select the *Training Data* and *Version*.
. Select *Classification* or *Regression*, depending on your model type. If your Training Set code includes prediction data defined in `target_columns`, select the model type that matches your Training Set:
* If `target_columns` is a `categorical` column, select *Classification*.
* If `target_columns` is a `numerical` column, select *Regression*.

. Click *Save*.

NOTE: When using the *Configure Data* window to register a model for monitoring, the default binning strategy will be used to create bins for both numerical and categorical variables. To change the binning strategy, including the number of bins to use for numerical or categorical variables, reach out to your Domino representative. See link:21503d[Supported binning methods] for more details on binning.

[[validate-setup]]

== Validate your Setup

Domino captures prediction data in a Domino Dataset in hourly batches.

Validate your setup without waiting for the Scheduled Check to run:

[[tr2]]
// Validate setup of integrated monitoring for a Domino endpoint

. On the Endpoint's Monitoring tab, go to *Configure Monitoring > Target Ranges*.
. If the Configure Tests and Thresholds page populates with data, then your setup is complete. See link:f33b63[Analyze Data Drift] to learn how to configure tests and thresholds.

----- user_guide/model-monitoring/3-set-up-domino-endpoints/3-set-up-model-qual-monitor.txt -----
:page-version: 6.1
:page-title: Set up model quality monitoring
:page-sidebar: Model quality monitoring
:page-permalink: ce3835
:page-order: 30

Ground truth data consists of known correct outputs for each input. Model quality monitoring compares the model's predicted values against the actual results (or labels for the predictions) using ground truth data to generate quality metrics.

[[tr1]]
// Ingest Ground Truth data for a Domino endpoint with integrated monitoring

After you set up your endpoint to link:93e5c0[capture prediction data], you can ingest the model's ground truth data to monitor the quality of predictions made by the model. For this analysis, you must have declared a `row_identifier` column for the prediction data schema. This `row_identifier` column must also be present in the ground truth data to map a prediction uniquely to the ground truth.

When you apply a date filter, Domino uses the timestamp values in the prediction data to filter the results. Then, it matches the filtered predictions with the ground truth labels ingested in the last 90 days and calculates the model quality metrics for the matched predictions.

[[ingest-gtd]]
== Ingest ground truth data

[[tr]]

You can use the Domino application or a REST API to ingest ground truth data. For details about using the REST API, see link:a94c1c[GET /v2/api/model/{model_id}/traffic/ground-truth].

. In the navigation pane, click *Model Monitor*.
. Click the model for which you want to ingest ground truth data.
. Click *Model Quality*.
. On the Model Quality page of the registered model, click *Register Ground Truth < Upload Ground Truth Config*.
+
In the Register Ground Truth window, you can upload a Ground Truth Config file or paste a JSON config. This JSON must capture all the information needed to register ground truth data.
+
The following is a sample file. Your file must define the `variableType` as `ground_truth`. Do not include any other variableTypes. The configuration file must always include the `datasetDetails` and `modelMetadata` information. See link:bb88ca[Monitoring config JSON] for details about each field in the JSON.
[[tr2]]
// Configuration file content in documentation is correct
+
[source,console]
----
{
"variables": [
  {
    "name": "y_gt",
    "variableType": "ground_truth",
    "valueType": "categorical",
    "forPredictionOutput": "y"
  }
],
"datasetDetails": {
  "name": "GT-labels.csv",
  "datasetType": "file",
  "datasetConfig": {
    "path": "GT-labels.csv",
    "fileFormat": "csv"
  },
  "datasourceName": "abc-shared-bucket",
  "datasourceType": "s3"
}
}
----

. Click *Register Ground Truth*. On the Model Quality Analyze page, the Model Monitor starts to calculate model quality metrics for this dataset.

. Click the bell icon to disable the alerts. Doing this reduces noise by controlling the metrics included in the Scheduled Checks.

=== Check the status of ground truth data

After you register the ground truth data for ingestion, you might want to check its status.

. From the navigation pane, click *Model Monitor*.
. Click the name of the model for which you set up ground truth data ingestion.
. Click *Ingest History* to check to see if the status is Done.


After the data is ingested, you can perform an ad-hoc link:d4f9b7[model quality analysis] to validate your setup. Then, link:4da05b[set up a schedule] to perform model quality analysis automatically.

----- user_guide/model-monitoring/3-set-up-domino-endpoints/4-set-up-cohort-analysis.txt -----
:page-version: 6.1
:page-title: Set up Cohort Analysis for Domino endpoints
:page-sidebar: Cohort Analysis
:page-permalink: b1b1ca
:page-order: 40

Cohort analysis provides information about your model, including feedback about what might be going wrong with it. Cohort analysis is a job that runs on the model's prediction and ground truth data for a regression model. The result is a PDF report and raw data in JSON format.

To use Cohort Analysis, enable data analysis for each regression model. Doing this for a model starts a series of ingest and analysis jobs.

[NOTE]
====
This feature is only available for regression models with numerical features.
====

== Prerequisites

* link:93e5c0[Set up prediction data capture]
* The model is link:8dbc91#Deploy-a-model[published].
* The Domino endpoint is running.
* link:ce3835[Ground truth data] is available.
* The model is a regression type model

== Set up Cohort Analysis for a Domino endpoint

[[tr1]]
// Set up Cohort Analysis for a Domino endpoint

. From the navigation pane, click *Domino endpoint*.
. Click the name of the model for which you want to set up Cohort Analysis.
. Click *Grafana Monitoring*.
. Click *Configure Monitoring > Data*.
. On the Configure Data page, click *open this model* to open the configuration in the Model Monitor.

. Click *Register Ground Truth Data > Upload Ground Truth Config*.
. In the Register Ground Truth window, click *Register with Cohort Analysis*.


[[di-status]]

== Check the status of the data ingestion

After you set up the data analysis for a regression model, you might want to check its status.

. From the navigation pane, click *Model Monitor*.
. Click the name of the model for which you set up Cohort Analysis.
. Click *Ingest History* to check to see if the status is Done.

After the data is ingested, Domino finds underperforming cohorts and the features that make those cohorts distinct from the rest of the data.

== Configure the Cohort Analysis

You can configure the cohort analysis for a model to customize the report.

[[tr2]]
// Configure the cohort analysis for a Domino endpoint

=== Prerequisites

* Check the status of the link:b1b1ca#di-status[data ingestion] to confirm the data analysis is complete.

=== Configure the Cohort Analysis

. In the navigation pane, click *Projects*.
. Click the *DominoCohortAnalysis* project. The DominoCohortAnalysis Project is created automatically as a private project under the model's project owner's account.
. In the navigation pane, click
*Code*.
. Click `config.yaml` and then click *Edit*.
+
You can configure the following parameters:
+
min_k::
  The minimum number of cohorts.
max_k::
  The maximum number of cohorts.
max_samples_for_clustering::
  The maximum number of samples to use to find cohorts
num_bins::
  The number of bins to use to compute the feature histograms and the contrast score.
max_num_top_cohorts_for_report::
  The maximum number of cohorts to show in the Cohort Summary and Detailed Cohort Analysis sections of the Cohort Analysis report.
max_num_top_cohorts_for_report::
  The maximum number of features to show per cohort in the Detailed Cohort Analysis section of the Cohort Analysis report.

. Click *Save*.
. In the navigation pane, click *Jobs*.
. Select the *cohort_analysis* job and click *Run* to generate new results.

See the additional link:71d6ad#cohort-analysis-options[Cohort Analysis options] in the _Administration Guide_.

----- user_guide/model-monitoring/3-set-up-domino-endpoints/5-set-up-notifications.txt -----
:page-version: 6.1
:page-title: Domino endpoint notifications
:page-sidebar: Notifications
:page-permalink: 287438
:page-order: 50

A scheduled check is an automated instance of a running drift or model quality computation over a range of data. You can view the results on the monitoring dashboard, and automated email notifications are sent if thresholds are breached.

[[tr1]]
// Configure Monitoring notifications for a specific Domino endpoint

This topic describes how to configure notifications.

== Set up notifications for a specific Domino endpoint:

. In the navigation pane, click *Endpoints*.
. Click the name of the model for which you want to set up notifications.
. Click *Grafana Monitoring*.
. Click *Configure Monitoring > Alert Recipients*.
. Type or paste a list of comma-separated or semicolon-separated email addresses.
. Click *Save*.

----- user_guide/model-monitoring/3-set-up-domino-endpoints/6-set-scheduled-checks.txt -----
:page-version: 6.1
:page-title: Domino endpoint scheduled checks
:page-sidebar: Scheduled checks
:page-permalink: 4da05b
:page-order: 60

Use Scheduled Checks to ensure that you are notified if data drift or model quality metrics degrade beyond the threshold for any period. For data drift, Domino uses the timestamps of the predictions to select data for the scheduled checks. For model quality, Domino uses the timestamp of the prediction data to select the prediction and ground truth data on which to run periodic model quality checks. You can specify how often to repeat the checks and the time range of the data to be used for calculations for those checks. When a check fails, emails are sent. See link:287438[Set up Notifications].


[[tr1]]
// Configure Data Drift Monitoring scheduled checks for a specific Domino endpoint

[[tr2]]
// Configure Model Quality Monitoring scheduled checks for a specific Domino endpoint

== Prerequisites

You must have ground truth data available for the date ranges selected for the Schedule Checks. To do this, set up the link:8c7833[data source] and notify Domino of new link:ce3835#ingest-ground-truth-data[ground truth data].

== Create a scheduled check

. From the navigation pane, click *Endpoints*.
. Click the name of the model for which you want to set up scheduled checks.
. Click *Grafana Monitoring*.
. Go to *Configure Monitoring > Schedule*.
. Type a name for the check.
. Set up the frequency at which the check must run.
. In the *Select Data to Check* area, select one of the following:
+
[CAUTION]
====
* When you make a selection, remember that you must update the data manually and ensure that there is enough time for the data to be ingested. For example, if you set the check to repeat every day at 11 PM but you load new ground truth data at 10:45 PM, this might not be enough time for the data to be ingested.

* If you select *Use new data since last check*, the system assumes new data was ingested. If none was ingested, you will see an empty dashboard for the specified date and time on the Analyze tab.
====

** *Use new data since last check time*

*** For data drift, this checks predictions with timestamps later than the last scheduled check.
*** For model quality, this checks only ground truth labels ingested into the Model Monitor after the last scheduled check ran and matches them with historical predictions made by the model.

** *Data since last x <time period>*

*** For data drift, this checks predictions whose timestamps are within the last specified interval (for example, the last three days). For model quality, this checks only the ground truth labels ingested within the last specified interval (for example, within the last three days), and matches them with historical predictions made by the model.

. Click *Save*.

Domino automatically creates data in a Domino Dataset named `prediction_data` for every project. When you link:74f8ed#reproduce-the-environment[reproduce a workspace], this data is included. Predictions are in Parquet format and are updated hourly as the Domino endpoint processes inputs. All the data captured is stored in the file and populated in a workspace so you can diagnose flaws in the model and retrain the model as needed.

----- user_guide/model-monitoring/3-set-up-domino-endpoints/index.txt -----
:page-version: 6.1
:page-title: Set up monitoring for Domino endpoints
:page-sidebar: Monitoring for Domino endpoints
:page-permalink: 2a7c3b
:page-order: 30

//test links

You must set up model monitoring for each Domino endpoint that you want to monitor.

== Prerequisites


* When you link:9c4dec[trained the model^], you registered or used a Domino Training Set in the same project from which you're looking to publish a Domino endpoint.
* If you will perform Model Quality analysis, set up your data sources for ground truth data. See link:8c7833[Connect a Data Source].

=== Set up monitoring for Domino endpoints

The following topics explain the steps:


. link:93e5c0[Set up Prediction Data Capture]. Prediction data is required for monitoring both data drift and model quality.

. link:8dbc91#Deploy-a-model[Publish the Domino endpoint]. Domino starts to log prediction data (and convert it to Parquet files), but it does not yet produce monitoring data.

. link:c97091[Set up Drift Detection]. Register a training set to monitor data drift.

* Optional: Configure link:287438[Set up Notifications] or change the link:4da05b[scheduled checks].

. link:c97091#validate-setup[Validate your Setup]. Confirm that your predication data is being captured.

. link:ce3835[Set up Model Quality Monitoring]. Ingest ground truth data to monitor the quality of the model's predictions.

* Optional: Configure link:287438[Set up Notifications] or change the link:4da05b[scheduled checks].

. link:b1b1ca[Set up Cohort Analysis]. Get insights into model quality so you can find underperforming data hotspots for model remediation.

----- user_guide/model-monitoring/4-set-up-model-monitor/1-install-packages-for-model-monitoring.txt -----
:page-version: 6.1
:page-permalink: be1fd2
:page-title: Install packages for Model Monitoring
:page-sidebar: Packages for Model Monitoring
:page-order: 10



Model Monitoring in Domino 5.0 requires packages that are specific to the 5.0 release.
When you upgrade to Domino 5.0 from a previous release, a 5.0 DSE with these packages is created automatically, but it is not the default Environment.

To take advantage of features in the 5.0 release, you can either link:a8e081#compute-environment[configure your projects to use the 5.0 DSE] or add individual 5.0 packages to your existing environments.

This topic explains how to add 5.0 packages to your existing environments.

== Install the Domino Data Capture client

You must use the Domino Data Capture client for integrated model monitoring in Domino 5.0.
Install it as follows:

[source,python]
----
RUN pip install domino-data-capture
----

[source,r]
----
RUN R -e "install.packages(c('DominoDataCapture'))"
----

=== Install the Domino Data API

The link:f35c19[Domino Data API] currently supports Python only.
Install it as follows:

[source,shell]
----
RUN python -m pip install "dominodatalab[data]"
----

----- user_guide/model-monitoring/4-set-up-model-monitor/10-model-permissions.txt -----
:page-version: 6.1
:page-permalink: a1056b
:page-title: Configure Model permissions
:page-order: 100

By default, models registered for monitoring are viewable and editable by all licensed Domino users.
The model owner can choose to make a model private, and limit access to users that have been explicitly added as collaborators.

Model permissions can only be configured by the owner of the model or a Domino administrator.

Start by navigating to the model permissions page:

. In the navigation pane, click *Model Monitor*.
. Click the name of the model for which you want to configure permissions.
. Click *Permissions*.

[NOTE]
====
Monitoring permissions for link:8dbc91[models deployed as REST APIs] can be configured from the link:8dbc91#Sharing-and-collaboration[Domino endpoint collaborator settings].
====

.Make a model private:
* Turn off the *Enable access for all licensed Domino users* option.
+

.Add collaborators:
. Find and select the desired user(s) by typing usernames into the search box.
. Select between *Viewer* and *Editor* access for the user(s) being added.
. Click the *Add* button.

.Change collaborator role:
. Find the desired collaborator in the *Current Collaborators* list.
. In the *Role* column, select the desired role from the dropdown.

.Remove a collaborator:
. Find the desired collaborator in the *Current Collaborators* list.
. Click the *Delete* icon.

[[tr1]]

----- user_guide/model-monitoring/4-set-up-model-monitor/11-unregister-a-model.txt -----
:page-version: 6.1
:page-title: Unregister a Model
:page-permalink: c2eba7
:page-order: 110

If you must modify the link:bb88ca[Monitoring Config JSON], you can unregister the model. Make the necessary corrections to Config JSON and then link:d1f8bb[register the model] again.

. From the navigation pane, click *Models*.
. On the Models page, go to the model that you want to unregister.
. Click the three vertical dots at the end of the model's row, and click *Unregister*.
. Click *Yes* to confirm your action. The model and its checks are removed from monitoring.

----- user_guide/model-monitoring/4-set-up-model-monitor/2-connect-a-data-source.txt -----
:page-version: 6.1
:page-title: Connect a Data Source
:page-permalink: 8c7833
:page-order: 20

Model monitoring ingests and processes a model's training, prediction, and ground truth data to monitor the model. You must set up the data source from which the data is read. After you set up the data source, it is available to all users and can be used in any model.

You can have multiple data sources linked to the model monitor, and multiple instances for each data source type.

[NOTE]
====
Enable *read* and *list* access for each data source.
====

The Model Monitor supports data from the following sources:

* Amazon S3
* Generic S3
* Azure Blob
* Azure Data Lake Gen 1
* Azure Data Lake Gen 2
* Google Cloud Storage
* HDFS
* Snowflake

== Connect a Data Source

. If you haven't set up a data source, go to the *Data* page (from the navigation pane, go to *Model Monitor > Monitoring Data Sources*).
. Click *Add Data Source*.
. Complete the details as needed. The following describes source-specific configurations:
+
[cols="2a,2a,4a",options="header"]
|===
^|Data source ^|Required fields ^|Authentication
|Amazon S3 |
Data Source Name

S3 Bucket Name

S3 Region
|
If the S3 buckets can be authenticated to using IAM roles, select the *Load Credentials from IAM Role* attached to the instance checkbox.

Enter an *AWS Access Key* and an *AWS Secret Key*.

|Generic S3 |
Data Source Name

S3 Bucket Name

S3 Endpoint
|Enter an *Access Key* and a *Secret Key*.

|Azure Blob Store |
Data Source Name

Account name

Container

|
Access Key

Saas Token

|Azure Data Lake Gen 1 |
Data Source name

Container

|
Select the *Authentication Type*:

. If you select *Client Credentials*, enter:

* Token Endpoint

* Application ID

* Secret Key

. If you select *Managed Identity* , you can enter an optional *Port Number*.

This method applies when the Model Monitor is deployed on Azure VMs configured with service identities that can access Azure Data Lake.

|Azure Data Lake Gen 2 |
Data Source name

Account Name

Container

|
Select the *Authentication Type*:

. *Shared Key*.

. *Client Credentials*. Then, enter:

* *Endpoint*

* *Client ID*

* *Client Secret*

. *Username Password*. Then, enter:

* *Endpoint*

* *Username*

* *Password*

. *Refresh Token*. Then, enter:

* *Refresh Token*

* *Client ID*

. *Managed Identity*. This method applies when the Model Monitor has been deployed on Azure VMs configured with service identities that can access Azure Data Lake Gen2. Then, you can enter the following optional information:

* *Tenant ID*

* *Client ID*


|Google Cloud Storage |
Data Source Name

Bucket

|JSON Key File
|HDFS |
Data Source Name

Host

Port (optional)

|

|Snowflake
|All fields
|Enter the following:
[[tr7]]
// Configure Snowflake Data source

. *Account URL* which uniquely identifies the Snowflake account in your organization.
. Enter your *Username* and *Password*.
. In *Database*, enter the name of the Snowflake database that contains the data.
. In *Schema*, enter the name of the active schema for the session.
. In *Warehouse*, enter the name of a compute resource cluster that provides the resources in Snowflake.
. Select a *Role*.
+
NOTE: All fields must match Snowflake's requirements for https://docs.snowflake.com/en/sql-reference/identifiers-syntax.html[object identifiers^].
|===


. Click *Add*.

[[tr1]]

// Add to S3 Data source

[[tr2]]

// Add to Azure Blob

[[tr3]]

// Add to Azure Data Lake Gen 1

[[tr4]]

// Add to Azure Data Lake Gen 2

[[tr5]]

// Add to Google Cloud Storage

[[tr6]]

// Add to HDFS

----- user_guide/model-monitoring/4-set-up-model-monitor/3-register-a-model.txt -----
:page-version: 6.1
:page-title: Register a Model
:page-permalink: d1f8bb
:page-order: 30

Before monitoring a model, it must be registered so that Domino captures information about the model through a link:bb88ca[Monitoring Config JSON]. Use the JSON config to describe the model's schema, as well as the source of the training data, and metadata about the model. For models with many features, you can include only those features that you want to monitor.

. From the navigation pane, click *Model Monitor*.
. From the Models page, click *Register Model > Upload Model Config File*.
. In the Register Model window, upload or paste a JSON configuration that describes the features to monitor, the location of the training dataset, and the model metadata.
+
The following is a sample file. Your file must define all features that you want to monitor for drift in this configuration. These features must be specified as `“variableType”: “feature”`. Other variableTypes are optional when registering a model, although `sample_weight` and `prediction_probability` don't apply. The configuration file must always include the datasetDetails and modelMetadata information.
+
See link:bb88ca[Monitoring Config JSON] for details about each field in the JSON.
+
[source,console]
----
{
 "variables": [
   {
     "name": "age",
     "valueType": "numerical",
     "variableType": "feature"
   },
   {
     "name": "y",
     "valueType": "categorical",
     "variableType": "prediction"
   },
   {
     "name": "date",
     "valueType": "datetime",
     "variableType": "timestamp"
   },
   {
     "name": "RowId",
     "valueType": "string",
     "variableType": "row_identifier"
   }
 ],
 "datasetDetails": {
   "name": "TrainingData.csv",
   "datasetType": "file",
   "datasetConfig": {
     "path": "TrainingData.csv",
     "fileFormat": "csv"
   },
   "datasourceName": "abc-shared-bucket",
   "datasourceType": "s3"
 },
 "modelMetadata": {
   "name": "test_psg",
   "modelType": "classification",
   "version": "2",
   "description": "",
   "author": "testadmin"
 }
}
----

. Click *Register Model* and the model is listed on the Models page.

[[tr1]]

// Register a Model for monitoring.

[[tr2]]

// Register model with default code.

----- user_guide/model-monitoring/4-set-up-model-monitor/4-set-up-drift-detection-mm.txt -----
:page-version: 6.1
:page-title: Drift detection for monitored models
:page-sidebar: Set up drift detection
:page-permalink: 86bc1f
:page-order: 40


link:93e5c0#capture-pd[Prediction data] is the output produced by a model. You must add data that was trained on historical data to your model's code to start ingesting and storing prediction data for monitoring.

NOTE: You can register prediction data for a model that uses a Snowflake data source once per table.

[[tr4]]
// Snowflake datasource for Prediction data with timestamp
If you registered a `timestamp` variable or if you register it with the ground truth dataset, Domino will automatically retrieve new data every 24 hours at 12:00 AM UTC. See link:bb88ca[Monitoring Config JSON].
When you deregister the model, Domino stops retrieving the data.

[[tr5]]
// Snowflake datasource for Prediction data without timestamp
If you don't register the `timestamp` variable, the prediction data is ingested once, and for every new batch, you must register a new dataset.

. Click the registered model and go to the Data Drift page.
+
NOTE: If you want to do this from the Overview page, go to *Add Data* > *Prediction Data*.

. Click *Register Prediction < Upload Prediction Config*.
. In the Register Prediction window, upload or paste a JSON configuration that describes the prediction variables and the location of the prediction dataset.
+
The following is a sample file. This file doesn't require any variable information. You can include prediction variableTypes, although, you typically include these when you define your training data to get a complete set of metrics. If you define the variables here, you will only get drift predictions. You can also define the following variables: `sample_weight`, `prediction_probability`, and `timestamp`. The configuration must always include the `datasetDetails` and `modelMetadata` information.
+
See link:bb88ca[Monitoring Config JSON] for details about each field in the JSON.
+
[source,console]
----
{
 "variables": [
   {
     "name": "campaign",
     "variableType": "sample_weight",
     "valueType": "numerical"
   }
 ],
 "datasetDetails": {
   "name": "Predictions.csv",
   "datasetType": "file",
   "datasetConfig": {
     "path": "Predictions.csv",
     "fileFormat": "csv"
   },
   "datasourceName": "abc-shared-bucket",
   "datasourceType": "s3"
 }
}
----

. Click *Register Prediction* and the Model Monitor will start calculating drift on the registered dataset.

[[tr1]]

// Register prediction data to the Model for monitoring.

[[tr2]]

//Register Prediction with the default code.


== Check the status of prediction data

After you add the prediction data, you might want to check its status.

. From the navigation pane, click *Model Monitor*.
. Click the name of the model for which you set up ground truth data ingestion.
. On the Overview page's *Data Traffic* tab, you can see the history of the ingested data and review the model's metadata and schema.
. Click *Ingest History* to check to see if the status is `Done`. The following are other statuses you might see:
`Completed with Warnings`:: Click the three vertical dots at the end of the row and click *View Details*.

`Failed`:: Click the three vertical dots at the end of the row and click *View Details*.
+
If the ingestion failed, Domino stops the continuous ingestion. After you fix the issue, re-register the prediction data and Domino restarts continuous ingestion from this point.
+
NOTE: If you re-register the same dataset, Domino processes the entire table again. This causes duplicate records which affects the drift and model quality calculations.

`Skipped`:: The dataset had no rows to fetch.
+
You can click *Status* to select checkboxes to filter by status.
. Click *Refresh* to see a graph of the *Data Ingest Timeline*.
+

After the data is ingested, you can perform an ad-hoc link:f33b63[model quality analysis] to validate your setup. Then, link:afc767[set up a schedule] to perform model quality analysis automatically.


[[analyze-data-drift]]
==  Analyze data drift

After the drift calculations are complete, you can see the divergence value for each feature in the Drift column. The computation used the default *Test Type* and *Threshold* values.

. To access this page, in the navigation pane, click *Model Monitor*.
. Go to the model whose data drift you want to see and then click *Data Drift*.
+
[NOTE]
====
To change the default settings to experiment with other Test Types, Conditions, and Default Threshold Values, in the navigation pane click *Settings*. Then, click link:66f1f0[Test Defaults], make your changes and click *Save*.
====
+
image::/images/5.0/data-drift-labeled.png[alt="The Data Drift page labeled with A, B, and C"]
+
A - Use the Date Filter to refine the data and review the Drift and its trends for specific periods.
+
B - If your model had a timestamp column declared, it's used to get the timestamp of different predictions in the dataset. If the timestamp wasn't declared, then the data's ingestion time in the Model Monitor is used as its timestamp.
+
C - To reduce alert noise, click the bell icon next to features to disable alerts to exclude them from the Scheduled Checks.
+
D - If you made changes to the test types, thresholds, and so on and want to save them as the default configuration for the model to be used for running scheduled checks, go to *Other Actions > Save Checks Config* to save the changes. If you experimented with configurations and wanted to load the model's default configuration to reset the Analyze table, go to *Other Actions > Restore Checks Config*.

[[tr3]]

// Analyse the data drift of a Model with training and prediction data

----- user_guide/model-monitoring/4-set-up-model-monitor/5-set-up-model-qual-monitor-mm.txt -----
:page-version: 6.1
:page-title: Set up model quality monitoring
:page-permalink: 3851db
:page-order: 50


Ground truth data consists of known correct outputs for each input. Model quality monitoring compares the model's predicted values against the actual results (or labels for the predictions) using ground truth data to generate quality metrics.

After you set up your model to link:93e5c0#capture-prediction-data[capture prediction data], you can ingest the model's ground truth data to monitor the quality of predictions made by the model. For this analysis, you must have declared a `row_identifier` column for the prediction data schema. This `row_identifier` column must also be present in the ground truth data to map a prediction uniquely to the ground truth.

When you apply a date filter, Domino uses the timestamp values in the prediction data to filter the results. Then, it matches the filtered predictions with the ground truth labels ingested in the last 90 days and calculates the model quality metrics for the matched predictions.

NOTE: You can register ground truth data for a model that uses a Snowflake data source once per table.

[[tr4]]
// Snowflake datasource for GT data with timestamp
If you registered a `timestamp` variable or if you register it with the ground truth dataset, Domino will automatically retrieve new data every 24 hours at 12:00 AM UTC.
See link:bb88ca[Monitoring Config JSON].
When you deregister the model, Domino stops retrieving the data.

[[tr5]]
// Snowflake datasource for GT data without timestamp
If you don't register the timestamp variable, the ground truth data is ingested once, and for every new batch, you must register a new dataset.

[[ingest-gtd-mm]]
== Ingest ground truth data

You can use the Domino application or a REST API to ingest ground truth data. See link:a94c1c[Model Monitoring APIs] for details.

. In the navigation pane, click *Model Monitor*.
. Click the model for which you want to ingest ground truth data.
. Click *Model Quality*.
+
NOTE: If you want to do this from the Overview page, go to *Add Data > Ground Truth Data*.
. On the Model Quality page of the registered model, click *Register Ground Truth > Upload Ground Truth Config*.
+
In the Register Ground Truth window, you can upload a Ground Truth Config file or paste a JSON config. This JSON must capture all the information needed to register ground truth data.
+

The following is a sample file. You must specify the target column, shown below as `y_gt` and the row identifier that maps to a corresponding prediction record, shown below as `gt_uuid`. Note that your prediction dataset needs to have a column that contains unique row identifiers that match. When you registered your prediction dataset with Domino, you also supplied a variable that denotes the `row_identifier` similarly. The configuration must always include the `datasetDetails` and `modelMetadata` information. See link:bb88ca[Monitoring Config JSON] for details about each field in the JSON.
+
[source,console]
----
{
"variables": [
  {
    "name": "y_gt",
    "variableType": "ground_truth",
    "valueType": "categorical",
    "forPredictionOutput": "y"
  },
  {
     "name": "gt_uuid",
     "variableType": "row_identifier",
     "valueType": "string"
  }
],
"datasetDetails": {
  "name": "GT-labels.csv",
  "datasetType": "file",
  "datasetConfig": {
    "path": "GT-labels.csv",
    "fileFormat": "csv"
  },
  "datasourceName": "abc-shared-bucket",
  "datasourceType": "s3"
}
}
----

. Click *Register Ground Truth*. On the Model Quality Analyze page, the Model Monitor starts to calculate model quality metrics for this dataset.

. Click the bell icon to disable the alerts. Doing this reduces noise by controlling the metrics included in the Scheduled Checks.

[[tr1]]

// Register ground truth data to Model.

[[tr2]]

// Register ground truth data with the default JSON

=== Check the status of ground truth data

After you register the ground truth data for ingestion, you might want to check its status.

. From the navigation pane, click *Model Monitor*.
. Click the name of the model for which you set up ground truth data ingestion.
. On the Overview page's *Data Traffic* tab, you can see the history of the ingested data and review the model's metadata and schema.
. Click *Ingest History* to check to see if the status is `Done`. The following are other statuses you might see:
`Completed with Warnings`:: Click the three vertical dots at the end of the row and click *View Details*.

`Failed`:: Click the three vertical dots at the end of the row and click *View Details*.
+
If the ingestion failed, Domino stops the continuous ingestion. After you fix the issue, re-register the prediction data and Domino restarts continuous ingestion from this point.
+
NOTE: If you re-register the same dataset, Domino processes the entire table again. This causes duplicate records which affects the drift and model quality calculations.

`Skipped`:: The dataset had no rows to fetch.
+
You can click *Status* to select checkboxes to filter by status.
. Click *Refresh* to see a graph of the *Data Ingest Timeline*.
+

After the data is ingested, you can perform an ad-hoc link:f33b63[model quality analysis] to validate your setup. Then, link:afc767[set up a schedule] to perform model quality analysis automatically.



[[tr3]]

// Analyse ground truth data.

----- user_guide/model-monitoring/4-set-up-model-monitor/6-set-up-cohort-analysis-mm.txt -----
:page-version: 6.1
:page-title: Set up Cohort Analysis for Model Monitor
:page-sidebar: Set up Cohort Analysis
:page-permalink: c1dce7
:page-order: 60

Cohort analysis provides information about your model, including feedback about what might be going wrong with it. Cohort analysis is a job that runs on the model's prediction and ground truth data for a regression model. The result is a PDF report and raw data in JSON format.

To use Cohort Analysis, you must register ground truth data to enable data analysis for each regression model. Doing this for a model starts a series of ingest and analysis jobs.

[NOTE]
====
This feature is only available for regression models with numerical features.
====

== Set up Cohort Analysis for this Model Monitor

=== Prerequisites

* The model is a regression type model
* link:3851db[Ground truth data] is available.

=== Set up Cohort Analysis
. From the navigation pane, click *Model Monitor*.
. Click the name of the model for which you want to set up Cohort Analysis.
. Click *Model Quality*.

. Click *Register Ground Truth Data > Upload Ground Truth Config*.
. In the Register Ground Truth window, click *Register with Cohort Analysis*.


[[di-status-mm]]
[[tr1]]

// Register ground truth data for Cohort Analysis

== Check the status of the data ingestion

After you set up the data analysis for a regression model, you might want to check its status.

. From the navigation pane, click *Deploy > Model Monitor*.
. Click the name of the model for which you set up Cohort Analysis.
. Click *Ingest History* to check to see if the status is Done.

After the data is ingested, Domino finds underperforming cohorts and the features that make those cohorts distinct from the rest of the data.

[[tr2]]

// Check the Ingest history for Ground truth and Cohort registration.

== Configure the Cohort Analysis

You can configure the cohort analysis for a model to customize the report.

=== Prerequisites

* link:c1dce7[Set up Cohort Analysis].
* Check the status of the link:b1b1ca#di-status[data ingestion] to confirm the data analysis is complete.

=== Configure the Cohort Analysis

. In the navigation pane, click *Projects*.
. Click the *DominoCohortAnalysis* project. The DominoCohortAnalysis Project is created automatically as a private project under the model's project owner's account.
. In the navigation pane, click
*Code*.
. Click `config.yaml` and then click *Edit*.
+
You can configure the following parameters:
+
min_k::
  The minimum number of cohorts.
max_k::
  The maximum number of cohorts.
max_samples_for_clustering::
  The maximum number of samples to use to find cohorts
num_bins::
  The number of bins to use to compute the feature histograms and the contrast score.
max_num_top_cohorts_for_report::
  The maximum number of cohorts to show in the Cohort Summary and Detailed Cohort Analysis sections of the Cohort Analysis report.
max_num_top_cohorts_for_report::
  The maximum number of features to show per cohort in the Detailed Cohort Analysis section of the Cohort Analysis report.

. Click *Save*.
. In the navigation pane, click *Jobs*.
. Select the *cohort_analysis* job and click *Run* to generate new results.

See the additional link:71d6ad#cohort-analysis-options[Cohort Analysis options] in the _Administration Guide_.

[[tr3]]

// Check Cohort Analysis reports in workbench jobs and files.

----- user_guide/model-monitoring/4-set-up-model-monitor/7-set-up-notifications-mm.txt -----
:page-version: 6.1
:page-title: Set up notifications
:page-permalink: 9ebe39
:page-order: 70

A scheduled check is an automated instance of a running drift or model quality computation over a range of data. You can view the results on the monitoring dashboard, and automated email notifications are sent if thresholds are breached.

This topic describes how to configure notifications.


== Prerequisites

Set up the SMTP server:

. In the navigation pane, click *Model Monitor*.
. Click *Settings* and then click *Notification Channels*.
. Click *Edit*.
. Type the SMTP details in the fields provided.
. To use SMTPS instead of SMTP+STARTTLS, switch the *Enable TLS* toggle to the off position.
. Click *Save Mail Config*.

[[tr1]]

//Configure global notification settings

== Configure global notification settings for monitored models

These notification settings are used for all monitored models that do not override them.

. In the navigation pane, click *Model Monitor*.
. Click the name of the model for which you want to set up notifications.
. Click *Notifications*.
. If you do not want to *Use email address from global settings*, switch the toggle to the off position. See link:386451[Notification Channels].
. Click *Edit*.
. In the *To Addresses* field, type or paste a comma- or semicolon-separated list of email addresses.
. Click *Save Mail Config*.

[[tr2]]

//User can configure model-level email notifications

== Override the global notification settings for a monitored model

To use different notification settings for specific models, you can override the global notification settings:

. In the navigation pane, click *Model Monitor*.
. Click *Settings* and then click *Notification Channels*.
. Click *Edit*.
. Type the SMTP details in the fields provided.
. Type or paste a comma- or semicolon-separated list of email addresses.
. To use SMTPS instead of SMTP+STARTTLS, switch the *Enable TLS* toggle to the off position.
. Click *Save Mail Config*.

CAUTION: The global list was set up by your administrator during installation. This procedure overrides the global default notifications for all Model Monitors.

== Send notifications with the API

In your project code, you can use the Domino API to send alerts or notifications.
The `link:8c929e#_sendMetricAlert[/api/metricAlerts/v1]` REST API endpoint sends email when any of your monitoring metrics falls outside the thresholds you have defined in your code.
Deploy the code in a link:942549[job] (for one-time execution) or as a link:5dce1f[scheduled job] (for continuous evaluation) to compute your model's metrics and send alerts.

Alerts are sent to the recipients configured in the global notification settings for monitored models, except for models that override those notification settings as described above.

NOTE: When notifications are sent using the API, Domino uses the link:71d6ad#_email_notifications[SMTP server for global notifications], _not_ the server configured for model monitoring notifications.

The code examples below show how to construct the link:8c929e#_MetricAlertRequestV1[payload] and send it to the `/api/metricAlerts/v1` endpoint.

NOTE: You need your link:d982cc[API key] to access this endpoint.

[source, python]
----
import requests
import json

url = "https://<DOMINO_HOST>/api/metricAlerts/v1"
api_key = "<DOMINO_API_KEY>"

payload = json.dumps({
  "modelMonitoringId": "62471956481e88a77ae91210",
  "metric": "testMetric",
  "value": 1.0,
  "targetRange": {
    "lowerLimit": 2.0,
    "upperLimit": 3.0,
    "condition": "between"
  }
})

headers = {
  'X-Domino-Api-Key': api_key,
  'Content-Type': 'application/json'
}

response = requests.request("POST", url, headers=headers, data=payload)
response.ok
----
[source,curl]
----
curl --location --request POST 'https://<DOMINO_HOST>/api/metricAlerts/v1' 
--header 'X-Domino-Api-Key: <DOMINO_API_KEY>' 
--header 'Content-Type: application/json' 
--data-raw '{
    "modelMonitoringId": "62471956481e88a77ae91210",
    "metric": "testMetric",
    "value": 1.0,
    "targetRange": {
        "lowerLimit": 2.0,
        "upperLimit": 3.0,
        "condition": "between"
    }
}'
----

The values for `condition` are:

* `lessThan`
* `lessThanEqual`
* `greaterThan`
* `greaterThanEqual`
* `between`

You can also add an optional `description` string to include in the notifications.

TIP: Make sure that `modelMonitoringId` refers to the _monitoring_ model ID and not the ID of the Domino endpoint.
To find the monitoring model ID, select your model in the Model Monitor and scroll down on the model's Overview page.


----- user_guide/model-monitoring/4-set-up-model-monitor/8-set-scheduled-checks-mm.txt -----
:page-version: 6.1
:page-title: Set scheduled checks
:page-permalink: afc767
:page-order: 80

In a production setting, APIs typically push prediction and ground truth data into the Model Monitor.

You can use Scheduled Checks to ensure that you are notified if data drift or model quality metrics degrade beyond the threshold for any period. For data drift, Domino uses the timestamps of the predictions to select data for the scheduled checks. For model quality, Domino uses the ingestion time of the ground truth labels to select data. You can specify how often to repeat the checks and the time range of the data to be used for calculations for those checks. When a check fails, email notifications are sent. See link:9ebe39[Set up Notifications].

== Prerequisites

For scheduled checks on data drift and model quality, link:86bc1f[prediction] and link:3851db[ground truth data] must be available periodically.

== Set scheduled checks
. From the navigation pane, click *Model Monitor*.
. Click the name of the model for which you want to set up scheduled checks.
. Click *Data Drift* or *Model Quality* and then click *Scheduled Checks*.
. Type a name for the check.
. Set up the frequency at which the check must run.
. In the *Select Data to Check* area, select one of the following:
+
[CAUTION]
====
* When you make a selection, remember that you must update the data manually and ensure that there is enough time for the data to be ingested. For example, if you set the check to repeat every day at 11 PM but you load new ground truth data at 10:45 PM, this might not be enough time for the data to be ingested.

* If you select *Use new data since last check*, the system assumes new data was ingested. If none was ingested, you will see an empty dashboard for the specified date and time on the Analyze tab.
====
** *Use new data since last check time*

*** For data drift, this checks predictions with timestamps later than the last scheduled check.
*** For model quality, this checks only ground truth labels ingested into the Model Monitor after the last scheduled check ran and matches them with historical predictions made by the model.
** *Data since last x <time period>*
*** For data drift, this checks predictions whose timestamps are within the last specified interval (for example, the last three days). For model quality, this checks only the ground truth labels ingested within the last specified interval (for example, within the last three days), and matches them with historical predictions made by the model.
. Click *Save*.

In the Model Monitor, you can click *Checks History* to see historical data about scheduled checks.

[[tr1]]

// Schedule check for Data drift.

[[tr2]]

// Schedule Check for Model Quality.

----- user_guide/model-monitoring/4-set-up-model-monitor/9-model-monitoring-tags.txt -----
:page-version: 6.1
:page-permalink: ae638c
:page-title: Model Monitoring tags
:page-order: 90

Model Monitoring tags help collaborators organize and discover monitored models in Domino. You could use tags to generate summary views of monitored models for a specific audience - like an "executive summary" for executives or "production" models for developers.

== Tag a model

You can add and remove model tags by going to *Model Monitor* > Select the model you want to edit > Click the *Edit* icon.

== Manage tags

Domino admins can manage the tags for the Domino Model Monitor through the link:a94c1c#_modeltag[Model Monitoring API]. 

Admins can archive a tag to remove it from all models with that particular tag.

[source,python]
----
import requests

headers = {"X-Domino-Api-Key": f"{API_TOKEN}"}

# Fetch available tags that can be added to a given model
response = requests.get(f"{DOMINO_URL}/model-monitor/v2/api/model/{DMM_MODEL_ID}/tags", headers = headers)
print(response.json())

# Archive the first tag from the list
tag_id = response.json()[0]['id']
requests.patch(f"{DOMINO_URL}/model-monitor/v2/api/model-tag/{tag_id}/archive", headers = headers)
----

== Limitations

Model Monitoring tags are distinct and cannot be used with link:3b6ae5[model registry tags] or link:f9be5e[Project tags].

----- user_guide/model-monitoring/4-set-up-model-monitor/index.txt -----
:page-version: 6.1
:page-title: Set up Model Monitor
:page-permalink: 679cc1
:page-order: 40

Use the Model Monitor to configure monitoring for models that aren't deployed on Domino endpoints or for those models deployed outside Domino.

The following topics explain the steps:

. link:be1fd2[Install packages for Model Monitoring]. Domino 5.0 requires packages that are specific to the 5.0 release.

. link:8c7833[Connect a Data Source]. Connect to external Data Sources to access your training, prediction, and ground truth data.

. link:d1f8bb[Register a Model]. Register external models so that Domino can capture information about them through their Monitoring Config JSON files.

. link:86bc1f[Set up Drift Detection]. Add data that was trained on historical data to your model's code to start ingesting and storing prediction data for monitoring.

* Optional: link:9ebe39[Configure notifications] or change the link:afc767[scheduled checks].

. link:86bc1f#analyze-data-drift[Analyze data drift]. See the divergence value for features and experiment with test types, thresholds, and other conditions.

* Optional: link:9ebe39[Configure notifications] or change the link:afc767[scheduled checks].

. link:ce3835[Set up Model Quality Monitoring]. Ingest ground truth data to monitor the quality of the model's predictions.

* Optional: link:9ebe39[Configure notifications] or change the link:afc767[scheduled checks].

. link:c1dce7[Set up Cohort Analysis]. Get insights into model quality so you can find underperforming data hotspots for model remediation.

. link:ae638c[Configure model tags] to add metadata to your Model for better organization and searchability.

. link:a1056b[Configure model permissions] to define who can view or edit your model.

. If you must modify the monitoring config JSON, you can link:c2eba7[unregister the model] and then link:d1f8bb[register it] again.

----- user_guide/model-monitoring/5-use-monitoring/1-access-monitor-dashboard.txt -----
:page-version: 6.1
:page-title: Access the Monitor dashboard
:page-permalink: 2f17e3
:page-order: 10

This topic describes how to access Domino's dashboard which shows all endpoints being monitored.

To see all models, in the navigation pane, click *Deploy > Model Monitor*.

You can see a list of all models, their health status, and prediction traffic. If one or more features for the drift value or model quality for a model doesn't meet the test criteria, the model status is marked in red to indicate the features have drifted beyond the threshold. You can hold your mouse over the red circle in the appropriate column to see more details about the issues:

You can set up notifications to alert on breached thresholds for link:287438[drift] or link:9ebe39[model quality].
// Marty -- I tried to re-work the above sentence as best I could. The two pages linked have identical titles yet very different content.

.See details for a specific model:
[#domino-endpoint-details]
.Domino endpoint monitoring
--
. In the navigation pane, click *Endpoints*.
. Click the endpoint whose dashboard you want to see.
. Click *Grafana Monitoring*. The dashboard opens and shows the Data Drift and Model Quality charts, if both are enabled.
--
[#model-monitor-details]
.Model Monitor
--
. In the navigation pane, click *Model Monitor*.
. Click the model whose dashboard you want to see.
. Click *Data Drift* or *Model Quality* to see the respective dashboard.
--
[[tr1]]

// Validate the Model dashboard

[[tr2]]

// Navigate to a specific domino endpoint's monitoring dashboard

[[tr3]]

// Navigate to specific model's dashboard in Model Monitor

----- user_guide/model-monitoring/5-use-monitoring/2-custom-metrics.txt -----
:page-version: 6.1
:page-permalink: 4ceec8
:page-title: Custom Model Monitoring metrics
:page-order: 20

Use Domino's Model Monitoring custom metrics Python SDK to define custom metrics and use them alongside out-of-the-box drift and model quality metrics that are monitored in Domino Model Monitor.
With this SDK, you can register new metrics and define the logic to compute them.
You can author this logic and evaluate it from within a Domino project.

For every model that you register for monitoring, you can select a registered metric, associate the data sources from which the metric is computed, and set up the execution environment to compute this metric on a periodic basis.
//Domino stores these metrics and presents them in dashboards.
You are notified by email when a metric behaves abnormally based on threshold definitions.

For end-to-end working code with a description of the workflow, see the `/custom metrics example` folder in the quick-start project.

== Use custom metrics

Use the following library functions from the custom metrics Python SDK to build and deploy your own monitoring metrics.

. Instantiate the client.
Enter your `DMM model ID`:
+
[source, python]
----
import domino
dmm_model_id = "XXXXXXXXXXXXX"
d = domino.Domino("integration-test/quick-start")
metrics_client = d.custom_metrics_client()
----

. Log the custom metrics:
+
[source, python]
----
metrics_client.log_metric(dmm_model_id, "accuracy", 7.1234, "2022-10-08T00:00:00Z", { "example_tag1" : "value1", "example_tag2" : "value2" })

metrics_client.log_metrics([
{ "modelMonitoringId" : dmm_model_id, "metric" : "accuracy", "value" : 7.1234,
"timestamp" : "2022-10-08T00:00:00Z",
"tags" : { "example_tag1" : "value1", "example_tag2" : "value2" }
]
},
{ "modelMonitoringId" : dmm_model_id, "metric" : "accuracy", "value" : 8.4567,
"timestamp" : "2022-10-09T00:00:00Z" }
])
----
+
* `modelMonitoringId`: ID of the monitored model to send metric alerts for
* `metric`: Name of the metric to send alert for
* `value`: Value of the metric
* `timestamp`: Timezone is in UTC in ISO 8601 format.
* `tags`: Custom metadata for metric represented as key-value string pairs

. Send a custom metrics alert:
+
[source, python]
----
metrics_client.trigger_alert(dmm_model_id, "metric_name", 3.14,
condition = metrics_client.BETWEEN,
lower_limit=2.0, upper_limit=3.0,
description = "Breached 2.0-3.0 range." )
----
+
* `modelMonitoringId`: ID of the monitored model for which to send metric alerts.
* `metric`: Name of the metric for which to send the alert.
* `value`: Value of the metric.
* `condition`: Target range for the metric defined by lower and upper limit bounds. +
The following are potential values for the `condition` argument:
** `metrics_client.LESS_THAN = "lessThan"`
** `metrics_client.LESS_THAN_EQUAL = "lessThanEqual"`
** `metrics_client.GREATER_THAN = "greaterThan"`
** `metrics_client.GREATER_THAN_EQUAL = "greaterThanEqual"`
** `metrics_client.BETWEEN = "between"`
* `lower_limit`: The lower limit for the `condition`.
* `upper_limit`: The upper limit for the `condition`.
* `description`: Optional message included in the alert.

. Retrieve the custom metrics:
+
[source, python]
----
res = metrics_client.read_metrics(dmm_model_id, "accuracy",
"2022-10-08T00:00:00Z", "2022-10-10T00:00:00Z" )
----
+
* `modelMonitoringId`: ID of the monitored model for which to retrieve the metric values.
* `metric`: Name of the metric for which to retrieve the metric values.
* `start_timestamp`: The start timestamp of the range when the metrics were logged. The timezone is in UTC in ISO 8601 format.
* `end_timestamp`: The end timestamp of the range when the metrics were logged. The timezone is in UTC in ISO 8601 format.

== Sample output

----
{
  'metricValues': [
      {'timestamp': '2022-10-08T00:00:00Z', 'value': 3.123, 'tags': []},
      {'timestamp': '2022-10-08T00:00:00Z', 'value': 3.123, 'tags': []},
      {'timestamp': '2022-10-08T00:00:00Z', 'value': 3.123, 'tags': []},
      {'timestamp': '2022-10-08T00:00:00Z', 'value': 3.123, 'tags': []},
{'timestamp': '2022-10-08T00:00:00Z', 'value': 7.1234, 'tags': {'example_tag1': 'value1', 'example_tag2': 'value2'}},
      {'timestamp': '2022-10-09T00:00:00Z', 'value': 8.4567, 'tags': []}
   ],
   'metadata': {'requestId': '97e61f63-3be5-4766-a07a-06c931d91268', 'notices': []}
}
----

.Results:

* `timestamp`: The time this metric was logged.
* `value`: The value of the metric.
* `tags`: Custom metadata for metric represented as key-value string pairs.

See also the link:8c929e#_custommetrics[custom metrics REST API endpoints].

----- user_guide/model-monitoring/5-use-monitoring/3-analyze-data-drift.txt -----
:page-version: 6.1
:page-title: Analyze data drift
:page-permalink: f33b63
:page-order: 30

Domino's monitoring feature uses the training data to calculate the probability distributions of all features and prediction columns to detect data drift for input features and output predictions of your model. It approximates these columns by creating bins, and then counting the frequency for each bin. This acts as the reference pattern.

The monitor feature uses prediction data to calculate the probability distribution, using the same bins, and then applies the specified statistical divergence or distance test to quantify the dissimilarity between the training and prediction distributions for each column.

By default, drift checks are scheduled to occur daily at 12 PM UTC.

[NOTE]
====
Prediction data is analyzed through 23:59 of the previous day. Data from the current day is not included. Domino reads the timestamps in the dataset if they are present; if not, it uses the ingestion timestamp.
====

== Prerequisites

* Model is link:d1f8bb[registered] for monitoring
* link:93e5c0[Prediction data] is available for Domino to monitor


[[modify-the-data-drift-analysis]]
== Modify the data drift analysis

Out-of-the-box, the Model Monitor supports several statistical tests. Each feature can have a different test.
When a new model is registered, it inherits the link:66f1f0[global default test settings].
However, you can change the test types and thresholds to values suitable for each model.

If you save these changes, they will be used for subsequent automated checks, such as when new predication data is uploaded or when Scheduled Checks run for the model.

[[modify-the-test-features]]
=== Modify the test features

* If you are using the monitoring feature for a Domino endpoint, go to *Configure monitoring > Target Ranges* to access the Configure Tests and Thresholds page.
* If you are using the Model Monitor, in the navigation pane, click *Model Monitor*. Select the model that you want to monitor and then click *Data Drift*.

image::/images/5.0/drift-labeled.png[alt="The Data Drift page is labeled with the letters A through E"]

[NOTE]
====
If your model had a timestamp column declared, it's used to get the timestamp of different predictions in the dataset. If the timestamp wasn't declared, then the data's ingestion time in the Model Monitor is used as its timestamp.
====

A - Use the *Date Filter* to refine the date range and review the Drift and its trends for specific periods.

B - To reduce alert noise, click the bell icon next to features to disable alerts to exclude them from the Scheduled Checks.

C - Select the *Test Type*, the *Acceptable Range*, and the *Threshold*.
See link:#Data-drift-statistical-tests[Data drift statistical tests] for more information about test types.

Click *Recalculate* to see the results.

D - If you made changes to the tests and want to set them as the defaults for the model in subsequent automated checks, go to *Other Actions > Save Checks Config* to save the changes.

E - If you experimented with changes to the tests and wanted to load the model's default configuration to reset the table, go to *Other Actions > Restore Checks Config*.

If you are using the monitoring feature for a Domino endpoint, click the X in the top right corner to close the page when you are finished.

[[tr1]]

// Modify data drift features.

[[tr2]]

// Test Save Checks config

[[tr3]]

// Test Restore Checks config

[[Data-drift-statistical-tests]]
== Data drift statistical tests

Out-of-the-box, the Model Monitor supports several statistical tests.

[NOTE]
====
The lower the Population Stability Index (PSI) value, the better the prediction data matches the training data. If there is no difference between the data sets, the value is 0. Typically:

* PSI < 0.1 = no significant change
* PSI < 0.2 = moderate change
* PSI >= 0.2 = significant change
====

Kullback–Leibler Divergence (Recommended):: Kullback–Leibler divergence (also called relative entropy) is a measure of how one probability distribution is different from a second, reference probability distribution.
+
The divergence can range from zero to infinity. A value of zero means there is no difference between the data sets.
+
This is a robust test that works for different distributions and therefore is most commonly used to detect drift.


Chi-square Statistic::
  Chi-square test in another popular divergence test well-suited for categorical data.
+
The chi square statistic is a statistical hypothesis testing technique to test how two distributions of categorical variables are related to each other. Specifically, the chi-square statistic is a single number that quantifies the difference between the observed counts versus the counts that are expected if there was no relationship between the variables at all.
+
The divergence can range from zero to infinity. A value of zero means there is no difference between the data sets.
Population Stability Index::
  Population Stability Index (PSI) is a popular metric in the finance industry to measure changes in distribution for two datasets. It produces less noise and has the advantage of a generally accepted threshold of 0.2-0.25.

[[tr4]]

// Comapare the drift calculated for feature with golden data set.

----- user_guide/model-monitoring/5-use-monitoring/4-analyze-model-quality.txt -----
:page-version: 6.1
:page-title: Analyze model quality
:page-permalink: d4f9b7
:page-order: 40

The monitoring feature uses the row identifiers in the prediction data and ground truth data to match the predicted and expected value. Based on those matches, it calculates the model quality metrics.

By default, model quality checks are scheduled to occur daily at 12 PM UTC.

[NOTE]
====
Prediction data is analyzed through 23:59 of the previous day. Data from the current day is not included. Domino reads the timestamps in the dataset if they are present; if not, it uses the ingestion timestamp.
====

== Prerequisites


* link:93e5c0[Set up Prediction Data]

* link:8dbc91[Publish the Model]

* link:86bc1f[Set up Drift Detection]

* link:3851db[Set up Model Quality Monitoring]

* link:c1dce7[Set up Cohort Analysis]

== Modify the Model Quality Analysis

Classification models and regression models show different prediction quality metrics. See link:#metrics-for-classification-and-regression-models[Metrics for classification and regression models] for details.

If you are looking at a classification model, a confusion matrix and classification report for the data in the selected time range is shown in a Charts section under the metrics table. See link:#classification-models[classification models] for details.

For classification models:

* Model monitoring uses link:#weighted-method[weighted method] to calculate the metrics.

* AUC, ROC, Log Loss, and Gini Norm are only calculated if the Predication Probability type is declared as part of the schema. See the link:bb88ca#forPredictionOutput[Note] in link:bb88ca[Monitoring Config JSON].
* Sample Weights is used in calculations for the Gini Norm metric only.


[[modify-the-test-features]]
=== Modify the test features

* If you are using the monitoring feature for a Domino endpoint:

** Go to *Configure monitoring > Data*.
** In the Configure Data dialog, in the Ground Truth Data section, click *open this model*.
* If you are using the Model Monitor, in the navigation pane, click *Model Monitor*. Select the model that you want to monitor and then click *Model Quality*.
+
image::/images/5.0/model-quality-labeled.png[alt="Model Quality page labeled from A-E"]
+
A - Use the *Date Filter* to refine the date range and review the Model Quality and its trends for specific time periods.
+
B - To reduce alert noise, click the bell icon next to a feature to disable alerts and exclude it from the Scheduled Checks.
+
C - Select the *Acceptable Range* and the *Threshold*. Click *Recalculate* to see the results.
+
D - If you made changes to the tests and want to set them as the defaults for the model in subsequent automated checks, go to *Other Actions > Save Checks Config* to save the changes.
+
E - If you experimented with changes to the tests and wanted to load the model's default configuration to reset the table, go to *Other

[[tr1]]

// Analyse Model quality metrics

[[metrics-for-classification-and-regression-models]]
=== Metrics for classification and regression models

[[classification-models]]
==== Classification models

The following are the supported prediction quality metrics for Classification models. All metrics are calculated using the weighted average method with the weight relating to the proportion of occurrences of a prediction class in the dataset.

[[weighted-method]]

Accuracy::
  Accuracy is the most common metric used to evaluate the performance of a classification model. It is calculated as the proportion of examples in the evaluation dataset that were predicted correctly, divided by all predictions that were made on the evaluation dataset.
  +
  A confusion matrix represents the model's true positives, true negatives, false positives, and false negatives for each prediction class. The classification accuracy can be calculated from this confusion matrix as the sum of true positives and true negatives divided by the sum of the true positives, true negatives, false positives and false negatives.
Precision::
  Precision quantifies the number of positive identifications that were actually correct. It is defined as the ratio of true positives to the sum of true positives and false positives
Recall::
  Recall quantifies the proportion of positive identifications was actually correct. It is defined as the ratio of true positives to the sum of true positives and false negatives
F1::
  The F(n) score allows us to give precision or recall a higher weight based on what is important to the use case being evaluated. For F1, both precision and recall are given equal weight and is calculated as the harmonic mean of the precision and recall.
  +
  The formula for F1 is (2 * precision * recall) / (precision + recall)
ROC::
  An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters: true positive rate on the y-axis and false positive rate on the x-axis. Typically, lowering the classification threshold results in increasing both false positives and true positives and the curve is used to decide a classification threshold that is a good tradeoff between the two.
AUC::
  AUC stands for Area under the ROC curve and measures the 2D area under the ROC curve. The AUC provides an aggregate measure of performance across all possible classification thresholds. AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0 and one whose predictions are 100% correct has an AUC of 1. The AUC is desirable as it measures the quality of the model's predictions irrespective of what classification threshold is chosen.
Log Loss::
  Log-loss is indicative of how close the prediction probability is to the corresponding actual/true value (0 or 1 in case of binary classification). The more the predicted probability diverges from the actual value, the higher is the log-loss value. It's hard to interpret raw log-loss values, but log-loss is a good metric for comparing models. For any given model, a lower log-loss value means better predictions
Gini coefficient (Normalized)::
  The (normalized) Gini coefficient gets its inspiration from the Lorentz curve in economics and is typically used when the datasets are imbalanced. In the context of Machine Learning the equivalent of the Lorenz curve is built using the values of the (predicted) class probabilities as opposed to “income” that is used in economics models. The Gini coefficient is then defined as the area under the Lorentz curve. Dividing the Gini coefficient computed from the Lorenz curve of the predicted class probabilities by the Gini coefficient computed from the actual class probabilities results in a normalized Gini coefficient for classification models.

==== Regression models

The following are the supported prediction quality metrics for Regression models.

Mean Square Error (MSE)::
  Mean squared error (MSE) measures the amount of error (also called residual) in regression models. It assesses the average squared difference between the observed and predicted values. When a model has no error, the MSE equals zero. As model error increases, its value increases. The mean squared error is also known as the mean squared deviation (MSD) and is calculated by dividing the sum of squared errors by the sample size.
Mean Absolute Error (MAE)::
  The MAE is computed by taking the average of the absolute difference between the actual or true values and the values that are predicted by the regression model for every datapoint in the evaluation dataset. It is therefore measuring the average magnitude of errors of the predictions irrespective of whether the prediction was less than or greater than the actual value.
Mean Absolute Percentage (MAPE)::
  Mean absolute percentage error (MAPE) is commonly used as a loss function for regression problems and in model evaluation. It is the percentage equivalent of MAE and as it is a percentage is an intuitive way to quantify on average how far the model's predictions are off from the actual values. Both MAPE and MAE are robust to outliers but have the disadvantage of being biased towards predictions that are less than the actual values themselves.
R-Squared (R2)::
  R-Squared (R2) is a statistical measure of how close the data is to the fitted regression hyperplane.It is defined as the percentage of variation in the target variable that is explained by the regression model. R-Squared (R2) lies between 0 and 1 ( or 0% and 100%) and 0 (0%) indicates that the model explains none of the variability of the target data around its mean and 1 (100%) indicates that the model explains all the variability of the target data around its mean
Gini (Normalized)::
  The (normalized) Gini coefficient gets its inspiration from the Lorentz curve in economics and is typically used when the datasets are imbalanced i.e there are big (and) small values in the dataset. In the context of regression models, the equivalent of the Lorenz curve is built using the predicted values of the target variable as opposed to “income” that is used in the case of economics models. The Gini coefficient is then defined as the area under the Lorenz curve. Dividing the Gini coefficient computed from the Lorenz curve of the predictions by the Gini coefficient computed from the actual values results in a normalized Gini coefficient for regression models.

[[tr2]]

// Compare the Metrics calculated against the golden set.

----- user_guide/model-monitoring/5-use-monitoring/5-exclude-features.txt -----
:page-version: 6.1
:page-title: Exclude features from scheduled checks
:page-sidebar: Exclude features
:page-permalink: 149cfc
:page-order: 50


You might want to reduce the notifications sent to ensure that recipients are receiving information about the most important features.

* Click the bell icon next to a feature to disable its alert. This excludes the feature from Scheduled Checks.

[[tr1]]

// Enable/disable Features for Schedule checks.

[[tr2]]

// Enable/disable Metrics for Schedule checks.

----- user_guide/model-monitoring/5-use-monitoring/6-model-quality-methodology.txt -----
:page-version: 6.1
:page-permalink: d888d6
:page-title: Model quality methodology
:page-sidebar: Monitoring methodology
:page-order: 60

Domino Model Monitoring provides a set of metrics to assess your model's performance. Learn about the statistical techniques that Domino uses so you can interpret the metrics as new prediction and ground truth data sets are produced.

== Matching ground truth and prediction datasets

To monitor your models effectively, you need to set up Domino Model Monitor and provide corresponding records for both prediction and ground truth data sets. You need to separately set up Domino Model Monitor to ingest prediction datasets (for link:93e5c0[Domino hosted models], for
link:86bc1f[externally hosted models]) and link:3851db#ingest-gtd-mm[ground truth datasets] for each model.

Domino identifies matching records using the 'row_identifier' link:bb88ca#_variabletype[variable type] registered in both the prediction and ground truth datasets. 

For any duplicate entries in the ground truth data, Domino will process the latest record.


If you only ingest partial ground truth data, it will only reflect the accuracy for the predictions that have actuals associated with them.
For example, if only 100 out of 1000 entries have ground truth, then the accuracy will be calculated based on the 100 values.

If at some point, you re-ingest ground truth data, Domino will update the metrics assuming two different values for the same record ID.

== How Domino computes quality metrics

For the matched rows, Domino creates "aggregates," which are summaries over hourly periods. Aggregates serve as the basis for the model quality metrics. These aggregates capture different information depending on whether the model is for regression or classification.

When the user selects a date start/end period, or when a scheduled check happens, the model quality statistics are calculated on the fly, combining multiple hourly aggregates into a single per-day aggregate, one for each day in the selected period.

The sums are added up trivially. The two distributions are added by an algorithm that combines two distributions. To do so, the lower-sized
bin width is chosen as the new bin width, an overall min/max is decided and then values from the distributions being added are redistributed
into the closest new bin.

=== Re-ingesting ground truth data

If ground truth data is re-ingested and there exist aggregates for the period that has just been re-ingested, for a given hour, the existing record is updated. This implies that new information is added to the existing record. I.e., the system is designed to always process new data but not overwrite existing data. 

If you wish to override this behavior and perform a clean re-ingestion, it is recommended that you start with re-registering the model, starting afresh. If you need to clear a portion of data saved for an existing model,
contact your Domino support representative.


== Regression models

Each hourly aggregate described above is saved with the following information:

* Number of records
* Sum of actual values
* Sum of squares
* Sum of MSE
* Sum of MAE
* Sum of MAPE
* Prediction distribution (binned discrete counts)
* Ground truth distribution (binned discrete counts)

Since the sums of actual values, squares, MSE, MAE, and MAPE are additive, Domino then uses them to calculate final MSE, MAE,
MAPE, and R^2^ model quality statistics in the user interface.

In turn, prediction distributions and ground truth distributions are used to calculate the Gini Norm model quality statistic.


== Classification models

The model quality methodology for classification models is similar to regression models, except for the types of hourly aggregates saved.

For classification models, each aggregate is saved with the following information:

* Number of records
* Confusion matrix
* Log loss numerator
* Log loss denominator
* Counts of true positives & false positives
* Counts of true negatives & false negatives

When combined, these allow the system to calculate AUC/ROC, accuracy, precision, recall, F1, log loss and Gini Norm model quality statistics.

----- user_guide/model-monitoring/5-use-monitoring/index.txt -----
:page-version: 6.1
:page-title: Use monitoring
:page-permalink: d42ae6
:page-order: 50

Use Domino's dashboard to monitor your data science models. The model monitoring feature detects and tracks data drift in the model's input features and output predictions. If you have ground truth data in the model, Domino can calculate and track the model's prediction quality using standard measures such as accuracy, precision, and more.

Domino's monitoring feature checks if the characteristics of the predictions versus the target data that were used to train the model are significantly different. You can set up scheduled checks, use APIs to ingest data, configure notification recipients to continuously monitor your models across your organization, and review historical data about the checks. APIs give you a way to integrate into existing business processes, as well as a programmatic option to automatically retrain models.

Find out more about how to use monitoring in these topics:

* link:2f17e3[Access the Monitor dashboard].
* link:4ceec8[Custom Model Monitoring metrics].
* link:f33b63[Analyze data drift].
* link:d4f9b7[Analyze model quality].
* link:149cfc[Exclude features from scheduled checks].
* link:d888d6[Model quality methodology].

----- user_guide/model-monitoring/6-remediation/1-review-cohort-analysis.txt -----
:page-version: 6.1
:page-title: Use Cohort Analysis
:page-permalink: ac36e3
:page-order: 10

When the Model Monitor detects data drift or a reduction in quality for a regression model, the Cohort Analysis gives you details so you can take remedial action. It identifies underperforming cohorts of data, determines what differentiates this data, and shows you specific features of the data that require examination. See link:c1dce7[Set up Cohort Analysis] for information about how to enable this feature.


[NOTE]
====
This feature is only available for regression models with numerical features.
====

[[tr1]]

// Cohort Analysis works only with regression model with numerical values.

== Review the Cohort Analysis

You can access the job or report for the Cohort Analysis from the Model Monitor.

* The job gives you access to the JSON files that contain the code used to perform the analysis, as well as the cohort statistics. You can copy the code and statistics to create custom reports or to use when remediating your model.
* The report shows a detailed cohort analysis. Use this to drill down into each cohort to understand the primary features that impact the model's performance.

Review the Cohort Analysis:

. From the navigation pane, click *Model Monitor*.
. Select the model for which you generated the Cohort Analysis.
. Click *Model Quality*.
. In the Cohort Analysis Report area, click one of the following:
* *Go to Job* to see the job. Click the *Results* tab to see the JSON data that was used to generate the report. In the navigation pane, you can click *Files* to see the related files.
* *View Report* to open a Files page that has the .pdf. Click `<filename>.pdf` to view the report. The report has the following sections:
+
.. Section 1 shows a visual representation and summary statistics about the model quality for the worst-performing cohorts. You can investigate these cohorts.
.. Section 2 shows information about the most distinctive features (quantified by a contrast score) for the top worst-performing cohorts. You can investigate a feature or a combination of features for remediation.
.. Section 3 shows summary statistics for all cohorts listed in the order of model performance. It also lists the features for a cohort that contribute to the cohort's distinctiveness.
+
[cols="4a,4a,4a",options="header"]
|===
|image:/images/5.0/cohort-analy-pdf1.png[alt="The cohort summary shows the top 50 worst performing cohorts", width=500]|image:/images/5.0/pdf-ca-2.png[alt="The detailed cohort analysis shows the top 50 worst performing cohorts", width=500]|image:/images/5.0/pdf-ca-3.png[alt="The cohort summary statistics show a table of the model quality statistics", width=500]
|===

//tr1

// Validate the cohort analysis report.
----- user_guide/model-monitoring/6-remediation/2-remediate-a-domino-endpoint.txt -----
:page-version: 6.1
:page-title: Remediate a Domino endpoint
:page-permalink: 74f8ed
:page-order: 20



After reviewing the link:f33b63[data drift] and link:d4f9b7[model quality monitor], use the monitoring results and link:d8e27b[Cohort Analysis] to determine whether there are concerns with your model. If you do have concerns, link:#review-your-data[review your data].


If you want to review the model and its associated code to investigate further, you can link:#reproduce-the-environment[reproduce the original code commits and artifacts] in the environment in which you deployed the model. If necessary, you can update the model code or retrain it with the latest production data. Then you can link:#publish-a-new-domino-endpoint[deploy a new, improved version of the Domino endpoint].

[[review-your-data]]

== Review the Domino endpoint predictions

Domino automatically creates a prediction dataset named `prediction_data` for every project that can be accessed from any workspace. The predictions are in Parquet format and are updated hourly as the Domino endpoint processes inputs. If there is no data in an hour, no file is created. If you configured link:f33b63[data drift monitoring] or link:d4f9b7[Set up Model Quality Monitoring] then the stored prediction data is automatically consumed by the Model Monitor.

By default, a daily job deletes data older than 30 days. Your administrator defines the retention policy for predictions. See the link:71d6ad#model-monitoring[model monitoring configuration options].

=== Review your data:

. In your workspace, open the IDE.
. Use the following paths to read the data:
* To load individual Parquet files:

[#dfs-projects]
.To load individual Parquet files for DFS-based projects
--
[source,console]
----
/domino/datasets/local/prediction_data/<model_version_id>/$$date$$=<date_in_utc>/$$hour$$=<hour_in_utc>/predictions_<uuid>.parquet
----
--
[#git-projects]
.To load individual Parquet files for GIT-based projects
--
[source,console]
----
/mnt/data/prediction_data/<model_version_id>/$$date$$=<date_in_utc>/$$hour$$=<hour_in_utc>/predictions_<uuid>.parquet
----
--
[#entire-DFS]
.To load the entire Parquet dataset for DFS-based projects
--
[source,console]
----
/domino/datasets/local/prediction_data/<model_version_id>
----
--
[#entire-git]
.To load the entire Parquet dataset for Git-based projects
--
[source,console]
----
/mnt/data/prediction_data/<model_version_id>/$$date$$=<date_in_utc>/$$hour$$=<hour_in_utc>/predictions_<uuid>.parquet
----
--
[CAUTION]
====
Do not rename the generated Parquet files and directories. Doing so can cause inconsistent behavior.
====

[[reproduce-the-environment]]

== Reproduce the environment

=== Prerequisites

//tabbed
For Git-based projects

* This feature is only enabled for models published in Domino 5.0 and higher.

For Domino File System-based projects

* This feature is available for pre-5.0 published models as well as newly-published models, as long as no additional Git repositories are involved.

[[reproduce-the-original-code-commits-and-artifacts]]

=== Remediate the model

. From the navigation pane, click *Endpoints*.
. Click the model that you want to remediate and then click *Open in Workspace*.
. From the Open in New Workspace and Branch window, type a name for the workspace.
. Select a *Hardware Tier*.
. In New Branch Name, type a name for the code branch.
. Click *Open*. A Domino workspace opens and is ready for you to take remedial action.
+
[NOTE]
====
If you manage file changes and code commits outside of Domino (such as in an external Git client) and only use Domino to publish the Endpoint, the window will show a list of tools. Select from these tools to create a new workspace.
====

[[publish-a-new-domino-endpoint]]

== Publish a new Domino endpoint

When you reproduce a workspace, as you did in the link:#reproduce-the-environment[Reproduce the environment] topic, Domino creates a branch in every repository involved in the project.

To publish a new model based on this reproduced branch, you must apply the link:0d2247[commit to the master branch] because Domino supports Git-based projects. The way that you do this depends on whether you are working with a Domino File System-based project or a Git-based project.
//tabbed

Commit projects based on the Domino File System (DFS):

. Go to the Project.
. Click
*Code*.
in the navigation bar.
. From the *Branch* list, select the reproduced branch.
. Click *Revert Project* to ensure that the commits made in this branch are added in the Master branch.
+
[NOTE]
====
*Revert Project* only works for the DFS files. For imported Git repositories or projects, you must revert the artifacts independently.
====
//tabbed

Commit Git-based projects:

* In a Domino workspace or the Git tool of your choice, merge your latest code update into the master branch.
+
[NOTE]
====
Perform the same merge operation for any artifacts (such as `.pkl` files) that you maintain in a separate repository. You can use Domino for the artifacts.

. In your Git-based project, in the navigation pane, click *Artifacts*.
. Click *Revert Project* to restore the artifacts from your reproduced branch to the master branch.
====

Go to the *Domino endpoint* section of your project to publish a new Domino endpoint or a new version of an existing Domino endpoint. See link:8dbc91[Publish the Domino endpoint].

----- user_guide/model-monitoring/6-remediation/index.txt -----
:page-version: 6.1
:page-title: Remediate model issues
:page-permalink: 7080da
:page-order: 60


Domino automatically analyzes prediction data to identify cohorts that are influencing a model's performance.

For models deployed as Domino endpoints, you can use this cohort analysis to do an in-depth investigation into the production data that is in Domino Datasets to get to the root of the model's performance issues. Then, you can make changes to your model code or retrain the model with new data and deploy a new version.

Find out more about how to remediate model issues in these topics:

* link:ac36e3[Use Cohort Analysis].
* link:74f8ed[Remediate a Domino endpoint].

----- user_guide/model-monitoring/7-monitor-settings/1-api.txt -----
:page-version: 6.1
:page-title: API token
:page-permalink: bd3a62
:page-order: 10

DMM API keys have been deprecated in Domino 5.6, and you can no longer generate new DMM API keys. Instead, use Domino API keys to authenticate requests to the Model Monitoring API using the `X-Domino-Api-Key` header. Learn how to link:661f8e[generate Domino API Keys].

Existing DMM API keys will continue to authenticate requests to the Model Monitoring API with the `X-DMM-API-KEY` header. You can revoke DMM API keys based on your security policy by clicking *Revoke Token* from the Model Monitor Settings page.



----- user_guide/model-monitoring/7-monitor-settings/2-health.txt -----
:page-version: 6.1
:page-title:  Health dashboard
:page-permalink: dacce4
:page-order: 20

Use this section to configure the number of checks and number of days of prediction traffic shown in the Health Status area on the Model page.

. From the navigation pane, click *Model Monitor*.
. Click *Settings*.
. Click *Health Dashboard*.
. Select the *No. of checks* and *Prediction traffic (Days)* as needed.

== View the health status on the Model page

From the navigation pane, click *Models*.

[[tr1]]

//User can configure Health check and data traffic on Health Dashboard.

----- user_guide/model-monitoring/7-monitor-settings/3-notification.txt -----
:page-version: 6.1
:page-title:  Notification channels
:page-permalink: 386451
:page-order: 30


Use this section to set the global communication channel to send notifications to your stakeholders. You can override these global settings at the model level of any registered model. See link:287438[Set up Notifications] (for Domino endpoints) or link:9ebe39[Set up Notifications] (for the Model Monitor).

. From the navigation pane, click *Deploy > Model Monitor*.
. Click *Settings*.
. Click *Notification Channels*.

. Click *Edit*.
. Type the SMTP details in the fields provided.
. In the *From Address* field, type email address from which the notifications are sent.
. In the *To Addresses* field, type or paste a comma- or semicolon-separated list of email addresses.
. To use SMTPS instead of SMTP+STARTTLS, switch the *Enable TLS* toggle to the off position.
. Click *Save Mail Config*.

[[tr1]]

//User can configure global email notifications

[[tr2]]

//User can configure model-level email notifications

----- user_guide/model-monitoring/7-monitor-settings/4-tests.txt -----
:page-version: 6.1
:page-title: Test defaults
:page-permalink: 66f1f0
:page-order: 40


Use this section to set global default threshold conditions. You can override these global settings at the model level of any registered model.

* link:f33b63#modify-the-data-drift-analysis[To modify the test features for data drift], change the data drift default settings at the model level.
* link:d4f9b7#modify-the-model-quality-analysis[To modify the test features for model quality analysis], change the model quality analysis at the model level.

== Test the defaults
. From the navigation pane, click *Deploy > Model Monitor*.
. Click *Settings*.
. Click *Test Defaults*.
. Click the arrow to expand or collapse the following sections:
** *Data Drift Test Defaults*
** *Model Quality Test Defaults*
. For Data Drift, you can do the following:
** Select the default test type to use when a new model is registered.
** Set the default test condition and threshold for each test type to use when a new model is registered.
. For Model Quality, you can do the following:
** Set the default test condition and threshold for each metric to use when a new model is registered.
. Click *Save*.

[[tr1]]

// Configure global data drift and model quality test defaults

----- user_guide/model-monitoring/7-monitor-settings/index.txt -----
:page-version: 6.1
:page-title: Apply monitor settings
:page-permalink: 8608f7
:page-order: 70

The following settings impact both the Model Monitor and monitoring in the Domino endpoints.

== Access the Model Monitor settings

. From the navigation pane, click *Model Monitor*.
. Click *Settings*.
. From the Settings page, click the settings to manage.

[[tr1]]

// Navigate to settings page

Learn more about the different sections in the following topics:

* link:bd3a62[API token].
* link:dacce4[Health dashboard].
* link:386451[Notification channels].
* link:66f1f0[Test defaults].

----- user_guide/model-monitoring/8-json/1-supported-binning.txt -----
:page-version: 6.1
:page-title: Supported binning methods
:page-permalink: 21503d
:page-order: 10

Bins are used to represent probability distributions and divergence values for data drift. The number of bins impacts the quality of drift values and in general, Model Monitor's performance itself. If you have more than 20 bins, this can cause false alarms which can impact performance.

By default, Model Monitor uses the Freedman Diaconis Estimator method to calculate the number of bins for numerical variables. If this method returns a count higher than 20, then the count is capped at 20.

For numerical variables, the Model Monitor automatically adds one guard bin for values that fall outside the minimum and maximum range of the values present in the training data. For training data this guard bin will have a zero count (unless the user uses the ‘binEdges' override strategy). However, for Prediction data, values might fall in this bin, indicating that prediction data has values outside the min-max seen on the training data.


For categorical variables, all unique class values are used as bins. The Model Monitor automatically adds one guard bin ‘Untrained Classes'. For training data, this guard bin will have zero counts (unless the `binCategories` override strategy is used). However, for Prediction data, counts of all classes that were not present in the training data will fall in this bin. You can use this bin to detect new classes previously unseen during training.

Use the following attributes in the link:bb88ca[Monitoring config JSON] to override these defaults and fine tune the bin creation.

[NOTE]
====
After a model is registered, you can't change bins.
====

For numerical data columns, you can use one of the following approaches:

* *binsNum* 

** This takes a positive integer >= 2 and > 20 as input.
** The Model Monitor will create that number of equal sized bins for the numerical variable.
** The Model Monitor uses the max and min value in the training dataset to determine the bin widths.
** The Model Monitor will add two guard bands in addition to the user-defined bins.
** For example: 
*** `“binsNum”: 10``

* *binsEdges* 

** This takes an array of real numbers as input.
** Edges can be both positive and negative decimal numbers (except Infinity).
** These correspond to actual bin edges.
** To create N user-defined bins, users must provide N+1 bin edges.
** You can provide a minimum of 3 and maximum of 20 numbers or edges in the array.
** They must monotonically increase (lowest to highest) from the start of the array to end of the array.
** This is similar to histogram_bin_edges method used in Numpy.
** The Model Monitor will add two guard bands in addition to the user-defined bins.
** All provided values must be unique.
** For example:
*** `“binsEdges”: [-10, -4.5, -0.25, 0, 3.2, 5.11111]`

** Examples of invalid `“binsEdges”`:
*** `“binsEdges”: [-10, 4, -0.25, 0, 3.2, 5.11111]` –> not monotonically increasing
*** `“binsEdges”: [-10, XYZ, -0.25, 0, 3.2, 5.11111]` –> string value present
*** `“binsEdges”: [1,2]` –> less than 3 edges provided
*** `“binsEdges”: [1,2,2,4,6]` –> duplicates present

For categorical data columns, you can use the following approach:

* *binsCategories* 

** This takes an array of strings as input (length must be less than 100) and creates a bin for each of them.
** The values must ideally correspond to class values present in the data column in the training data or class values that you expect to find in the prediction data.
** Counts of all other class values of the training and prediction data columns will fall in the 'Untrained Classes' guard bin.
** If the user has specified an `Untrained Classes` bin as part of the `binsCategories`, then it will correspond to the internal `Untrained Classes` bin.
** Example:
*** `“binsCategories”: [“red”, “blue”, “green”, “white”, “yellow”]`

[[tr1]]

// configure binning strategy for model registry

----- user_guide/model-monitoring/8-json/index.txt -----
:page-version: 6.1
:page-title: Use the Monitoring Config JSON
:page-permalink: bb88ca
:page-order: 80

The Monitoring Config JSON captures all information required to register a model, a prediction dataset, or a ground truth dataset. This section describes the structure of the Monitoring Config JSON file. The following is a sample config:

[source,JSON]
----
{
        "variables": [
                {
                        "name": "age",
                        "valueType": "numerical",
                        "variableType": "feature",
                        "featureImportance": 0.9
                },
                {
                        "name": "y",
                        "valueType": "categorical",
                        "variableType": "prediction"
                },
                {
                        "name": "date",
                        "valueType": "datetime",
                        "variableType": "timestamp"
                },
                {
                        "name": "RowId",
                        "valueType": "string",
                        "variableType": "row_identifier"
                }
        ],
        "datasetDetails": {
                "name": "TrainingData.csv",
                "datasetType": "file",
                "datasetConfig": {
                        "path": "TrainingData.csv",
                        "fileFormat": "csv"
                },
                "datasourceName": "abc-shared-bucket",
                "datasourceType": "s3"
        },
        "modelMetadata": {
                "name": "test_psg",
                "modelType": "classification",
                "version": "2",
                "description": "",
                "author": "testadmin"
        }
}
----

== variables
An array of variables that declare all features and prediction columns that you want to analyze. For each member in the array, specify the `name`, `variableType`, and `valueType`.

=== name
Name of the column.

=== variableType

`variableType` provides the attribute that identifies the column. Supported types are:

- *feature*
** Can only be of `valueType` numerical or categorical.
** Input feature of the model.
** Data drift will be calculated for this data column.
** Must be declared while registering the model along with its training data.
** The column must be present in all training and prediction datasets registered with the model.

- *prediction* (optional)
** When declared, there can only be one Prediction column.
** Can only be of `valueType` numerical or categorical.
** Output prediction of the model.
** Data drift and model quality metrics are calculated for this data column. Include this column when registering your model to ensure both data drift and model quality analysis can be run. If it isn't included, model quality metrics won't be computed.

- *timestamp* (optional)
** When present, there can be only one `timestamp` column.
** Can only be of datetime `valueType`.
** Although you can declare this column when adding prediction data for the first time, Domino recommends that it be declared during model registration.
** Identifies the column that contains the timestamp for the prediction made. If not declared, the ingestion time of the data in the Model Monitor is used as the timestamp of the prediction.
** Must contain the date/time when the prediction was made. Column values must follow the ISO 8601 time format.
** When it is not declared, the ingestion time of the prediction dataset into the Model Monitor is substituted as the timestamp of prediction.
** To use automatic ingestion for Snowflake, you must include this column. https://docs.snowflake.com/en/user-guide/spark-connector-use.html#working-with-timestamps-and-time-zones[Snowflake's documentation^] recommends setting the timezone to UTC for both the Spark cluster and the Snowflake deployment.

- *row_identifier* (optional)
** Can only be of string `valueType`.
** Uniquely identifies each prediction row. Typically referred to as prediction ID, transaction ID, and so on.
** When present, there can be only one `row_identifier` column.
** Although you can declare this column when adding prediction data for the first time, Domino recommends that it be declared during model registration.
** Values are used to match ground truth data to the predictions to calculate model quality metrics. Model quality metrics will not be calculated if this column is not present. If used, must be present in both prediction and ground truth datasets.

- *ground_truth*
** Identifies the column that contains the ground truth labels in the ground truth datasets.

- *sample_weight*
** Column that contains the weight to be associated with each prediction to calculate the Gini Norm metric.

- *prediction_probability*
** Column that contains the probability value for the model's prediction. Can be a single value (maps to the probability value of the positive class) or a list of values (the length of the list must match the number of unique prediction labels or classes present in the training dataset).

[[forPredictionOutput]]

[NOTE]
====
For a field of this type, include `forPredictionOutput` to indicate the prediction column for which you are specifying the probability. This column is required if you want to measure AUC, ROC, Log Loss, and Gini Norm as part of the model quality analysis.
====

=== valueType

Identifies the value of the column. Supported types are `categorical`, `numerical`, `datetime`, or `string`. Can only be set to `datetime` for one column.

=== forPredictionOutput

Specifies which prediction column the ground truth variable represents in Ground Truth Config.

== datasetDetails

=== name

The name to associate with this dataset instance. You can use the same name as the file you are selecting.

=== datasetType
The supported type is `file`.

=== datasetConfig

Defines the actual location of the file.

- `path`: The name of the file.
If you are using Snowflake, the path is the name of the Snowflake table.
- `fileFormat`: .csv and parquet are supported.

If you are using Snowflake, the path must be `snowflake`.

=== datasourceName
Name you provided when you created the link:8c7833[data source].

=== datasourceType

The supported data source types are:
`amazon_s3`, `generic_s3`,
`gcs`, `azure_blob`, `azure_data_lake_gen1`, `azure_data_lake_gen2`,
`hdfs`, or `snowflake`.

== modelMetadata

Captures metadata related to the model.

Specify the `name`, `modelVersion`, `modelType`, `dataset`, `dateCreated`, `description`. and `author` attributes.

`dateCreated` must be in a valid UTC format (ISO 8601). Valid values for `modelType` are `classification` and `regression`.

== featureImportance

Highlights the overall impact of the feature on predictions made to the model, relative to other features.

[[tr1]]

// Configure json for model registration

[[tr2]]

// Configure json for prediction data upload

[[tr3]]

// Configure json for ground truth data upload

== Supported binning methods

Bins are used to represent probability distributions and divergence values for data drift.
The number of bins impacts the quality of drift values and in general, Model Monitor’s performance itself.

For more information, see link:21503d[Supported binning methods].

----- user_guide/model-monitoring/9-model-monitoring-apis.txt -----
:page-version: 6.1
:page-title: Use the Model Monitoring APIs
:page-permalink: a94c1c
:page-order: 90
:page-api-schema: dmm.yaml

The Model Monitoring API gives you programmatic access to the link:715969[Model Monitor].

To invoke these APIs, you must pass the API token in the header. See link:bd3a62[API Token] for more information.

Domino Model Monitoring can only ingest prediction data for Domino endpoints in the form of link:ae1654[Domino Datasets] captured using link:93e5c0[Domino's prediction capture library]. To ingest different data types, create a new model in Domino Model Monitor and use a link:a94c1c#_datasource[Model Monitor Data Source].

----- user_guide/model-monitoring/index.txt -----
:page-version: 6.1
:page-title: Monitor models
:page-permalink: 715969
:page-order: 290

Domino's Model Monitoring uses data from several supported link:8c7833[data sources] to analyze models in production and alerts you when a model's performance falls outside the specified range.

Model Monitoring uses training and prediction data to track drift for the model's input features and prediction variables, and alerts you about every feature that exceeds a configurable threshold. If you have ground truth data for the model's predicted values, Domino can ingest it to produce model quality metrics using standard measures such as accuracy, precision, and recall. Domino can also alert you about every metric that exceeds a threshold.

You can also:

* Use the monitoring dashboard to observe your data science models.
* Set up your model to be continuously monitored, using APIs to ingest prediction and ground truth data when available, and define scheduled checks to alert about drift and model quality metrics periodically.
* Use APIs to integrate into existing business processes, and programmatically retrain models.

To find out more, see:

* link:85f76d[Monitor workflows].
* link:b08113[Data drift and quality monitoring].
* link:2a7c3b[Set up monitoring for Domino endpoints].
* link:679cc1[Set up Model Monitor].
* link:d42ae6[Use monitoring].
* link:7080da[Remediate model issues].
* link:8608f7[Apply monitor settings].
* link:bb88ca[Use the Monitoring Config JSON].
* link:a94c1c[Use the Model Monitoring APIs].
* link:f66a1b[Troubleshoot the Model Monitor].

----- user_guide/on-demand-distributed-computing/dask/configure-prerequisites.txt -----
:page-version: 6.1
:page-title: Configure Dask prerequisites
:page-sidebar: Configure prerequisites
:page-permalink: 3e137b
:page-order: 20

[[configuring_dask_prerequisites]]
Before you can start using on-demand Dask clusters on Domino, you must ensure that this functionality is enabled and properly configured on your deployment.

[NOTE]
====
Domino on-demand Dask functionality is available starting with Domino 4.6.
====

[[creating_base_dask_env]]
== Create a base Dask cluster environment

[[tr1]]
// As a Domino user, I can create a Dask cluster environment using a daskdev external base image

When using on-demand Dask in Domino you will have two separate environments: one for the Dask cluster (base or worker environment) and one for the workspace/job execution (compute environment).

You can get the latest released Dask images from link:799193[compute Environment catalog].

To create a new base Dask cluster environment, follow the general link:f51038[Environment management] instructions with the following `environment_attributes`:

* *Base Image*
+
Select *Custom Image* and enter an image URI that points to a deployable Dask image.
+
From the options published at link:799193[compute Environment catalog], select the Dask cluster image available in the https://quay.io/repository/domino/dask-cluster-environment?tab=tags[cluster-environment-images repository^].
Domino's repository contains the latest Dask images curated for Domino.
+
The available images include the full set of Dask components and common dependencies like Pandas and NumPy.

* *Supported Clusters*
+
Select the *Domino managed Dask* option (required).
This ensures that the environment will be available for use when creating Dask clusters from workspaces and jobs.

* *Visibility*
+
Set this attribute the same way you would for any other link:f51038[compute environment].

* *Dockerfile Instructions*
+
Leave blank to use the base image provided by the Dask community.
+
You can modify this section to include additional packages that might be necessary for your workloads and must be available on the Dask cluster nodes.
+
See link:3dcdb2[Manage dependencies] to learn more.

* *Pluggable Notebooks / Workspace Sessions*
+
Leave this section blank as the Dask base environments are not intended to include notebook configuration.

[[creating_compute_dask_env]]
== Prepare your Dask execution compute environment

[[tr2]]
// As a Domino user, I can update a base compute environment for use with a Dask cluster workload (in the environment dockerfile or at runtime using a requirements.txt file)

In addition to the base Dask cluster environment, you must also configure the Dask compute environments for workspaces and/or jobs that will connect to your cluster.

Domino recommends that you use the Dask base image to create a compatible workspace. See link:799193[compute Environment catalog].

=== Customize this Workspace compute environment:

Use the image mentioned previously and add **Pluggable Workspace Tools**.

[source,yaml]
----
jupyter:
  title: "Jupyter (Python, R, Julia)"
  iconUrl: "/assets/images/workspace-logos/Jupyter.svg"
  start: [ "/opt/domino/workspaces/jupyter/start" ]
  supportedFileExtensions: [ ".ipynb" ]
  httpProxy:
    port: 8888
    rewrite: false
    internalPath: "/{{ownerUsername}}/{{projectName}}/{{sessionPathComponent}}/{{runId}}/{{#if pathToOpen}}tree/{{pathToOpen}}{{/if}}"
    requireSubdomain: false
jupyterlab:
  title: "JupyterLab"
  iconUrl: "/assets/images/workspace-logos/jupyterlab.svg"
  start: [  "/opt/domino/workspaces/jupyterlab/start" ]
  httpProxy:
    internalPath: "/{{ownerUsername}}/{{projectName}}/{{sessionPathComponent}}/{{runId}}/{{#if pathToOpen}}tree/{{pathToOpen}}{{/if}}"
    port: 8888
    rewrite: false
    requireSubdomain: false
vscode:
  title: "vscode"
  iconUrl: "/assets/images/workspace-logos/vscode.svg"
  start: [ "/opt/domino/workspaces/vscode/start" ]
  httpProxy:
    port: 8888
    requireSubdomain: false
rstudio:
  title: "RStudio"
  iconUrl: "/assets/images/workspace-logos/Rstudio.svg"
  start: [ "/opt/domino/workspaces/rstudio/start" ]
  httpProxy:
    port: 8888
    requireSubdomain: false
----

----- user_guide/on-demand-distributed-computing/dask/index.txt -----
:page-version: 6.1
:page-title: Use on-demand Dask
:page-permalink: 747a51
:page-order: 20

https://dask.org[Dask^] is a distributed computing library that tightly integrates with the Python ecosystem and allows for multi-core and distributed parallel execution on larger-than-memory datasets.
Dask makes it simple to scale up a single machine Python workload to a multi-machine cluster with little or no changes even if the application was never developed with Dask in mind initially.

Dask offers the following:

* *Low-level scheduling and execution APIs*: Dask provides a set of APIs and facilities for scheduling and parallel execution of task graphs.
This execution engine powers the high-level collections mentioned below but can also be used to develop and execute custom, user-defined distributed workloads.
These low-level capabilities are an alternative to direct use of `threading` or `multiprocessing` Python libraries or other task scheduling systems like `Luigi` or `IPython parallel`.
+
For additional information, see
https://docs.dask.org/en/latest/delayed.html[Dask delayed^] and
https://docs.dask.org/en/latest/futures.html[Dask futures^].
* *High-level distributed libraries*: Dask provides distributed equivalents for popular Python collection libraries such as NumPy arrays, Python lists, and Pandas data frames.
The Dask equivalents provide API-level compatibility and can be used as drop-in replacement when one needs to work with large datasets.
Additionally, Dask provides similar compatibility with scikit-learn and integration with other popular model frameworks to enable scalable training and prediction on large models and datasets.
+
For additional information, see https://docs.dask.org/en/latest/array.html[Dask arrays^], https://docs.dask.org/en/latest/dataframe.html[Dask dataframes^], and https://ml.dask.org/[Dask ML^].

== Orchestrate Dask on Domino

Domino offers the ability to dynamically provision and orchestrate a Dask cluster directly on the infrastructure backing the Domino instance.
This allows Domino users to get quick access to Dask without having to rely on their IT team.

When you start a link:e6e601[Domino workspace]
for interactive work or a link:942549[Domino job] for batch processing, Domino will create, manage, and provide a containerized Dask cluster for execution.

//DOCS-1006
See Domino's https://github.com/dominodatalab/domino-quickstart-dask[quick-start-dask^] project.

== Suitable use cases

Domino on-demand Dask clusters are suitable for the following workloads:

Working with large datasets:: Dask excels at scaling up Python data analysis or transformation code where the data that needs to be processed exceeds the resources that can be provided by a single machine.
With a compatible API for commonly used Python libraries, Dask is a suitable tool for Python-first data scientists who have R&D Python code for data cleaning, manipulation, and advanced analytics which needs to be scaled to a much larger production dataset.
This can be done with minimal modification and without having to switch over to a different ecosystem like Spark.
Distributed training::
https://www.dask.org/[Dask^] provides a simple drop-in replacement to train parallelizable scikit-learn models at scale. It's possible to run Dask in single-node and distributed modes to enable parallelization and distribution separately and in conjunction. This approach is an excellent fit for moderate memory footprint models that are CPU/GPU-bound with many individual operations and can be parallelized beyond the limits of a single machine. For larger memory-bound workloads, there are Dask-specific ML libraries (for example, Parallel Meta-estimators and Incremental Hyperparameter Optimizers) that use algorithms; they are specifically optimized to work with the Dask scalable NumPy and DataFrame equivalents.
Custom distributed computations::
If the available Dask machine learning algorithms or large data representations are insufficient, use the low-level Dask scheduling APIs to build custom algorithms that can benefit from parallelism. Developers control the business logic, while Dask handles task dependencies, network communication, workload resilience, diagnostics, and support functions.

== Next steps

* Find out more about the link:09ba07[Validated Dask version].
* Learn how to enable and configure the functionality on your deployment in link:3e137b[Configure Dask prerequisites].
* Learn how to link:aaa2c1[create an on-demand Dask cluster] with the desired cluster settings attached to a Workspace or Job.
* Find out how you can link:3dcdb2[manage Dask dependencies].
* Learn how to link:0919a6[Access data with Dask].

----- user_guide/on-demand-distributed-computing/dask/manage-dependencies.txt -----
:page-version: 6.1
:page-title: Manage Dask dependencies
:page-sidebar: Manage dependencies
:page-permalink: 3dcdb2
:page-order: 40

[[dask_managing_dependencies]]
While Dask base images come with the full set of Dask collection components and common dependencies like Pandas and NumPy, you might still have to modify your environments if you need additional packages or when you need a specific version of a given package.

Domino allows you to easily package and manage dependencies as part of your link:3e137b#creating_base_dask_env[Dask-enabled compute environments].
This approach creates the flexibility to manage dependencies for individual projects or workloads without having to deal with the complexity of a shared cluster.

To add a new dependency, you need to add the appropriate statements in the *Docker instructions* section of the relevant Dask base and Dask execution compute environments.

For example, if you wanted to add a particular version of Dask ML, you might include the following.

[source,Docker]
----
### Optionally specify version if desired
RUN pip install dask-ml==1.9.0
----

There are several optional dependencies that might be required for subsets of Dask functionality.
You can find more information in the https://docs.dask.org/en/latest/install.html#optional-dependencies[Dask documentation^].

----- user_guide/on-demand-distributed-computing/dask/validated-dask-version.txt -----
:page-version: 6.1
:page-title: Validated Dask version
:page-permalink: 09ba07
:page-order: 10

For each minor release (starting with Domino 4.6), Domino performs a set of validation tests against the *most recent released version* of Dask.
The purpose of the tests is to cover the integration surface between the Domino platform and the on-demand Dask clusters that the platform orchestrates.
The testing is not intended to be a comprehensive test of Dask itself since Domino does not modify that software.

[NOTE]
====
The validated and supported version of Dask is 2022.1.0.

Other Dask versions are likely to function properly, but Domino will not be able to offer technical support.
====

----- user_guide/on-demand-distributed-computing/dask/work-with-data.txt -----
:page-version: 6.1
:page-title: Access data with Dask
:page-sidebar: Work with data
:page-permalink: 0919a6
:page-order: 50

When using a Domino on-demand Dask cluster, any data that will be created or modified as part of the interaction needs to go into an external data store.

[WARNING]
====
On-demand clusters in Domino are ephemeral.
Any data that is stored on cluster local storage and not externally will be lost upon termination of the workload and the cluster.
====

== Using Domino Datasets

[[tr1]]
// Dask cluster workers have access to datasets attached to a workspace
[[tr2]]
// Dask cluster workers have access to datasets attached to a job

When you create a Dask cluster attached to a Domino workspace or job, any link:0a8d11[Domino dataset] accessible from the workspace or job will also be accessible from all components of the cluster under the same dataset mount path.
You will then be able to access the files from your code using the same path regardless of whether your code runs on your workspace or job container or in a Dask task on the cluster.

For example, to read a file you would use the following.

[source,python]
----
import dask.dataframe as dd

df = dd.read_parquet("/mnt/data/my_dataset/large_dataset.parquet")
----

== Using S3

[[tr3]]
// Dask cluster workers are able to access S3

To access Amazon S3 (or S3 compatible object store) data with Dask, you can use any of the libraries you already use (for example, `boto3`, `s3fs`) to pull down files from S3.

For structured data, you can also read it directly into Dask dataframes of bags.
You would need to specify the `s3://` as the protocol.
The following is a basic example.

[source,python]
----
import dask.dataframe as dd

df = dd.read_parquet("s3://bucket/path/data-*.parquet")
df = dd.read_csv("s3://bucket/path/data-*.csv")
----

[NOTE]
====
Dask uses `s3fs` and the underlying `boto3` for S3 access, so you will need to make sure that the optional `s3fs` package is installed on your base Dask environment and your execution environment when using Dask to access S3 data.
====

Additional parameters (for example, auth keys) can be passed through the
`storage_options`.
For full documentation of the S3 specific options (including loading data from S3 compatible services), refer to the relevant section of the https://docs.dask.org/en/latest/remote-data-services.html#amazon-s3[Dask documentation^].

=== AWS credential propagation

[[tr4]]
// Dask cluster workers have the same AWS credentials and permissions enabled for the user

When link:eb6a88[AWS credential propagation] is enabled for your deployment, temporary AWS credentials corresponding to the roles enabled for you in your company identity provider will be automatically available on all Dask workers and your execution.

The credentials will be automatically refreshed and available under a profile name corresponding to each role in an AWS credential file.
The location of the file is stored in the `AWS_SHARED_CREDENTIALS_FILE`
environment variable, which puts in the proper search path for `s3fs` and
`boto3`.

You will be able to specify the name of the profile that corresponds to the role that you would want to use for authentication.
You can do the following:

[source,python]
----
import dask.dataframe as dd

df = dd.read_parquet("s3://bucket/path/data-*.parquet",
    storage_options={
       "profile_name"="my-role-profile",
    })
----

== Using other data stores

[[tr5]]
// Dask cluster workers are able to access data sores other than S3/AWS

Similar to S3, Dask can load data from Microsoft Azure Storage, Google Cloud Storage, HDFS, HTTP, NFS, and your local file system.

Detailed documentation describing the protocol to use, the required packages, and the available `storage_options` can be found in the
https://docs.dask.org/en/latest/remote-data-services.html#remote-data[Remote data] section of the Dask documentation.

----- user_guide/on-demand-distributed-computing/dask/work-with-your-cluster.txt -----
:page-version: 6.1
:page-title: Work with clusters
:page-permalink: aaa2c1
:page-order: 30

== Create a cluster with Workspaces

[[tr1]]
// As a Domino user, I can launch a workspace with a Dask cluster attached and execute a worklodad requiring that cluster

To create an on-demand Dask cluster attached to a Domino Workspace, click *New Workspace* from the *Workspaces* menu.
On the Launch New Workspace dialog select *Compute Cluster*.
Specify the desired link:#dask_cluster_settings[cluster settings] and launch your workspace.
Once the workspace is up, it will have access to the Dask cluster you configured.

== Create a cluster with Jobs

[[tr2]]
// As a Domino user, I can start a batch job with a Dask cluster attached and successfully execute job code requiring that cluster

Similarly to workspaces, to create an on-demand Dask cluster attached to a Domino job, click on *Run* from the *Jobs* menu.
On the Start a Job dialog, select *Compute Cluster*.
Specify the desired
link:#dask_cluster_settings[cluster settings] and launch your job.
The job will have access to the Dask cluster you configured.

As your command, you can use any Python script that interacts with your Dask cluster.

[[dask_cluster_settings]]
== Understand cluster settings

Domino makes it simple to specify key settings when creating a Dask cluster.

[[tr3]]
// As a Domino user, I can change the Dask cluster settings (number of workers, worker and head hardware tiers, compute environment, and local storage per executor) used to create a Dask cluster

* *Min workers*
+
The number of Dask node workers that will make up the Dask cluster when it starts.
If *Auto-scale workers* is not enabled, this will always be the size of the cluster.
The combined capacity of the workers will be available for your workloads.

* *Max workers*
+
The maximum number of Dask node workers that the cluster can reach when *Auto-scale workers* is enabled.
See
link:#dask_cluster_autoscaling[cluster autoscaling] for more details.

* *Cluster size: Limit*
+
[[tr4]]
// As a Domino user, I can only set up to the maximum concurrent executions as set by the Domino admin; the cluster settings field will not allow me to exceed the remaining executions available
The maximum number of workers that you can make available to your cluster is limited by the number of per-user executions that your Domino administrator has configured for your deployment or by the maximum simultaneous executions of the underlying hardware tier used for workers.
+
In addition to the number of Dask node workers, you will need 1 slot for your cluster master and 1 slot for your workspace or job.

* *Worker hardware tier*
+
The amount of compute resources (CPU, GPU, and memory) that will be made available to each Dask node worker.

* *Scheduler hardware tier*
+
Same mechanics as the worker hardware tier, but applied to the resources that will be available for your Dask cluster scheduler node.
+
As the name suggests, the Dask scheduler is responsible for determining the worker on which a given task needs to execute.
A serialized version of the task is held in the scheduler memory until it is possible to assign the task.
This is not necessarily the same amount of memory needed to actually execute the task, but limiting the scheduler memory may prevent work from being scheduled for executions with a very complex task graph.
+
From a CPU perspective, the needs of the scheduler are likely much lower than your workers.
+
[[tr13]]
// As a Domino admin, I can create hardware tiers for use only by Dask (or multiple) cluster types; if such tiers exist, only those tiers will be selectable, and no standard (non-Dask-cluster) hardware tiers will be available
By default, any hardware tier is available when selecting resources for your executor and scheduler.
Domino administrators can optionally configure compute cluster
link:eca4b2#compute-cluster-specific-hardware-tiers[dedicated hardware tiers].

* *Cluster compute environment*
+
Designates your compute environment for the Dask cluster.

* *Dedicated local storage per executor*
+
The amount of dedicated storage in Gigabytes (2^30 bytes) that will be available to each Dask worker.
+
The storage will be automatically mounted to `/tmp`.
+
The storage will be automatically provisioned when the cluster is created and de-provisioned when it is shut down.
+
[WARNING]
====
The local storage per worker should not be used for storing any data that must be available after the cluster is shut down.
====


[[dask_cluster_autoscaling]]
== Cluster auto-scaling

[[tr5]]
// As a Domino user, I can set my Dask cluster to autoscale from a minimum to a maximum number of workers


Cluster auto-scaling allows you to start with a small cluster, which then automatically scales up and down in response to the resource consumption of your workload.
This approach utilizes resources more efficiently for bursty workloads.

[[tr6]]
// As a Domino user, I can see my cluster size increase when my workload uses more than the CPU or memory threshold
By default, the cluster size will increase when the average CPU utilization of your workload reaches 80%.
A Domino administrator can further refine the
link:71d6ad#compute-cluster-auto-scaling[auto-scaling settings] by including memory utilization or changing the desired scaling thresholds.

[[tr7]]
// As a Domino user, I can see my cluster size decrease when my usage falls below the CPU or memory threshold
Scale down happens if resource utilization is low for a period of at least 5 minutes.
Note that depending on the workload that you are executing, scale down may terminate cluster nodes that contain intermediate results which would need to be recomputed.

== Connect to a cluster

[[tr8]]
// As a Domino user, I can execute code to connect to an use the Dask cluster I have initiated

When provisioning your on-demand Dask cluster, Domino sets up environment variables that hold the information needed to easily connect to your cluster.

The following snippet can be used to connect:

[source,python]
----
from dask.distributed import Client
import os
...

service_host = os.environ["DASK_SCHEDULER_SERVICE_HOST"]
service_port = os.environ["DASK_SCHEDULER_SERVICE_PORT"]
client = Client(f"{service_host}:{service_port}")

# you should now be connected to the cluster
# Dashboard link from the client object is clickable but will not route in Domino
# Use the embedded Dask Web UI tab instead
----

[NOTE]
====
* You can optionally wait until the desired number of workers is available with `client.wait_for_workers(<desired num workers>)`.

* If you must close your client connection, use `client.close()`.
Be careful not to use `client.shutdown()`, since that will terminate the cluster workers and you will not be able to continue working with the cluster unless you restart your execution.
====

Printing the client properties in a notebook will give you a Dask dashboard link.
While that link is clickable, it will not return an appropriately formatted dashboard link that will work from Domino.
Use the embedded Dask web UI as described below instead.

== Access the Dask web UI

Dask provides a built-in diagnostics dashboard with access to metrics, charts, and other features that helps you understand the components of the Dask cluster and the execution of your workloads.

The dashboard is extremely useful to diagnose performance issues and help improve your understanding of what is happening across your cluster.

Domino makes the Dask web UI available for active on-demand clusters attached to both workspaces and jobs.

NOTE: In a hybrid Domino deployment, if the link:95520d[data plane] is not configured for workspaces, then the user interface link is disabled.
An admin can link:491fe8[Enable a data plane for workspaces].

For a more in-depth introduction to the Dask dashboard, refer to the official https://docs.dask.org/en/latest/diagnostics-distributed.html#dashboard[Dask documentation^].

=== Dask web UI from Workspaces

[[tr9]]
// As a Domino user, I can view the Web UI associated with my Dask cluster from its tab in my workspace

The Dask web UI is available from a dedicated tab in your workspace.


=== Dask web UI from Jobs

[[tr10]]
// As a Domino user, I can view the Web UI associated with my Dask cluster from a link in my job details

The Dask web UI is also available for running jobs from the *Details* tab.


== Cluster lifecycle

[[tr11]]
// As a Domino admin, I can view the Dask cluster resources created to execute a job or workspace and see that they are cleaned up when the workload is complete

On workspace or job startup, a Domino on-demand Dask cluster with the desired link:#dask_cluster_settings[cluster settings] is automatically provisioned and attached to the workspace or job as soon as the cluster becomes available.

On workspace or job termination, the on-demand Dask cluster and all associated resources are automatically terminated and de-provisioned.
This includes any compute resources and storage allocated for the cluster.

== Cluster network security

[[tr12]]
// As a Domino user, I cannot use another user's active Dask cluster

The on-demand Dask clusters created by Domino are not meant for sharing between multiple users.
Each cluster is associated with a given workspace or a job instance.
Access to the cluster and the Dask web UI is restricted only to users who can access the workspace or the job attached to it.
This restriction is enforced at the networking level and the cluster is only reachable from the execution that provisioned it.

----- user_guide/on-demand-distributed-computing/index.txt -----
:page-version: 6.1
:page-title: On-demand distributed computing
:page-permalink: 8b4418
:page-order: 270

//This needs an introduction to describe how these relate to environments
// Also let's vary the sentences below, and not use the subjective term "easily".

Use Domino Cluster Environments to scale out compute-intensive workloads. Domino Cluster Environments are similar to Domino Standard Environments (DSEs), but with added libraries for specific cluster types. Just like DSEs, they include the same tools and packages, and you can use them to spin up Workspaces or Jobs. However, Domino Cluster Environments are mutually exclusive from non-cluster environments - you cannot use a Domino Cluster Environment in a non-cluster configuration.

The following clusters can be accessed with Domino:

* link:68faaa[Spark on Domino]
* link:747a51[On-Demand Dask]
* link:d13903[On-Demand Ray]
* link:d60880[On-Demand Open MPI]

== Clusters with Domino Nexus

link:c65074[Domino Nexus] has full support for Domino compute clusters in remote executions.
If the selected cluster framework includes a user interface, you can only access that user interface if the link:95520d[data plane] is configured for Workspaces.
If the data plane is not configured for Workspaces, clusters can still be used with Jobs, but the user interface link is disabled.
An admin can link:491fe8[Enable a data plane for Workspaces].


----- user_guide/on-demand-distributed-computing/mpi/configure-mpi-prerequisites.txt -----
:page-version: 6.1
:page-title: Configure MPI prerequisites
:page-permalink: 402c65
:page-order: 20

Before using on-demand Message Passing Interface (MPI) clusters on Domino, this functionality must be enabled and properly configured on your deployment.

== Enable MPI on your deployment
[[tr1]]
// As a Domino admin, I can make MPI cluster functionality globally available or unavailable by setting the global feature flag

Before you can access MPI clusters, your Domino Administrator must set the `ShortLived.MpiClustersEnabled` feature flag to `true` to enable the on-demand MPI functionality. The flag is `false` by default.

== Create a base Open MPI cluster environment
[[tr2]]
// As a Domino user, I can create an MPI cluster environment using a compatible external base image (e.g. Horovod or TensorFlow).

Domino does not include an Open MPI compatible link:0d73c6[compute environment] for use with the components of the cluster, by default. You can not create a cluster unless you have a compatible compute environment.

NOTE: When using on-demand MPI in Domino, you have one environment for the MPI cluster (cluster compute environment) and one for the workspace/job execution (compute environment). These environments must have the same version of Open MPI installed.

. To setup base image, click *Start from a custom base image* and enter an image URI for a deployable MPI image.
+
For workloads using Horovod, Tensorflow, or Pytorch, Domino recommends basing both the cluster and compute environments on an NGC image containing Open MPI. An example is https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tensorflow[TensorFlow^]. The current URI is `nvcr.io/nvidia/tensorflow:22.02-tf2-py3`. You can use other Open MPI images such as https://github.com/horovod/horovod[Horovod^]. For simple, CPU-only workloads not needing ML libraries, use the pre-built Domino MPI images: `quay.io/domino/mpi-cluster-environment:ubuntu22-mpi4.1.4-domino6.1`.

. To enable the environment for use when creating MPI clusters from workspaces and jobs, click *MPI* in the *Supported Clusters* list.
+
NOTE: The *Automatically make compatible with Domino* checkbox is removed when you click *MPI* in the *Supported Clusters* list.

. For visibility, set this attribute to the same value as any other link:f51038[compute environment].

. To create the environment, click *Create Environment*. See link:85ae58[Manage dependencies] to add packages to the environment.

=== Prepare your Open MPI execution compute environment
[[tr3]]
// As a Domino user, I can update a base compute environment for use with an MPI cluster workload (in the environment dockerfile or at runtime using a requirements.txt file)

You must also configure the Open MPI compute environments for workspaces and jobs that connect to your cluster. For an Open MPI compute environment you can:

* Recommended: Use the same base image as the cluster environment to create a new environment.
* Enhance the *Docker Instructions* section of an existing compute environment.

==== Create a new environment with the same image as the cluster environment:

. For your base image, click *Start from a custom base image* and enter a deployable MPI image.
+
If you are using Horovod, Tensorflow, or Pytorch, Domino recommends basing both the cluster and compute environments on a suitable NGC image containing Open MPI. An example of this is https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tensorflow[TensorFlow^]. The current URI is `nvcr.io/nvidia/tensorflow:22.02-tf2-py3`. For simple, CPU-only workloads not needing ML libraries, use the pre-built Domino MPI image: `quay.io/domino/mpi-cluster-environment:ubuntu22-mpi4.1.4-domino6.1`.

. Check *Automatically make compatible with Domino* unless you are using a Domino image.

. For visibility, set this attribute to the same value as any other link:f51038[compute environment].

. To create the environment, click *Create Environment*. See link:85ae58[Manage dependencies] to add packages to the environment.

==== Add Open MPI to an existing environment

. Open the environment and click *Edit Definition*.

. Add the following instructions to the *Dockerfile Instructions*, editing the instructions as needed:

+
[source,dockerfile]
----
ENV OMPI_VERSION=4.1.2
ENV OMPI_MAJOR_VERSION=4.1
ENV OMPI_SHA256=a400719b04375cd704d2ed063a50e42d268497a3dfede342986ab7a8d7e8dcf0

ENV DOMINO_USER=ubuntu
ENV DOMINO_GROUP=ubuntu

# Create ubuntu user
RUN if ! id 12574 &> /dev/null; then 
        groupadd -g 12574 ${DOMINO_GROUP}; 
        useradd -u 12574 -g 12574 -m -N -s /bin/bash ${DOMINO_USER}; 
    fi

WORKDIR /opt

#NOTE:build essentials is already present in most Domino distributions.

RUN apt-get -y update && apt-get -y install curl libdigest-sha-perl build-essential

# see https://www.open-mpi.org/faq/?category=running#mpirun-prefix to find config options.
# https://www.open-mpi.org/faq/?category=building#where-to-install
RUN curl -o openmpi-${OMPI_VERSION}.tar.gz https://download.open-mpi.org/release/open-mpi/v${OMPI_MAJOR_VERSION}/openmpi-${OMPI_VERSION}.tar.gz && 
    echo "${OMPI_SHA256}  openmpi-${OMPI_VERSION}.tar.gz" | shasum -a 256 -c && 
    tar -xf openmpi-${OMPI_VERSION}.tar.gz && 
    cd openmpi-${OMPI_VERSION} && 
    ./configure 
        --prefix=/opt/mpi 
        --enable-mpirun-prefix-by-default && 
    make -j $(nproc) all && 
    make install

# set the PATH vars appropriately using domino-defaults
# https://www.open-mpi.org/faq/?category=running#run-prereqs
RUN 
    echo "export PATH=/opt/mpi/bin:$PATH" >> /home/${DOMINO_USER}/.domino-defaults && 
    if [ -z "$LD_LIBRARY_PATH" ]; then 
        echo "export LD_LIBRARY_PATH=/opt/mpi/lib" >> /home/${DOMINO_USER}/.domino-defaults; 
    else 
        echo "export LD_LIBRARY_PATH=/opt/mpi/lib:$LD_LIBRARY_PATH" >> /home/${DOMINO_USER}/.domino-defaults; 
    fi && 
    echo "export OMPI_VERSION=$OMPI_VERSION" >> /home/${DOMINO_USER}/.domino-defaults

WORKDIR /
----
+
. Click *Build*.

----- user_guide/on-demand-distributed-computing/mpi/file-sync-mpi-clusters.txt -----
:page-version: 6.1
:page-title: File sync MPI clusters
:page-permalink: 71871d
:page-order: 30

NOTE: The Open MPI feature is only available with Domino versions 5.1.1 and later. Previous versions of Domino do not support this feature.

[[tr1]]
// As a Domino user, I must sync any file changes to MPI cluster workers before those changes will be picked up by a workload using that cluster.

When using Domino on-demand MPI clusters, data and code from the workspace must be synced with the worker nodes before submitting an `mpirun` run.

. Click the *File Changes* tab on the navigation bar.
+
NOTE: The tab displays changes since the last sync operation. The `mpirun` command does not detect changes since the last sync. You must manually sync to use updated code on the cluster worker nodes. Do not execute `mpirun` commands while files are syncing.

. Click *Sync* under *MPI Cluster File Sync*.

+
[[tr2]]
// As a Domino user, when I use the sync UI controls, any changes to /mnt or /repos will be applied to the file system on each MPI cluster worker.
The MPI file sync applies to the following directories:

* `/mnt` and subdirectories including `/imported` for Domino file system projects.

* `/mnt/code`, `/mnt/artifacts`, `/mnt/imported` and subdirectories for Git-based projects.

* `/repos`.

WARNING: On-demand clusters in Domino are temporary. Any data stored on cluster local storage, and not externally, is lost when the workload and the cluster shut down.

== Domino datasets
[[tr3]]
// MPI cluster workers have access to datasets attached to a workspace
[[tr4]]
// MPI cluster workers have access to datasets attached to a job

When you create an MPI cluster attached to a Domino workspace or job, any link:0a8d11[Domino dataset] accessible from the workspace or job is accessible from all components of the cluster under the same dataset mount path.
Worker nodes can also write to attached read/write Domino datasets.

== AWS credential propagation
[[tr5]]
// MPI cluster workers have the same AWS credentials and permissions enabled for the user

When link:eb6a88[AWS credential propagation] is enabled, temporary AWS credentials are automatically available on all MPI workers and your session. These credentials correspond to the roles enabled in your company's identity provider.

The credentials are automatically refreshed and available with a profile name corresponding to each role in the AWS credential file. The `AWS_SHARED_CREDENTIALS_FILE` environment variable contains the location of the file. This variable is in the proper search path for `s3fs` and `boto3`. See link:947ddd[Connect to generic S3] for more info.

----- user_guide/on-demand-distributed-computing/mpi/index.txt -----
:page-version: 6.1
:page-title: Distributed GPUs with Open MPI
:page-permalink: d60880
:page-order: 40

NOTE:  The Open MPI feature is only available with Domino versions 5.1.1 and later. Previous versions of Domino do not support this feature.

Message Passing Interface (https://www.mpi-forum.org/[MPI^]), is a communication protocol for distributed parallel computing. Domino validates the use of https://www.open-mpi.org/doc/current/[Open MPI^], a popular open-source MPI distribution that is widely used in high performance computing.

Open MPI has these features:

* *Leading open source MPI distribution:* Open MPI provides low-latency and high bandwidth, gradual parallelism, and flexibility.

* *Support for machine learning in high performance environments:* MPI is the underlying communication mechanism for higher-level machine learning training libraries. MPI is often used in Horovod to train models in high-performance environments.

== Orchestrate Open MPI on Domino

Domino can dynamically provision and orchestrate an MPI cluster directly on the infrastructure backing the Domino deployment. You get quick access without needing an IT team.

Starting a link:e6e601[Domino workspace] for interactive work or link:942549[Domino job] for batch processing, Domino creates, manages, and makes available a containerized MPI cluster to your execution.

== Use cases

Domino on-demand MPI clusters are suitable for the following workloads:

Distributed multi-GPU training::
Open MPI is ideal for distributed multi-GPU and multi-CPU training for Tensorflow, PyTorch, Keras, or MXNet models.

High performance computing::
MPI clusters have lower overhead than other distributed computing systems and are highly customizable.

== Next steps

* Learn how to enable and configure the functionality on your deployment in link:402c65[Configure MPI prerequisites].
* Find out how to link:71871d[file sync MPI clusters].
* Find out more about the link:9e50b2[Validated MPI version].
* Learn how to link:594e3c[create an on-demand MPI cluster] with the desired cluster settings attached to a Workspace or Job.
* Find out how you can link:85ae58[manage MPI dependencies].

----- user_guide/on-demand-distributed-computing/mpi/manage-dependencies.txt -----
:page-version: 6.1
:page-title: Manage MPI dependencies
:page-sidebar: Manage dependencies
:page-permalink: 85ae58
:page-order: 60

NOTE: The Open MPI feature is only available with Domino versions 5.1.1 and later. Previous versions of Domino do not support this feature.

The Domino Open MPI base image contains common dependencies like Pandas and NumPy. You can modify your environments to add additional packages or a specific package version.

You can package and manage dependencies for your link:402c65[MPI-enabled compute environments]. This creates flexibility in managing dependencies for individual projects or workloads without shared cluster worker complexities.

* To add a new cluster worker, add the appropriate statements in the *Docker Instructions* section of both the MPI cluster and Open MPI execution environments.
For example, to add a particular version of `PyTorch` or the `mpi4py` package include the following:

[source,Docker]
----
### Optionally specify version if desired
RUN pip install PyTorch==1.9.0
RUN pip install mpi4py
----

----- user_guide/on-demand-distributed-computing/mpi/validate-mpi-version.txt -----
:page-version: 6.1
:page-title: Validate MPI version
:page-permalink: 9e50b2
:page-order: 40

NOTE: The Open MPI feature is only available with Domino versions 5.1.1 and later. Previous versions of Domino do not support this feature.

For each minor release, Domino performs validation tests on a specific version of Open MPI. These tests cover the integration surface between the Domino platform and the on-demand MPI clusters the platform orchestrates. The testing is not a comprehensive test of Open MPI, because Domino does not modify that software.

NOTE: The current validated and supported version of Open MPI is `4.1.2`. Other Open MPI versions might function properly, but are not supported. Domino does not validate other distributions of MPI.

----- user_guide/on-demand-distributed-computing/mpi/work-with-your-cluster.txt -----
:page-version: 6.1
:page-title: Work with your MPI cluster
:page-sidebar: Work with your cluster
:page-permalink: 594e3c
:page-order: 50

NOTE: The Open MPI feature is only available with Domino versions 5.1.1 and later. Previous versions of Domino do not support this feature.

You can use MPI clusters with Domino workspaces, jobs, and scheduled jobs.

NOTE: MPI clusters are not available for apps, launchers, or Domino endpoints.

== Create an MPI cluster with Workspaces
[[tr1]]
// As a Domino user, I can launch a workspace with an MPI cluster attached and execute a worklodad requiring that cluster

=== Create an on-demand MPI cluster attached to a Domino Workspace

. Go to *Workspaces* > *New Workspace*.
. In the *Launch New Workspace* dialog click *Compute Cluster*. For your compute cluster, set the following cluster configuration attributes after selecting *MPI*:

* *Number of Workers* - The number of MPI node workers making up the cluster when it starts. The combined capacity of the workers is available for your workloads.
* *Worker Hardware Tier* - The amount of compute resources (CPU, GPU, and memory) made available to each MPI node worker.
* *Cluster Compute Environment* - Specifies the compute environment for the MPI cluster.
* *Dedicated local storage per executor* - The amount of dedicated storage in Gigabytes (2 ^ 30 bytes) available to each MPI worker. The storage is automatically mounted to `/tmp`. The storage is automatically provisioned when the cluster is created and de-provisioned when it is shut down.
+
WARNING: The local storage per worker must not be used for storing any data that must be available after the cluster is shut down.

. To launch your workspace, click *Launch*.

After the workspace launches it can access the MPI cluster you configured.

[[tr10]]
// As a Domino user, I can find the status of my MPI cluster shown at the top of my workspace.
NOTE: The cluster worker nodes can take longer to spin up than the workspace. The status of the cluster is displayed in the navigation bar.

=== Connect to your cluster

When provisioning your on-demand MPI cluster, Domino initializes environment variables. These variables contain the information to connect to your cluster. A `hostfile` is created with the worker node name. That file is located at `/domino/mpi/hosts`. Submit your `mpirun` jobs from your preferred IDE. The jobs use these default settings:

* Hosts are set to all workers. This does not include the workspace/job container.

* Slots are allotted based on the CPU cores in the selected worker hardware tier. The minimum is one. If you are using GPU worker nodes this is set to the minimum CPU/GPU requests from the hardware tier.

* Defaults are set for mapping (socket) and binding (core).

NOTE: You must perform an MPI file sync before you run commands on the worker nodes. See the link:71871d[File sync MPI clusters] for more information.

. Run a Python script:

[source,shell]
----
mpirun python my-training.py
----

You can pass arguments to the `mpirun` command. See https://www.open-mpi.org/doc/v4.0/man1/mpirun.1.php[Open MPI docs^]. The following are some frequently used parameters:

* `--x <env>` - Distribute an existing environment variable to all workers. You can specify only one environment variable per `-x` option.

* `--hostfile <your hostfile>` - Specify a new hostfile with your custom slots.

* `--map-by <core>` - Map to the specified object. The default is `socket`. Create a `rmaps_base_mapping_policy` environment variable to set a new mapping policy.

. Create a `hwloc_base_binding_policy` environment variable to set a new binding policy.

For example to add `-bind-to none`, to specify that a training process not be bound to a single core, and `-map-by slot`, use the following code:

[source,shell]
----
mpirun 
--bind-to none 
--map-by slot 
python my-training.py
----

NOTE: If you are using Horovod, Domino recommends running it directly using MPI. If not, enter a command specifying the number of worker nodes. For example, `-H server1:1,server2:1,server3:1` specifies three worker nodes. See the https://horovod.readthedocs.io/en/stable/mpi_include.html[Horovod docs^] for instructions on converting Horovod commands to MPI.

== Create an MPI cluster with Jobs

[[tr2]]
// As a Domino user, I can start a batch job with an MPI cluster attached and successfully execute job code requiring that cluster

=== Create an on-demand MPI cluster attached to a Domino Job

. Go to *Jobs* > *Run*.
. In the *Start a Job* dialog, click *Compute Cluster*.
+
[[tr3]]
// As a Domino user, I can change the MPI cluster settings (number of workers, worker hardware tier, compute environment, and local storage per worker) used to create an MPI cluster
[[tr4]]
// As a Domino user, I can only set up to the maximum concurrent executions as set by the Domino admin; the cluster settings field will not allow me to exceed the remaining executions available
+
For your compute cluster, set the following cluster configuration attributes:

* *Number of Workers* - The number of MPI node workers for the cluster when it starts. The combined capacity of the workers is available for your workloads.
+
[[tr5]]
// As a Domino admin, I can create hardware tiers for use only by MPI (or multiple) cluster types; if such tiers exist, only those tiers will be selectable, and no standard (non-MPI-cluster) hardware tiers will be available
* *Worker Hardware Tier* - The amount of compute resources (CPU, GPU, and memory) available to each MPI node worker.

* *Cluster Compute Environment* - Specifies the compute environment for the MPI cluster.

* *Dedicated local storage per executor* - The amount of dedicated storage in Gigabytes (2 ^ 30 bytes) available to each MPI worker. The storage is automatically mounted to `/tmp`. The storage is automatically provisioned when the cluster is created and de-provisioned when it is shut down.
+
WARNING: Do not store any data in the local storage that must be available after the cluster is shut down.

. To launch your job, click *Launch*.
+
. Enter the cluster settings and launch your job.
+
[[tr6]]
// As a Domino user, I can execute a command as a batch job and Domino will prefix mpirun to the command if an MPI cluster is attached to the job.
+
The job start command is wrapped in a `mpirun` prefix. The defaults were described previously.
+
NOTE: The Domino job is executing the `mpirun` command. Other scripts cannot run `mpirun` while that job is executing.
+
When writing scripts that are used in a job MPI cluster, you must save artifacts to an external location or to a Domino dataset, not to the project's files page (`/mnt`). Each project contains a default link:0a8d11[read/write dataset], accessed using the `/domino/datasets/local/${DOMINO_PROJECT_NAME}` path.
+
[[tr7]]
// As a Domino user, I can specify parameters to be used for mpirun commands by creating a file called `mpi.conf`` in my project's root directory.
To tune the Open MPI run in jobs, create an `mpi.conf` file at your root directory with the parameters you want. The root of your project is the `/mnt` working directory. This is linked to the `Files` page, or the `/mnt/code` directory if you are using a Git-based project. See https://www.open-mpi.org/faq/?category=tuning#setting-mca-params[Open MPI run-time tuning^] for more information.
+
Parameters are set one per line, for example:
+
* `mca_base_env_list = EXISTING_ENV_VAR`, where the environment variable is set in the workspace.

* `orte_hostfile = <new hostfile location`, copy the default hostfile from `domino/mpi/hosts` to `/mnt` and set `orte_hostfile=/mnt/hosts`.

* `rmaps_base_mapping_policy core`, equivalent to `--map-by <core>`

* `hwloc_base_binding_policy socket`, equivalent to `--bind-to socket`

== Cluster lifecycle
[[tr8]]
// As a Domino admin, I can view the MPI cluster resources created to execute a job or workspace and see that they are cleaned up when the workload is complete

When the workspace or job starts, a Domino on-demand MPI cluster is automatically provisioned with your MPI cluster settings. The cluster is attached to the workspace or job when the cluster becomes available.

When the workspace or job shut down the on-demand MPI cluster and all associated resources are automatically stopped and de-provisioned. This includes any compute resources and storage allocated for the cluster.

== Cluster network security
[[tr9]]
// As a Domino user, I cannot use another user's active MPI cluster

The on-demand MPI clusters created by Domino are not meant for sharing between multiple users. Each cluster is associated with a given workspace or job instance. Access to the cluster is restricted to users who can access the workspace or the job attached to the cluster. This restriction is enforced at the networking level and the cluster is only reachable from the execution that provisioned the cluster.

----- user_guide/on-demand-distributed-computing/ray/configure-prerequisites.txt -----
:page-version: 6.1
:page-title: Configure Ray prerequisites
:page-sidebar: Configure prerequisites
:page-permalink: 190175
:page-order: 20

[[configuring_ray_prerequisites]]

Ray functionality is available with Domino-provided images.
The Domino Ray Environment is built specifically for workspaces that control a Ray cluster.
It includes Ray on top of the typical DSE functionality.

You can get the latest released Ray images from link:799193[compute Environment catalog].

== Create an environment
[[tr1]]

// As a Domino user, I can create a Ray cluster environment using Domino-provided Ray images

To set up a Ray cluster, the user needs to first create two environments, one for the Ray cluster (base or worker environment) and one for the workspace/job execution (compute environment).

To create a new base Ray cluster environment, follow the general link:fa8137[environment creation steps] with the following `environment_attributes`:

* *Base Image*
+
Select *Custom Image* and enter an image URI that points to a deployable Ray image.
+
Domino recommends that you use the latest release tag for your version of Ray from the options published at link:799193[compute Environment catalog].
+
For example, for Domino 6.0 the following cluster images are available:

** `quay.io/domino/ray-cluster-environment:ray2.36.0-py3.10-domino6.0`
** `quay.io/domino/ray-cluster-gpu-environment:ray2.36.0-py3.10-gpu-domino6.0`

* *Supported Clusters*
+
Select *Domino managed Ray* (Required). This ensures that the environment will be available for use when creating Ray clusters from workspaces and jobs.

* *Visibility*
+
You can set this attribute the same way you would for any other link:f51038[compute environment] based on your desired visibility.

* *Dockerfile Instructions*
+
Leave blank to use the images as provided by Ray project.
+
You can modify this section to include additional packages that might be necessary for your workloads and must be available on the Ray cluster nodes.
+
See link:bb6af0[Manage dependencies] to learn more.

* *Pluggable Notebooks / Workspace Sessions*
+
Leave this section blank because the Ray base environments are not intended to include any workspace configuration.

[[creating_compute_ray_env]]
== Prepare your Ray execution compute environment
[[tr2]]
// As a Domino user, I can update a base compute environment for use with a Ray cluster workload (in the environment dockerfile or at runtime using a requirements.txt file)

In addition to the base Ray cluster environment, you must connect your workspace to your cluster.

Domino recommends that you use the Ray base image to create a compatible workspace.
See link:799193[compute Environment catalog].

For example, for Domino 6.0, the available compute image is:

`quay.io/domino/domino-ray-environment:ubuntu22-py3.10-r4.4-ray2.36.0-domino6.0`

=== Customize this Workspace compute environment

Use the image mentioned previously and add **Pluggable Workspace Tools**.

[source,yaml]
----
jupyter:
  title: "Jupyter (Python, R, Julia)"
  iconUrl: "/assets/images/workspace-logos/Jupyter.svg"
  start: [ "/opt/domino/workspaces/jupyter/start" ]
  supportedFileExtensions: [ ".ipynb" ]
  httpProxy:
    port: 8888
    rewrite: false
    internalPath: "/{{ownerUsername}}/{{projectName}}/{{sessionPathComponent}}/{{runId}}/{{#if pathToOpen}}tree/{{pathToOpen}}{{/if}}"
    requireSubdomain: false
jupyterlab:
  title: "JupyterLab"
  iconUrl: "/assets/images/workspace-logos/jupyterlab.svg"
  start: [  "/opt/domino/workspaces/jupyterlab/start" ]
  httpProxy:
    internalPath: "/{{ownerUsername}}/{{projectName}}/{{sessionPathComponent}}/{{runId}}/{{#if pathToOpen}}tree/{{pathToOpen}}{{/if}}"
    port: 8888
    rewrite: false
    requireSubdomain: false
vscode:
  title: "vscode"
  iconUrl: "/assets/images/workspace-logos/vscode.svg"
  start: [ "/opt/domino/workspaces/vscode/start" ]
  httpProxy:
    port: 8888
    requireSubdomain: false
rstudio:
  title: "RStudio"
  iconUrl: "/assets/images/workspace-logos/Rstudio.svg"
  start: [ "/opt/domino/workspaces/rstudio/start" ]
  httpProxy:
    port: 8888
    requireSubdomain: false
----

----- user_guide/on-demand-distributed-computing/ray/index.txt -----
:page-version: 6.1
:page-title: Use on-demand Ray
:page-permalink: d13903
:page-order: 30

https://ray.io[Ray.io^] is a distributed execution framework that makes
it easy to scale your single machine applications, with little or no
changes, and to leverage state-of-the-art machine learning libraries.

Ray provides a set of core low-level primitives as well as a family of pre-packaged libraries that take advantage of these primitives
to enable solving powerful machine learning problems.

The following libraries come packaged with Ray:

* https://docs.ray.io/en/master/tune/index.html[Tune: Scalable Hyperparameter Tuning^]
* https://docs.ray.io/en/releases-1.11.0/raysgd/raysgd.html[RaySGD: Distributed Training Wrappers^]
// Marty - The above link was dead
* https://docs.ray.io/en/latest/rllib/index.html[RLlib: Industry-Grade Reinforcement Learning^]
// Marty - The above link was also dead.
* https://docs.ray.io/en/master/serve/index.html#rayserve[Ray Serve: Scalable and Programmable Serving^]

Additionally, Ray has been adopted as a foundational framework by a large number of open source ML frameworks which now have community-maintained Ray integrations.

// do we have a sme who can update the link that was: <https://docs.ray.io/en/master/ray-libraries.html#ray-oss-list>`_ - otherwise we can leave the link out.

== Orchestrate Ray on Domino

Domino can dynamically provision and orchestrate a Ray
cluster directly on the infrastructure backing the Domino instance. This
allows Domino users to get quick access to Ray without having to rely on
their IT team.

When you start a link:e6e601[Domino workspace]
for interactive work or a link:942549[Domino job] for batch
processing, Domino will create, manage, and make available a containerized Ray cluster to your execution.

//DOCS-1006
See Domino's https://github.com/dominodatalab/domino-quickstart-ray[quick-start-ray^] project.

== Suitable use cases

Domino on-demand Ray clusters are suitable for the following workloads:

Distributed multi-node training:: RaySGD provides a lightweight
mechanism for taking existing PyTorch and Tensorflow models and scaling
them across multiple machines to dramatically reduce training times. Ray
is suitable for both distributed CPU and GPU training.
Hyperparameter optimization::
Launch a distributed hyperparameter sweep with just a few lines of code and no adaptation of your existing training harness, and take advantage of a large number of advanced parameter search algorithms.
Reinforcement learning:: Ray, in combination with the RLlib library,
allows you to take advantage of a number of built-in reinforcement
learning algorithms, but also provides a general framework for developing
your own.

[NOTE]
====
While Ray offers a scalable serving capability, the ephemeral nature of
the Domino Ray clusters does not make it a good fit for this use case.
====

== Next steps

* Find out more about the link:cae013[Validated Ray version].
* Learn how to enable and configure the functionality on your deployment in link:190175[Configure Ray prerequisites].
* Learn how to link:c50248[create an on-demand Ray cluster] with the desired cluster settings attached to a Workspace or Job.
* Find out how you can link:bb6af0[manage Ray dependencies].
* Learn how to link:a5638c[Access data with Ray].

----- user_guide/on-demand-distributed-computing/ray/manage-dependencies.txt -----
:page-version: 6.1
:page-title: Manage Ray dependencies
:page-sidebar: Manage dependencies
:page-permalink: bb6af0
:page-order: 40

[[ray_managing_dependencies]]

While the Ray base images (especially the `ray-ml` flavor) come with a comprehensive set of packages frequently used for machine learning, you may still need to modify your environment when you need additional packages or when you need a specific version of a given package.

Domino allows you to easily package and manage dependencies as part of your link:190175#creating_base_ray_env[Ray-enabled compute environments]. This approach creates the flexibility to manage dependencies for individual projects or workloads without having to deal with the complexity of a shared cluster.

To add a new dependency, you must add the appropriate statements in the *Docker Instructions* section of the relevant Ray base and Ray execution compute environments.

For example, if you wanted to add a particular version of PyTorch you might include the following.

[source,Docker]
----
### Optionally specify version if desired
RUN pip install torch==1.8.0 torchvision==0.9.0
----



----- user_guide/on-demand-distributed-computing/ray/validated-ray-version.txt -----
:page-version: 6.1
:page-title: Validated Ray version
:page-permalink: cae013
:page-order: 10

For each minor release (starting with Domino 4.5), Domino performs a set
of validation tests against the *most recent released version* of Ray.
The purpose of the tests is to cover the integration surface between the
Domino platform and the on-demand Ray clusters that the platform
orchestrates. The testing is not intended to be a comprehensive test of
Ray itself since Domino does not modify that software.

[NOTE]
====
The validated and supported version of Ray is
`2.0`.
Other Ray versions are likely to function properly, but Domino will not
be able to offer technical support.
====

----- user_guide/on-demand-distributed-computing/ray/work-with-data.txt -----
:page-version: 6.1
:page-title: Access data with Ray
:page-sidebar: Work with data
:page-permalink: a5638c
:page-order: 50


When using a Domino on-demand Ray cluster, any data that will be used, created, or modified as part of the interaction needs to go into an external data store.

[WARNING]
====
On-demand clusters in Domino are ephemeral. Any data that is stored on cluster local storage and not externally will be lost upon termination of the workload and the cluster.
====

== Use Domino Datasets
[[tr1]]
// Ray cluster workers have access to datasets attached to a workspace
[[tr2]]
// Ray cluster workers have access to datasets attached to a job

When you create a Ray cluster attached to a Domino workspace or job, any link:0a8d11[Domino dataset] accessible from the workspace or job will also be accessible from all components of the cluster under the same dataset mount path.
You will then be able to access the files from your code using the same path regardless of whether your code runs on your workspace of job container or in a Ray task on the cluster.

For example, to read a file you would use the following:

[source,python]
----
file = open("/mnt/data/my_dataset/file.csv")
----

== Using S3
[[tr3]]
// Ray cluster workers are able to access S3

To access Amazon S3 (or S3 compatible object store) data with Ray you can use any of the libraries you already use (for example, `boto3`, `s3fs`, or `pandas`). When access will happen from Ray workers, the only prerequisite is to make sure that the relevant libraries and dependencies are available on both the base cluster environment and the execution environment.

=== AWS credential propagation
[[tr4]]
// Ray cluster workers have the same AWS credentials and permissions enabled for the user

When link:eb6a88[AWS credential propagation] is enabled for your deployment, temporary AWS credentials corresponding to the roles enabled for you in your company identity provider will be automatically available on all Ray worker nodes.

The credentials will be automatically refreshed and available under a
profile name corresponding to each role in an AWS credential file. Ray
worker code can then utilize these credentials for seamless and secure
access.

----- user_guide/on-demand-distributed-computing/ray/work-with-your-cluster.txt -----
:page-version: 6.1
:page-title: Work with your Ray cluster
:page-sidebar: Work with your cluster
:page-permalink: c50248
:page-order: 30

== Create a cluster with Workspaces
[[tr1]]
// As a Domino user, I can launch a workspace with a Ray cluster attached and execute a workload requiring that cluster

Create an on-demand Ray cluster attached to a Domino Workspace:

. Go to *Workspaces* > *New Workspace*.
. From Launch New Workspace, select the *Compute Cluster* step.
. Specify the link:#ray_cluster_settings[cluster settings] and launch your workspace. After the workspace is up, it will have access to the Ray cluster you configured.


== Create a cluster with Jobs
[[tr2]]
// As a Domino user, I can start a batch job with a Ray cluster attached and successfully execute job code requiring that cluster

Create an on-demand Ray cluster attached to a Domino Job:

. Go to *Jobs* > *Run*.
. From Start a Job, select the *Compute Cluster* step.
. Specify the link:#ray_cluster_settings[cluster settings] and launch your job. The job will have access to the Ray cluster you configured.


You can use any Python script that interacts with your Ray cluster.

[[ray_cluster_settings]]
== Set your cluster settings

[[tr3]]
// As a Domino user, I can change the Ray cluster settings (number of workers, worker and head hardware tiers, compute environment, and local storage per executor) used to create a Ray cluster

Set the following cluster settings:

Min Workers::
The number of Ray node workers in the Ray cluster when it starts. If *Auto-scale workers* is not enabled, this will always be the size of the cluster. The combined capacity of the workers will be available for your workloads.

Max Workers::
The maximum number of Ray node workers that the cluster can reach when
*Auto-scale workers* is enabled. See link:#ray_cluster_autoscaling[cluster autoscaling].

Cluster size: Limit::
[[tr4]]
// As a Domino user, I can only set up to the maximum concurrent executions as set by the Domino admin; the cluster settings field will not allow me to exceed the remaining executions available
The maximum number of workers that you can make available to your
cluster is limited by the number of per-user executions that your Domino
administrator has configured for your deployment or by the maximum
simultaneous executions of the underlying Hardware Tier used for
workers.

In addition to the number of Ray node workers, you will need one slot for
your cluster master and one slot for your workspace or job.

Worker Hardware Tier::
The amount of compute resources (CPU, GPU, and memory) that will be made
available to each Ray node worker.
Head Hardware Tier::
Same mechanics as the worker hardware tier, but applied to the resources
that will be available for your Ray cluster head node.
+
The Ray head node coordinates the Ray workers, so it does not need a significant amount of CPU resources. It will host the Ray Global Control Store. The amount of required memory will depend on the complexity of your application.
+
[[tr13]]
// As a Domino admin, I can create hardware tiers for use only by Ray (or multiple) cluster types; if such tiers exist, only those tiers will be selectable, and no standard (non-Ray-cluster) hardware tiers will be available
By default, any hardware tier is available when selecting resources for
your worker and head. Domino administrators can optionally configure
compute cluster
link:eca4b2[dedicated hardware tiers].

Cluster Compute Environment::
Designates your compute environment for the Ray cluster.
Dedicated local storage per executor::
The amount of dedicated storage in Gigabytes (2^30 bytes) that will be
available to each Ray worker.
+
The storage will be automatically mounted to `/tmp`.
+
The storage will be automatically provisioned when the cluster is
created and de-provisioned when it is shut down.
+
[WARNING]
====
The local storage per worker must not be used for storing any data
that must be available after the cluster is shut down.
====

[[ray_cluster_autoscaling]]
== Cluster auto-scaling
[[tr5]]
// As a Domino user, I can set my Ray cluster to autoscale from a minimum to a maximum number of workers


Use cluster auto-scaling to start with a small cluster which automatically scales up and down in response to the resource consumption of your workload. This approach uses resources more efficiently for bursty workloads.

[[tr6]]
// As a Domino user, I can see my cluster size increase when my workload uses more than the CPU or memory threshold
By default, the cluster size will increase when the average CPU utilization of your workload reaches 80%. A Domino administrator can further refine the
link:71d6ad#compute-cluster-auto-scaling[auto-scaling settings]
by including memory utilization or changing the scaling thresholds.

[[tr7]]
// As a Domino user, I can see my cluster size decrease when my usage falls below the CPU or memory threshold
Scale down happens if resource utilization is low for a period of at least 5 minutes. Depending on the workload that you are executing, scale down might terminate cluster nodes that contain intermediate results which must be recomputed.


== Connect to your cluster
[[tr8]]
// As a Domino user, I can execute code to connect to an use the Ray cluster I have initiated

When provisioning your on-demand Ray cluster, Domino sets up environment
variables that hold the information needed to connect to your cluster.

Use the following snippet to connect:


[NOTE]
====
The preferred method to connect to a Ray cluster uses a modified version of `ray.init()`.
For more information, including the proper way to connect to a Ray cluster depending on the version, see the relevant section of the https://docs.ray.io/en/latest/index.html[Ray docs^].
====


[source,python]
----
import ray
import ray.util
import os

...

if ray.is_initialized() == False:
   service_host = os.environ["RAY_HEAD_SERVICE_HOST"]
   service_port = os.environ["RAY_HEAD_SERVICE_PORT"]
   ray.init(f"ray://{service_host}:{service_port}")
----

== Access the Ray web UI

Ray provides a built-in dashboard with access to metrics, charts, and
other features that helps users understand the Ray cluster, libraries, and
workloads.

Use the dashboard to:

* View cluster metrics.
* View logs, error, and exceptions across many machines in a single
pane.
* View resource utilization, tasks, and logs per node and per actor.
* Kill actors and profile Ray jobs.
* See tune jobs and trial information.

Domino makes the Ray web UI available for active on-demand clusters
attached to both workspaces and jobs.

NOTE: In a hybrid Domino deployment, if the link:95520d[data plane] is not configured for workspaces, then the user interface link is disabled.
An admin can link:491fe8[Enable a data plane for workspaces].

=== Ray web UI from Workspaces
[[tr9]]
// As a Domino user, I can view the Web UI associated with my Ray cluster from its tab in my workspace

The Ray web UI is available from a dedicated tab in your workspace.


=== Ray UI from Jobs
[[tr10]]
// As a Domino user, I can view the Web UI associated with my Ray cluster from a link in my job details

The Ray web UI is also available for running jobs from the *Details* tab.


== Cluster lifecycle
[[tr11]]
// As a Domino admin, I can view the Ray cluster resources created to execute a job or workspace and see that they are cleaned up when the workload is complete

On workspace or job startup, a Domino on-demand Ray cluster with its
link:#ray_cluster_settings[cluster settings] is automatically
provisioned and attached to the workspace or job as soon as the cluster
becomes available.

When the workspace or job terminates, the on-demand Ray cluster and all
associated resources are automatically terminated and deprovisioned.
This includes any compute resources and storage allocated for the
cluster.

== Cluster network security
[[tr12]]
// As a Domino user, I cannot use another user's active Ray cluster

The on-demand Ray clusters created by Domino are not meant to be shared
between users. Each cluster is associated with a given workspace or a job instance. Access to the cluster and the Ray web UI is restricted only to users who can access the workspace or the job attached to it. This restriction is enforced at the networking level and the cluster can only be reached from the execution that provisioned it.

----- user_guide/on-demand-distributed-computing/spark/hadoop-and-spark/amazon-emr-cluster.txt -----
:page-version: 6.1
:page-title: Connect to an Amazon EMR cluster from Domino
:page-sidebar: Amazon EMR cluster
:page-permalink: b18456
:page-order: 40

Domino supports connecting to an https://aws.amazon.com/emr/[Amazon EMR^]
cluster through the addition of cluster-specific binaries and
configuration files to your
link:f51038[Domino environment].

At a high level, the process is as follows:

. Connect to the EMR Master Node and gather the required binaries and
configuration files needed by Domino.
. Create a new Domino environment that uses the uploaded files to enable
connections to your cluster.
. Enable YARN integration for the Domino projects that you want to use
with the EMR cluster.

Domino supports the following types of connections to an EMR cluster:

* https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.0/bk_cloud-data-access/content/using-fs-shell.html[FS shell^] 
* https://books.japila.pl/apache-spark-internals/[spark-shell^]
* https://spark.apache.org/docs/latest/submitting-applications.html[spark-submit^]
* https://spark.apache.org/docs/latest/api/python/index.html[PySpark^]

== Requirements

These instructions are written with the following requirements:

* Domino needs to be routable from the EMR cluster by private EC2 IP.
This can be achieved by launching EMR directly into Domino's VPC or via
VPC Peering.
* Your security groups are configured to allow traffic between EMR and
Domino. The Domino node security group, the EMR Master Node, and the EMR
Worker Node security groups all need to allow TCP traffic between them.

[[tr1]] 
// As a Domino admin/user. I can create a Domino environment compatible with an Amazon EMR cluster

== Gather and serve the required binaries and configuration files

You will find the necessary files for setting up your Domino environment on the https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-master-core-task-nodes.html[EMR Master Node^]. To get started, https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-connect-master-node-ssh.html[connect to your Master Node via SSH^].

After you are connected to the Master Node, use `vi` or the editor of your choice
to create a script called `domino-emr-config-maker.sh`. Copy in the
following code and save the script.

NOTE: This script also captures Hive binaries. If you do not have Hive
installed on your EMR cluster, you may need to remove the Hive-related
commands from this script.

[source,sh]
----
#!/bin/bash

rm -rf www
rm -rf /tmp/hadoop-binaries-configs

mkdir -p www
mkdir -p /tmp/hadoop-binaries-configs/configs

cp -rL /etc/hadoop /tmp/hadoop-binaries-configs/configs
cp -rL /etc/hive /tmp/hadoop-binaries-configs/configs
cp -rL /etc/spark /tmp/hadoop-binaries-configs/configs

cp -r /usr/lib/hadoop /tmp/hadoop-binaries-configs
cp -r /usr/lib/hadoop-lzo /tmp/hadoop-binaries-configs
cp -r /usr/lib/spark /tmp/hadoop-binaries-configs
cp -r /usr/share/aws /tmp/hadoop-binaries-configs
cp -r /usr/share/java /tmp/hadoop-binaries-configs

cd /tmp/hadoop-binaries-configs/configs/hadoop/conf/
sed -i '$ d'  hdfs-site.xml
echo "<property>" >> hdfs-site.xml
echo "<name>dfs.client.use.datanode.hostname</name>" >> hdfs-site.xml
echo "<value>true</value>" >> hdfs-site.xml
echo "</property>" >> hdfs-site.xml
echo "</configuration>" >> hdfs-site.xml

cd /tmp
tar -zcf hadoop-binaries-configs.tar.gz hadoop-binaries-configs
cd ~
mv /tmp/hadoop-binaries-configs.tar.gz www/
cd www
/usr/bin/python3 -m http.server
----

This script bundles together all of the required binaries and
configurations and serves it via a web server on port 8000 of the Master
Node. You will need to open port 8000 in your cluster's security group
if you have not already.

Before moving on, note the private IP address of your EMR Master Node.
This will be available in your connection's prompt.

Execute this script via the command `bash domino-emr-config-maker.sh` to
begin the bundling and launch the web server. You will want to leave your
`ssh` connection to the Master Node open while finishing the rest of
this setup.

== Create a Domino Environment for connecting to EMR

. Create a new Domino environment with the latest version of the Domino
Analytics Distribution as its base image.
. Edit this environment, and add the following code to the environment's
Dockerfile. Be sure to replace `<MASTER_NODE_PRIVATE_IP>` with the
private IP address you noted earlier.
+
[source,dockerfile]
----
ENV EMR_MASTER_PRIVATE_IP <MASTER_NODE_PRIVATE_IP>

USER root
RUN echo "ubuntu ALL=(ALL:ALL) NOPASSWD: ALL" >> /etc/sudoers

RUN mkdir /tmp/domino-hadoop-downloads

# Download the binaries and configs gzip from EMR master.
#
# This downloaded gzip archive should contain a configs directory with
# hadoop, hive, and spark subdirectories directories.
#
# You may need to edit this depending on where you are running the web server on your EMR master.
RUN wget -q http://$EMR_MASTER_PRIVATE_IP:8000/hadoop-binaries-configs.tar.gz -O /tmp/domino-hadoop-downloads/hadoop-binaries-configs.tar.gz && 
    tar xzf /tmp/domino-hadoop-downloads/hadoop-binaries-configs.tar.gz -C /tmp/domino-hadoop-downloads/

RUN cp -r /tmp/domino-hadoop-downloads/hadoop-binaries-configs/configs/hadoop /etc/hadoop && 
    cp -r /tmp/domino-hadoop-downloads/hadoop-binaries-configs/configs/hive /etc/hive && 
    cp -r /tmp/domino-hadoop-downloads/hadoop-binaries-configs/configs/spark /etc/spark

RUN mv /tmp/domino-hadoop-downloads/hadoop-binaries-configs/aws /usr/share/aws
RUN mv /tmp/domino-hadoop-downloads/hadoop-binaries-configs/hadoop /usr/lib/hadoop
RUN mv /tmp/domino-hadoop-downloads/hadoop-binaries-configs/hadoop-lzo /usr/lib/hadoop-lzo
RUN mv /tmp/domino-hadoop-downloads/hadoop-binaries-configs/spark /usr/lib/spark
RUN cp -r /tmp/domino-hadoop-downloads/hadoop-binaries-configs/java/* /usr/share/java/

RUN 
echo 'export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64' >> /home/ubuntu/.domino-defaults && 
echo 'export HADOOP_HOME=/usr/lib/hadoop' >> /home/ubuntu/.domino-defaults && 
echo 'export SPARK_HOME=/usr/lib/spark' >> /home/ubuntu/.domino-defaults && 
echo 'export PYTHONPATH=${PYTHONPATH:-}:${SPARK_HOME:-}/python/' >> /home/ubuntu/.domino-defaults && 
echo 'export PYTHONPATH=${PYTHONPATH:-}:${SPARK_HOME:-}/python/lib/py4j-0.10.7-src.zip' >> /home/ubuntu/.domino-defaults && 
echo 'export PATH=${PATH:-}:${SPARK_HOME:-}/bin' >> /home/ubuntu/.domino-defaults && 
echo 'export PATH=${PATH:-}:${HADOOP_HOME:-}/bin' >> /home/ubuntu/.domino-defaults && 
echo 'export HADOOP_CONF_DIR=/etc/hadoop/conf' >> /home/ubuntu/.domino-defaults && 
echo 'export YARN_CONF_DIR=/etc/hadoop/conf' >> /home/ubuntu/.domino-defaults && 
echo 'export SPARK_CONF_DIR=/etc/spark/conf' >> /home/ubuntu/.domino-defaults
RUN mkdir -p /var/log/spark/user/ubuntu
RUN chown ubuntu:ubuntu /var/log/spark/user/ubuntu
RUN chmod -R 777 /usr/lib/spark/conf

USER ubuntu
----

. Build the new revision of the environment by clicking the button on
the bottom of the page. You may want to follow along by viewing the
Build Logs of the build, accessible from the Revisions table of the
environment.
+
NOTE: If the build hangs or fails, you may need to adjust the inbound
rules of your security groups as described at the start of this doc.
. After the environment builds successfully, you may stop the web server
on your EMR Master Node that was launched earlier and close the `ssh`
connection.

[[tr2]] 
// As a Domino user. I can connect to an Amazon EMR cluster via a properly configured project
[[configure-a-domino-project-for-use-with-an-emr-cluster]]
== Configure a Domino Project for use with an EMR cluster

This procedure assumes that an environment with the necessary client
software has been created according to the instructions above. Ask your
Domino admin for access to such an environment. Note that you may need
to provide Domino with additional options when setting up your project.
Your Domino or AWS administrators should be able to provide you with the
correct values for these options.

. Open the Domino project you want to use with your EMR cluster, then
click *Settings* from the project menu.
. On the *Integrations* tab, click to select *YARN* integration from the
Apache Spark panel.
. Use `root` as the *Hadoop username*.
. If your work with the cluster generates many warnings about missing
Java packages, you can suppress these by adding the following to *Spark
Configuration Options*.
+
Key: `spark.hadoop.yarn.timeline-service.enabled`
+
Value: `false`

. After inputting your *YARN* configuration, click *Save*.
. On the *Hardware & Environment* tab, change the project default
environment to the one you built earlier with the binaries and
configuration files.

You are now ready to start Runs from this project that interact with
your EMR cluster.

----- user_guide/on-demand-distributed-computing/spark/hadoop-and-spark/cloudera-cdh5-cluster.txt -----
:page-version: 6.1
:page-title: Connect to a Cloudera CDH5 cluster from Domino
:page-sidebar: Cloudera CDH5 cluster
:page-permalink: 83e94a
:page-order: 10

Domino supports connecting to a
https://www.cloudera.com/services-and-support/tutorials/how-to-create-a-cdp-private-cloud-base-development-cluster.html[Cloudera CDH5^]
cluster through the addition of cluster-specific binaries and
configuration files to your
link:f51038[Domino environment].

At a high level, the process is as follows:

. Connect to your
https://www.cloudera.com/documentation/enterprise/latest/topics/cdh_sg_gateway_setup.html[CDH5
edge or gateway node^] and gather the required binaries and configuration
files, then download them to your local machine.
. Upload the gathered files into a Domino project to allow access by the
Domino environment builder.
. Create a new Domino environment that uses the uploaded files to enable
connections to your cluster.
. Enable YARN integration for the Domino projects that you want to use
with the CDH5 cluster.

Domino supports the following types of connections to a CDH5 cluster:

* https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.4/bk_cloud-data-access/content/using-fs-shell.html[FS shell^]
// Marty - 403 error in old link
* https://www.cloudera.com/documentation/spark2/latest/topics/spark_running_apps.html[spark2-shell^]
* https://www.cloudera.com/documentation/spark2/latest/topics/spark_running_apps.html[spark2-submit^]
* https://spark.apache.org/docs/latest/api/python/index.html[PySpark^]
* https://hadoop.apache.org/docs/r3.1.1/hadoop-yarn/hadoop-yarn-site/YarnCommands.html[YARN shell^]


[[tr1]]
// As a Domino admin/user. I can create a Domino environment compatible with a Cloudera CDH5 cluster

== Gather the required binaries and configuration files

You will find most of the files for setting up your Domino
environment on your
https://www.cloudera.com/documentation/enterprise/latest/topics/cdh_sg_gateway_setup.html[CDH5
edge or gateway node^]. To get started, connect to the edge node via SSH,
then follow the steps below.

. Create a directory named `hadoop-binaries-configs` at `/tmp`.
+
[source,shell]
----
mkdir /tmp/hadoop-binaries-configs
----
. Create the following subdirectories inside
`/tmp/hadoop-binaries-configs/`.
+
[source,shell]
----
mkdir /tmp/hadoop-binaries-configs/configs

mkdir /tmp/hadoop-binaries-configs/parcels
----
. Optional: If your cluster uses Kerberos authentication, create the
following subdirectory in `/tmp/hadoop-binaries/configs/`.
+
[source,shell]
----
mkdir /tmp/hadoop-binaries-configs/kerberos
----
+
Then, copy the `krb5.conf` Kerberos configuration file from `/etc/` to
`/tmp/hadoop-binaries-configs/kerberos`.
+
[source,shell]
----
cp /etc/krb5.conf /tmp/hadoop-binaries-configs/kerberos/
----
. Copy the `CDH` and `SPARK2` directories from `/opt/cloudera/parcels/`
to `/tmp/hadoop-binaries-configs/parcels/`. These directories will have
a version number appended to their names, so complete the appropriate
directory name in the commands shown below.
+
[source,shell]
----
cp -R /opt/cloudera/parcels/CDH-<version>/ /tmp/hadoop-binaries-configs/parcels/
cp -R /opt/cloudera/parcels/SPARK2-<version>/ /tmp/hadoop-binaries-configs/parcels/
----
. Copy the `hadoop`, `hive`, `spark`, and `spark2` directories from
`/etc/` to `/tmp/hadoop-binaries-configs/configs/`.
+
[source,shell]
----
cp -R /etc/hadoop /tmp/hadoop-binaries-configs/configs/
cp -R /etc/hive /tmp/hadoop-binaries-configs/configs/
cp -R /etc/spark2 /tmp/hadoop-binaries-configs/configs/
cp -R /etc/spark /tmp/hadoop-binaries-configs/configs/
----
. On the edge node, run the following command to identify the version of Java running on the cluster.
+
[source,shell]
----
java -version
----
+
You should then
https://www.oracle.com/technetwork/java/javase/downloads/index.html[download
a JDK .tar file from the Oracle downloads page^] that matches that
version. The filename will have a pattern like the following.
+
`jdk-8u211-linux-x64.tar.gz`
+
Keep this JDK handy on your local machine for use in a future step.
. Compress the `/tmp/hadoop-binaries-configs/` directory to a gzip
archive.
+
[source,shell]
----
cd /tmp

tar -zcf hadoop-binaries-configs.tar.gz hadoop-binaries-configs
----
+
When finished, use `SCP` to download the archive to your local machine.
. Next, you'll need to extract the archive on your local machine, add a
`java` subdirectory, then add the JDK .tar file you downloaded earlier to
the `java` subdirectory.
+
[source,shell]
----
tar xzf hadoop-binaries-configs.tar.gz

mkdir hadoop-binaries-configs/java

cp jdk-8u211-linux-x64.tar.gz hadoop-binaries-configs/java/
----
. When finished, your `hadoop-binaries-configs` directory should have the
following structure:
+
[source,console]
----
hadoop-binaries-configs/
  ├── configs/
        ├── hadoop/
        ├── hive/
        ├── spark/
        └── spark2/
  ├── java/
        └── jdk-8u211-linux-x64.tar.gz
  ├── parcels
        ├── CDH-version/
        └── SPARK-version/
  └── kerberos/  # optional
        └── krb5.conf
----
. If your directory contains all the required files, you can now
compress it to a gzip archive again in preparation for uploading to
Domino in the next step.
+
[source,shell]
----
tar -zcf hadoop-binaries-configs.tar.gz hadoop-binaries-configs
----




== Upload the binaries and configuration files to Domino

Use the following procedure to upload the archive you created in the
previous step to a public Domino project. This will make the file
available to the Domino environment builder.

. Log in to Domino, then create a new public project.

. Open the
*Code*
page for the new project, then click to *browse for
files* and select the archive you created in the previous section. Then
click *Upload*.
. After the archive has been uploaded, click the gear menu next to it on
the Files page, then right click *Download* and click *Copy Link
Address*. Save the copied URL in your notes, as you will need it in the
next step.
+
After you have recorded the download URL of the archive, you're ready to
build a Domino environment for connecting to your CDH5 cluster.




== Create a Domino environment for connecting to CDH5

. Click *Environments* from the Domino main menu, then click *Create Environment*.
. Give the environment an informative name, then choose a base
environment that includes the version of Python that is installed on the
nodes of your CDH5 cluster. Most Linux distributions ship with Python
2.7 by default, so you will see the
link:0d73c6[Domino analytics distribution for Python 2.7]
used as the base image in the following examples. Click *Create* when
finished.
. After creating the environment, click *Edit Definition*. Copy the
following example into your Dockerfile instructions, then be sure to edit it
wherever necessary with values specific to your deployment and cluster.
+
In this Dockerfile, wherever you see a hyphenated instruction enclosed
in angle brackets like `<paste-your-domino-download-url-here>`, be sure to
replace it with the corresponding value you recorded in previous steps.
+
You may also need to edit commands that follow to match downloaded
filenames.
+
[source,dockerfile]
----
USER root

# Give user ubuntu ability to sudo as any user including root
RUN echo "ubuntu ALL=(ALL:ALL) NOPASSWD: ALL" >> /etc/sudoers

# Set up directories
RUN mkdir -p /opt/cloudera/parcels && 
    mkdir /tmp/domino-hadoop-downloads && 
    mkdir /usr/java

# Download the binaries and configs gzip you uploaded to Domino.
# This downloaded gzip file should have the following
# - CDH and Spark2 parcel directories in a 'parcels' sub-directory.
# - java installation tar file in 'java' sub-directory
# - krb5.conf in 'kerberos' sub-directory
# - hadoop, hive, spark2 and spark config directories a 'configs' sub-directory
RUN wget --no-check-certiticate <paste-your-domino-download-url-here> -O /tmp/domino-hadoop-downloads/hadoop-binaries-configs.tar.gz && 
    tar xzf /tmp/domino-hadoop-downloads/hadoop-binaries-configs.tar.gz -C /tmp/domino-hadoop-downloads/

# Install kerberos client and update the kerberos configuration file
RUN apt-get -y install krb5-user telnet && 
    cp /tmp/domino-hadoop-downloads/hadoop-binaries-configs/kerberos/krb5.conf /etc/krb5.conf

# Install version of Java that matches hadoop cluster and update environment variables
# Your JDK may have a different filename depending on your cluster's version of Java
RUN tar xvf /tmp/domino-hadoop-downloads/hadoop-binaries-configs/java/jdk-8u162-linux-x64.tar -C /usr/java
ENV JAVA_HOME=/usr/java/jdk1.8.0_162
RUN echo "export JAVA_HOME=/usr/java/jdk1.8.0_162" >> /home/ubuntu/.domino-defaults && 
    echo "export PATH=$JAVA_HOME/bin:$PATH" >> /home/ubuntu/.domino-defaults

# Install CDH hadoop-client binaries from cloudera ubuntu trusty repository.
# This example shows client binaries for CDH version 5.15 here.
# Update these commands with the CDH version that matches your cluster.
RUN echo "deb [arch=amd64] http://archive.cloudera.com/cdh5/ubuntu/trusty/amd64/cdh trusty-cdh5.15.0 contrib" >> /etc/apt/sources.list.d/cloudera.list && 
    echo "deb-src http://archive.cloudera.com/cdh5/ubuntu/trusty/amd64/cdh trusty-cdh5.15.0 contrib" >> /etc/apt/sources.list.d/cloudera.list && 
    wget http://archive.cloudera.com/cdh5/ubuntu/trusty/amd64/cdh/archive.key -O /tmp/domino-hadoop-downloads/archive.key && 
    apt-key add /tmp/domino-hadoop-downloads/archive.key && 
    apt-get update && 
    apt-get -y -t trusty-cdh5.15.0 install zookeeper && 
    apt-get -y -t trusty-cdh5.15.0 install hadoop-client

# Copy CDH and Spark2 parcels to correct directories and update symlinks
# Note that the version strings attached to your directory names may be different than the below examples.
RUN mv /tmp/domino-hadoop-downloads/hadoop-binaries-configs/parcels/CDH-5.15.0-1.cdh5.15.0.p0.21 /opt/cloudera/parcels/ && 
    mv /tmp/domino-hadoop-downloads/hadoop-binaries-configs/parcels/SPARK2-2.3.0.cloudera3-1.cdh5.13.3.p0.458809 /opt/cloudera/parcels/ && 
    ln -s /opt/cloudera/parcels/CDH-5.15.0-1.cdh5.15.0.p0.21 /opt/cloudera/parcels/CDH && 
    ln -s /opt/cloudera/parcels/SPARK2-2.3.0.cloudera3-1.cdh5.13.3.p0.458809 /opt/cloudera/parcels/SPARK2

# Copy hadoop, hive and spark2 configurations
RUN mv /etc/hadoop /tmp/domino-hadoop-downloads/hadoop-binaries-configs/configs/hadoop-etc-local.backup && 
    mv /tmp/domino-hadoop-downloads/hadoop-binaries-configs/configs/hadoop /etc/hadoop && 
    mv /tmp/domino-hadoop-downloads/hadoop-binaries-configs/configs/hive /etc/hive && 
    mv /tmp/domino-hadoop-downloads/hadoop-binaries-configs/configs/spark2 /etc/spark2 && 
    mv /tmp/domino-hadoop-downloads/hadoop-binaries-configs/configs/spark /etc/spark

# Create alternatives for hadoop configurations. Update the extensions with the same strings as found in your edge node
# Example: In the command 'update-alternatives --install /etc/hadoop/conf hadoop-conf /etc/hadoop/conf.cloudera.yarn 55'
# make sure that /etc/hadoop/conf.cloudera.yarn is named the same as the corresponding file on your edge node.
# Sometimes in the CDH5 edgenode, that is named something like /etc/hadoop/conf.cloudera.yarn_
RUN update-alternatives --install /etc/hadoop/conf hadoop-conf /etc/hadoop/conf.cloudera.yarn 55 && 
    update-alternatives --install /etc/hive/conf hive-conf /etc/hive/conf.cloudera.hive 55 && 
    update-alternatives --install /etc/spark2/conf spark2-conf /etc/spark2/conf.cloudera.spark2_on_yarn 55 && 
    update-alternatives --install /etc/spark/conf spark-conf /etc/spark/conf.cloudera.spark_on_yarn 55

# These instructions are for Spark2
# Creating alternatives for Spark2 binaries, also create symlink for pyspark pointing to pyspark2
RUN update-alternatives --install /usr/bin/spark2-shell spark2-shell /opt/cloudera/parcels/SPARK2/bin/spark2-shell 55 && 
    update-alternatives --install /usr/bin/spark2-submit spark2-submit /opt/cloudera/parcels/SPARK2/bin/spark2-submit 55 && 
    update-alternatives --install /usr/bin/pyspark2 pyspark2 /opt/cloudera/parcels/SPARK2/bin/pyspark2 55 && 
    ln -s /usr/bin/pyspark2 /usr/bin/pyspark

# Update SPARK and HADOOP environment variables. Make sure py4j file name is correct per your edgenode
ENV SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2
RUN echo "export HADOOP_HOME=/opt/cloudera/parcels/CDH/lib/hadoop" >> /home/ubuntu/.domino-defaults && 
    echo "export HADOOP_CONF_DIR=/etc/hadoop/conf" >> /home/ubuntu/.domino-defaults && 
    echo "export YARN_CONF_DIR=/etc/hadoop/conf" >> /home/ubuntu/.domino-defaults && 
    echo "export SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2" >> /home/ubuntu/.domino-defaults && 
    echo "export SPARK_CONF_DIR=/etc/spark2/conf" >> /home/ubuntu/.domino-defaults && 
    echo "export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip" >> /home/ubuntu/.domino-defaults

# Change spark-defaults.conf file permission
RUN mv /etc/spark2/conf/spark-defaults.conf /etc/spark2/ && 
    chmod 777 /etc/spark2/conf.cloudera.spark2_on_yarn

# Copy hive-site.xml to /etc/spark2/conf to access hive tables from Spark2.
RUN cp /etc/spark2/conf/yarn-conf/hive-site.xml /etc/spark2/conf/

USER ubuntu
----

. Scroll down to the *Pre Run Script* field and add the following lines:
+
----
cat /etc/spark2/spark-defaults.conf >> /etc/spark2/conf/spark-defaults.conf
sed -i.bak '/spark.ui.port=0/d' /etc/spark2/conf/spark-defaults.conf
----
. Scroll down and click *Advanced* to expand additional fields. Add the
following line to the *Post Setup Script* field:
+
----
echo "export YARN_CONF_DIR=/etc/hadoop/conf" >> /home/ubuntu/.bashrc
----
. Click *Build* when finished editing the Dockerfile instructions. If
the build completes successfully, you are ready to try using the
environment.



[[tr2]]
// As a Domino user. I can connect to a Cloudera CDH5 cluster via a properly configured project

== Configure a Domino project for use with a CDH5 cluster

This procedure assumes that an environment with the necessary client
software has been created according to the instructions above. Ask your
Domino admin for access to such an environment.

. Open the Domino project you want to use with your CDH5 cluster, then
click *Settings* from the project menu.
. On the Integrations tab, click to select *YARN* integration from the
Apache Spark panel, then click *Save*. You do not need to edit any of
the fields in this section.
. If your cluster uses Kerberos authentication, you can
link:ed5f5a[configure credentials at the user level or project level ].
Do so before attempting to use the environment. Note that if you
followed the instructions above on creating your environment, your
Kerberos configuration file has already been added to it.
. On the *Hardware & Environment* tab, change the project default
environment to the one with the cluster's binaries and configurations
files installed.

You are now ready to start Runs from this project that interact with
your CDH5 cluster.

----- user_guide/on-demand-distributed-computing/spark/hadoop-and-spark/hortonworks-cluster.txt -----
:page-version: 6.1
:page-title: Connect to a Hortonworks cluster from Domino
:page-sidebar: Hortonworks cluster
:page-permalink: aa0f53
:page-order: 20

Domino supports connecting to a
https://hortonworks.com/products/data-platforms/hdp/[Hortonworks
cluster^] through the addition of cluster-specific binaries and
configuration files to your
link:f51038[Domino environment].

At a high level, the process is as follows:

. Connect to your Hortonworks cluster edge node and gather the required
binaries and configuration files, then download them to your local
machine.
. Upload the gathered files into a Domino project to allow access by the
Domino environment builder.
. Create a new Domino environment that uses the uploaded files to enable
connections to your cluster.
. Enable YARN integration for the Domino projects that you want to use
with the Hortonworks cluster.

Domino supports the following types of connections to a Hortonworks
cluster:

* https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.4/bk_cloud-data-access/content/using-fs-shell.html[FS shell^]
* https://books.japila.pl/apache-spark-internals/[spark-shell^]
* https://spark.apache.org/docs/latest/submitting-applications.html[spark-submit^]
* https://spark.apache.org/docs/latest/api/python/index.html[pyspark^]
* https://hadoop.apache.org/docs/r3.1.1/hadoop-yarn/hadoop-yarn-site/YarnCommands.html[YARN shell^]

[[tr1]]
// As a Domino admin/user. I can create a Domino environment compatible with a Hortonworks cluster

== Gather the required binaries and configuration files

You will find most of the necessary files for setting up your Domino
environment on your edge node. To get started, connect to the edge node
via SSH, then follow the steps below.

. Create a directory named `hadoop-binaries-configs` at `/tmp`.
+
[source,shell]
----
mkdir /tmp/hadoop-binaries-configs
----
. Create the following subdirectory inside
`/tmp/hadoop-binaries-configs/`.
+
[source,shell]
----
mkdir /tmp/hadoop-binaries-configs/configs
----
. (Optional) If your cluster uses Kerberos authentication, create the
following subdirectory in `/tmp/hadoop-binaries/configs/`.
+
[source,shell]
----
mkdir /tmp/hadoop-binaries-configs/kerberos
----
+
Then, copy the `krb5.conf` Kerberos configuration file from `/etc/` to
`/tmp/hadoop-binaries-configs/kerberos`.
+
[source,shell]
----
cp /etc/krb5.conf /tmp/hadoop-binaries-configs/kerberos/
----
. Copy the `hadoop`, `hive`, `spark`, and `spark2` directories from
`/etc/` to `/tmp/hadoop-binaries-configs/configs/`.
+
[source,shell]
----
cp -R /etc/hadoop /tmp/hadoop-binaries-configs/configs/
cp -R /etc/hive /tmp/hadoop-binaries-configs/configs/
cp -R /etc/spark2 /tmp/hadoop-binaries-configs/configs/
cp -R /etc/spark /tmp/hadoop-binaries-configs/configs/
----
. On the edge node, run the following command to identify the version of
Java running on the cluster.
+
[source,shell]
----
java -version
----
+
You should then https://www.oracle.com/technetwork/java/javase/downloads/index.html[download a JDK .tar file from the Oracle downloads page^] that matches that version.
The filename will have a name like the following:
+
[source,shell]
----
jdk-8u211-linux-x64.tar.gz
----
+
Keep this JDK handy on your local machine for use in a future step.
. Compress the `/tmp/hadoop-binaries-configs/` directory to a gzip
archive.
+
[source,shell]
----
cd /tmp

tar -zcf hadoop-binaries-configs.tar.gz hadoop-binaries-configs
----
+
When finished, use SCP to download the archive to your local machine.
. Next, you'll need to extract the archive on your local machine, add a
java subdirectory, then add the JDK .tar file you downloaded earlier to
the java subdirectory.
+
----
tar xzf hadoop-binaries-configs.tar.gz

mkdir hadoop-binaries-configs/java

cp jdk-8u211-linux-x64.tar.gz hadoop-binaries-configs/java/
----
. When finished, your `hadoop-binaries-configs` directory should have the
following structure:
+
[source,console]
----
hadoop-binaries-configs/
  ├── configs/
        ├── hadoop/
        ├── hive/
        ├── spark/
        └── spark2/
  ├── java/
        └── jdk-8u211-linux-x64.tar.gz
  └── kerberos/  # optional
        └── krb5.conf
----
. If your directory contains all the required files, you can now
compress it to a gzip archive again in preparation for uploading to
Domino in the next step.
+
[source,shell]
----
tar -zcf hadoop-binaries-configs.tar.gz hadoop-binaries-configs
----

== Upload the binaries and configuration files to Domino

Use the following procedure to upload the archive you created in the
previous step to a public Domino project. This will make the file
available to the Domino environment builder.

. Log in to Domino, then create a new public project.

. Open the
*Code*
page for the new project, then click to *browse for files* and select the archive you created in the previous section. Then
click *Upload*.
. After the archive has been uploaded, click the gear menu next to it on
the Files page, then right click *Download* and click *Copy Link
Address*. Save the copied URL in your notes, as you will need it in the
next step.
+
After you have recorded the download URL of the archive, you're ready to
build a Domino environment for connecting to your Hortonworks cluster.

== Create a Domino environment for connecting to Hortonworks

. Click *Environments* from the Domino main menu, then click *Create Environment*.

. Give the environment an informative name, then choose a base
environment that includes the version of Python that is installed on the
nodes of your Hortonworks cluster. Most Linux distributions ship with
Python 2.7 by default, so you will see the
link:0d73c6[Domino Analytics Distribution for Python 2.7]
used as the base image in the following examples. Click *Create* when
finished.
. After creating the environment, click *Edit Definition*. Copy the
below example into your Dockerfile Instructions, then be sure to edit it
wherever necessary with values specific to your deployment and cluster.
+
In this Dockerfile, wherever you see a hyphenated instruction enclosed
in angle brackets like `<paste-your-domino-download-url-here>`, be sure to
replace it with the corresponding value you recorded in previous steps.
+
Additionally, follow the instructions in the comments carefully, as you
may need to modify the commands based on versions and filenames from
your system. You may also need to edit commands that follow to match
downloaded filenames.
+
[source,dockerfile]
----
USER root

# Give user ubuntu ability to sudo as any user including root in the compute environment
RUN echo "ubuntu ALL=(ALL:ALL) NOPASSWD: ALL" >> /etc/sudoers

# Setup directories
RUN mkdir /tmp/domino-hadoop-downloads && 
    mkdir /usr/jdk64

# This downloaded gzip file should have the following
# - java installation tar file in 'java' sub-directory
# - krb5.conf in 'kerberos' sub-directory
# - hadoop, hive, spark2 and spark config directories from hadoop edgenode in a 'configs' sub-directory
# Make sure your URL is updated to reflect where you uploaded your configs.
# You should have this saved from your preparation steps
RUN wget --no-check-certiticate <paste-your-domino-download-url-here> -O /tmp/domino-hadoop-downloads/hadoop-binaries-configs.tar.gz && 
    tar xzf /tmp/domino-hadoop-downloads/hadoop-binaries-configs.tar.gz -C /tmp/domino-hadoop-downloads/

# Install kerberos client and update the kerberos configuration file
RUN apt-get update && 
    apt-get -y install krb5-user telnet && 
    cp /tmp/domino-hadoop-downloads/hadoop-binaries-configs/kerberos/krb5.conf /etc/krb5.conf

# Install version of java that matches hadoop cluster and update environment variables
RUN tar xvf /tmp/domino-hadoop-downloads/hadoop-binaries-configs/java/jdk-8u112-linux-x64.tar -C /usr/jdk64 && 
    ln -s /usr/jdk64/jdk1.8.0_112 /usr/jdk64/default
ENV JAVA_HOME=/usr/jdk64/default
RUN echo "export JAVA_HOME=/usr/jdk64/default" >> /home/ubuntu/.domino-defaults && 
    echo "export PATH=$JAVA_HOME/bin:$PATH" >> /home/ubuntu/.domino-defaults

# Install HDP hadoop-client and spark binaries from Hortonworks Ubuntu repository.
# Update the repo URL based for the version that matches what's running on your cluster.
# This example shows version 2.6.5.
RUN wget http://public-repo-1.hortonworks.com/HDP/ubuntu14/2.x/updates/2.6.5.0/hdp.list -O /etc/apt/sources.list.d/hdp.list && 
    apt-key adv --keyserver keyserver.ubuntu.com --recv 07513CAD && 
    apt-get update && 
    apt-get -y install hadoop-client spark2-python spark-python

# Copy hadoop, hive, spark and spark2 configurations
RUN mv /etc/hadoop /tmp/domino-hadoop-downloads/hadoop-binaries-configs/configs/hadoop-etc-local.backup && 
    mv /etc/spark /tmp/domino-hadoop-downloads/hadoop-binaries-configs/configs/spark-etc-local.backup && 
    mv /etc/spark2 /tmp/domino-hadoop-downloads/hadoop-binaries-configs/configs/spark2-etc-local.backup && 
    cp -r /tmp/domino-hadoop-downloads/hadoop-binaries-configs/configs/hadoop /etc/hadoop && 
    cp -r /tmp/domino-hadoop-downloads/hadoop-binaries-configs/configs/hive /etc/hive && 
    cp -r /tmp/domino-hadoop-downloads/hadoop-binaries-configs/configs/spark2 /etc/spark2 && 
    cp -r /tmp/domino-hadoop-downloads/hadoop-binaries-configs/configs/spark /etc/spark

# Update symlinks to point to correct configurations
# When you are creating these symlinks make sure that right versions are specified.
# Example: In the command 'ln -s /etc/spark2/2.6.5.0-292/0 /etc/spark2/conf'
# make sure that 2.6.5.0-292/0 is right version according to the edgenode and set the correct version similar to the hortonwork edgenode.
RUN rm /etc/spark2/conf && 
    rm /etc/spark/conf && 
    rm /etc/hadoop/conf && 
    rm /etc/hive/conf && 
    ln -s /etc/spark2/2.6.5.0-292/0 /etc/spark2/conf && 
    ln -s /etc/spark/2.6.5.0-292/0 /etc/spark/conf && 
    ln -s /etc/hadoop/2.6.5.0-292/0 /etc/hadoop/conf && 
    ln -s /etc/hive/2.6.5.0-292/0 /etc/hive/conf

# Update SPARK and HADOOP environment variables. Make sure py4j file name is correct as per your edgenode
ENV SPARK_HOME=/usr/hdp/2.6.5.0-292/spark2
RUN echo "export HADOOP_HOME=/usr/hdp/2.6.5.0-292/hadoop" >> /home/ubuntu/.domino-defaults && 
    echo "export HADOOP_CONF_DIR=/etc/hadoop/conf" >> /home/ubuntu/.domino-defaults && 
    echo "export YARN_CONF_DIR=/etc/hadoop/conf" >> /home/ubuntu/.domino-defaults && 
    echo "export SPARK_HOME=/usr/hdp/2.6.5.0-292/spark2" >> /home/ubuntu/.domino-defaults && 
    echo "export SPARK_CONF_DIR=/etc/spark2/conf" >> /home/ubuntu/.domino-defaults && 
    echo "export SPARK_MAJOR_VERSION=2" >> /home/ubuntu/.domino-defaults && 
    echo "export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.6-src.zip" >> /home/ubuntu/.domino-defaults

# Backup existing spark-defautls.conf file.
# Change spark-defaults.conf directory permission as a new spark-defaults.conf file gets created by Domino's spark integration
RUN mv /etc/spark2/conf/spark-defaults.conf /etc/spark2/ && 
    chmod 777 /etc/spark2/2.6.5.0-292/0

USER ubuntu
----

. Scroll down to the *Pre Run Script* field and add the following lines.
+
[source,shell]
----
cat /etc/spark2/spark-defaults.conf >> /etc/spark2/conf/spark-defaults.conf
sed -i.bak '/spark.ui.port=0/d' /etc/spark2/conf/spark-defaults.conf
----
. Scroll down and click *Advanced* to expand additional fields. Add the
following line to the *Post Setup Script* field.
+
[source,shell]
----
echo "export YARN_CONF_DIR=/etc/hadoop/conf" >> /home/ubuntu/.bashrc
----
. Click *Build* when finished editing the Dockerfile instructions. If
the build completes successfully, you are ready to try using the
environment.

[[tr2]]
// As a Domino user. I can connect to a Hortonworks cluster via a properly configured project

== Configure a Domino project for use with a Hortonworks cluster

This procedure assumes that an environment with the necessary client
software has been created according to the instructions above. Ask your
Domino admin for access to such an environment.

. Open the Domino project you want to use with your Hortonworks cluster,
then click *Settings* from the project menu.
. On the *Integrations* tab, click to select *YARN* integration from the
Apache Spark panel, then click *Save*. You do not need to edit any of
the fields in this section.
. If your cluster uses Kerberos authentication, you can
link:ed5f5a[configure credentials at the user level or project level].
Do so before attempting to use the environment. Note that if you
followed the instructions above on creating your environment, your
Kerberos configuration file has already been added to it.
. On the *Hardware & Environment* tab, change the project default
environment to the one with the cluster's binaries and configurations
files installed.

You are now ready to start Runs from this project that interact with
your Hortonworks cluster.

----- user_guide/on-demand-distributed-computing/spark/hadoop-and-spark/index.txt -----
:page-version: 6.1
:page-title: Hadoop and Spark
:page-permalink: 0c042e
:page-order: 10

https://hadoop.apache.org/[Apache Hadoop^] is a collection of open source cluster computing tools that supports popular applications for data science at scale, such as https://spark.apache.org/[Spark^].

To interact with Hadoop from your Domino executors, configure your link:f51038[Domino environment]
with the necessary software dependencies and credentials.
Domino supports most providers of Hadoop solutions, including MapR, Cloudera, and Amazon EMR.
After a Domino environment is set up to connect to your cluster, Domino projects can use the environment to work with Hadoop applications.

== Use a Hadoop-enabled environment in your Domino project

If your Domino administrators have already created an environment for connecting to a Hadoop cluster, you can follow these subsections of the setup instructions to use that environment in your Domino project.

*For users setting up projects to work with an existing environment, read these subsections:*

* link:83e94a#tr2[Configure a Domino project for use with a Cloudera CDH5 cluster]
* link:aa0f53#tr2[Configure a Domino project for use with a Hortonworks cluster]
* link:cdbf96#configure-a-domino-project-for-use-with-a-mapr-cluster[Configure a Domino project for use with a MapR cluster]
* link:b18456#configure-a-domino-project-for-use-with-an-emr-cluster[Configure a Domino project for use with an Amazon EMR cluster]

After your project is set up to use the environment, you can execute code in your Domino Runs that connects to the cluster for Spark, HDFS, or Hive functionality.

image::/images/4.x/hadoop-enabled-environment.png[alt="Use a Hadoop-enabled environment"]

== Set up Domino to connect to a new Hadoop cluster

To connect to your existing Hadoop cluster from Domino, you must create a Domino environment with the necessary dependencies installed.
Some of these dependencies, including binaries and configuration files, will come directly from the cluster itself.
Others will be external software dependencies like Java and Spark, and you will need to match the version you install in the environment to the version running on the cluster.

The basic steps for setting up an environment to connect to your cluster are:

. Gather binaries and configuration files from your cluster.
. Gather dependencies from external sources, like Java JDKs and Spark binaries.
. Upload all dependencies to a Domino project, to make them accessible to the Domino environment builder.
. Author a new Domino environment that pulls from the Domino project, then installs and configures all required dependencies.
+
image::/images/4.x/configure-environment-dependencies.png[alt="Configure environment dependencies"]

*For Domino admins setting up a Domino environment to connect to a new cluster, read the full provider-specific setup guides:*

* link:83e94a[Connect to a Cloudera CDH5 Cluster from Domino]
* link:aa0f53[Connect to a Hortonworks cluster from Domino]
* link:cdbf96[Connect to a MapR cluster from Domino]
* link:b18456[Connect to an Amazon EMR cluster from Domino]

== Additional capabilities

Domino also supports running Spark on a Domino executor in local mode, querying Hive tables with JDBC, and authenticating to clusters with Kerberos.
See the following guides for more information.

* link:d982f4[Run Local Spark on a Domino Executor]
* link:bf6681[Interactive PySpark notebooks]
* link:ed5f5a[Kerberos Authentication]

----- user_guide/on-demand-distributed-computing/spark/hadoop-and-spark/kerberos-authentication.txt -----
:page-version: 6.1
:page-title: Kerberos authentication
:page-permalink: ed5f5a
:page-order: 70

Domino supports Kerberos authentication, allowing users to authenticate
as themselves when connecting to Kerberos-secured systems.

Users can enable Kerberos authentication at the project-level or
user-level by uploading a Kerberos keytab and principal into Domino.
After set up, Runs started by Kerberos-enabled users or in
Kerberos-enabled projects in Domino will automatically run kinit and
retrieve the ticket to be able to authenticate.

== Adding your Kerberos configuration file to Domino

There are two ways to add your
https://web.mit.edu/kerberos/krb5-1.12/doc/admin/conf_files/krb5_conf.html[krb5.conf^]
file to Domino.

* *Add it to your project in a folder named* `kerberos`.
[[tr1]]
// As a Domino user, I can enable Kerberos authentication by adding a Kerberos configuration file to a project folder
* *Add it to your environment at* `/etc/krb5.conf`:
[[tr2]]
// As a Domino user, I can enable Kerberos authentication by creating a Kerberos configuration file using dockerfile commands in my environment

[source,dockerfile]
----
RUN echo "[libdefaults]" >> /etc/krb5.conf && 
    echo "    default_realm = [YOUR-DEFAULT-REALM]" >> /etc/krb5.conf && 
    echo "    default_tkt_enctypes = des3-hmac-sha1 des-cbc-crc" >> /etc/krb5.conf && 
    echo "    default_tgs_enctypes = des3-hmac-sha1 des-cbc-crc" >> /etc/krb5.conf && 
    echo "    dns_lookup_kdc = true" >> /etc/krb5.conf && 
    echo "    dns_lookup_realm = false" >> /etc/krb5.conf && 
    echo "" >> /etc/krb5.conf && 
    echo "[realms]" >> /etc/krb5.conf && 
    echo "    EXAMPLE.COM = {" >> /etc/krb5.conf && 
    echo "        kdc = kerberos.example.com" >> /etc/krb5.conf && 
    echo "        admin_server = kerberos.example.com" >> /etc/krb5.conf && 
    echo "    }" >> /etc/krb5.conf
----

== Adding Kerberos credentials to your user
[[tr3]]
// As a Domino user, I can enable Kerberos authentication by adding credentials to my user account settings

To add a keytab and principal that will be used for Runs started by your
user:

. Go to *Account Settings* > *Kerberos Integration*.
. Select *Keytab file based authentication*, paste your
keytab and principal, then click *Save*.

== Adding Kerberos credentials to your Project
[[tr4]]
// As a Domino user, I can enable Kerberos authentication by adding credentials to my project settings

To add a keytab and principal that will be used for Runs started by a
specific project:

. Open the project *Settings* and click to open the
*Integrations* tab.
. In the *Kerberos* panel, click *Keytab file based authentication*,
supply your keytab and principal, then click *Save*.

----- user_guide/on-demand-distributed-computing/spark/hadoop-and-spark/local-spark-cluster.txt -----
:page-version: 6.1
:page-title: Run local Spark on a Domino Executor
:page-sidebar: Local Spark cluster
:page-permalink: d982f4
:page-order: 50

== Use a local Spark cluster

[[tr1]]
// As a Domino user, I can configure Spark for local mode and execute a parallelized workload on a local Spark cluster

Typically, users interested in Hadoop and Spark have data volumes and workloads that demand the power of cluster computing.
However, some people use Spark for its expressive API, even if their data volumes are small or medium.
Because Domino lets you run code on powerful VM infrastructure, with up the 32 cores in AWS, you can use Domino to create a local Spark cluster and easily parallelize your tasks across all 32 cores.

== Configure Spark in Local mode

To configure Spark integration in Local mode, open your project and go to *Project settings*. Under *Integrations*, choose the *Local mode* option for Apache Spark.
Click *Save* to save your changes.

----- user_guide/on-demand-distributed-computing/spark/hadoop-and-spark/mapr-cluster.txt -----
:page-version: 6.1
:page-title: Connect to a MapR cluster from Domino
:page-sidebar: MapR cluster
:page-permalink: cdbf96
:page-order: 30

Domino supports connecting to a https://mapr.com/[MapR^] cluster through
the addition of cluster-specific binaries and configuration files to
your link:f51038[Domino environment].

At a high level, the process is as follows:

. Connect to a
https://mapr.com/docs/52/AdvancedInstallation/PlanningtheCluster-node-types.html[MapR
Edge node^] and gather the required binaries and configuration files,
then download them to your local machine.
. Upload the gathered files into a Domino project to allow access by the
Domino environment builder.
. Create a new Domino environment that uses the uploaded files to enable
connections to your cluster.
. Enable YARN integration for the Domino projects that you want to use
with the MapR cluster.

Domino supports the following types of connections to a MapR cluster:

* https://mapr.com/docs/61/MapROverview/c_maprfs.html[FS shell for MapRFS^]
* https://books.japila.pl/apache-spark-internals/[spark-shell^]
* https://spark.apache.org/docs/latest/submitting-applications.html[spark-submit^]
* https://spark.apache.org/docs/latest/api/python/index.html[pyspark^]
* https://hadoop.apache.org/docs/r3.1.1/hadoop-yarn/hadoop-yarn-site/YarnCommands.html[YARN shell^]
* https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-Beeline%E2%80%93CommandLineShell[Hive with Beeline^]

[[tr1]]
// As a Domino admin/user. I can create a Domino environment compatible with a MapR cluster

== Gather the required binaries and configuration files

You will find most of the necessary files for setting up your Domino environment on your https://mapr.com/docs/52/AdvancedInstallation/PlanningtheCluster-node-types.html[MapR Edge node^]. To get started, https://mapr.com/docs/61/AdvancedInstallation/PreparingEachNode-connectivity.html[connect to the Edge node via SSH^], then follow the steps below.

. Create a directory named `hadoop-binaries-configs` at `/tmp`.
+
[source,shell]
----
mkdir /tmp/hadoop-binaries-configs
----
. Copy `hive-site.xml` from `/opt/mapr/spark/spark-<version>/conf` to
`/tmp/hadoop-binaries-configs/`. Be sure to replace the `<version>`
string in the command below with the number that matches the folder name
on your edge node.
+
[source,shell]
----
cp /opt/mapr/spark-<version>/conf /tmp/hadoop-binaries-configs/
----
. Copy the `ssl_truststore` from `/opt/mapr/conf` to
`/tmp/hadoop-binaries-configs/`.
+
[source,shell]
----
cp /opt/mapr/conf/ssl_truststore /tmp/hadoop-binaries-configs/
----
. After you've copied the above files into
`/tmp/hadoop-binaries-configs`, zip the directory for transfer to
your local machine.
+
[source,shell]
----
cd /tmp
tar -zcf hadoop-binaries-configs.tar.gz hadoop-binaries-configs
----
+
Then use `SCP` from your local machine to download the zipped archive.
After transfer, extract the files to your local filesystem and keep them
handy for a future step where they will be uploaded to Domino.
. On the MapR edge node, run the following command to identify the
version of Java running on the cluster.
+
[source,shell]
----
java -version
----
+
You must then https://www.oracle.com/technetwork/java/javase/downloads/index.html[download a JDK .tar file from the Oracle downloads page^] that matches that version.
The filename will have a name like the following:
+
[source,shell]
----
jdk-8u211-linux-x64.tar.gz
----
+
Keep this JDK handy for use in a future step.

== Upload the binaries and configuration files to Domino

Use the following procedure to upload the files you retrieved in the
previous step to a public Domino project. This will make the files
available to the Domino environment builder.

. Log in to Domino, then create a new public project.

. Open the
*Code*
page for the new project, then click *browse for
files* and select the files you downloaded from the MapR edge node, and
the JDK you downloaded from Oracle. Then click *Upload*.
. From the
*Code*
page of your project, click *New File*. Name the file
`run-client.sh`, and in its contents you must construct an invocation of
the MapR `configure.sh` script that is valid for setting up a client to
connect to your cluster. A full explanation of how to invoke this script
is beyond the scope of this document.
Read the full https://mapr.com/docs/61/ReferenceGuide/configure.sh.html[documentation on the script from MapR^], and consider the following example.
+
`run-client.sh`
+
----
#!/bin/bash
/opt/mapr/server/configure.sh -N <clustername> -c -secure -C <host1>:7222,<host2>:7222,<host3>:7222 -HS <historyServer>
----
. After your project contains the files from the MapR edge node, the
correct JDK, and a `run-client.sh` script that wraps the MapR
configuration script, click the gear menu next to each of those files,
then right click *Download* and click *Copy Link Address*. Save these
URLs in your notes, as you will need them in the next step.
+
After you have recorded the download URL of the binaries and
configuration files, you're ready to build a Domino environment for
connecting to MapR.

== Create a Domino Environment for connecting to MapR

. Click *Environments* from the Domino main menu, then click *Create Environment*.
. Give the environment an informative name, then choose a base
environment that includes the version of Python that is installed on the
nodes of your MapR cluster. Most Linux distributions ship with Python
2.7 by default, so you will see the
link:0d73c6[Domino Analytics Distribution for Python 2.7]
used as the base image in the following examples. Click *Create* when
finished.
. After creating the environment, click *Edit Definition*. Copy the
below example into your Dockerfile instructions, then be sure to edit it
wherever necessary with values specific to your deployment and cluster.
+
In this Dockerfile, wherever you see a hyphenated instruction enclosed
in angle brackets like `<paste-your-domino-download-url-here>`, be sure to
replace it with the corresponding values you recorded in previous steps.
You may also need to edit commands that follow to match downloaded
filenames.
+
[source,dockerfile]
----
# Base Image: quay.io/domino/base:Ubuntu16_DAD_Py2.7_R3.4-20180727
USER root

# Give the ubuntu user ability to sudo as any user including root in the compute environment
RUN echo "ubuntu ALL=(ALL:ALL) NOPASSWD: ALL" >> /etc/sudoers

# Set up directories
RUN mkdir /tmp/mapr-cluster-downloads && 
   mkdir /usr/jdk64

# Create a mapr user and group
RUN groupadd -g 5000 mapr
RUN useradd -u 5000 -g mapr mapr
RUN usermod -s /bin/bash mapr

# Use the following wget commands to download the four files you added to Domino in the previous section.
# You should have copied down the URLs to download a JDK .tar, the two files from the edge node, and the run-client.sh script you created.
# The example below will use a JDK file named jdk-8u112-linux-x64.tar.gz. If you're using a different version or have a different filename, replace it wherever it occurs.
RUN cd /tmp/mapr-cluster-downloads && 
   wget <paste-your-run-client-dot-sh-download-url-here> -O /tmp/mapr-cluster-downloads/run-client.sh.gz && 
   wget <paste-your-hive-site-dot-xml-download-url-here> -O /tmp/mapr-cluster-downloads/hive-site.xml.gz && 
   wget <paste-your-jdk-tar-download-url-here> -O /tmp/mapr-cluster-downloads/jdk-8u112-linux-x64.tar.gz && 
   wget <paste-your-ssl-truststore-download-url-here> -O /tmp/mapr-cluster-downloads/ssl_truststore.gz && 
   gunzip run-client.sh.gz && 
   gunzip hive-site.xml.gz && 
   gunzip jdk-8u112-linux-x64.tar.gz && 
   gunzip ssl_truststore.gz && 
   cd ~

# Install Java from the JDK
RUN tar xvf /tmp/mapr-cluster-downloads/jdk-8u112-linux-x64.tar -C /usr/jdk64 && 
   ln -s /usr/jdk64/jdk1.8.0_112 /usr/jdk64/default
ENV JAVA_HOME=/usr/jdk64/default
RUN echo "export JAVA_HOME=/usr/jdk64/default" >> /home/ubuntu/.domino-defaults && 
   echo "export PATH=$JAVA_HOME/bin:$PATH" >> /home/ubuntu/.domino-defaults

# Install mapr-client and Spark binaries from the MapR ubuntu repository.
# These examples are for MapR 6.1.0.
# If you are using a different version of MapR, replace these URLs with the correct versions from http://archive.mapr.com/releases/.
RUN echo "deb https://package.mapr.com/releases/v6.1.0/ubuntu binary trusty" >> /etc/apt/sources.list
RUN echo "deb https://package.mapr.com/releases/MEP/MEP-6.0.0/ubuntu binary trusty" >> /etc/apt/sources.list
RUN wget -O - https://package.mapr.com/releases/pub/maprgpg.key | sudo apt-key add -
RUN apt-get update
RUN apt-get -y install mapr-client mapr-spark mapr-hive

# Copy the ssl_truststore file from the /opt/mapr/conf directory on the cluster to the /opt/mapr/conf directory on the client
RUN cp /tmp/mapr-cluster-downloads/ssl_truststore /opt/mapr/conf/

# Make your customized script from the previous section executable
RUN chmod +x /tmp/mapr-cluster-downloads/run-client.sh

# Update SPARK and HADOOP environment variables.
# Make sure the Spark and Hadoop version numbers match what is installed on your cluster
# The examples below show Spark 2.3.1 and Hadoop 2.7.0.
# If you are using different versions, be sure to edit the file and directory names to match.
# Make sure the py4j file name is correct per your edgenode.
ENV SPARK_HOME=/opt/mapr/spark/spark-2.3.1
RUN echo "export HADOOP_HOME=/opt/mapr/hadoop/hadoop-2.7.0" >> /home/ubuntu/.domino-defaults && 
   echo "export HADOOP_CONF_DIR=/opt/mapr/hadoop/hadoop-2.7.0/etc/hadoop" >> /home/ubuntu/.domino-defaults && 
   echo "export YARN_CONF_DIR=/opt/mapr/hadoop/hadoop-2.7.0/etc/hadoop" >> /home/ubuntu/.domino-defaults && 
   echo "export SPARK_HOME=/opt/mapr/spark/spark-2.3.1" >> /home/ubuntu/.domino-defaults && 
   echo "export SPARK_CONF_DIR=/opt/mapr/spark/spark-2.3.1/conf" >> /home/ubuntu/.domino-defaults && 
   echo "export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip" >> /home/ubuntu/.domino-defaults

# Change spark configuration directory permission as a new spark-defaults.conf file gets created by Domino's spark integration
RUN chmod 777 /opt/mapr/spark/spark-2.3.1/conf

# Add symlinks for Spark binaries
RUN ln -s /opt/mapr/spark/spark-2.3.1/bin/pyspark /usr/bin/pyspark
RUN ln -s /opt/mapr/spark/spark-2.3.1/bin/spark-shell/usr/bin/spark-shell
RUN ln -s /opt/mapr/spark/spark-2.3.1/bin/spark-submit /usr/bin/spark-submit

# Update Java path for R
RUN export LD_LIBRARY_PATH=/usr/jdk64/default/jre/lib/amd64/server && R CMD javareconf

# Install Python and R JDBC packages
RUN pip install jaydebeapi
RUN R --no-save -e 'install.packages(c("RJDBC"))'

USER ubuntu
----

. Scroll down to the *Pre Run Script* field and add the following lines,
being sure to match the Spark version in the directory name to the one
being set up by the Dockerfile instructions.
+
----
# Configure mapr-client with your customized script.
sudo bash /tmp/mapr-cluster-downloads/run-client.sh

# Copy `hive-site.xml` to the spark configuration directory
# Be sure to match the Spark version in this folder name to match what you set up above.
cp /tmp/mapr-cluster-downloads/hive-site.xml /opt/mapr/spark/spark-2.3.1/conf
----
. (Optional) If you want to store and access https://mapr.com/docs/61/SecurityGuide/Tickets.html[MapR user tickets^] as link:6ac5a1[Domino environment variables], follow these additional steps.
.. Request a long-running MapR ticket from your cluster administrator,
and copy its contents to your local machine. The ticket will be
formatted as:
+
`<cluster-name> <token>`
.. Add that token as a
link:6ac5a1[Domino environment variable]
to your Domino user account with the name `USERTICKET`.
.. Add the following lines to the bottom of the *Pre Run Script* field for
the environment you edited previously:
+
----
## Write maprticket in environment variable to a file during runtime
echo $USERTICKET > /tmp/maprticket_12574
chown ubuntu:ubuntu /tmp/maprticket_12574
chmod 600 /tmp/maprticket_12574
----
+
If you do this, every user that wants to use this environment
must set up a `USERTICKET` environment variable as described in the
previous step.
. Click *Build* when finished editing the Dockerfile instructions. If
the build completes successfully, you are ready to try using the
environment.

[[tr2]]
// As a Domino user. I can connect to a MapR cluster via a properly configured project
[[configure-a-domino-project-for-use-with-a-mapr-cluster]]
== Configure a Domino Project for use with a MapR cluster

This procedure assumes that an environment with the necessary client
software has been created according to the instructions above. Ask your
Domino administrator for access to such an environment.

. Open the Domino project you want to use with your MapR cluster, then
click *Settings* from the project menu.
. On the Integrations tab, click to select *YARN* integration from the
Apache Spark panel, then click *Save*. You should not need to edit any
of the fields in this section.
. On the *Hardware & Environment* tab, change the project default
environment to the one you built earlier with the binaries and
configuration files.

You are now ready to start Runs from this project that interact with
your MapR cluster.

----- user_guide/on-demand-distributed-computing/spark/hadoop-and-spark/pyspark-in-jupyter-workspaces.txt -----
:page-version: 6.1
:page-title: Use PySpark in Jupyter Workspaces
:page-sidebar: PySpark in Jupyter Workspaces
:page-permalink: bf6681
:page-order: 60

You can configure a
link:e6e601[Domino workspace] to launch a Jupyter notebook with a connection to
link:0c042e[your Spark cluster].

This allows you to operate the cluster interactively from Jupyter with
https://archive.apache.org/dist/spark/docs/0.9.0/python-programming-guide.html[PySpark^].

The instructions for configuring a PySpark workspace are below.
To use them, you must have a Domino environment that meets the following prerequisites:

* The environment must use one of the
link:0d73c6[Domino Standard Environments]
as its base image.
* The necessary binaries and configurations for connecting to your Spark cluster must be installed in the environment.
See
link:0c042e[the provider-specific guides] for setting up the environment.

[NOTE]
====
PySpark 2 does not support Python 3.8 or higher.
Build PySpark 2 compute environments from images with Python versions before 3.8 or use PySpark 3.
====

== Add a PySpark Workspace option to your Environment

[[tr1]]
// As a Domino admin/user, I can add PySpark to the available pluggable workspace IDE tools for a Domino environment

. From the Domino main menu, click *Environments*.
. Click the name of an environment that meets the prerequisites listed previously.
It must use a Domino standard base image and already have the necessary binaries and configuration files installed for connecting to your spark cluster.
. On the environment overview page, click *Edit Definition*.
. In the *Pluggable Workspace Tools* field, paste the following YAML configuration.
+
[source,yaml]
----
pyspark:
   title: "PySpark"
   start: [ "/var/opt/workspaces/pyspark/start" ]
   iconUrl: "https://raw.githubusercontent.com/dominodatalab/workspace-configs/develop/workspace-logos/PySpark.png"
   httpProxy:
      port: 8888
      internalPath: "/{{ownerUsername}}/{{projectName}}/{{sessionPathComponent}}/{{runId}}/{{      rewrite: false
      requireSubdomains: false
   supportedFileExtensions: [ ".ipynb" ]
----
+
When finished, the field should look like this:
+
image::/images/4.x/pyspark-pluggable-workspace-tools.png[alt="Pluggable workspace tools"]
+
. Click *Build* to apply the changes and build a new version of the environment.
Upon a successful build, the environment is ready for use.

[NOTE]
====
If you are using an older version of a
link:0d73c6[Domino Standard Environment]
you might require a different *Pluggable Workspace Tool* definition for PySpark.
The safest way to do this is to copy the Jupyter pluggable workspace definition for your base image (see the `README`` for your base image at the
https://github.com/dominodatalab/Domino_Base_Images/tree/master/Domino_Analytics_Distribution[Domino
Analytics Distribution] git repo), but replace Jupyter in the *title* and
*start* fields with PySpark.
You can use the same *iconUrl* specified above to get the correct PySpark icon.
====

== Launching PySpark Workspaces

[[tr2]]
// As a Domino user, I can launch an interactive PySpark session in a workspace

. Open the project you want to use a PySpark workspace in.
. Open the project settings, then follow the link:0c042e[provider-specific instructions from the Hadoop and Spark overview ]
on *setting up a project to work with an existing Spark connection environment*.
This will involve enabling YARN integration in the project settings.
. On the *Hardware & Environment* tab of the project settings, choose the environment you added a PySpark configuration to in the previous section.
. After the previous settings are applied, you can launch a PySpark workspace from the workspaces dashboard.
+

image::/images/4.x/pyspark-pluggable-workspace-tools.png[alt="Pluggable workspace tools"]
----- user_guide/on-demand-distributed-computing/spark/index.txt -----
:page-version: 6.1
:page-title: Use Spark on Domino
:page-permalink: 68faaa
:page-order: 10

Apache Spark is a fast and general-purpose cluster computing system that offers a unified analytics engine for large-scale data processing and machine learning.

Domino provides flexibility on how to use Spark.
You can dynamically provision an on-demand Spark cluster orchestrated by Domino or you can connect to an existing Spark cluster outside of Domino.

link:0c042e[Hadoop and Spark]:: Domino projects can use the environment to work with Hadoop applications.

link:482ec5[On-demand Spark]:: Use Domino to dynamically provision and orchestrate a Spark cluster directly on the infrastructure that backs the Domino instance.

== Use Spot Instances for Spark Clusters (PREVIEW)

Spark clusters can use link:3813fa[Spot instances] to save the infrastructure costs. We recommend to use Spot instances only for the driver nodes as they can recover in case of failure. For Master note, always use on-demand nodes.

If AWS interrupts a spot instance, the on-demand or scheduled job on the Spark cluster may slow down the execution. If this happens, and until AWS spot instances of the requested type become available again, the remediation is to change the hardware tier of the job to use a non-spot node pool.

----- user_guide/on-demand-distributed-computing/spark/on-demand-spark/configure-prerequisites.txt -----
:page-version: 6.1
:page-title: Configure Spark prerequisites
:page-sidebar: Configure prerequisites
:page-permalink: 1962f3
:page-order: 20

[[configuring_spark_prerequisites]]
Before you can start using on-demand Spark clusters on Domino, you must enable and configure the functionality on your deployment.

== Create a base Spark cluster environment

[[tr1]]

By default, Domino provides Spark compatible images, as listed in link:799193[compute Environment catalog], that can be used for the components of the cluster.

When using on-demand Spark in Domino, you need one environment for the Spark cluster (base or worker environment) and one environment for the workspace/job execution (compute environment).

=== Create a new base Spark cluster environment

. Follow the instructions to link:fa8137[create an environment].

. In the *Base Image* section, select *Custom Image* and specify an image URI that points to a deployable Spark image.
+
Domino recommends that you use a Domino-provided Spark image from link:799193[compute Environment catalog] for versions of Spark and Python.
+
[NOTE]
====
Image compatibility:

Domino currently republishes the Spark base images from `bitnami/spark`.

Domino's on-demand Spark functionality has been developed and tested using https://hub.docker.com/r/bitnami/spark/[open-source Spark images
from Bitnami^].
====

. Required: In the *Supported Clusters* area, select the *Domino managed Spark* checkbox.
+
This ensures that the environment is available for use when you create Spark clusters from workspaces and jobs.

. Set the *Visibility*.
+
You can set this attribute the same way you would for any other link:f51038[compute environment].

. Leave the *Dockerfile Instructions* blank to use the Hadoop client libraries included with the image or follow the instructions to configure link:#creating_base_spark_env_custom_hadoop[custom Hadoop client libraries].
+
Domino-provided images are installed with all pre-requisites.
+
See link:146b80[Manage dependencies] to learn more.

. Leave *Pluggable Notebooks / Workspace Sessions* blank as the Spark base environments are not intended to also include notebook configuration.

=== Base Spark cluster environment - default Hadoop client libraries

Leave the *Docker Instructions* section blank if you want a thin base image that only contains core Spark with the default Hadoop client libraries.

[[creating_base_spark_env_custom_hadoop]]
=== Base Spark cluster environment (advanced) - custom Hadoop client libraries

The Hadoop client libraries, pre-bundled with your Spark version, might not be appropriate for your needs.
This is common if you want to use cloud object store connector improvements introduced post Hadoop 2.7.

Add the following to the *Docker Instructions* section, and adjust the Spark and Hadoop version as needed:

[source,docker]
----
### Needed if using the recommended Bitnami base image
USER root

### Make sure wget is available
RUN apt-get update && apt-get install -y wget && rm -r /var/lib/apt/lists /var/cache/apt/archives

### Modify the Hadoop and Spark versions below as needed
### NOTE: The HADOOP_HOME and SPARK_HOME locations should not be modified
ENV HADOOP_VERSION=3.1.1
ENV HADOOP_HOME=/opt/bitnami/hadoop
ENV HADOOP_CONF_DIR=/opt/bitnami/hadoop/etc/hadoop
ENV SPARK_VERSION=3.2.0
ENV SPARK_HOME=/opt/bitnami/spark
ENV PATH="$PATH:$SPARK_HOME/bin:$HADOOP_HOME/bin"

### Enable access to AWS and ADLS Gen2. Can modify as needed
ENV HADOOP_OPTIONAL_TOOLS="hadoop-aws,hadoop-azure,hadoop-azure-datalake"

### Remove the pre-installed Spark since it is pre-bundled with Hadoop but preserves the Python env
WORKDIR /opt/bitnami
RUN [ -d ${SPARK_HOME}/venv ] && mv ${SPARK_HOME}/venv /opt/bitnami/temp-venv
RUN rm -rf ${SPARK_HOME}

### Install the desired Hadoop-free Spark distribution
RUN wget -q https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-without-hadoop.tgz &&
    tar -xf spark-${SPARK_VERSION}-bin-without-hadoop.tgz &&
    rm spark-${SPARK_VERSION}-bin-without-hadoop.tgz &&
    mv spark-${SPARK_VERSION}-bin-without-hadoop ${SPARK_HOME} &&
    chmod -R 777 ${SPARK_HOME}/conf

### Restore the virtual Python environment
RUN [ -d /opt/bitnami/temp-venv ] && mv /opt/bitnami/temp-venv ${SPARK_HOME}/venv

### Install the desired Hadoop libraries
RUN wget -q http://archive.apache.org/dist/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz &&
    tar -xf hadoop-${HADOOP_VERSION}.tar.gz &&
    rm hadoop-${HADOOP_VERSION}.tar.gz &&
    mv hadoop-${HADOOP_VERSION} ${HADOOP_HOME}

### Setup the Hadoop libraries classpath
RUN echo 'export SPARK_DIST_CLASSPATH="$(hadoop classpath):'"${HADOOP_HOME}"'/share/hadoop/tools/lib/*"' >> ${SPARK_HOME}/conf/spark-env.sh
ENV LD_LIBRARY_PATH="${LD_LIBRARY_PATH}:$HADOOP_HOME/lib/native"

### This is important to maintain compatibility with Bitnami
WORKDIR /
RUN /opt/bitnami/scripts/spark/postunpack.sh
WORKDIR ${SPARK_HOME}

USER 1001
----

[[creating_compute_spark_env]]
== Prepare your PySpark execution compute environment

You must configure the PySpark compute environments for workspaces and/or jobs that will connect to your cluster.

Domino recommends that you use the pre-built base image to create a compatible workspace from link:799193[compute Environment catalog].

=== Customize this Workspace compute environment

Use the image mentioned previously and add **Pluggable Workspace Tools**.

[source,yaml]
----
jupyter:
  title: "Jupyter (Python, R, Julia)"
  iconUrl: "/assets/images/workspace-logos/Jupyter.svg"
  start: [ "/opt/domino/workspaces/jupyter/start" ]
  supportedFileExtensions: [ ".ipynb" ]
  httpProxy:
    port: 8888
    rewrite: false
    internalPath: "/{{ownerUsername}}/{{projectName}}/{{sessionPathComponent}}/{{runId}}/{{#if pathToOpen}}tree/{{pathToOpen}}{{/if}}"
    requireSubdomain: false
jupyterlab:
  title: "JupyterLab"
  iconUrl: "/assets/images/workspace-logos/jupyterlab.svg"
  start: [  "/opt/domino/workspaces/jupyterlab/start" ]
  httpProxy:
    internalPath: "/{{ownerUsername}}/{{projectName}}/{{sessionPathComponent}}/{{runId}}/{{#if pathToOpen}}tree/{{pathToOpen}}{{/if}}"
    port: 8888
    rewrite: false
    requireSubdomain: false
vscode:
  title: "vscode"
  iconUrl: "/assets/images/workspace-logos/vscode.svg"
  start: [ "/opt/domino/workspaces/vscode/start" ]
  httpProxy:
    port: 8888
    requireSubdomain: false
rstudio:
  title: "RStudio"
  iconUrl: "/assets/images/workspace-logos/Rstudio.svg"
  start: [ "/opt/domino/workspaces/rstudio/start" ]
  httpProxy:
    port: 8888
    requireSubdomain: false
----

----- user_guide/on-demand-distributed-computing/spark/on-demand-spark/index.txt -----
:page-version: 6.1
:page-title: On-demand Spark
:page-permalink: 482ec5
:page-order: 20

You can use Domino to dynamically provision and orchestrate a Spark cluster directly on the infrastructure that backs the Domino instance.
This gives you quick access to Spark without having to rely on your IT team.

== Orchestrate Spark on Domino

Domino supports fully containerized execution of Spark workloads on the Domino Kubernetes cluster.
You can interact with Spark through a link:e6e601[Domino workspace] or in batch mode through a link:942549[Domino job], as well as directly with https://spark.apache.org/docs/latest/submitting-applications.html[spark-submit^].

When you start a workspace or a job that uses an on-demand cluster, Domino orchestrates a cluster in http://spark.apache.org/docs/latest/spark-standalone.html[standalone mode^].
The master and workers are newly deployed containers and the driver is your Domino workspace or job.

== Suitable use cases

The Domino on-demand Spark cluster is suitable for the following workloads:

Distributed machine learning::
https://spark.apache.org/[Spark^] provides a simple way to parallelize compute-heavy workloads such as distributed training. Spark comes with powerful machine learning algorithms bundled in https://spark.apache.org/mllib/[MLlib^] for this purpose.
Interactive exploratory analysis::
Efficiently load a large dataset in a distributed manner to explore and understand the data using familiar query techniques with https://spark.apache.org/sql/[Spark SQL^].
Featurization and data transformation _(for experienced Spark users)_::
Sample, aggregate, relabel, or otherwise manipulate large datasets to make them more suitable for analysis or training.
+
[NOTE]
====
For optimal performance, you must have a cluster with sufficient resources and you must be adept at tuning your Spark application and writing performant Spark transforms.
====

== Unsuitable use cases

The following are usage patterns that are presently not suitable for on-demand Spark on Domino:

Stream processing pipeline::
+
While Spark offers a robust stream processing engine, the ephemeral nature of the on-demand clusters on Domino makes it not a great fit for long-lived stream processing applications.
+
For such cases, consider using an externally managed Spark cluster.
Collocated Spark and HDFS::
+
The Domino on-demand clusters do not come with an HDFS installation and are generally not suitable for collocating data and compute.
//What is HDFS?
+
Data in Domino clusters is intended to reside outside the cluster (for example, object store or Domino data set).
When you want to use the cluster as long-term HDFS storage, consider using an externally managed Spark cluster.
Data pipelines with strict performance SLA::
+
While Domino orchestrates Spark on Kubernetes reliably, no extensive performance tuning or optimization has been performed.
The cluster configuration and default context configuration parameters might not be optimized for such workloads.
+
[NOTE]
====
If you intend to explore on-demand Domino spark clusters for such workloads, perform extensive validation and tuning of your jobs.
====

== Next steps

* Find out more about the link:03c30f[Validated Spark version].
* Learn how to enable and configure the functionality on your deployment in link:1962f3[Configure Spark prerequisites].
* Learn how to link:f11f6a[create an on-demand Spark cluster] with the desired cluster settings attached to a Workspace or Job.
* Find out how you can link:146b80[manage Spark dependencies].
* Learn how to link:a3b42e[Access data with Spark].

----- user_guide/on-demand-distributed-computing/spark/on-demand-spark/manage-dependencies.txt -----
:page-version: 6.1
:page-title: Manage Spark dependencies
:page-sidebar: Manage dependencies
:page-permalink: 146b80
:page-order: 40

In a shared Spark cluster, it can be challenging for teams to manage their dependencies (for example, Python packages or JARs). Installing every dependency that a Spark application may need before it runs and dealing with version conflicts can be complex and time-consuming.

Domino allows you to easily package and manage dependencies as part of your link:1962f3#creating_base_spark_env[Spark-enabled compute environments]. This approach creates the flexibility to manage dependencies for individual projects or workloads without having to deal with the complexity of a shared cluster.

To add a new dependency, add the appropriate statements in the _Docker Instructions_ section of the relevant Spark and execution compute environments.

For example to add `numpy`, include the following.

[source,Docker]
----
### Optionally specify version if desired
RUN pip install numpy
----

== Next steps

link:a3b42e[Access data with Spark].
----- user_guide/on-demand-distributed-computing/spark/on-demand-spark/validated-spark-version.txt -----
:page-version: 6.1
:page-title: Validated Spark version
:page-permalink: 03c30f
:page-order: 10

For each minor release (for example, Domino 4.4 or 4.5), Domino performs a set of validation tests against the *most recently released version* of Apache Spark.
The purpose of the tests is to cover the integration surface between the Domino platform and the on-demand Spark clusters that the platform orchestrates.
The testing is not intended to be a comprehensive test of Spark, as Domino does not modify that software.

See the link:41faed[release notes] for more information on the validated framework for each Domino version.

----- user_guide/on-demand-distributed-computing/spark/on-demand-spark/work-with-data.txt -----
:page-version: 6.1
:page-title: Access data with Spark
:page-sidebar: Work with data
:page-permalink: a3b42e
:page-order: 50

When using a Domino on-demand Spark cluster, any data that will be used,
created, or modified as part of the interaction must go into an
external data store.

[NOTE]
====
On-demand Spark clusters are not intended as a permanent store of any
data or collocating a big data layer such as HDFS. Any data that is not
stored externally from the cluster will be lost upon termination.
====

== Use Domino datasets
[[tr1]]
// Ray cluster workers have access to datasets attached to a workspace
[[tr2]]
// Ray cluster workers have access to datasets attached to a job

When you create a Spark cluster attached to a Domino workspace or job,
any link:0a8d11[Domino dataset] accessible from the workspace or job
will also be accessible from all components of
the cluster under the same dataset mount path.
Data can be accessed using the following path prefix:

[source,shell]
----
file:///
----

// Marty - The above text was being rendered as an unclickable link

For example, to read a file you would use the following.

[source,python]
----
rdd = sc.textFile("file:///path/to/file")
----

No additional configuration of the Spark cluster environment or the
execution environment is required.

== Use S3
[[tr3]]
// Ray cluster workers are able to access S3

To enable working with data in Amazon S3 (or S3 compatible object store) you must ensure that your link:1962f3#creating_base_spark_env[base Spark cluster environment] and compatible link:1962f3#creating_compute_spark_env[PySpark compute environment] are configured with the https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html[Hadoop-AWS module^].

The environments created when
link:1962f3[configuring prerequisites] will at a
minimum include Hadoop 2.7.3 client libraries, which are sufficient for
basic access. A number of additional commonly used features (for example,
temporary credentials, SSE-KMS encryption, more efficient committers,
etc.) are only available in more recent Hadoop-AWS module versions.

Consult the documentation for the relevant version to determine what may
be the best fit for you.

* https://hadoop.apache.org/docs/r2.7.3/hadoop-aws/tools/hadoop-aws/index.html[Hadoop-AWS Module 2.7.3^]
* https://hadoop.apache.org/docs/r2.8.5/hadoop-aws/tools/hadoop-aws/index.html[Hadoop-AWS Module 2.8.5^]
* https://hadoop.apache.org/docs/r2.9.2/hadoop-aws/tools/hadoop-aws/index.html[Hadoop-AWS Module 2.9.2^]
* https://hadoop.apache.org/docs/r3.1.3/hadoop-aws/tools/hadoop-aws/index.html[Hadoop-AWS Module 3.1.3^]
* https://hadoop.apache.org/docs/r3.2.1/hadoop-aws/tools/hadoop-aws/index.html[Hadoop-AWS Module 3.2.1^]

For Spark 3.1.1, a good advanced option would be Hadoop 3.2.0 or later.

=== S3 usage examples

Now that you have your environments properly setup, you can interact
with S3. The following are several common access patterns.

*Access bucket with AWS credentials in environment variables*

[source,python]
----
import os
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

# the default configuration will pick up your credentials from environment variables
# No additional configuration is necessary

# test reading
df = spark.read.json("s3a://bucket/prefix1/prefix2/people.json")
df.show()
----

*Access bucket with SSE-KMS encryption*

[NOTE]
====
Requires Hadoop-AWS
2.9.2+
====

[source,python]
----
import os
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

# for write operations you will need the ARN of the key to use
# Note that the credentials used need to have proper access to use the key
kms_key_arn = "<your key ARN here>"

# configure the connector
# This example assumes credentials from environment variables so no need to configure
# Note: The encryption config is not needed for read only operations
hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()
hadoop_conf.set("fs.s3a.server-side-encryption-algorithm", "SSE-KMS")
hadoop_conf.set("fs.s3a.server-side-encryption.key", kms_key_arn)

# test reading
df = spark.read.json("s3a://bucket/prefix1/prefix2/people.json")
df.show()

# test writing
df.write.mode("overwrite").parquet("3a://bucket/prefix1/prefix2/write-test/output")
----

*Access a bucket with Domino assumed temporary credentials*

[NOTE]
====
Requires Hadoop-AWS
2.9.2+

It is important that no AWS credential variables are set in your user
profile or project.
====

[source,python]
----
import os
from pyspark.sql import SparkSession

try:
    spark.stop()
except:
    pass
spark = SparkSession.builder.getOrCreate()

#The name of one of the roles you are entitled to
profile_name="my-role-name-read-write"

# use boto3 for convenience to get credentials form credentials file populated by Domino
# can use any method desirable to extract the credentials
import boto3
role_creds = boto3.Session(profile_name=profile_name).get_credentials().get_frozen_credentials()

# configure the connector
# Use the TemporaryAWSCredentialsProvider
hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()
hadoop_conf.set("fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider")
hadoop_conf.set("fs.s3a.access.key", role_creds.access_key)
hadoop_conf.set("fs.s3a.secret.key", role_creds.secret_key)
hadoop_conf.set("fs.s3a.session.token", role_creds.token)

# test reading
df = spark.read.json("s3a://bucket/prefix1/prefix2/people.json")
df.show()

# test writing
df.write.mode("overwrite").parquet("s3a://bucket/prefix1/prefix2/write-test/output")
----

For a full set of configuration options, see the documentation for the https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html[Hadoop-AWS module^].

== Using Azure Data Lake Storage Gen2

To enable working with data in Azure Data Lake Storage (ADLS)
Gen2 you need to configure your base Spark environment and your compute
environment with the https://hadoop.apache.org/docs/current/hadoop-azure/abfs.html[Hadoop-Azure ABFS connector^].

The ABFS connector requires Hadoop 3.2+.

To accomplish this, set `SPARK_VERSION=3.0.0` and `HADOOP_VERSION=3.2.1`
when following the advanced instructions for
link:1962f3#creating_compute_spark_env[base Spark cluster environment] and
compatible
link:1962f3#creating_compute_spark_env_custom_hadoop[PySpark compute environment].

[NOTE]
====
You must also enable the
`ENV HADOOP_OPTIONAL_TOOLS=hadoop-azure` directive in your environments.
====


[[aws-resources]]
== Access AWS Resources from a Spark Cluster
[[tr4]]
// Ray cluster workers have the same AWS credentials and permissions enabled for the user


[NOTE]
====
This feature requires Hadoop 2.9.2 or higher. If you're using an earlier
version of Hadoop, you'll need to
link:1962f3#creating_base_spark_env_custom_hadoop[configure
your base Spark environment] or your
link:1962f3#creating_compute_spark_env_custom_hadoop[PySpark
environment] to use Hadoop 2.9.2+.
====

You can configure on-demand Spark clusters in your Domino workspace to
access AWS resources using temporary credentials issued by AWS. To do
this, your Domino deployment must use single sign-on (SSO) with a
trusted identity provider (IdP). The credentials can also be
continuously refreshed, allowing your Spark cluster to have continuous
access to AWS resources.

The specific credentials (and associated privileges) issued by AWS to
your Spark cluster are based on role profiles defined in your IdP by a
Domino administrator. These profiles include identity attributes used by
AWS to issue appropriate temporary credentials corresponding to a role
profile. The temporary credentials are then automatically distributed to
your on-demand Spark cluster.

For more details on this credential propagation architecture, see the link:eb6a88[AWS
credential propagation] section in the _Domino Administrator's Guide_.

To take advantage of this feature, you can either configure your
Spark context dynamically to work with profile role credentials in your
code, or configure the desired profile in your project settings.
Both methods are described below and you should select the option that
best matches your use case.

*Configure your Spark context dynamically in your code*

This method provides you with more flexibility and is recommended if you
need to frequently change role profiles. Recall that your Spark clusters
must use Hadoop 2.9.2 or higher and may need to be configured
accordingly prior to implementing the following code snippet.

[source,python]
----
import os
from pyspark.sql import SparkSession

try:
    spark.stop()
except:
    pass

# First, set the AWS_PROFILE environment variable to the name of the profile found in the credentials file $AWS_SHARED_CREDENTIALS_FILE.
# If you're doing this in a notebook, first stop your Spark session or context for the change to take effect.
os.environ['AWS_PROFILE'] = 'name-of-profile-to-use'

# Next, configure the Spark connector by setting up the provider type and the name of the profile to use. Be sure to replace the .appName() argument with the name of your app.
spark = SparkSession.builder 
          .appName("Credential Spark Test") 
          .config("spark.hadoop.fs.s3a.aws.credentials.provider", "com.amazonaws.auth.profile.ProfileCredentialsProvider") 
          .config("spark.executorEnv.AWS_PROFILE", os.environ['AWS_PROFILE']) 
          .getOrCreate()

# Read some data from AWS (replace with your S3 URI)
df = spark.read.json("s3a://foobar/bazbux.json")
df.show()
----

*Configure your Spark context from your Domino project settings*

You can also enable this feature by adding Spark configuration options
in your Domino project settings. This method provides less flexibility
and is recommended for projects that will utilize one consistent role
profile. To enable this feature:


. Go to *Settings* in your Domino project.
. Click the *Integrations* tab.
. In the *Apache Spark mode* section, select *Domino managed on-demand
cluster*.
. In the *Spark Configuration Options* text area, add the keys and
values specified below. Ensure one whitespace between the key and the
value.
+
----
spark.hadoop.fs.s3a.aws.credentials.provider com.amazonaws.auth.profile.ProfileCredentialsProvider
spark.executorEnv.AWS_PROFILE name-of-profile-to-use
----


. Go to your Domino account settings and click
*User Environment Variables*. Under *Set user environment variable*, set
*Name* to `AWS_PROFILE` and set *Value* to the name of the profile you'd
like to use (`name-of-profile-to-use` in the previous step). Click *Set
Variable*.



== Kerberos keytab propagation
[[tr5]]
// If a Kerberos keytab file is configured, it will be automatically distributed to all Ray cluster workers

When Kerberos authentication is enabled either in user settings or in
project settings, the uploaded keytab will be automatically distributed
to all cluster containers at a well-known location.

By default, the keytab will be available at:
`/etc/security/keytabs/keytab`

Alternatively, a Domino administrator can change the path where the
keytab will be available by using the
`com.cerebro.domino.integrations.kerberos.keytabMountPath`
link:71d6ad[configuration records]
setting.

----- user_guide/on-demand-distributed-computing/spark/on-demand-spark/work-with-your-cluster.txt -----
:page-version: 6.1
:page-title: Work with your Spark cluster
:page-sidebar: Work with your cluster
:page-permalink: f11f6a
:page-order: 30

Create an on-demand Spark cluster with the desired cluster settings attached to a Workspace or Job.

[[tr1]]
// As a Domino user, I can launch a workspace with a Spark cluster attached and execute a workload requiring that cluster

== Create an on-demand Spark cluster attached to a Domino Workspace
. Click *New Workspace*.
. From Launch New Workspace, select
*Compute Cluster*.
. Specify the link:#spark_cluster_settings[cluster settings] and launch you workspace.
After the workspace is running, it has access to the Spark cluster you configured.

The hardware tier for your workspace determines the compute resources available to your Spark driver process.


== Create a cluster with Jobs

[[tr2]]
// As a Domino user, I can start a batch job with a Spark cluster attached and successfully execute job code requiring that cluster

. From the *Jobs* menu, click *Run*.
. From *Start a Job* select
*Compute Cluster*.
. Specify the link:#spark_cluster_settings[cluster settings] and launch your job.
The job has access to the Spark cluster you configured.


You can use any Python script that contains a PySpark job.

You can also use `spark-submit` to submit jobs. However, this is not recognized automatically as a Domino supported job type, so you must wrap it with a shell script unless you included a copy of `spark-submit.sh` when you link:1962f3#creating_compute_spark_env[prepared your compute environment].

The following is an example of a simple wrapper `my-spark-submit.sh`.

[source,bash]
----
#!/usr/bin/env bash

spark-submit $@
----

[[spark_cluster_settings]]
== Understand your cluster settings

Domino makes it simple to specify key settings when creating a Spark cluster.

[[tr3]]
// As a Domino user, I can change the Spark cluster settings (number of workers, worker and head hardware tiers, compute environment, and local storage per executor) used to create a Spark cluster

* *Min Executors*
+
Number of Executors that will be available to your Spark application when the cluster starts.
If *Auto-scale workers* is not enabled, this will always be the size of the cluster.
The combined capacity of the executors will be available for your workloads.
+
When you instantiate Spark context with the default settings, the
`spark.executor.instances` https://spark.apache.org/docs/latest/configuration.html[Spark setting^]
is set to the number specified in the previous window.

* *Max Executors*
+
The maximum number of Spark executors that the cluster can reach when
*Auto-scale workers* is enabled.
See link:#spark_cluster_autoscaling[Cluster autoscaling] for more details.

* *Cluster size: Limit*
+
[[tr4]]
// As a Domino user, I can only set up to the maximum concurrent executions as set by the Domino admin; the cluster settings field will not allow me to exceed the remaining executions available
The maximum number of executors that you can make available to your cluster is limited by the number of per-user executions that your Domino administrator has configured for your deployment.
+
In addition to the number of Spark application executors, you will need 1 slot for your cluster master and 1 slot for your workspace or job.

* *Executor Hardware Tier*
+
The amount of compute resources (CPU and memory) that will be made available to Spark executors.
+
When you instantiate Spark context with the default settings, the
`spark.executor.cores` and `spark.executor.memory`
https://spark.apache.org/docs/latest/configuration.html[Spark settings^]
will be set to the values corresponding to the Hardware Tier.
+
For cores, the number of cores will be rounded up to the nearest integer.
For example, a Hardware Tier with 1.5 cores will result in
`spark.executor.cores` set to 2 when creating the default context.

* *Master Hardware Tier*
+
Same mechanics as the Executor Hardware Tier, but applied to the resources that will be available for your Spark cluster master.
+
The master in the Domino configuration has a pretty limited role in Spark application scheduling, and it does not need a significant amount of resources.
+
[[tr13]]
// As a Domino admin, I can create hardware tiers for use only by Spark (or multiple) cluster types; if such tiers exist, only those tiers will be selectable, and no standard (non-Spark-cluster) hardware tiers will be available
By default, any Hardware Tier is available when selecting resources for your Executor and Master.
Domino administrators can optionally configure compute cluster
link:eca4b2[dedicated hardware tiers].

* *Cluster compute environment*
+
Designates your environment.
Projects already using this environment will be allowed to continue to use it until a new project environment is set.

* *Dedicated local storage per executor*
+
The amount of dedicated storage in Gigabytes (2^30 bytes) that will be available to each executor.
+
The storage will be automatically mounted to [.title-ref]#/tmp# which is the default location for `spark.local.dir` that serves as the location for RDDs, when they need to be on disk, and for shuffle data.
+
The storage will be automatically provisioned when the cluster is created and de-provisioned when it is shut down.
+
[WARNING]
====
The local storage per executor should not be used for storing any data which needs to be available after the cluster is shut down.
====

== Advanced cluster configuration topics

=== Spark executors versus workers

Domino gives you the ability to have flexible resource allocation through Hardware Tiers which makes it convenient to plan your cluster resources needed for your application in terms of executors as opposed to workers.
When creating on-demand Spark clusters, Domino will automatically manage the relationship between Spark executors and workers (see
https://spark.apache.org/docs/latest/cluster-overview.html#glossary[Spark
definitions^]).
By default, there will be one worker for each executor.

=== Resource overhead

When setting up worker and executor memory limits, Domino ensures that the worker containers have additional memory on top of the Hardware Tier memory definition to account for the recommended overhead of 10% of executor memory with a minimum of 384MB.
The usable executor memory will always be based on the value from the actual Hardware Tier.

You might have to take this into account when designing Hardware Tiers to achieve optimal packing of Spark worker containers on the physical instances of the Domino Kubernetes cluster.

Because of the worker memory overhead and the approach of using 1 worker per executor, it is possible that using lots of executors with smaller Hardware Tiers may result in excessive memory overhead.

If this is undesirable, an experienced data scientist can choose a large Hardware Tier when creating the cluster and then overwrite the executor core and memory settings to achieve the desired effect.
See link:#overwriting_spark_parameters[Adding and overwriting parameters].


[[spark_cluster_autoscaling]]
== Cluster auto-scaling

[[tr5]]
// As a Domino user, I can set my Spark cluster to autoscale from a minimum to a maximum number of workers


Cluster auto-scaling allows you to start with a small cluster, which then automatically scales up and down in response to the resource consumption of your workload.
This approach utilizes resources more efficiently for bursty workloads.

[[tr6]]
// As a Domino user, I can see my cluster size increase when my workload uses more than the CPU or memory threshold
By default, the cluster size will increase when the average CPU utilization of your workload reaches 80%.
A Domino administrator can further refine the
link:71d6ad#compute-cluster-auto-scaling[auto-scaling settings]
by including memory utilization or changing the desired scaling thresholds.

[[tr7]]
// As a Domino user, I can see my cluster size decrease when my usage falls below the CPU or memory threshold
Scale down happens if resource utilization is low for a period of at least 5 minutes.
Note that depending on the workload that you are executing, scale down may terminate cluster nodes that contain intermediate results which would need to be recomputed.


== Connect to your cluster

[[tr8]]
// As a Domino user, I can execute code to connect to an use the Spark cluster I have initiated

When provisioning your on-demand Spark cluster, Domino sets up key default cluster configuration parameters (for example, `spark.master`,
`spark.driver.host`) to the appropriate cluster URLs so that creating [.title-ref]#SparkSession# or [.title-ref]#SparkContext# with default context will properly connect the cluster to your workspace or job.

=== Create SparkSession

You can create `SparkSession` using the following:

[source,python]
----
from pyspark.sql import SparkSession

spark = SparkSession 
        .builder 
        .appName("MyAppName") 
        .getOrCreate()

 # You can examine the full config
spark.sparkContext.getConf().getAll()
----

=== Create SparkContext

Alternatively, you can create `SparkContext` using the following:

[source,python]
----
from pyspark import SparkContext

conf = SparkConf().setAppName(appName).setMaster(master)
sc = SparkContext(conf=conf)
----

[[overwriting_spark_parameters]]
=== Add and overwrite parameters

It is possible to add additional config parameters or overwrite any of the defaults by manipulating your context configuration.

[source,python]
----
from pyspark.sql import SparkSession

spark = SparkSession 
        .builder 
        .appName("MyAppName") 
        .config("spark.some.config.option", "some-value") 
        .getOrCreate()
----

You can also supply additional configuration settings that will apply for any clusters associated with a Domino project from
*Settings* > *Integrations* > *Domino managed on-demand cluster*


[WARNING]
====
You should not overwrite the default configuration values for
`spark.master`, `spark.driver.host` since you may lose proper connectivity between your cluster and workspace or job.
====

== Access the Spark UI

Spark provides a suite of
https://spark.apache.org/docs/latest/web-ui.html[web user
interfaces^] that you can use to monitor the status and resource consumption of your Spark cluster.

Domino makes the Spark web UI available for active on-demand clusters attached to both workspaces and jobs.

NOTE: In a hybrid Domino deployment, if the link:95520d[data plane] is not configured for workspaces, then the user interface link is disabled.
An administrator can link:491fe8[Enable a data plane for workspaces].

=== Spark UI from Workspaces

[[tr9]]
// As a Domino user, I can view the Web UI associated with my Spark cluster from its tab in my workspace

The Spark UI is available from a dedicated tab in your workspace.


=== Spark UI from Jobs

[[tr10]]
// As a Domino user, I can view the Web UI associated with my Spark cluster from a link in my job details

The Spark UI is also available for running jobs from *Details* tab.



=== Spark UI compatibility mode

[[tr14]]
// As a Domino admin, I can configure Domino to run Spark in compatibility mode
[[tr15]]
// As a Domino user, I can access Spark web UI when using an earlier Spark version (<=3.0.1) in a Domino deployment configured to use compatibility mode

With Domino 4.5 and higher, if an on-Demand Spark cluster uses Spark 3.0.1 or earlier, some links in the Spark UI will not function properly.
This will not affect actual Spark processing.

For cases where you want to run an older version, you can configure Domino in Spark web UI compatibility mode using one of the following options:

* *Set for the entire Domino instance*
+
A Domino administrator can set the Domino
link:71d6ad[configuration records]
setting
`com.cerebro.domino.computegrid.computeCluster.spark.proxyCompatibility`
to `legacy`.
* *Set on a Domino Project level*
+
Alternatively, this can be set at a project level from
*Settings* > *Integrations* > *Domino managed on-demand cluster*
+

[WARNING]
====
When Compatibility mode is set, the UI for Spark 3.1.1 and higher will not function properly.
====

== Cluster lifecycle

[[tr11]]
// As a Domino admin, I can view the Spark cluster resources created to execute a job or workspace and see that they are cleaned up when the workload is complete

On workspace or job startup, a Domino on-demand Spark cluster with the desired link:#spark_cluster_settings[cluster settings] is automatically provisioned and attached to the workspace or job as soon as the cluster becomes available.

On workspace or job termination, the on-demand Spark cluster and all associated resources are automatically terminated and de-provisioned.
This includes any compute resources and storage allocated for the cluster.

== Cluster network security

[[tr12]]
// As a Domino user, I cannot use another user's active Spark cluster

The on-demand Spark clusters created by Domino are not meant for sharing between multiple users.
Each cluster is associated with a given workspace or a job instance.
Access to the cluster and the Spark web UI is restricted only to users who can access the workspace or the job attached to it.
This restriction is enforced at the networking level and the cluster is only reachable from the execution that provisioned it.

----- user_guide/overview.txt -----
:page-version: 6.1
:page-title: What is Domino?
:page-permalink: 71a047
:page-order: 10
:page-section: Overview

Domino is an open platform for data science that unifies various programming languages, integrated development environments (IDEs), data sources, and tools in one location. 

== Domino as a system of record
Domino provides a central hub for AI operations and knowledge across the enterprise, enabling best practices, cross-functional collaboration, faster innovation, and efficiency. This integration enhances the research, development, and deployment of data science and works well to:

* Unify teams, tools, data, and infrastructure; and democratize all AI. 
* Orchestrate model life cycles and industrialize AI from pilot to scale.
* Govern data, models, and processes to make AI responsible by default. 

Domino makes it easier to use your favorite tools by setting up a few basic computing environments with partner technologies. We build and test these environments and run security checks to ensure they are safe. 

== Domino delivers value for all teams

Domino integrates seamlessly with your existing stack, allowing access to various open-source and commercial tools. Domino compounds knowledge, serving as a reliable source of organizational information and enhancing team collaboration. 

* *Data Science teams*: Domino is designed and optimized for unique and complex data science workflows. It gives you self-service access to data, tools, and infrastructure. You can reuse and collaborate with other teams and the business, enforce best practices, and compound knowledge and efficiency.  
* *IT and DevOps teams*: Domino has a secure, managed infrastructure with minimal IT support burden. The smart controls and full visibility allow you to slash cloud costs, and you can run Domino on any cloud, on-prem, hybrid, or multi-cloud environment.
* *MLOps and Risk teams*: Domino allows you to track, review, and validate all models using robust processes. It also offers flexible deployment options within any environment and turnkey model monitoring with easy remediation. Automatic versioning of code, data, environments, and results and customizable templates for best practices make compliance easier to monitor.

image::/images/6.1/ecosystem-new.png[alt="Domino machine learning ecosystem", role=noshadow, width=1200]

== How do we do it?
Domino is an open system that provides self-service access to data and tools, enables the reuse of materials, and facilitates collaboration with other teams in your organization while enforcing best practices, enhancing knowledge, and improving efficiency.

* link:16d9c1[Data]: Domino connects to external data sources like databases, data warehouses, and data lakes. You can find a list of supported data sources under link:fbb41f[Data Source Connectors].
* link:d2ba79[LLMs]: Our deep learning models are trained on extensive datasets for language processing tasks. Based on their training data, they generate new text that mimics human language.
* link:a8e081[Software]: You can connect your Domino projects to link:40f92c[Jira], link:910370[GitHub], link:da707d[MLflow], or link:02ec6d[Sagemaker]. This enables seamless integration for your data science workflows and allows users to track progress on data science projects.
* link:08a636[Languages]: Domino allows data scientists to use their preferred languages and tools, such as Python, SAS, Matlab, and R.
* link:867b72[IDEs and Tools]: You can use IDEs and tools like Jupyter Notebook, JupyterLab, RStudio, VS Code, MATLAB, and SAS with Domino.
* link:bfa148[Packages and Libraries]: We support a range of packages and libraries, including open-source options such as Python, R, TensorFlow, PyTorch, and others.
* link:799193[Compute Environments]: You can add different environments to any Domino installation by visiting https://quay.io/[quay.io^]. To get started, download the image from the repository link. Our documentation has step-by-step instructions for setting up these environments.

== Domino framework

We seamlessly integrate your data, infrastructure, and frameworks by employing diverse strategies and techniques. Additionally, we focus on creating comprehensive frameworks tailored to your specific needs, ensuring that your existing IT ecosystem functions cohesively and efficiently. Through these methods, we aim to optimize your operations and streamline workflows for improved performance and productivity:

* link:f35c19[APIs]: Use Domino APIs to expand and facilitate innovation and efficiency.
* link:71635d[Applications]: Configure web applications to optimize scalability and performance for several popular frameworks.
* link:95520d[Edge]: Capture, process, store, and analyze data locally rather than fetching it from a distant server.
* link:06da1b[NetApp Volumes]: Share data more easily across projects by storing files on external NetApp-backed storage.
* link:f12554[External Data Volumes(EDVs)]: Volumes from network-attached storage systems that are mounted to the Domino system.

----- user_guide/projects/add-existing-data.txt -----
:page-version: 6.1
:page-permalink: ec0164
:page-title: Add existing data to Projects
:page-order: 100

Sometimes the data you need is already available in your Domino deployment.
This topic explains how to discover available data in your deployment and add it to your project.

== Find available data

In the left navigation pane of your project, click *Data* to view datasets and data sources that you have permission to use across all projects in the deployment:

// EDVs never appear on this page. https://dominodatalab.slack.com/archives/C019MDUG9S6/p1686340040254189?thread_ts=1686339075.779479&cid=C019MDUG9S6
NOTE: External Data Volumes do not appear here; you can find them by adding them to your project, as explained on the page linked below.

If you don't see the data you need, it might need to be link:df7044[shared with you] or added to Domino.

[[add-data]]
== Add data to a Project

If the data is already set up in Domino and you have the appropriate permissions, you can add it to your project. See the following articles on adding different data types to your project.

- link:fa5f3a[Add Data Sources to a project].
- link:6942ab[Add Domino Dataset to a project].
- link:ee8d01[Add External Data Volumes (EDVs) to a project].
- link:56938d[Add Project Artifacts to a project].


== What if I don't see the data I need?

If the data you're looking for is not listed, contact the project owner or your Domino admin.

* If the data is in Domino but you can't see it, it might need to be link:df7044[shared with you].
* If the data is not yet in Domino, you or your administrator might need to add it.

== Next steps

Once you've added data to your project, you can work with it in your code.
The method for working with data in your code depends on how it's stored:

* link:fa5f3a[Work with data sources]
* link:ba5bad[Work with Datasets]
* link:f12554[Work with external data volumes]
* Learn about link:df7044[data security and sharing]

----- user_guide/projects/best-practices-for-projects.txt -----
:page-permalink: d2b098
:page-version: 6.1
:page-title: Best practices for working with Projects
:page-sidebar: Best practices for Projects
:page-order: 20

Domino Projects help you follow best practices in your work. They are flexible and can fit your organization’s workflow. From idea creation to production, Projects support you through the entire process of making different analytical assets.

We have collected a series of best practices to help you succeed in creating and managing your Domino Projects.

== Data for Projects

Here is a compilation of best practices for effective data management in Projects:

=== Use Git-based Projects whenever possible

Git-based Projects provide enhanced control over code synchronization, allowing you to use features from external Git providers and offer a more modern development experience.

*Reason*: Git-based Projects help keep your repository organized so you can follow best practices for your field.

*Solution*: Follow these ideas for using Git-based Projects:

* Use Git-based Projects for greater control over code synchronization.
* Leverage features from external Git providers to improve the development experience.

=== Use data sources or external data volumes

Data sources in a Domino deployment have a global scope and are accessible to anyone with the appropriate permissions in any project.

*Reason*: Domino automatically generates an link:85fbb1[audit trail] for its data sources, allowing you to track and review all activities related to those data sources.

*Solution*: Use these tips to use data sources:

* Ask your Domino administrator to create, configure, and manage your data sources.
* Use a set of fixed login details that are automatically used for all users who have permission to access a specific data source.

== Creating Projects

Domino Projects help you follow best practices and adapt to your organization’s workflow, supporting you from idea creation to production of analytical assets. Here is a collection of best practices for creating Domino projects:

=== Use Project templates to get started quickly

You can create link:5fed45[Project templates] that can be shared across the organization, which allows users to kickstart their Projects from an existing prototype rather than starting from scratch.

*Reason*: Projects often lack consistency between them, causing slower project setups and less adherence to best practices. Users can get started quickly by using existing templates rather than beginning from scratch.

*Solution*: Here are some ideas for using Project templates:

* Create Project templates tailored to specific use cases.
* Use templates to implement internal best practices or enforce company standards.
* Foster stronger collaboration, faster project setup, and increase consistency between Projects by using a template.

=== Create new Projects by cloning existing Projects

Before you start your Project, look for existing content in your organization that you can use to get started.

*Reason*:  Projects created by others in Domino can help you reach your analytical goals faster.

*Solution*: Follow these tips to clone existing Projects:

* link:d9abb7[Search your Domino deployment] to discover Domino Projects that you can base your Project on.
* On your Projects home page, explore the various tabs to see Projects that you are assigned as a collaborator, as well as other Projects recommended by Domino.
* Rename your cloned projects something different from the original to avoid confusion.

== Collaborating on Projects

Domino makes it easy to collaborate on Projects and share outputs. Here is a collection of best practices to help you collaborate on Domino Projects:

=== Invite the original project owner to collaborate

The original project owner can often guide you on ways to get the most out of the cloned Project.

*Reason*: Complicated Projects are frequently more difficult to ramp up.

*Solution*: To invite the original project owner to your Project:

* Make the original project owner a contributor to your Project.
* Contributors can read and write project environment variables, and they can invite new collaborators.
* Use suggestions from the original project owner for best practices for compliance.

=== Use individual forks for each project contributor

Forking a Project helps you stay organized and track changes if you have many contributors to a Project.

*Reason*: If you have many collaborators on a Project, forking the Project into a smaller copy will ensure that collaborators are contributing consistently to a Project.

*Solution*: Here are some reasons for using forked Projects:

* Fork a Project to create copies of its files into a new, separate Project.
* Changes in the forked Project can later be reviewed and merged back into the original Project.

== Project permissions

To allow others to access a Project, you can add them as collaborators. You must be a contributor or the project owner to add collaborators.

=== Audit Project permissions regularly

Verify that the right collaborators have the correct permissions regularly.

*Reason*: Allowing anyone to find, use, or run code on your Projects can be dangerous.

*Solution*: Keep your Project permissions locked down:

* Audit collaborators regularly to verify permissions.
* Keep permissions on your Project as locked down as possible.
* Verify that users have the needed permissions to work on your Project.
* Use the Visibility setting to specify who can view your Project.

== Next Steps

* link:299798[Create a Project] to help you follow best practices in your work. Projects are flexible and can fit your organization’s workflow and support you through the entire process of creating different analytical assets.
* link:16685e[Copy a Project] to create a duplicate of an existing Project, which does not include Project settings, run history, or scheduled Jobs.
* link:314004[Import Git repositories] into your Project to access the repository and commit changes to repository contents.
* link:e6ed48[Export and import Project content] shows you how to share content among Projects by exporting and importing.

----- user_guide/projects/collaborate-on-projects/add-comments.txt -----
:page-version: 6.1
:page-title: Add comments
:page-permalink: 06ceeb
:page-order: 40

With Domino, document your decision rationale, analysis, suggestions, and next steps. Easily collaborate and coordinate using comments on these artifacts:



[#Files]
.Files
--
[[tr1]]
. In your project, go to *Files* and click the desired file.
. At the bottom of the page, enter your comment in the *Discussion* box.
. Click the Send icon on the right to save the comment.
--

[#Jobs]
.Jobs
--
[[tr2]]
. In your project, go to *Jobs* and click the desired job.
. In the right pane, click the *Comments* tab and enter your comment.
. Click the Send icon on the right to save the comment.
--

[#Job-results]
.Job results
--
[[tr3]]
. In your project, go to *Jobs* and click the desired job.
. In the right pane, click the *Results* tab and enter your comment.
. Click the Send icon on the right to save the comment.
--

[#Job-run-comparisons]
.Job run comparisons
--
[[tr4]]
. In your project, go to *Jobs* and select two jobs to compare.
. In the navigation tabs above the job list, click the *Compare* icon.
. Scroll to the *Discussion* box and enter your comment.
. Click *Comment* below the preview to save the comment.
--
[#Activity-feed]
.Activity feed
--
[[tr5]]
. In your project, go to *Activity*.
. For entries about jobs and workspaces, click *Add Comment* to write comments for your colleagues.
+
Comments (by anyone) also appear as entries in the feed.
--

[#Reviews]
.Reviews
--
[[tr6]]
. When you or your collaborators are ready to link:4a6d0f[merge] a link:ef261b[forked project], go to *Reviews* in your project to access the requests.
. Click the desired merge request.
. Click *Discussion*.
. Enter your text, then click *Comment* to save.
--

[TIP]
====
* To plan for the future, use keywords in projects with public or organization visibility to help others find your work through link:d9abb7[searches].
* As a good practice, check the *Notify me when others add collaborators to projects I own* box under *Collaboration and Sharing* in your and your collaborators' *Account Settings*.
====

[[tr7]]
== Format your comments
[[tr8]]
Domino also supports link:https://www.mathjax.org/[MathJax^] and link:https://daringfireball.net/projects/markdown/syntax[Markdown^] for richly formatted comments. Preview your message below the *Discussion* box.

[[mathjax-formatting]]
=== MathJax

// _mathjax-formatting added for DOCS-2206.
[[_mathjax_formatting]]

MathJax uses LaTex notation to format mathematical symbols and equations in your comments. See the official link:http://docs.mathjax.org/en/latest/[MathJax Documentation^]. For quick reference and examples of commonly used MathJax notation, Domino recommends link:https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference[this tutorial^] available on the link:https://math.meta.stackexchange.com/[Mathematics meta stack exchange^].

image::/images/6.0/mathjax.png[Example comment with MathJax only,width=600]

[[markdown-formatting]]
=== Markdown

// _markdown-formatting added for DOCS-2206.
[[_markdown_formatting]]

Markdown is a lightweight markup language used to format plaintext documents. The official Markdown link:https://www.markdownguide.org/cheat-sheet/[cheat sheet^] covers the basic syntax. Use Markdown in Domino to format longer comments or create README files.

image::/images/6.0/markdown.png[Example comment with Markdown only,width=600]

== Notify other users

For real-time communication, type `@` followed by a username in a comment to send an email notification to another user based on the link:71afc6[project visibility] rules:

[[tr9]]

* *Private* - link:4b6411[Collaborators] can be notified.
* *Organization* - The link:526a62[organization]'s members can be notified.
* *Public* - Any user can be notified.

----- user_guide/projects/collaborate-on-projects/collaborator-permissions.txt -----
:page-version: 6.1
:page-title: Collaborator permissions
:page-permalink: 7876f1
:page-order: 30


These tables describe the project permissions for each type of collaborator.


== File permissions

[cols="3a,2a,2a,2a,2a,2a, options="header"]
|===
|Action |Project Importer |Launcher User |Results Consumer|Contributor |Owner

|Read files | | |x |x |x

|Write files | | | |x |x

|Add an external Git repository | | | |x |x
|===

== Run permissions

[cols="3a,2a,2a,2a,2a,2a,options="header"]
|===
|Action |Project Importer |Launcher User |Results Consumer|Contributor |Owner

|Start a Run | | | |x |x

|View Completed Runs | |x |x |x |x
|Start a Workspace^1^ | | | |x |x
|View Workspaces | | |x |x |x
|Schedule a Run | | | |x |x
|View Scheduled Runs | | |x |x |x
|===
^1^ Start a Workspace: A user can start a workspace they've created.

== Publishing permissions

[cols="3a,2a,2a,2a,2a,2a,options="header"]
|===
|Action |Project Importer |Launcher User |Results Consumer|Contributor |Owner

|Run a Launcher | |x | |x |x

|View an App | |x |x |x |x

|Publish an App | | | |x |x

|Unpublish an App | | | |x |x

|Invite users to an App | | | |x |x

|Change an App hardware tier | | | |x |x

|Publish a Domino endpoint | | | |x |x

|Create a Launcher | | | |x |x
|===

== Settings permissions

[cols="3a,2a,2a,2a,2a,2a,options="header"]
|===
|Action |Project Importer |Launcher User |Results Consumer|Contributor |Owner

|Set an environment variable | | | |x |x

|Invite a collaborator | | | |x |x

|Change the project stage | | | |x |x

|Raise a blocker | | | |x |x

|Set the project status as complete | | | | |x

|Manage collaborator permissions | | | |x |x

|Change the visibility setting | | | | |x

|Change the default environment | | | |x |x
|Change the default hardware tier | | | |x |x

|Change the project name | | | | |x

|Handle a merge request | | | |x |x

|Transfer project ownership | | | | |x

|Archive a project | | | | |x
|===

[[ds-permissions]]
== Dataset permissions

See link:8f5b7e#_dataset_roles[Dataset Roles] for more information.

[cols="3a,2a,2a,2a,2a,2a,options="header"]
|===
|Action |Project Importer |Launcher User |Results Consumer|Contributor |Owner

|Create a new Dataset in a project | | | |x |x

|Mount a shared Dataset in a project | | | |x |x
|===

== Import permissions

[cols="3a,2a,2a,2a,2a,2a,options="header"]
|===
|Action |Project Importer |Launcher User |Results Consumer|Contributor |Owner

|Import a project |x | |x |x |x
|===

[[experiment-permissions]]
== Experiment permissions
[cols="3a,2a,2a,2a,2a,2a,options="header"]
|===
|Action |Project Importer |Launcher User |Results Consumer|Contributor |Owner

|Create a new experiment or experiment run | | | |x |x

|View, list, or search experiments or runs | | |x |x |x

|Update an experiment or run (includes logging to a run or adding tags) | | | |x |x

|Archive an experiment or run | | | |Only the original owner |x
|===

----- user_guide/projects/collaborate-on-projects/export-import-project.txt -----
:page-version: 6.1
:page-title: Export and import Project content
:page-sidebar: Export and Import Projects
:page-permalink: e6ed48
:page-order: 70



You can import content from one Domino project into another.
The importing project might have access to the exporting project’s files, environment variables, or both, depending on configuration.
During runs with imported files, each project directory is located at `/mnt/<username>/<project name>`.
When a run or workspace is started, these files are pulled in with the current project’s files.
Imported directories are read-only.

NOTE: The path of your project will also change from `/mnt` to `/mnt/<username>/<project name>` when you have imported projects.
If you have hardcoded any paths in your project code to `/mnt`, replace them with paths that use the `$DOMINO_WORKING_DIR` environment variable.



== Export Project content


. Go to project from which you want to export.
. In the navigation pane, go to *Settings > Exports*.
. Select the check boxes to enable exports for the project’s *Files* and *Environment variables* separately, or export the project files as a Python or R *Code Package*.
If you do not enable any of these options, other projects cannot import anything from this project.
+
By default, projects make their latest revision available for export.
+
You can also tag jobs with `release` to make revisions produced by specific jobs available for import.
To do this, go to the Jobs page of a project, select the check box for the Job that you want to export, then click Tag. Enter the exact string `release` to mark the revision created by the selected job as available for export.


== Import Project content



To import a project:

* You must have Project Importer, Contributor, or Owner permissions on the project.
* The project must be configured for export.


. Go to project to which you want to import.
. In the navigation pane, go to 
*Code*
and click *Other Projects*.
. Click *Import a Project* and enter the path to the project that you want to import.
Use the format `<username>/<project-name>`.
. Click *Import*.

Only the files from the directly imported project can be seen when you import.
For example if project A is imported into project B, and then your project imports B, only the contents of B will be accessible to your project.

See link:5c1ab6[Troubleshoot Imports] if necessary.

----- user_guide/projects/collaborate-on-projects/index.txt -----
:page-version: 6.1
:page-title: Collaborate on a Project
:page-permalink: d7731d
:page-order: 120

Domino makes it easy to collaborate on projects and share project outputs.

link:71afc6[Set Project visibility]::
Use the Visibility setting to specify who can view your project.

link:4b6411[Invite collaborators]::
Add users as collaborators to grant them access to a project.

link:7876f1[Collaborator permissions]::
Find the project permissions for each type of collaborator.

link:06ceeb[Add comments]::
Document your decision rationale, analysis, suggestions, and next steps.

link:b889e6[View event notifications]::
Administrators send notifications to alert you about Domino events.

link:5c2374[Transfer Project ownership]::
Transfer ownership of a Project to an individual or organization.

link:e6ed48[Export and Import Project Content]::
You can export content from a project and import it into other projects.

link:5fed45[Project templates]::
Use project templates as a way to efficiently share best practices across the organization.

----- user_guide/projects/collaborate-on-projects/invite-collaborators.txt -----
:page-version: 6.1
:page-title: Invite collaborators
:page-permalink: 4b6411
:page-order: 20


To grant other users access to a project, you can add them as collaborators.
To add collaborators, you must be a Contributor to the project, or the project Owner.

[[tr5]]
//add by user name
[[tr6]]
//add by email address-existing Domino user
[[tr7]]
//add by email address - not existing Domino user

. In the Project, go to *Settings* > *Access & Sharing*.
. In the *Collaborators and permissions* section, enter a username, email address, or organization name.
+
If you enter an email address for a Domino user, they will be invited to join the project as a collaborator.
If you enter an email address that is not associated with an existing Domino user, they are invited to join Domino.
+
. Click *Invite*.
. Set the *Role* for the collaborator.
+
--
[[tr9]]
Contributors::
Can read and write project files, and start runs.
On the *Settings* page, Contributors can read and write project environment variables, and they can invite new collaborators.
Contributors can also change hardware tier and environments.
[[tr10]]
Results Consumers::
Can only read files and access link:71635d[published apps].
[[tr11]]
Launcher Users::
Can only view and run Launchers, and see the launcher runs results.
They cannot view project files.
[[tr12]]
Project Importers::
Can link:37158c[import the project], but otherwise cannot access it.
[[tr13]]
Owners::
You cannot select this role as this is the project owner. These are the only users who can archive a project, change the owner, change collaborator types, import or export to share environment variables and files between projects, or set automatic workspace shutdown times.

See link:7876f1[Collaborator Permissions] for specific project permissions for each type of collaborator.
--
+
[[tr8]]
. In *Notifications preference*, select how to link:ae32e7[notify] the collaborator when runs complete.
This keeps your collaborators up-to-date about the work that each person is doing.

----- user_guide/projects/collaborate-on-projects/project-templates.txt -----
:page-permalink: 5fed45
:page-version: 6.1
:page-title: Project templates
:page-sidebar: Project templates
:page-order: 80

A *Project template* can be created from an existing project by selecting which assets to include (code, datasets, apps, etc.). With templates created, users can kickstart their projects from a collection of existing prototypes rather than beginning from scratch. The use of project templates help to enable:

* *Collaboration* - Easily share and reuse methods, techniques, and working prototypes with colleagues across the organization or company.
* *Efficiency* - Accelerate time to impact by leveraging internal examples as a starting point for your projects.
* *Standardization* - Implement internal best practices that can be easily promoted/enforced as company standards.

NOTE: Project templates are only supported for Git-based projects.

== Create a template

To create a project template:

. Navigate to the Project you would like to templatize and click *Create Template* on the top right. Note that For GitHub and GitHub Enterprise, the button only appears if the code repository for the Project is a link:https://docs.github.com/en/repositories/creating-and-managing-repositories/creating-a-template-repository[template repository].
+
image::/images/6.0/project-templates/create-template.png[alt="Create Template Button", width=1000, role=noshadow]

. Enter a *Template Name*, an optional *Description*, and set the *Access* level.
+
image::/images/6.0/project-templates/create-1.png[alt="Create Template - Step 1", width=800, role=noshadow]

. Select the assets to copy into the template. Also enter a *Default Billing Tag* (optional), *Default Environment*, and *Default Hardware Tier*. Note that Activities, Reviews, Workspaces, Jobs, Experiments and Comments cannot be copied to a template.
+
image::/images/6.0/project-templates/create-2.png[alt="Create Template - Step 2", width=800, role=noshadow]

. Choose where to store the code files for the new template, specifying details for the provider, credentials (only PAT credentials are supported), and the repository.
+
When choosing to store in an existing repository, the repository must be empty. If there is a `README.md` file in there, it will be overwritten. If there are other files besides that, the operation will fail.
+
image::/images/6.0/project-templates/create-3.png[alt="Create Template - Step 3", width=800, role=noshadow]

. Create the Project template.

== Create a Project from a template

NOTE: For templates stored in GitHub or GitHub Enterprise, the code repository must be marked as a link:https://docs.github.com/en/repositories/creating-and-managing-repositories/creating-a-template-repository[template repository] before projects can be created from it.

To create a Project from a template:

. Navigate to the *Projects* page and click on the *Templates* tab.

. Browse for the relevant template and click on it to create the project.
+
image::/images/6.0/project-templates/template-home.png[alt="Project Template Home", width=1000, role=noshadow]

. When creating the project, choose between a number of options for where the code will be stored:

* *New repository*: Create a completely new repository for the project code.
* *Existing repository (empty)*: Use an existing empty repository. The template code files will be copied into the repository. If there is a `README.md` file in there, it will be overwritten. If there are other files besides that, the operation will fail.
* *Existing repository (with code)*: Use a repository with existing code. Template code files will not be copied into the new repository. Only the project assets will be included.

+
image::/images/6.0/project-templates/project-create.png[alt="Create Project", width=800, role=noshadow]

== Mark official templates

Project templates can be marked with an *official* flag to indicate an approved or recommended template for usage across the organization.

NOTE: Only an admin or librarian role can toggle the official flag on a template. Once a template has been flagged as official, practitioner roles can no longer modify or delete the templates.

To flag a template as official:

. Navigate to the *Projects* page and click on the *Templates* tab.

. Hover over the relevant template and click on *Edit Details*.

. Check the *Mark this as an official template* box and click *Save*.
+
image::/images/6.0/project-templates/edit-template.png[alt="Project Template Edit", width=500, role=noshadow]

. Filter the view by *Official templates* to see all available templates that are marked as official.
+
image::/images/6.0/project-templates/official-template.png[alt="Project Template Official", width=1000, role=noshadow]

== Known Issues

[cols="2*",options="header"]
|===
|Issue |Workaround
| SysAdmins can't view Template details
| Share the Template with the SysAdmin by adding them as a collaborator
|===

== Next steps

link:910370[Manage a Git-based Project]
----- user_guide/projects/collaborate-on-projects/set-project-visibility.txt -----
:page-version: 6.1
:page-title: Set Project visibility
:page-permalink: 71afc6
:page-order: 10



Use the Visibility setting to specify who can view your project.

. In the Project, go to *Settings* > *Access & Sharing*.

. Select the type of *Visibility* you want for your project:
[[tr1]]
Public::

* Anyone with your project's URL can view your files and runs even if they don't have a Domino account.
* If you enable file exports, anyone can import your project files.
* Only collaborators can modify files, start runs, and import environment variables unless you select the *allow runs by anonymous users* check box.
+
[[tr4]]
If your project is visible to the public, you can select the *Allow runs by anonymous users* check box.
This lets users start runs even if they don't have a Domino account.
Runs started by anonymous users show as started by the project owner.
+
//Tara: Why do we have this option? What is the benefit?
CAUTION: It is dangerous to allow anyone to run your code.
When allowing anonymous users to perform runs, consider the information that you are revealing such as environment variables in your project that contain bearer tokens, API keys, or passwords.
+
[[tr2]]
Searchable::
//Tara: Is this still an option? I don't see it in cloud-develop.
* Anyone can see this project and see its name and description in search results, but only collaborators can see the project's contents.

Private::

[[tr3]]
* Only collaborators can find and view this project.
//what is the difference between these two bullets?
* Only collaborators can find this project in search results or view this project.

----- user_guide/projects/collaborate-on-projects/transfer-project-ownership.txt -----
:page-version: 6.1
:page-title: Transfer Project ownership
:page-permalink: 5c2374
:page-order: 60

You can transfer any Project you own to another user or link:526a62[organization].
For example, a project that started out as a private scratch workspace can be shared with colleagues by transferring ownership to your organization.

== Transfer ownership to another user
. Go to the project settings page.
. Select the *Archive Project or Transfer Ownership* tab.
. Click *Transfer Ownership*.
You can enter the new owner's user or organization name.
[[tr1]]
//transfer ownership to new user
[[tr2]]
//transfer ownership to new organization
+
[[tr3]]
//new owner (user not organization) receives email
After ownership is transferred, the new owner receives an email about the change.
Organizations do not receive emails.

== API endpoints and ownership transfer

If you were using an API endpoint in the original project, it will stop serving requests after ownership transfer.
The new owner can re-publish it for further use, but note that the endpoint URL will change and your API key will no longer work (unless you are given Contributor access).
So be sure to re-point any external system that calls it, and to coordinate around the downtime.

[[tr4]]
To avoid downtime, you can instead follow these steps:

. Have the new owner fork the project (you will need to link:d7731d[give them access as a Contributor or Results Consumer].
. Re-publish the endpoint in the new project.
. Point your system to the new URL.
. Shut down the original endpoint (and archive the original project, if you like).

== Transfer ownership to an organization
Organization members can transfer ownership of a project to the Organization.

. Go to the project.
. Click *Settings*.
. Click *Archive Project or Transfer Ownership* tab and click *Transfer Ownership*.
Enter the current owner and project name, then for the *New owner username or email* use the name of the Organization.

----- user_guide/projects/collaborate-on-projects/view-event-notifications.txt -----
:page-version: 6.1
:page-title: View event notifications
:page-permalink: b889e6
:page-order: 50

Administrators send notifications to alert you about Domino events.
For example, you might need to know about planned system maintenance, be reminded to save your work, or receive other targeted messages.

[NOTE]
====
Notifications are automatically refreshed every 30 seconds.
====

== View your notifications

. On the Domino home page, you can see *Notifications* in the bottom left of the page.
. Click *View All* to see messages from the past 30 days.
+
* Use the *Priority list* to filter the notifications.
* Messages are sorted chronologically and prioritized by criticality in the following order:
.. Critical and unread
.. Default and unread
.. Read and expired
* If a message is expired, it is tagged EXPIRED and is disabled.
* If an administrator deletes a message, it is removed from your list.
. Click *Mark all as read* to acknowledge your notifications and lower their priority.
This prioritizes new notifications higher in your list.

----- user_guide/projects/configure-projects/index.txt -----
:page-version: 6.1
:page-title: Configure a Project
:page-permalink: 716281
:page-order: 40

Configuring your Projects involves defining security settings, enabling discovery, and allocating resources. These sections will guide you in configuring your Domino projects: 

link:dba65c[Project settings]::
Settings determine many critical aspects of a Project. Learn about Project settings and how to configure it.

link:f9be5e[Tag a Project]::
Tags assist colleagues and consumers in organizing projects of interest in Domino. Project tags also provide an easy way to add freeform metadata to a project.

link:8590a9[Rename a Project]::
Project names help with the discoverability and organization of work. 

link:d8dde6[Store Project credentials]::
Learn how to safely store Project credentials and how to use environment variables to securely execute a Project.

link:db2820[Set notifications]::
Configure email notifications for long-running Jobs and Workspaces. 
----- user_guide/projects/configure-projects/rename-a-project.txt -----
:page-version: 6.1
:page-title: Rename a Project
:page-permalink: 8590a9
:page-order: 30


Changing the name of a project in Domino is a quick and simple process, 
but it cannot be undone.
There are repercussions involved in changing a project's name.
For example, when a project is renamed, its endpoints are automatically
unpublished, and they will all have new URLs when they are re-published.
Additionally, any local copies will have to be cloned again, since the
project metadata will point at the old project name.

== Change a Project's name

. Go to the project's *Setting* page under the *Access & Sharing* tab and enter a new project name. 
+
image::/images/6.0/rename-project.png[alt="Rename a project"]
+
Domino checks to ensure the new name is not currently in use by
another project.
. When you finish entering the new project name,
Domino will show you a warning.
[[tr1]]
//check name not already in use
[[tr2]]
//warning shows
Read the warning carefully. If any of these events are unacceptable, click *Cancel*.
[[tr3]]
//Cancel rename
[[tr4]]
//complete project rename
+
. If you want to proceed with renaming your project, click *Rename Project* to complete the process. Domino will rename your project, and you can continue working on it immediately.

----- user_guide/projects/configure-projects/set-notifications.txt -----
:page-version: 6.1
:page-permalink: db2820
:page-title: Set notifications for long-running Jobs and Workspaces
:page-sidebar: Set notifications
:page-order: 50

You can set how often you receive emails about long-running jobs and workspaces.
This lets you shut them down when they are no longer needed.

. In the navigation pane, click your username.
. Go to *Notifications* and in the *Jobs & Workspace Sessions*, select the *Notify me about long-running Jobs and Workspace sessions* checkbox.
. Select the time from the list.

When you receive an email, it  will include the number of sessions, links to the sessions, how long they have been running, how long you have before they are automatically shut down (if applicable), and links to shut them down.

----- user_guide/projects/configure-projects/set-project-settings.txt -----
:page-version: 6.1
:page-title: Set Project settings
:page-sidebar: Project settings
:page-permalink: dba65c
:page-order: 10

There are several important settings attached to every project.

== Access Project settings
. In the navigation pane click open a project overview and click *Projects*.
. Click the project for which you want to change the settings.
. In the navigation pane, click *Settings*.

[[hardware_tier]]
== Hardware tier

Use the *Hardware & Environment* tab of the project settings to set a specific hardware tier for your project.
Hardware tiers describe the compute hardware to use for project executors.
Executors can be virtual instances from a cloud services provider or a machine running in your deployment's on-premises data center.
Your administrator configures the hardware tiers available for your deployment.

When you select a hardware tier, it must provide the proper performance for your workflow. However, you must also consider the cost of the hardware in cloud deployments and the impact of your tenancy on local hardware in on-premises deployments.
Domino uses this hardware tier for all runs started in the project.

If the hardware tier is changed, it will be the default for future runs in the project. However, if a run starts that requires different hardware, Domino can override the default.

=== Workspace and Jobs volume size

Volumes represent the storage space dedicated to your Workspace or Job.

The default volume size for all Workspaces and Jobs in Domino is 10GiB, link:71d6ad#workspaces[configurable by an administrator].
When you launch a Workspace, you are able to select a volume size automatically recommended by Domino based on your project size and previous usage, if your administrator has enabled this feature.

NOTE: In a link:910370[Git-based project], the first Workspace you launch always uses the volume size link:e6e601#workspace-volume-size[configured in your project settings].
Subsequent Workspace launches receive volume provisioning recommendations.

You can change the size of the volume if you find that your Workspace (or Job) will require more storage space.

[[compute-environment]]
=== Compute environment

Compute environments are specifications for containers in which projects run.
You can create new environments and access public environments shared in your deployment or organization.
Whenever a new executor is launched or provisioned for use with a project, Domino loads the compute environment specified in the *Hardware & Environment* tab of the project settings:

image::/images/6.0/project-settings.png[alt="Project settings"]

[NOTE]
====
If you are using a custom environment, apply the same customizations after switching to another environment.
====

See how to link:e46d54[Customize the Domino Software Environment].

== Access and sharing

See link:d7731d[Sharing and Collaboration] for details about how to grant access to your projects.


== Mandatory commit messages

Admins and Project owners can enforce mandatory commit messages in Workspaces to ensure internal documentation best practices.


----- user_guide/projects/configure-projects/store-project-credentials.txt -----
:page-version: 6.1
:page-title: Store Project credentials
:page-permalink: d8dde6
:page-order: 40

When you need to connect your code to things like databases or Amazon S3, don't put passwords or secret keys directly in your source code.

Instead, use environment variables to keep sensitive info safe.
This way, you can:

* Share source files without sharing credentials.
* Keep credentials out of version control systems like Git or Domino.
* Let only privileged users (like the project owner) change sensitive variables.

If sensitive values like credentials are stored in your source code, anyone who can change the code can access it.
So, Domino suggests keeping this info separate and injecting it when the code runs with environment variables.

== Store environment variables in a secret store

Project environment variables can be stored in a secure credential store.

You must be the project owner to add, modify, or delete environment variables.

[[tr1]]
// Configure a project level environnment variable

. Go to the **Settings** tab on the project.
. In the **Environment variables** section, add the key/value pairs to inject as environment variables:
+
image::/images/6.0/env-variables.png[alt="Environment variables", width=500]
+
The values are passed verbatim, so escaping is not required.
The value has a 64K length limit.

NOTE: All collaborators in a project can see the environment variable values.

== Next steps

* Learn how to link:6ac5a1[define and inject environment variables] for Projects, Models, and users.

----- user_guide/projects/configure-projects/tag-a-project.txt -----
:page-version: 6.1
:page-title: Tag a Project
:page-permalink: f9be5e
:page-order: 20

Tags help colleagues and consumers organize and find projects that interest them in Domino.
Tags can be used to describe the subject matter explored by a project, the packages and libraries it uses, or the source of the data within.

== Tag a Project

[[tr1]]
//tag added
[[tr2]]
//tag deleted
[[tr3]]
//tag modified
Tags can be added from a project's overview page by clicking the *+* button in the *Tags & Description* section. 

image::/images/6.0/tag-plus.png[alt="Add tags", width=800]

Tags can be removed from a project by clicking the *x* next to the tag that should be removed.

image::/images/6.0/tag-x.png[alt="Remove tags", width=800]

[[tr4]]
While you can create a tag with whatever content you'd like, tags indicated in green have been marked as *approved* by your Domino admin or link:2611b7[librarian] to help reduce duplicate tags.

== Manage Tags

[[tr5]]
//admin or librarian add tag
[[tr6]]
//admin or librarian delete tag
[[tr7]]
//admin or librarian modify existing tag
[[tr8]]
//admin or librarian merge tags
[[tr9]]
//admin or librarian mark tag as approved
Domino admins and link:2611b7[librarians] can manage the tags in a Domino deployment.
From the overview page you can click *Manage Tags* to open the tags interface.

From this page you can add, delete, edit, or merge existing tags.
Admins or librarians can mark a tag as approved which will make it appear green to all users, and signal that its use is encouraged.

image::/images/6.0/approve-tag.png[alt="Approve a tag", width=800]

== Limitations

Project tags are distinct and cannot be used with link:3b6ae5[model registry tags] or link:ae638c[Domino Model Monitoring tags].

----- user_guide/projects/create-project.txt -----
:page-version: 6.1
:page-title: Create Projects
:page-permalink: 299798
:page-order: 30

Domino Projects help you follow best practices in your work. They are flexible and can fit your organization’s workflow. From idea creation to production, Projects support you through the entire process of making different analytical assets.

Domino Projects consist of Git-based Projects and Domino File System (DFS) Projects. Each type has a distinct workflow.

* *Git-based Projects* provide enhanced control over code synchronization, allowing you to use features from external Git providers and offer a more modern development experience.
* *Domino File System (DFS) Projects* organize all your Project's assets as either data or files. In DFS projects, your assets are categorized as data or code.

Domino recommends using Git-based Projects whenever possible. Git-based Projects offer greater control over code synchronization, enable access to the features of external Git providers, and create a more modern development experience.

There are four primary methods for creating a Project in Domino: starting from scratch, using a template, leveraging existing content, or duplicating a current Project.

== Create a Project from scratch

Many users prefer to start a Project from scratch to learn about the interface:

image::/images/6.0/create-new-project.png[alt="Create a new Project", width=600]

. Go to *Develop* > *Projects*  > *Create Project*.
. Fill in the following fields:
.. *Template*: Choose *None (start from scratch)*.
.. *Project Name*: Name your Project.
.. *Billing Tag*: If applicable, choose a *Billing Tag* from the menu.
.. *Project Visibility*: Choose *Private*, *Searchable*, or *Restricted Project*.
. Click *Next*.

Next, you’ll need to choose between a Domino File System (DFS) or a Git-based Project.

=== Choose a host for your Project

Domino recommends choosing a Git Service Provider for better code syncing, access to external Git features, and a modern development experience.

image::/images/6.0/choose-provider.png[alt="Choose a host for your Project", width=600]

. Click *Next* or choose *Code* from the left of the *Create New Project* window.
. Choose link:ca786d[Domino File System] or link:910370[Git Service Provider].
. *Domino File System*: if you choose this, just click *Create*.
. *Git Service Provider*: if you choose this, you’ll need to provide the following:
.. *Git Service Provider*: Select your provider from the dropdown.
.. *Git Credentials*: Provide any required link:d8dde6[credentials] from the dropdown.
.. *Git Repository URL*: Paste the repository URL here.
. Click *Create*.

Your project is created and ready for uploaded files or folders.

== Create a Project from a template

Users can create link:5fed45[Project templates] that can be shared across the organization, which allows you to kickstart your Project from an existing prototype rather than starting from scratch.

. Go to *Develop* > *Projects*  > *Create Project*.
. Fill in the following fields:
.. *Template*: Choose an option from the menu.
.. *Project Name*: Name your Project.
.. *Billing Tag*:  If applicable, choose a *Billing Tag* from the menu.
.. *Project Visibility*: Choose *Private*, *Searchable*, or *Restricted Project*.
. Click *Next*.

Since Projects created with a template automatically route to Git, you'll need to enter some information about your Git Service Provider next.

=== Enter Git credentials

Provide this information for your new Project:

. Click *Next* or choose *Code* from the left of the *Create New Project* window.
. *Git Service Provider*: Select your provider from the dropdown.
. *Git Credentials*: Provide any required link:d8dde6[credentials] from the dropdown.
. *Git Repository URL*: Paste the repository URL here.
. Click *Create*.

Your Project is created and ready for uploaded files or folders.

== Create Project from existing content

Before you start your Project, look for existing content in your organization that you can use to get started. Projects created by others in Domino can help you reach your analytical goals faster.

. link:d9abb7[Search your Domino deployment] to discover Domino Projects that you can base your Project on.
. On your Projects home page, explore the various tabs to see Projects that you are assigned as a collaborator, as well as other Projects recommended by Domino.
. link:123ae8[AI Hub example projects] can be deployed directly to your Domino instance. You can find a summary of these Projects on the link:55497f[Domino Data Lab AI Hub templates] page.

== Next Steps

* link:16685e[Copy a Project] to create a duplicate of an existing Project, which does not include Project settings, run history, or scheduled Jobs.
* link:ef261b[Fork a Project] to create a new Project and include everything from the source Project.
* link:314004[Import Git repositories] into your Project to access the repository and commit changes to repository contents.
* link:e6ed48[Export and import Project content] shows you how to share content among Projects by exporting and importing.

----- user_guide/projects/download-execution-results.txt -----
:page-version: 6.1
:page-title: Download execution results from a Project
:page-sidebar: Download execution results
:page-permalink: 31abd5
:page-order: 130

The results of an execution are the set of files that your code generates or modifies when it runs.
//Where is this? Does this mean the Jobs page and the results tab?
You can view the results of a given execution through the Runs tab of your project.
Or you can view the latest results on the Results tab of your project.

[[tr1]]
// View Results of a Run on the Run tab

[[tr2]]
// View Results of a Run on the Results tab

== Set results branching behavior

[[tr3]]
// Results files geenrated during a run are saved to the project folder

By default, when your code finishes running, Domino saves any new files it produced into your project folder.
Those changes will be downloaded the next time you sync your project.

[[tr4]]
// Save results files of a Run to an isolated results branch

If your code generates large results that you don't have to synchronize each time, you can set your project to save results to isolated results branches.
You can access your results through the application and they are permanently saved, but they won't automatically be accessible to subsequent runs, or automatically downloaded to your computer.

. Go to your project.
. Click *Settings*.
. Click the *Results* tab.
. Save your files generated by your runs (executions) to the main line or to isolated branches.

== Download results from an isolated branch

[[tr5]]
// Download results files from isolated branch via the UI

You can download isolated results from the application or the link:9355a5[CLI].

From the application, select the execution from the Jobs page, and click *Results*.
From there you can download each results file.

[[tr6]]
// Download results files from isolated branch via the CLI

With the CLI, you can use the `domino download-results` command.
For details on usage syntax, run `domino help download-results`.

----- user_guide/projects/index.txt -----
:page-version: 6.1
:page-title: Work with Projects
:page-permalink: a8e081
:page-order: 170
:page-separator: true
:page-section: Develop

Domino organizes work for data science and analytics teams using Projects. Projects allow teams to run experiments and enhance their code. Through Projects, you can manage data, code, artifacts, and user permissions effectively.

Projects encourage best practices in your work. They are flexible and can fit your organization’s unique workflow. From ideation to production, Projects guide you through the end-to-end lifecycle of creating a variety of analytical assets.

The topics in this section will teach you how to get the most out of Projects.

link:5b332c[Overview: Domino Projects]::
Domino offers two types of projects: Git-based projects and Domino File System (DFS) projects. 

link:d2b098[Best Practices for Projects]::
We have compiled a set of best practices to help you successfully create and manage your Domino Projects.

link:299798[Create a Project]::
Learn different methods for creating a Project in Domino.

link:716281[Configure Projects]::
Explore different configuration options for Domino projects, such as security settings, collaborating, and more.

link:ca786d[Manage a Domino File System Project]::
In Domino, the Domino File System (DFS) is the traditional method for storing project assets. 

link:910370[Manage a Git-based Project]::
Git-based projects provide greater control over code synchronization, enabling the use of features from the Git provider and delivering a modern development experience.

link:cc299e[Work with Project assets]::
The Assets Portfolio shows assets in projects that you own or in which you are a collaborator, such as Domino endpoints, Apps, and Launchers.

link:d95a3c[Work with Project files]::
Domino keeps a set of files for each project. When you change a project's files, it creates a new version of that project. When you run an execution from a Domino project, the files for that project are loaded onto the machine that is executing it.

link:56938d[Work with Project artifacts]::
An artifact is a file that is not source code or a data set. Artifacts typically contain the outputs from data analysis jobs, such as plots, charts, serialized models, and more.

link:ec0164[Add existing data to Projects]::
Learn how to locate available data in your deployment and incorporate it into your project.

link:f3cafc[Track Projects]::
You can track the progress of your projects as they move through various phases, including gates, stages, code reviews, validations, audits, project management, and ultimately, production pipelines.

link:d7731d[Collaborate on a Project]::
Domino simplifies collaboration on projects and facilitates the sharing of project outputs.

link:31abd5[Download execution results from a Project]::
The execution results are the files that your code creates or modifies during runtime. You can view these results by accessing the *Runs* tab of your project.


----- user_guide/projects/manage-dfs-projects/archive-projects.txt -----
:page-version: 6.1
:page-title: Archive a Project
:page-permalink: 3c528d
:page-order: 80

To clean up projects, you can archive them.
When you archive a project:

* Domino stops all scheduled runs and runs currently in progress.
* Collaborators cannot use the Domino CLI to upload or download files to or from this project.
* If you published an app to make it globally discoverable, Domino removes it.
//What happens if the app is running at the time?
//* You must link:c2eba7[unregister published models] using this project.
//should it be deactivate and not unregister? If so, how do you deactivate a model?

== Archive a Project
. Go to the project to archive.
. From the navigation bar, click *Settings*.
. Click *Archive Project or Transfer Ownership*.
. Click *Archive This Project*.

To unarchive the project, contact your Domino representative for assistance.

----- user_guide/projects/manage-dfs-projects/copy-projects.txt -----
:page-version: 6.1
:page-permalink: 16685e
:page-title: Copy a Project
:page-order: 40

[[tr1]]

When you copy a project, Domino creates a duplicate project under your account. The new project contains the same files, tasks, and link:0093e8#restricted_projects[restricted project status],
but does not include the following:

* Project settings
* Run history
* Scheduled jobs
* Task Assignee

[[tr2]]
== Copy a Project

. Go to the project and the *Overview* page opens.
. Click the copy icon.
+
image::/images/6.0/copy-project.png[Copy button]

. Type the name for the new project and click *Copy*. The new project's Overview page opens.

[NOTE]
====
To copy Git-based projects, see link:a96d2b[Project Templates].
====

----- user_guide/projects/manage-dfs-projects/domino-file-system.txt -----
:page-version: 6.1
:page-title: Domino File System
:page-permalink: de4abb
:page-order: 10

This topic describes the filesystem structure you will find when you use a Domino File System (DFS) project.

The filesystem root (`/`) contains the following directories.
[[tr1]]

[source,console]
----
/
├── bin
├── boot
├── dev
├── domino  # Contains the datasets directory.
├── etc
├── home
├── lib
├── lib32
├── lib64
├── media
├── mnt     # Default working directory.
├── opt
├── proc
├── root
├── run
├── sbin
├── scripts
├── srv
├── sys
├── tmp
├── usr
└── var
----

== Domino working directory

When you start a Run from a DFS project, Domino loads all the files needed for your project into the Domino working directory.
[[tr2]]
By default, your working directory is `/mnt`.
The folders and files from your project will be in that directory.
[[tr3]]
If your project is set up to link:37158c[import another project], your working directory is `/mnt/<project-owner-username>/<project-name>`.

[[tr4]]
NOTE: Domino sets the `DOMINO_WORKING_DIR` environment variable for all Runs.
This variable contains the path to your working directory.

[[tr5]]
In addition to your project files, the following files and folders are stored in the working directory:

[source,console]
----
DOMINO_WORKING_DIR/
├── ipynb_checkpoints   # Contains the auto-saved Jupyter states.
├── results             # Contains your generated results.
    └── stdout.txt      # Tail of the console output from your Run.
├── requirements.txt    # Optional: Specifies Python package dependencies.
├── .dominoresults      # Determines which files are rendered as results.
├── .dominoignore       # Contains file patterns for Domino to ignore.
├── .dominokeep         # Prevents directory removal of empty directory.
├── dominostats.json    # Values shown in the Jobs dashboard.
├── email.html          # Controls the formatting of notification emails.
├── .noLock             # Solves "too many open files" messages.
├── app.sh              # Contains app-launching code for Domino Apps.
├── domino.log          # Contains CLI logs for local CLI projects only.
└── .domino.vmoptions   # Contains proxy settings for local CLI projects only.
----

Learn more about:

* link:31abd5[.dominoresults]
* link:9c4f82[requirements.txt]
* link:bfeeae[.dominoignore]
* link:f7292e[.noLock]
* link:5b84c5[email.html]
* link:5b84c5[dominostats.json]
* link:0da1e9[.domino.vmoptions]

----- user_guide/projects/manage-dfs-projects/fork-projects.txt -----
:page-version: 6.1
:page-title: Fork Projects
:page-permalink: ef261b
:page-order: 50

Forking a project copies all of its files into a new, separate project,
allowing for independent development and experimentation. Changes in the
forked project can later be reviewed and merged back into the original
project.


== Fork a project
[[tr1]]
. Open the project overview and click *Fork*.
. Enter a name for the fork when prompted. You must be the project owner, or have
access to the project as a contributor or results consumer to
fork it. See link:d82ba3[Share and Collaborate on Projects] to learn more about project roles and access control.

image::/images/6.0/fork-button.png[alt="The fork button"]

[[tr2]]
The following are copied to the newly forked project:

- All files
- Revision history of all files
- Environment variables
- Launchers
- link:400957[Restricted project] status

[[tr3]]
These things are *not* copied to the newly forked project:

- Run history
- Project settings, including collaborators and compute environment
- Discussion history

[[tr4]]
Projects that have been forked, or were created by forking another
project, will link to related forks on the project overview page.

image::/images/6.0/fork-details.png[alt="Fork details"]


----- user_guide/projects/manage-dfs-projects/import-git-repositories.txt -----
:page-version: 6.1
:page-title: Import Git repositories
:page-permalink: 314004
:page-order: 10

Domino supports importing Git repositories to projects.
When you add a repository to a project, it is available to Runs that you start in the project.
This means you can access the contents of the repository, just as you would your Domino files.

This topic explains how you can import a Git repository to a project, access the repository from within a Workspace, and commit changes back to the repository.

Domino supports connecting to Git servers through HTTPS and SSH for both public and private repositories.


[NOTE]
====
If you want to use Git with the Git service provider of your choice, Domino recommends that you use link:910370[Git-based Projects].
====


[[step-1-create-credentials]]
== Step 1: Create credentials

If you are adding a private repository, want to write commits to remote, or are using SSH, you must add Git credentials to your Domino account.
Domino uses these credentials to authenticate with the service hosting your repository when you start a
link:942549[Run].

Domino stores the following types of credentials:

* Personal Access Tokens
* SSH private keys
* Username and password

[[tr1]]
=== Option 1: SSH key creation

To connect with SSH, you must have a private SSH key that corresponds to a public key that you've added to your Git service.
See the https://help.github.com/articles/connecting-to-github-with-ssh/[GitHub documentation^] for instructions about how to create and add keys.

=== Option 2: Personal Access Token creation

You must have a Personal Access Token to access a private repository through HTTPS.
You must have a Personal Access Token if the URI you want to use to interact with a repository is formatted as:

[source, shell]
----
https://<domain>/<user>/<repository>.git
----

Personal Access Tokens are supported by the following Git services.

[[tr2]]
* https://help.github.com/articles/creating-a-personal-access-token-for-the-command-line/[GitHub
Personal Access Tokens]:
The minimum scopes to grant for full functionalities are `repo` and `read:user`

[[tr3]]
* https://docs.gitlab.com/ee/user/profile/personal_access_tokens.html[GitLab Personal Access Tokens^]:
The minimum scopes to grant for full functionalities are `read_api' and `write_repo`.
If you are creating Git-based projects and plan to create a repository from Domino, you must grant `api` access to the Personal Access Token.

[[tr4]]
To connect to Bitbucket repositories through HTTPS from Domino, you must add a
https://confluence.atlassian.com/bitbucket/app-passwords-828781300.html[Bitbucket App Password^] credential to your Domino account.

If your GitHub organization requires SSO then you must authorize the PAT or SSH key to access private repositories through Domino.

See the GitHub documentation for instructions about https://help.github.com/articles/authorizing-an-ssh-key-for-use-with-a-saml-single-sign-on-organization/[authorizing keys for SSO on Github^].

[[tr5]]
=== Option 3: Username and password

For other Git service providers, including Amazon AWS CodeCommit, Azure DevOps, Domino supports authentication with App username and passwords.

[[step-2-add-credentials]]
== Step 2: Add your credentials to Domino

=== Option 1: SSH private key

You must have a SSH Private Key to access a repository through SSH.
You must have a SSH private key if the URI you want to use to interact with a repository is formatted as:

`<user>@<domain>:<username>/<repository>.git`

SSH access is supported by the following Git services.
[[tr6]]
* https://help.github.com/articles/connecting-to-github-with-ssh/[GitHub SSH Access^]
[[tr7]]
* https://docs.gitlab.com/ee/user/ssh.html[GitLab SSH Access^]
[[tr8]]
* https://confluence.atlassian.com/bitbucket/set-up-an-ssh-key-728138079.html[Bitbucket SSH Access^]

After setting up SSH access with your Git service, you will have a *public key* that you provided to the Git service, and a *private key*.
Use these steps to add the private key to Domino:

. Copy your SSH key.
+
From a terminal, run `pbcopy < ~/.ssh/id_rsa` to copy the key to your clipboard, if you set up your key with the standard name and in the standard location.
+
If you copy the key manually, remember to include the `-----BEGIN OPENSSH PRIVATE KEY-----` and `-----END OPENSSH PRIVATE KEY-----` header and footer.

. In the top navigation pane, click *Account*, then *Account Settings*.

. Go to *Git Credentials*, then click *Add Credentials*.
. In *Domain*, enter the exact domain of the service hosting your repository, such as `github.com` , `bitbucket.com`, or `your-internal-gitlab-url.com`.
. Specify a nickname and choose the applicable Git Service Provider (e.g. GitHub, GitLab, Bitbucket, etc.).
. For *Access Type*, click *Private SSH Key*.
. Paste your private key.
This is the contents of the private key file that matches the public key you provided to your Git service.
. If you set up your SSH keys to require a passphrase when used, enter it in the *Passphrase* field, then click *Add Credentials*.
Your credential is listed in *Git Credentials*.


[[tr9]]
=== Option 2: Personal Access Token

After generating a Personal Access Token in your Git service, use these steps to add it to Domino:

. In Domino, click your username, then click *Account Settings*.
. Go to *Git Credentials*, then click *Add a New Credential*.
. In the *Domain* field, enter the exact domain of the service hosting your repository, such as `github.com`, `bitbucket.com`, or `your-internal-gitlab-url.com`.
. For *Authentication Credential Type*, click *Personal Access Token*.
. Enter your Personal Access Token, then click *Add Credentials*.
Your credential is listed in Git Credentials.


[[tr10]]
[[add-a-repo]]
== Step 3: Add a repository to a Project

. Open the project to which you want to add a repository, then click *Code*.
. Click to open the *Git Repositories* tab, then click *Add a New Repository*.
+
. Enter a directory name and the HTTPS or SSH URI of the repository you want to add.
The directory name will be the directory in `/repos` that this repository clones into.
It defaults to the name of the repository.
. Select the branch of the repository that you want Domino to check out when it clones this repository into a run or workspace.
If you leave this setting at *Use default branch*, Domino will check out the branch specified as default by your Git service, typically `master`.
You can also specify a different branch name, tag name, commit ID, or supply a custom Git ref.
+
. Click *Add Repository*.


== Work with a Git repository in Domino

[[tr11]]
When you start a run or workspace in a project, any repositories added to the project are cloned into `/repos` and will have the branch or commit you specified checked out.

Remember that your link:de4abb[Domino working directory] is in `/mnt`, which is a sibling of `/repos`.
Both directories are in the filesystem root (`/`).
Scripts you have added as Domino files can interact with the contents of these repositories by specifying an absolute path to `/repo-name>/<file>`.

=== Commit back to Git repositories

[[tr12]]
When you start a Workspace session in a project that has added Git repositories, you will see those repositories listed in the *Session Overview* under *Git repos*.
If you make changes to the contents of those repositories while running the workspace, those changes will be itemized file-by-file under each repository.

[[tr13]]
If you want to commit those changes back to the repository, select the check box next to the repository name and then click *Full Sync*.


[[tr14]]
Enter a commit message.
This commit message is attached to commits to the selected Git repositories, and to a new revision of the Domino project if there are changes to Domino files.
Git commits are pushed to the default branch you specified when adding the repository.

[[tr15]]
If you attempt to stop your Workspace while there are uncommitted changes to your Git repositories, you are prompted to commit those changes.
This works the same as the *Session Overview* page.
Select the check box next to the repositories you want to commit to, enter a commit message, and click *Stop and Commit*.


[[tr16]]
If you try to commit when there are conflicts between your local changes and the state of the default branch in remote, Domino creates a new branch from its local state.
Domino will then push that new branch to remote.

[[tr17]]
After this happens, you must resolve those conflicts outside of Domino, or use the command line in your Workspace session to resolve them.
The next time you launch a Workspace session, Domino will check out the default branch from remote, not the new branch it pushed.

=== Git interaction from the Workspace command line

Both Jupyter and RStudio workspaces have command line tools.
You can use these to interact with your repositories with conventional Git commands.
Go to `/repos` in your command line to find your project's repositories.
See the https://git-scm.com/docs/gittutorial[official Git documentation^] to learn more about using Git on the command line.

[[tr18]]
To open the RStudio command line, click *Tools > Shell…*.

[[tr19]]
To open the Jupyter command line, from the *Files* tab click *New > Terminal*.

== Track changes to repositories made in Domino Runs

When viewing the Details tab of a Domino Run, at the bottom you will find a Repositories panel.
You can expand this panel to see details of how the repository changed during the Run.
Domino records the checked out commit at the start of the Run and the end of the Run.

== Troubleshooting

*Run Error*:

[source,text]
----
Errors occurred while processing dependencies. Contact support@dominodatalab.com:
Credentials are required for your repository: project-name (ssh://git@github.com/your-org/projectname.git)
----

*Solution*:

Your Git Credential added to Domino might have the incorrect Domain.
Double-check the domain field in your Git credential to ensure it matches your exact Git repository URL, like:

* `github.com`
* `bitbucket.com`
* `your-internal-gitlab-url.com`

*Run Error*:

[source,text]
----
Errors occurred while processing dependencies. Contact support@dominodatalab.com:
Authentication is required for your repository:
The repository provided requires credentials but none were found.
Add SSH or PAT authentication to your Domino account.
----

*Solution*:

There are a couple steps to check when encountering this error.
First, ensure your private SSH key or PAT has been added to the Git Credentials section of your Domino Account Settings page.
Second, if your organization's Git repository requires SSO access, you may need to authorize the key you have added.
See https://help.github.com/articles/authorizing-an-ssh-key-for-use-with-a-saml-single-sign-on-organization/[Authorizing an SSH key for use with SAML single sign-on^] for more details.

*Run Error*:

[source,text]
----
Errors occurred while processing dependencies. Contact support@dominodatalab.com:
remote: Invalid username or password.
fatal: Authentication failed for 'https://github.com/<your account>/<your repo>/'
----

*Solution*:

If your organization's Git repository requires SSO access, you may need to authorize the key you have added.
See https://help.github.com/articles/authorizing-an-ssh-key-for-use-with-a-saml-single-sign-on-organization/[Authorizing an SSH key for use with SAML single sign-on^] for more details.

----- user_guide/projects/manage-dfs-projects/index.txt -----
:page-version: 6.1
:page-title: Manage a Domino File System Project
:page-sidebar: Manage a DFS Project
:page-permalink: ca786d
:page-order: 50

In Domino, the Domino File System (DFS) is the traditional way to store project assets.
Domino File System (DFS) projects organize all your project's assets as either data or files.

link:de4abb[Domino File System]::
The filesystem structure for a Domino File System (DFS) project.

link:314004[Import Git Repositories]::
You can import a Git repository to a project.

link:f86aa5[Work from a Commit ID in Git]::
Import a specific commit from a repository to your project.

link:a834bb[Organize Domino File System Project Assets]::
Project assets are organized as data or files.

link:16685e[Copy Projects]::
Create a duplicate project under your account.

link:ef261b[Fork Projects]::
Make changes without affecting the original project.

link:4a6d0f[Merge Projects]::
Combine your changes into the original project.

link:afea00[Manage Project Files]::
link:7a0fee[Upload] files, link:c5fe4e[export] files as a package, link:bfeeae[exclude] files from syncing, or link:dae129[compare] file revisions.

link:3c528d[Archive a Project]::
Clean up the projects that you no longer need.

link:b8a061[Revert Projects and Files]::
Revert projects or individual files to an earlier version. This lets you maintain later versions while working with earlier projects and files.

----- user_guide/projects/manage-dfs-projects/manage-project-files/compare-file-revisions.txt -----
:page-version: 6.1
:page-title: Compare file revisions
:page-permalink: dae129
:page-order: 20

Domino can create a rich comparison of the differences between revisions of a file in your Domino project.

. Go to your project.
. In the navigation pane, click
*Code*.
. Click the file to open it.
. Click *Compare Revisions*.
+
[[tr2]]
Domino opens the file comparison tool.
+
image::/images/6.0/compare-revisions.png[alt="Compare versions"]
+
* The *Base* menu sets the starting version of the file for your comparison.
* From *Target*, select a version of the file to compare to the base version.

[[tr3]]
When you are viewing the
*Code*
page for your project, you can select between all revisions for the project.
When you are viewing an individual file, the *Revisions* menu is limited to only those revisions where a change was made to that file.

----- user_guide/projects/manage-dfs-projects/manage-project-files/exclude-project-files-from-sync.txt -----
:page-version: 6.1
:page-title: Exclude Project files from sync
:page-sidebar: Exclude Project files
:page-permalink: bfeeae
:page-order: 30

You can ignore files in your project which excludes them from the link:9355a5#to-synchronize-the-files-on-your-computer-with-the-server[domino sync] operation.

This includes if you sync from your local machine (with the link:9355a5[CLI] or link:ad1224[R package], or sync at the end of a run or during a workspace session.

[[tr1]]
//ignore individual file
[[tr2]]
//ignore folder
[[tr3]]
//ignore multiple files using * wildcard
== Ignore a file

The link:de4abb#_domino_working_directory[.dominoignore] is automatically created at the root of every new project.

A `.git/` directory is always ignored by the sync operation, even if not listed in `.dominoignore`.

NOTE: You can use the `*` symbol as a wildcard to match files of a similar pattern.
All paths must be relative to the project root.

. Enter the name of the file to be ignored in `.dominoignore`.
. To ignore a folder and its contents, add the folder's name to `.dominoignore`.


== Use isolated results branches

[[tr4]]
//use isolated branch for results
You can configure your project to save results to an isolated branch.
You can access these results from the application as well as CLI, but they will not be downloaded by `domino sync`, nor will they be used as input to future runs unless you explicitly specify this.
See link:31abd5[Download Execution Results] for more information.

== Use Project importing

[[tr5]]
//put code in one project and import to another
Sometimes, neither `.dominoignore` nor results branches are sufficient approaches.
For example, imagine you are developing a recurring data cleaning task that produces large files used by other projects.
You want to sync these results back to the main project at the end of the run, but don't want to sync them to your local machine during development.

To do this, put your code in one project and link:37158c[import it] into a different project in which the large-output runs will occur.
This way you can sync with the code project without downloading the results stored in the data output project.
You can even configure the code project to be link:c5fe4e[imported as a package], and it will be automatically installed and accessible at runtime.

----- user_guide/projects/manage-dfs-projects/manage-project-files/export-files-as-packages.txt -----
:page-version: 6.1
:page-title: Export files as Python or R package
:page-sidebar: Export files as packages
:page-permalink: c5fe4e
:page-order: 40

If you organize the files in a project as an installable package, then
you can choose to export it as such.
When another project imports this project, Domino will automatically install the package at runtime, making it available to your code.
[[tr1]]
//export as R package
[[tr2]]
//export as Python package

. Configure your project to link:17d088[export files].
. From *Code Package*, select the language.
+
image::/images/6.0/export-files-packages.png[alt="Export file packages"]

The following describes the language-specific pattern required for any package.

[#R]
--

See the https://cran.r-project.org/doc/manuals/r-release/R-exts.html[official manual^] for an in-depth guide to writing R extensions.

In summary, each R package requires:

* A directory called `R/` with code files.
* A directory called `man/` with documentation files.
* A file named `DESCRIPTION`, with each line following the pattern `link:key>: <value[]`. The required keys include:
+
  ** Package
  ** Version (for example, `0.1`)
  ** Title
  ** Description
  ** Author
  ** Maintainer (a name followed by an email address in angle brackets, for example, `Sample Maintainer <maintainer@example.com>`)
  ** License

* A file named `NAMESPACE` that describes the namespace of the package.
If you don't know what to put here, `exportPattern( "." )` works in many cases.
--

[#Python]
--

See https://packaging.python.org/tutorials/packaging-projects/#generating-distribution-archives[Packaging Python Projects^] for more details.

In summary, each Python package requires:

* A `setup.py` file.
This must contain a `setup()` function (imported from `setuptools`), with arguments as described in https://packaging.python.org/tutorials/packaging-projects/#creating-setup-py[Packaging Python Projects^].
* A folder containing your Python modules and packages.
Usually this is given the same name as the overall package.
* Domino recommends that you also include a `README` file.
--

----- user_guide/projects/manage-dfs-projects/manage-project-files/index.txt -----
:page-version: 6.1
:page-title: Manage Project files
:page-permalink: afea00
:page-order: 70

link:7a0fee[Upload files to Domino]::
If you must upload files larger than 550MB, you have to use alternate methods because of limitations to the application.

link:dae129[Compare file revisions]::
You can compare files to see the changes that you made.

link:bfeeae[Exclude Project files from sync]::
You can ignore files in your project so they do not synchronize to the server.

link:c5fe4e[Export files as Python or R package]::
You can export the files in your project as a package that will automatically install at runtime when it is imported into another project.

----- user_guide/projects/manage-dfs-projects/manage-project-files/upload-files-to-domino.txt -----
:page-version: 6.1
:page-title: Upload files to Domino
:page-permalink: 7a0fee
:page-order: 10

Domino limits the size of a file that can be uploaded through the Web application to 550MB.
You can drag and drop files that are less than this limit into Domino.

NOTE: If you are using an on-premises or VPC deployment of Domino, your administrator can configure this size limit so your limit might be different.


The following are the ways to upload files larger than 550MB:
[[tr1]]
//use cli

== Use the CLI

If your files are on your computer, use the command line interface (CLI) client.
You can quickly sync projects on your local machine with projects on Domino.

link:e21e55[Install the Domino CLI client] if you haven't done do already.

.. After the CLI client is installed and you have authenticated (per the installation instructions), go to the folder on your local machine that contains the files to upload to your Domino project.
.. Type the following to identify which Domino project to associate with this folder.
+
----
domino restore
----
It downloads project files that you have in Domino for that project.
This lets you easily sync files to and from the project.
.. Type the following to sync the files in your local directory to the project in Domino, thereby uploading any files over 550MB.
+
----
domino sync
----

== Use `wget` or a Workspace session in Domino

[[tr2]]
//use wget
[[tr3]]
//use shell script
If your file is on the web or accessible through a URL, Domino can download the file to your project.
To do this, use a shell script or a workspace session.

Create and save a new `.sh` file in your Domino code tab and run it in Domino.
See the following example.

image::/images/6.0/get-filesh.png[alt="Import downloaded files through wget", width=1000]

This imports the downloaded files through `wget`.
The
*Code*
tab lists the downloaded files.
Do this once only to bring the files into your project.

[NOTE]
====
If you are working with large data, for example, 10s of GB, you might see longer *Preparing* times.
During the *Preparing* stage of the run, your files are copied to the executor.

Larger amounts of data take longer.
For large data projects, Domino natively integrates with Hadoop, Spark and other big data platforms.
Contact support@dominodatalab.com for more information.
====

----- user_guide/projects/manage-dfs-projects/merge-projects.txt -----
:page-version: 6.1
:page-title: Merge Projects
:page-permalink: 4a6d0f
:page-order: 60


[[tr5]]
After you've made some changes to the new fork, you can initiate a merge
by clicking *Request Review* on the project overview page. You must be the
project owner, or have access to the project as a contributor in order
to request a merge review.

image::/images/6.0/merge-fork.png[alt="Merge a fork"]

[[tr6]]
You will be prompted to submit a review request, in which you can review
the changes and describe their effects with a message. Once submitted,
contributors to the main-line project are notified. The merge will occur
when a contributor accepts the review, and a new revision of the
main-line project will be written with the forked changes merged in.

[[tr7]]
To view a history of Review Requests, including the status of current
requests, click *Reviews* from the navigation pane. 

----- user_guide/projects/manage-dfs-projects/organize-project-assets.txt -----
:page-version: 6.1
:page-title: Organize Domino File System Project assets
:page-sidebar: Organize Project assets
:page-permalink: a834bb
:page-order: 30

In Domino File System (DFS) projects, your project's assets are organized as data or
code.

== Data
This section organizes and lists all data sources used in your project, including Domino datasets, external data volumes, and dataset scratch spaces.
For more information about how to use data with your project, see link:0a8d11[Domino Datasets].

image::/images/6.0/data-org.png[alt="Organize data"]

== Code
This section lists your local files, files imported from other projects, or the files in your Git repositories.

image::/images/6.0/code-org.png[alt="Organize code"]

----- user_guide/projects/manage-dfs-projects/revert-projects-and-files.txt -----
:page-version: 6.1
:page-title: Revert Projects and files
:page-permalink: b8a061
:page-order: 90

In Domino you can revert projects or individual files back to an earlier version.
This encourages experimentation and new problem-solving methodologies by removing the overhead that you encounter when recovering from failed experiments.

Domino supports the following actions:

* Revert a Project
* Revert a file

== Revert Projects

In Domino you can revert individual projects to earlier versions and still preserve changes made to other projects.

[[tr1]]
=== Revert to an earlier version of a Project

. Open your project in Domino.

. Go to the project whose files you want to revert.

. In the navigation bar, click
*Code*.
The
*Commit*
menu lists all the versions of this project.
The current version is the first item in the list.
+
image::/images/5.6/commits.png[alt="Commits"]

. Click an older version to view that project version.
+
[[tr2]]
NOTE: You can still access the version from which you reverted the project.
+
[[tr3]]
. Click *Revert Project* to return your project to an earlier version.
+
This creates a new version at the top of the version history.

NOTE: The version you reverted from is still accessible.

// image::/images/4.x/screen_shot_2018-10-26_at_12.55.27_PM.png[alt="Old version"]


== Revert files

In Domino, you can revert individual files to earlier versions.
This is useful when you want to revert to an earlier version of a file but preserve changes made in other files in the project.

[[tr4]]
=== Revert to an earlier version of a file

. Open the project that contains the file in Domino.

. From the navigation bar, click
*Code*.

. Click the file you want to revert.
. From the *Commit* menu, click to see all versions of the file.
The current version is the first item in the list.

. Click an older version to view that file version.

. Click *Revert File* to revert your file to the earlier version.
This creates a new commit in your project.
This new file is at the top of the version history.
+
image::/images/6.0/revert-file.png[alt="Revert the file"]
+
NOTE: The version you reverted from is still accessible.
[[tr5]]



----- user_guide/projects/manage-dfs-projects/work-from-a-commit-id.txt -----
:page-version: 6.1
:page-title: Work from a commit ID in Git
:page-permalink: f86aa5
:page-order: 20

When you link:314004[add an external Git repository to your
project], you can specify which state of the repository you want checked out by default in your runs and workspaces.
You can specify a branch, tag, commit ID, or custom ref.

[[tr1]]
Commit IDs are unique SHA-1 hashes that are created whenever a new https://git-scm.com/docs/git-commit[commit^] is recorded.
If you specify a commit ID when you add a repository, Domino will always pull the state of the repository specified by that commit in a https://git-scm.com/docs/git-commit[detached HEAD^] state.
This represents a fixed point in the repository's history, and will not evolve over time like a branch.

[[tr2]]
Domino cannot automatically push to repositories it has pulled in this way.
If you want to push changes to such a repository, you can use the workspace command line to manually commit and push to a new branch.
See link:314004[Import Git Repositories] to learn more about how to interact with Git in workspaces.

== Example

In this example, a repository called `domino-manual` was added to a project with a specified commit ID.
When the workspace is started, it is pulled to `/repos/domino-manual` with the target commit checked out in a detached HEAD state.
You can run `git status` to verify this in your workspace command line.

image::/images/4.x/detached-head-state.png[alt="Detached head state", width=1000]

Suppose during the course of your workspace session you make a change to the repository.
It will remain in the detached HEAD state, but Git will continue to track changes.

image::/images/4.x/detached-head-state-tracking.png[alt="Detached head state tracking", width=1000]

You can add and commit those changes as normal.

image::/images/4.x/detached-head-state-commit.png[alt="Detached head state commit", width=1000]

However, if you try to push from the detached HEAD state, you will encounter a fatal error.
Git must have a branch to push to.

image::/images/4.x/detached-head-state-push-error.png[alt="Detached head state push error", width=1000]

The solution is to create a local branch from your detached HEAD, check it out, and push to remote with `git push -u origin`.

image::/images/4.x/detached-head-new-branch.png[alt="Detached head state new branch", width=1000]

----- user_guide/projects/manage-git-projects/copy-project.txt -----
:page-version: 6.1
:page-permalink: a96d2b
:page-title: Copy a Git-based project
:page-order: 40

// Original content: https://docs.google.com/document/d/1OB3cB43mI9lfmOqxQdhCkCMsbTwymk7ZCwxeDnkQvC8/edit#

You can bootstrap your teams' Git-based projects by copying a Git-based project "template."
By sharing and copying Git-based projects, you can achieve better outcomes and more reliable results through these benefits:

* Increased consistency between projects
* Faster project setup
* Better adherence to best practices
* Stronger collaboration

In your Git-based project, you can organize your repository and your project to follow best practices for your field. Every team member can start on a new project by making a copy of that project.

== Which project artifacts are copied?

When you copy a Git-based project, certain configurations and materials are copied to the new Project while others are not:

[cols="1a,2a,2a",options="header"]
|====
|   |Copied |Not copied

|*Project overview*
|Tags, description, tasks, and README
|Entity links to tasks (Workspaces, Jobs, Apps, Domino endpoints, files)

|*Project materials*
|Files, artifacts, code, datasets and imported Git repository, imported projects
|External data sources, audit data, run history, discussion history

|*Project settings*
|Environment variables, hardware tier, environment, results, exports, integrations, and visibility (public, searchable, private)
, Project task stages
|Access and sharing

|*Other*
|
|Published applications, Domino endpoints, and Launchers

|====

NOTE: Datasets can only be copied to a new project through the Domino API with the parameter `copyDatasets:true`.  See the code examples below.

== Deep copy vs. reference copy

To support different needs, Git-based projects support two modes of copy: reference copy and deep copy.

Deep copy::
This creates an independent copy of the original repository.
Use this when building a brand new project from an existing project.
You can create a deep copy using the Domino UI or the API.

Reference copy::
All the changes in one project are reflected in all the referenced projects.
Use this when multiple parallel projects rely on the same main Git repository.
For example, this is a common scenario for statistical analysis in the pharmaceutical field.
You can only create a reference copy using the Domino API.

== Create a deep copy

Using the Domino UI or the API, you can create a deep copy of any Git-based project.
This creates a copy of the original repository that you can modify independently.

=== Deep copy using the Domino UI

. Navigate to the Git-based project you want to copy, then click the *Copy* button.
+
image::/images/6.0/copy-git-project.png[alt="Copy the Git project"]

. Enter the name for your new project, then click *Next*.
. Enter your Git credentials, then click *Copy*.

=== Deep copy using the Domino API

[source,python]
----
import requests

project_id = 'asdf1234'
code_repo_credential_id = 'asdf123'
domino_project_name = 'my_new_project'
shared_repo_url = 'https://github.com/user/repo.git'
url = f'https://host.com/api/projects/v1/projects/{project_id}/copy-project'
body = {
	'name': domino_project_name,
    'visibility': 'private',
	'copyDatasets': true,
	'gitCodeRepoSpec': {
		'credentialId': code_repo_credential_id,
        'deepCopy': {
            'visibility': 'private',
            'newRepoName': 'new_git_repo',
            'newRepoOwnerName': 'git_user'
		}
	}
}

headers = {
    'X-Domino-Api-Key': '<YOUR_API_KEY>',
    'Content-Type': 'application/json'
}

requests.post(url, data=json.dumps(body), headers=headers)
----

== Create a reference copy

When you create a reference copy, all the changes in one project are reflected in all the referenced projects.
Use this when multiple parallel projects rely on the same main Git repository.

The Python example below illustrates how you can automatically create a reference copy of a project using the Domino API.


[source,python]
----
import requests

project_id = 'asdf1234'
code_repo_credential_id = 'asdf123'
domino_project_name = 'my_new_project'
shared_repo_url = 'https://github.com/user/repo.git'
url = f'https://host.com/api/projects/v1/projects/{project_id}/copy-project'
body = {
	'name': domino_project_name,
    'visibility': 'private',
	'copyDatasets': true,
	'gitCodeRepoSpec': {
		'credentialId': code_repo_credential_id,
		'referenceCopy': {
			'mainRepoUrl': shared_repo_url
		}
	}
}

headers = {
    'X-Domino-Api-Key': '<YOUR_API_KEY>',
    'Content-Type': 'application/json'
}

requests.post(url, data=json.dumps(body), headers=headers)
----

== Troubleshooting: Dataset copy error

// https://docs.google.com/document/d/18CqUA51X_3ZQOYpnuUhHCdxywQqcTDinmocW4t3w2HE/edit#

----
java.lang.RuntimeException: One or more of the datasets in this project are already being copied elsewhere.
Please wait for them to copy and then try again.
----

When you copy a dataset from one project to another, it is locked in a “copying” state until the copying operation finishes.
To maintain the integrity of the dataset, it cannot be copied anywhere else while in this state.

If you receive the error above, your new project was not created.
Wait for the datasets to finish copying into the previous project and then try again with the same API call.
The error can occur if the dataset is already being copied into a different project, or if you try to copy the same project several times in a short interval with `copyDatasets=true`.

== Limitations & requirements

* Only GitHub, GitHub Enterprise, GitLab, and GitLab Enterprise are supported providers. Other providers such as Bitbucket are not supported.
* SSH credentials are not supported. You must use a Personal Access Token with repository read and write permissions enabled.
* In order to copy a Git-based project, the GitHub repository you want to copy must be a https://docs.github.com/en/repositories/creating-and-managing-repositories/creating-a-template-repository[template repository^]:
+
image::/images/5.6/github-template-repository-selection.png[alt="GitHub template repository selection"]
+
Collaborators on the parent project are not copied to the new dataset or project.

----- user_guide/projects/manage-git-projects/create-a-git-based-project.txt -----
:page-version: 6.1
:page-title: Create a Git-based Project
:page-sidebar: Create a Project
:page-permalink: eaab17
:page-order: 30



. In the navigation pane, click *Develop > Projects* and then click *New Project*.
. In the Create New Project window, enter a name for your project.
. Set your project's *Visibility*.
. Click *Next*.
. Under *Hosted By*, click *Git Service Provider*.
. Select the *Git Service Provider* currently hosting the repository you want to import.
This is the target repository.
. Select the *Git Credentials* authorized to access the target repository.
. Click to *Choose a repository* or *Input Git URL*.
If you are using PAT credentials with Github or GitLab, you can select *Create new repository*.
. Click *Create*
+
[IMPORTANT]
====
If the repository you use to store your code contains files that exceed 2 GB, Domino creates your Git-based project.
If it fails at workspace setup, consider using a Domino File System project.

Use the following tool to check the total size of a Git repository and the size of individual files in the repository: https://github.com/github/git-sizer[git sizer^].
====
[[tr3]]
+
During the project creation process, you can create a new repository for Github and GitLab.

----- user_guide/projects/manage-git-projects/directory-structure.txt -----
:page-version: 6.1
:page-title: Git-based Project directory structure
:page-sidebar: Project directory structure
:page-permalink: ccaee6
:page-order: 10

[[tr7]]

Git-based projects use the following directory structure.
The default working directory for your code is `/mnt/code`.

[IMPORTANT]
====
* If you have mounted External Data Volumes to your Git-based project, the directory structure lists them.
* If you've imported additional Git repositories into your Git-based project, the directory structure shows the `imported` directory.

* Git does not support committing empty directories.
If there are no files in a directory, the directory is removed from your repository.
====

[source,console]
----
/mnt
│
├── /code   # Git repository and default working directory.
│
├── /data
│   │   # Project Datasets
│   ├── /{dataset-name}   # Latest version of dataset.
│
│    # Project Artifacts
├── /artifacts
│
│    # External mounted volumes
├── /{external-volume-name}
│
└── /imported
    │   # Imported Git Repos
    ├── /code
    │   └── /{imported-repo-name}
    │
    ├── /data
    │   │   # Mounted Shared Datasets
    │   └── /{shared-dataset-name} # Contains contents of latest snapshot unless otherwise specified by yaml.
    │
    │    # Imported Project Artifacts
    └── /artifacts
        └── /{imported-project-name}
----

----- user_guide/projects/manage-git-projects/index.txt -----
:page-version: 6.1
:page-title: Manage a Git-based Project
:page-permalink: 910370
:page-order: 60

NOTE: Domino does not support Git Large File Storage (LFS).

Domino supports Git-based projects.
Git-based projects:

* Ensure https://www.atlassian.com/git/tutorials/comparing-workflows/feature-branch-workflow[common Git workflows^] are available natively in your workspace.
* Provide an easy way to engage in version-controlled, code-based collaboration with fellow project team members within Domino.
* link:5d0521[Organize your projects' assets] as either code, data, or artifacts to support common data science workflows.

Consider Git-based projects if you:

* Have a beginner's proficiency with Git.
* Have experience with hosted version control systems like GitHub and  Bitbucket.
* Collaborate with several data scientists on projects.

To learn more about common workflows using Git and branching, see
https://www.atlassian.com/git/tutorials/comparing-workflows/feature-branch-workflow[Git Feature Branch Workflow^].

If you are not familiar with Git or hosted version control systems, or
you want to reproduce data science research, then consider a
link:a8e081[Domino File System (DFS)] project.

[[create-a-gbp]]
[[tr1]]

[NOTE]
====
Before you use a private Git repository to store your code, you must link:314004#step-1-create-credentials[add the corresponding Git credentials] to your Domino account settings.
After you add the credentials you can create a Git-based project in Domino.

If you use a public Git repository to store your code, you do not have to add your Git credentials.
====

[[tr16]]
WARNING: If you run a job in a Git-based project, Domino only synchronizes and saves artifacts to the Domino File System (DFS).
In Git-based projects, you must manually sync code or push it to the Git repository.
This is intentional and supports the https://docs.github.com/en/actions/using-workflows/storing-workflow-data-as-artifacts[code, data, and artifacts workflow^].
To learn more, see running link:942549[Work with Jobs].

== Next steps

* link:ccaee6[Git-based Project directory structure.]
* link:5d0521[Organize Git-based Project assets.]
* link:eaab17[Create a Git-based Project.]
* link:a96d2b[Copy a Git-based project.]
* link:63ac71[Work with Git in your Project.]

----- user_guide/projects/manage-git-projects/organize-project-assets.txt -----
:page-version: 6.1
:page-title: Organize Git-based Project assets
:page-sidebar: Organize Project assets
:page-permalink: 5d0521
:page-order: 20

[[tr2]]

[[code-data-artifacts]]

In Git-based projects, your project's assets are organized as code, data, or artifacts.

== Access Project assets

. Go to the project.
. In the navigation pane, click *Code*, *Data*, or *Artifacts*.

== Code
This section organizes and lists all the Git-based repositories that store your project code and imported repositories.
See link:314004[Import Git Repositories] for more information.
Use common Git workflows to access files in these repositories.

When you interact with code in a Domino workspace, you can use common Git workflows, like committing, pushing, and pulling.
See link:0d2247[Use Git in Your Workspace] for more information.

The default working directory for your code is `/mnt/code`.

//Tara: Can we move this information elsewhere? Seems out of place here.

[[tr4]]
You can select any branch and the last 10 commits. You can browse the folders of linked Git repositories natively from the Code section on the Project page.

[NOTE]
====
Depending on the size of the repository, it can take up to 30 seconds to access the file structure on initial load.
====


[[tr5]]

== Data
This section organizes and lists all data sources used in your project, including Domino datasets, external data volumes, and dataset scratch spaces.
For more information about how to use data with your project, see
link:0a8d11[Domino Datasets].

[[tr6]]
== Artifacts
This section contains the results from your research and analysis like plots, charts, and serialized models.
You can also import artifacts from other projects.

----- user_guide/projects/manage-git-projects/work-with-git.txt -----
:page-permalink: 63ac71
:page-version: 6.1
:page-title: Work with Git in your Project
:page-order: 50

[[change-branches]]
== Change branches in the Workspace

You can change branches in your workspace by selecting a branch from the menu.

The menu lists a maximum of ten local branches followed by remote branches, in alphabetical order.
If your repositories have more than ten branches, you can search for additional branches.

TIP: Branches deleted in GitHub continue to appear in Domino's menu until you sync link:262fef[sync] or link:#pull-changes[pull] your latest changes.

[[pull-changes]]
== Pull changes

When you pull the latest changes from remote to your workspace, Domino fetches the latest content from the remote branch (git fetch), then applies your changes on top of the updated branch (git rebase).
When files conflict, you can resolve conflicts manually or use remote changes. *Use Remote Changes* discards changes in your workspace and overwrites files in your workspace with remote changes.

== Resolve merge conflicts

[[tr9]]
Merge conflicts occur when changes are made to the same line of a file, or when one person edits a file and another person deletes the same file.
They can also happen when you push multiple commits.
You can resolve merge conflicts in your workspace through a guided application for both the main code repository and any additional imported repositories.
If you were making multiple commits, the latest commit might overwrite changes that you made during conflict resolution.

If you are working in a Git-based project and sync some files, you might run into a merge conflict.
When this occurs, the *File Conflicts* window opens.

[[tr12]]
Select *Resolve Manually* to resolve conflicts by the filename.

For each file in conflict, you can *Mark as resolved*, *Use my changes* or *Use origin repo changes*:

* *Mark as resolved* means that you've edited the files to resolve conflict markers.
The latest change of the file is listed under *Uncommitted changes* and will be pushed to remote when you click *Continue sync*.
* *Use my changes* overwrites remote files with changes in your workspace.
The latest change of the file is listed under *Uncommitted changes* and will be pushed to remote when you click *Continue sync*.
* *Use origin repo changes* discards changes in your workspace and overwrites the file with remote changes.
The file are not listed under *Uncommitted changes* because there is no change to commit.
However, you must click *Continue sync* to complete the conflict resolution.

[[save-artifacts]]
== Save artifacts to the Domino File System

[[tr14]]
IMPORTANT: All files in *Artifacts* are saved exclusively to the Domino File System (DFS).
If you do not want to save a particular asset to the Domino File System, Domino recommends that you do not save it as an artifact.
See link:262fef[Sync Changes].

Artifacts are results from your research, like plots, charts, serialized models, and more.
In Domino, you can save these results in the *Artifacts* section of your project.

// tag::steps[]
=== Save artifacts and push changes

. In the navigation pane of your workspace, click *File Changes*.
. Under *Artifacts*, expand *File Changes*.
. Enter a commit message.
. Click *Sync to Domino*.
Domino saves your artifacts to the Domino File System (DFS).
// end::steps[]

== Pull artifacts from the Domino File System

//tr14
You can pull *Artifacts* from the Domino File System (DFS).

[[tr15]]

. From the navigation pane, click *File Changes*.
. Under *Artifacts*, click *Pull*.
Domino pulls the latest changes into your workspace.
----- user_guide/projects/projects-overview.txt -----
:page-version: 6.1
:page-permalink: 5b332c
:page-title: Projects overview
:page-order: 10

Domino uses Projects to organize work for data science and analytics teams. Projects help teams run experiments and improve code. Using Projects, you can manage data, code, artifacts, and user permissions.

Projects also help you follow best practices. They are flexible and can adapt to how your organization works. Projects support you through developing different analytical assets, from brainstorming ideas to launching them.

== Storing assets in Domino Projects

How you organize, store, and access files in Domino depends on your chosen project type. Domino has two types of Projects: link:910370[Git-based Projects] and link:ca786d[Domino File System (DFS) Projects]. Deciding on a Project-type will impact how you operate in Domino. We recommend using Git-based projects.

However, your organization may have policies that require you to use DFS or specific Git providers. Ephemeral and persistent disks on your platform's containers can fill up. High disk usage or continuous high usage may affect service performance.

=== Git-based Projects

Git-based Projects let you choose a Git provider and a repository. Domino helps you sync your work, giving you control over saving versions. These projects use an external Git provider to store your files, offering better control and access to the provider's features for a modern development experience.

=== Domino File System (DFS) Projects

Understanding how Domino manages files in a DFS repository is essential. These projects use the Domino File System (DFS) to store files. The workflow is similar to that of Git-based projects, but DFS does not have some features offered by external Git services.

The DFS automatically manages versioning with a Git server operated by Domino in the background. Domino also automatically generates specific files. link:d95a3c[Project Files] and link:56938d[Work with Project artifacts] have more information about these files and their management.

== Code, data, and artifacts workflow in Projects

Combining code, data, and artifacts into one workflow is crucial for improving efficiency and driving innovation. All Domino Projects organize the related code, data, and artifacts needed to accomplish a Project’s goals. Projects can achieve better results and success by creating strong connections between these elements.

* *Code*: Domino Projects have a Code repository that you can access from the left panel by clicking *Code*. DFS projects store artifacts directly in the Code repository.
* *Data*: link:fbb41f[Data sources], link:0a8d11[Domino Datasets], and link:f12554[external data volumes] can be accessed in the *Data* section of Projects. These tools are the preferred solutions for managing all your data needs. Each data offering operates independently from a Project to enhance collaboration and security.
* *Artifacts*: Git-based Projects have an additional Artifacts repository accessible from the side panel. This is a DFS repository designed for storing artifacts that need to be kept outside the Git-based project. 

Data files, like mapping dictionaries, are sometimes included in code or artifact projects. This works well if the files are small (less than 1 GB each) or if there are fewer than 50,000 files in total.

You can store data files in a repository. However, it’s best to limit the number of large files you store there. Repositories have storage limits, and exceeding these can slow down your future work. For details on storage limits and performance, talk to your Domino or external Git administrator.

== Next Steps

* link:d2b098[Best Practices for Projects] is a series of recommendations for effective data management in Projects.
* link:299798[Create Projects] walks you through different options for creating Domino Projects.
* link:716281[Configure a Project] contains information on the different configuration options for Projects.
* link:ec0164[Add Data to a Project] explains how to discover available data in your deployment and add it to your project.
* link:d7731d[Collaborate on a Project] teaches you how to collaborate on Domino Projects and share project outputs.




----- user_guide/projects/track-projects/add-project-goals.txt -----
:page-version: 6.1
:page-title: Add Project tasks
:page-permalink: 4e60ef
:page-order: 30

Project tasks offer an easy way to define milestones and track progress in your Domino Projects. Project tasks are flexible and can be used by Project owners to define outcomes and track subtasks. They are an important feature of project management in Domino.

Project tasks also link:#link-work[organize your work by linking artifacts to a specific task]. Linking artifacts like Workspace sessions, Jobs, and Apps together makes it easier to navigate Projects.

[[tr1]]

IMPORTANT: Before you add Project tasks, make sure you're satisfied with the link:#task-stages[Project task stages], you cannot create or edit task stages after adding tasks.

Adding tasks to your project is simple:

. In your Project, from the left panel, go to *Govern* > *Tasks*.
. Use the `@` in the *Description* to notify collaborators that you have set a task.
. Optionally, assign an *Owner* or leave a *Comment*.

After you create your tasks, edit them as your Project evolves and update the status to show progress:

[[tr2]]

. In your Project, from the left panel, go to *Govern* > *Tasks*.
. Locate the task that you want to update.
. Click the three vertical dots and select *Edit*.
. As you make progress towards a task, update the link:#task-stages[task stage] to communicate status.
. From within the *List Mode* view, you can make edits to multiple tasks at the same time.
. When you're done with the task, click *Mark task as Complete*. Marking the task as complete updates the *Tasks* column in the link:13a1a4[Projects Portfolio].

image::/images/6.0/edit-multiple-tasks.png[alt="Edit multiple tasks", width=1000, role=noshadow]

[[task-stages]]
== Task stages

Use task stages to indicate a task's current progress. Task stages are defined at the Project level, and can only be modified by *Admins* and *Project owners*.

To define task stages for the project:

. In the Project, click *Settings* > *Tasks*.
. Add new tasks and edit or delete existing tasks.


[[link-work]]
== Link work to tasks

As your Project achieves milestones, link the artifacts you create to tasks. Linking artifacts to tasks helps your stakeholders find, organize, validate, and recreate work.

[[tr3]]

=== Link a Workspace or Job
Select the check box for the *Workspace* or *Job* that you want to link and click *Link Tasks*.

=== Link Apps
[[tr4]]
Go to the *App* and click *Link to Task*.

=== Link Domino endpoints
[[tr5]]
Go to *Endpoints*. Click the three vertical dots in the *Actions* column and select *Link to Task*.

=== Link files
[[tr6]]
In the file view, click *Link to Task* in the file view.


== Use Project templates to reuse project tasks and stages

You can use link:a96d2b[Project templates] to implement project management standards in Domino.

Create a Project template with the Project task stages that meet your requirements, and use it to create new Projects. This is a good way to ensure that Projects follow the same structure and process for repeatable business deliveries.

== Next steps

link:a7420a[View Project activity]
link:11693a[Promote Projects to production]
link:40f92c[Integrate Jira]
----- user_guide/projects/track-projects/index.txt -----
:page-version: 6.1
:page-permalink: f3cafc
:page-title: Track Projects
:page-order: 110

The Project lifecycle in Domino is defined by your organization. Domino provides monitoring systems to give you visibility over your Projects.

== Define the Project lifecycle

The flow of work through gates, stages, code reviews, validations, audits, project management, and eventually production pipelines is different for each organization.

* link:1f1064[Define custom stages] and move your Project between Stages and view the current Stage.
* link:129510[Set Project status].
* link:4e60ef[Create Project tasks] for Project milestones or validation gates and insert Domino links to evidence of task completion with reproducible assets.
* link:11693a[Promote Projects to production].

== Track and monitor the Project lifecycle

* link:a7420a[View Project activity] across users.
* link:129510[View Project status].
* link:40f92c[Integrate with Jira] to create a bi-directional link between Domino tasks and Jira Tickets.
* link:e34be1[View Project audit logs] for troubleshooting, security, and regulatory purposes.
* link:cc299e[View Project assets] to track the inputs/outputs of your Projects.

----- user_guide/projects/track-projects/integrate-jira.txt -----
:page-version: 6.1
:page-title: Integrate Jira
:page-permalink: 40f92c
:page-order: 60

With Domino's Jira integration, you can perform common Jira actions from your Project.

[IMPORTANT]
====
To use this feature:
[[tr1]]
* Your Domino deployment must be configured to use Jira.
* A Jira administrator must configure this integration within Jira.
* Your Jira account must have the privileges to modify tickets in Jira.

To learn how to configure your Domino deployment to use Jira, see link:0792cf[Integrate With Atlassian Jira].
====

Images, Markdown, and any other Jira-compliant formatting are rendered as plain-text in Domino.

== Connect Domino and Jira

[[tr2]]
You must connect your Domino and Jira accounts, and then authorize your Domino account to take action in Jira on your behalf.

. In your Project, from the left panel, go to *Govern* > *Tasks*.
. Click *Add Jira Credentials*.
. Enter your Jira credentials and authorize read and write access for Domino.
+
The connection between your Domino and Jira accounts will persist until you clear your Domino-related browser cookies.
+
image::/images/6.0/jira.png[alt="Jira credentials", width=900, role=noshadow]

== Link a Jira ticket to your Project

[[tr3]]
After you've connected your Domino and Jira accounts, you can link a Jira ticket to your Project.
Only a single Jira ticket can be linked to a Project at any given time.

To link a Jira ticket to the Project, search by its Jira key or title text, browse among issues assigned to you, or browse issues in a given Jira project.

Linking a Jira ticket to your Project automatically pulls the ticket's metadata (such as description and comments) into Domino.
Changes made to a Jira ticket from Domino are reflected in Jira.

If you change a Jira ticket from within Jira _after_ it has been linked to a Project, you must click *Sync* in the *Manage* tab in Domino to synchronize the changes.

image::/images/6.0/link-jira-ticket.png[alt="Link a Jira ticket", width=500, role=noshadow]

== Jira subtasks or child issues become Domino tasks

[[tr4]]
A Project can be linked to a single Jira ticket.
If that ticket has child issues or Subtasks associated with it, they automatically become Domino tasks when linked in your Project.
The Domino-Jira linkage is bidirectional.
You can add, edit, or delete tasks from Domino and those changes are reflected in Jira (and vice versa).

[[tr5]]
If your Jira workflow commonly uses Epics with child issues like Tasks, Stories, and Bugs, Domino recommends that you link the Epic issue to your Project.
The child issues within the linked Epic will automatically become tasks in the Project.

[[tr6]]
If your Jira workflow does _not_ typically use Epics, or if a single Epic usually maps to multiple data science projects, then Domino recommends that you link a Jira ticket of type Task or Story to your Domino Project.
If the Task contains Subtasks then the Subtasks automatically become tasks in the Domino Project.

The following operations on a Domino task are reflected in the corresponding Jira ticket, and vice versa:
[[tr7]]
* Create a new task
[[tr8]]
* Edit the task title or description
[[tr9]]
* Delete an existing task
[[tr10]]
* Change status
[[tr12]]
* Assign an owner
+
IMPORTANT: You cannot use link:4e60ef#task-stages[Domino task stages] with Jira integration. Jira statuses replace the functionality of task stages.

To learn more about tasks in Domino, see link:4e60ef[Link tasks to work].

== Unlink a Jira ticket from your Domino Project

[[tr11]]
. In your Project, from the left panel, go to *Govern* > *Tasks*.
. Click *...* and click *Unlink Ticket* to unlink the Jira ticket.


After a Jira ticket is unlinked from your Domino Project, you must discard any associated Jira tasks in your Project.

[IMPORTANT]
====
Discard any tasks that were created in your Project as a result of _unintentionally_ linking the wrong Jira ticket to your Domino Project.
If you do not discard these tasks, they will be kept and added to the set of tasks of a subsequent Jira ticket.
====

== Next steps

link:d7731d[Collaborate on a Project]

----- user_guide/projects/track-projects/promote-projects-to-production.txt -----
:page-version: 6.1
:page-title: Promote Projects to production
:page-permalink: 11693a
:page-order: 50

When a project is ready to be promoted to production, you might want to continue experimenting with the model even after the project is deemed production-grade.
When that happens, you must make sure that others with access to the project do not make changes to the production version without careful consideration.

[[tr1]]
// Assign a project to an existing organization

[[tr2]]
// Change a project's assigned organization 

[[tr3]]
// Create an Organization

== Create a production-only organization

To do this, use the link:526a62[organizations] feature.
If an organization does not exist, create an organization that contains production-ready projects and projects in production only.
Transfer ownership of your production-grade project to this organization before you begin experimenting.

NOTE: All Domino users with access to this organization must understand its purpose.

To experiment on this project, link:ef261b[fork it into a new
project] and conduct your experiments there.

When you are ready to merge your changes back into the original, production-grade project, click *Request Review* to submit a merge request to the project owner.

NOTE: See link:ef261b[forking and merging] before using this feature.

Some Domino customers do this for models that are not in production, as a way of submitting their work to their managers.

== Other ways to promote to production

While changing organizational ownership of a project is usually the simplest way to promote a project, Domino offers other methods.

=== Change results behavior

[[tr4]]
// Save results to isolated branches

You can save runs to isolated branches rather than the main branch.
This  ensures that the results of each run is independent and not synchronized.
One user's run has no impact on the next user's run.

To change results behavior:

. In the navigation pane, click *Settings*.
. In Project Settings, click the *Results* tab.
. Under *Results behavior*, select *To isolated branches*.

=== Export the project

[[tr5]]
// Utilize Import/Export between projects

You can use *Imports/Exports* to export files from your production-designated project into a separate working project.
The code in the original project becomes read-only, while code execution occurs in the working project.

==== Promote a project to production
. In the project settings pane, click *Exports*.
. Under *Exports*, select *Files*.
This ensures that any project that imports this one uses the current set of files for your production-designated project.
. Create and name a new project, and then import the files from your original, production-designated project.
This new project is your working project.
+
[[tr6]]
// Tag a run as "release"
+
. Assign the "release" tag to runs that you want to make available to users in the working project.
Now, they can select the correct version of the production-ready project.

=== Project tagging

[[tr7]]
// Tag a project

You can use Domino's Project Tagging to designate a project as being production-ready.
This option is only available for on-premises or private deployments.

==== Tag a project as production-ready
. In the navigation pane, click *Overview*.
. In *Select Tags*, begin typing the tag (`prod` or `promote`, for example).
+
If the tag has already been used, Domino prompts you with an autofill option.
Otherwise, you can add a new tag.
+
After the project is tagged, make sure all users with access to the project understand the tag's meaning.

=== Promote-to-production workflow

You can use Domino's advanced routing option to test with one version of a model and manage production traffic on another version.

//This could be reusable content when we start doing this
See link:8c79ad#Route-your-domino-endpoint[Route your Domino endpoint] for more information.

== Next steps

link:40f92c[Integrate Jira]
link:d7731d[Collaborate on a Project]
----- user_guide/projects/track-projects/set-project-stages.txt -----
:page-version: 6.1
:page-title: Set Project stages
:page-permalink: 1f1064
:page-order: 10


You can label Projects with stages to track their progress through a data science life cycle.
If your Domino administrator has configured Project stages, you will see the current stage of your project in brackets under the project name in the project menu.

Tracking your Project through the stages used by your team helps your colleagues understand what kind of work is happening in the project and how they can contribute.

If you are an link:d7731d[owner or contributor] on the project, you can click the project name to open the menu and change the project stage
If you change the stage of a project, this event is listed on the Project’s Activity page.

== Set the current stage

. The current stage of the project is visible below the project name. 
. Click the text box with the current stage to view and select other project stages.
+
image::/images/6.0/project-stage.png[alt="Set project stage", width=600]

== Next steps

* link:129510[Set Project status]
* link:4e60ef[Add Project tasks]
* link:a7420a[View Project activity]
* link:11693a[Promote Projects to production]
* link:40f92c[Integrate Jira]
----- user_guide/projects/track-projects/set-project-status.txt -----
:page-version: 6.1
:page-title: Set Project status
:page-permalink: 129510
:page-order: 20

Projects have a status indicated by the colored dot next to the project name on the *Projects* page.
A project’s status can be:

* *Green* indicates an active and progressing project.
By default, new projects are set to green.
You can link:1b6083[unblock] a project or link:ecca88#reopen[reopen] a completed project to make it active again.

* *Red* indicates an active project that is link:cfbb90[blocked].

* *Grey* indicates a link:ecca88[completed project].

== View Project status

Use the Projects Portfolio to track the status of projects to which you have access, including:

* Projects you own.
* Projects you have been added to as a collaborator.
* All projects across Domino (if you are a system administrator).
[[tr1]]

=== Open the Projects Portfolio

. In the top navigation pane, click *Govern > Projects Portfolio*
+
image::/images/6.0/projects-portfolio.png[alt="Projects portfolio", width=500]
+
The buttons can be used to filter projects by stage or blocked status. 
The tabs filter by active or complete status. 

Use this page to quickly understand the state of work in your projects.
To maximize the usefulness of this tool, understand how administrators can link:6cde3a[configure project stages] for their teams, and link:1f1064[how to set project stages], link:cfbb90[raise] and link:1b6083[resolve] blockers, and link:129510[change project status].


== Set a Project as blocked or unblocked

If you are a project link:d7731d[owner or contributor], you can mark a project as blocked when you need assistance from colleagues or administrators to make progress.
Domino administrators and project collaborators receive an email notification when you mark a project as blocked.

Set a project as blocked when:

* You need assistance setting up additional tools in a link:f51038[Domino environment].

* You need access to new link:16d9c1[data sources].

* You need hardware capabilities not covered by your current link:908bd9[hardware tier].

The same menu used to mark a project as blocked can be used to unblock the project, which returns it to a green, active status.

. Click the current stage box below the project name at the top of the project menu.
+
image::/images/6.0/blocked-switcher.png[alt="Blocked switcher"]
. Click *I'm Blocked* or *Unblocked* and enter a message to describe the blocker.
. Click *Raise a Blocker*.
+
When you change the status of a project, this event is listed on the project’s link:a7420a[Activity] page with the comment so project collaborators can discuss blockers.
A blocked Project will show a red circle.


== Set a Project as complete

If you are a project link:d7731d[owner or contributor], you can mark a project as complete.

. Click the current stage box below the project name at the top of the project menu.
+
image::/images/6.0/blocked-switcher.png[alt="Blocked switcher"]
. Click *Complete* and enter a message to describe the conclusion.

Domino filters out complete projects from the active project views. However, a completed project is still a functioning project. You can modify its files and start runs, but before doing so you might want to reopen the project to indicate that work is continuing.

If you want to see your completed projects, on the Projects page, select the *Show completed projects* check box.
The indicator adjacent to the project is gray.

[[reopen]]

== Reopen a Project

If you want to reopen the project, click the project name and click *Reopen Project*.
The project returns to a green, active status.

== Next steps

link:4e60ef[Add Project tasks]
link:a7420a[View Project activity]
link:11693a[Promote Projects to production]
link:40f92c[Integrate Jira]
----- user_guide/projects/track-projects/view-project-activity.txt -----
:page-version: 6.1
:page-title: View Project activity
:page-permalink: a7420a
:page-order: 40


The Activity page shows the history of activity in the project, including:

* link:942549[Jobs] started.
* link:867b72[Workspaces] started.
* Comments left on Jobs or Workspaces.
* Comments left on files.
* Project stage changes.
* link:4e60ef[Project task] changes.
* Blockers raised or resolved.
* Files created, edited, or deleted in Domino.;
* Files modified in Workspace sessions.
* Models or Apps published.
* Scheduled Jobs published or edited.


== View Project activity
. Click
*Overview* > *Activity*
in the project menu.
+
NOTE: You can also download the activity feed as a PDF.
+
. Use the menu to filter the activities such as *Comments*, *Jobs*, and *Workspaces*.
. If you select two successfully completed Runs in the feed, you can use the comparison icon next to the filter menu to link:08118a[compare jobs].

----- user_guide/projects/work-with-project-artifacts.txt -----
:page-version: 6.1
:page-permalink: 56938d
:page-title: Work with Project artifacts
:page-order: 90

An artifact is a file whose purpose is not source code or a data set.
Artifacts usually contain the output from your data analysis jobs, such as plots, charts, serialized models, and so on.
Your workspaces, jobs, and other executables can access data in your project artifacts just as they would any other source of data.

Keep in mind that accessing project artifacts in your executions can impact their performance.
This is because many events can link:373d37#_how_data_is_stored_in_project_files[trigger a project file sync], and running executions must wait for the sync to complete before they can access the data again.

* In a DFS-based (Domino File System) project, artifacts are stored alongside the rest of your project files.
You can save them just as you would any other files in your project.
* In a Git-based project, artifacts are stored in a special *Artifacts* folder in the DFS.
See the instructions below for saving artifacts in a Git-based project.

== Save outputs

Use Project Artifacts and TrainingSets to store the output of your data analysis jobs or training runs.

link:56938d[Project artifacts]::

An artifact is a file whose purpose is not source code or a data set.
Artifacts usually contain the output from your data analysis jobs, such as plots, charts, serialized models, and so on.
Your workspaces, jobs, and other executables can access data in your project artifacts just as they would any other source of data.

link:440de9[TrainingSets]::

A TrainingSet is a versioned set of data, column information, and other metadata. Use TrainingSets to persist dataframes for model training and other analysis. You can store and load multiple versions of a given dataframe from a training set so you can connect a model to the specific version of a dataframe that was used to train it.

== Save artifacts and push changes

. In the navigation pane of your workspace, click *File Changes*.
. Under *Artifacts*, expand *File Changes*.
. Enter a commit message.
. Click *Sync to Domino*.
Domino saves your artifacts to the Domino File System (DFS).

If you are storing small data files as project artifacts, you can access them from your code just like any other data source.
This topic explains how to get the path to a project artifact file so you can refer to it in your code.

Keep in mind that accessing project artifacts in your executions can impact their performance.
This is because many events can link:373d37#_how_data_is_stored_in_project_files[trigger a project file sync], and running executions must wait for the sync to complete before they can access the data again.
Datasets are recommended for better efficiency and scalability; see link:ba5bad[Work with datasets in projects].

TIP: If you are working with a large number of project artifacts, consider using the link:30b067[Domino CLI] to manage them.

== Get the path to a Project artifact

* Your *Code* files are accessible at the path `/mnt/code`.
+
For example, if you have a file under *Code* in your project at `reference-project-ner/dataset/ner_dataset.csv`, you can refer to it in your code as `/mnt/code/reference-project-ner/dataset/ner_dataset.csv`.

* Your *Artifacts* are accessible at the path `/mnt/artifacts`.
+
For example, if you have a file under *Artifacts* in your project called `job_output.json`, you can refer to it in your code as `/mnt/artifacts/job_output.json`.

TIP: Replace `/mnt` with the `$DOMINO_WORKING_DIR` environment variable to make your code more portable.
This is especially useful when projects are link:e6ed48[exported and imported].

[[store-more-files-bigger-files-and-access-them-faster]]
== When to use Datasets instead of Project files

When you start a run or launch a workspace, Domino copies your project files to a Domino execution.
When working with large volumes of data, this presents the following potential issues:

* By default, you can store 10,000 files in a Domino project and you might exceed the limit.
* By default, you can only transfer individual files that are 8 GB to and from your Domino project files, and you might exceed the limit.
* The time required to transfer data to and from the Domino executions is proportional to the size of the data.
It can take a long time if the size of the data is large, which can lead to long startup and shutdown times for workspaces, jobs, apps, and launchers.

You can solve these problems with Domino link:ba5bad[datasets] because:

* Domino datasets do not have a limit on the number of files that can be stored.
* Domino datasets do not have a limit on the size of any individual file.
* Domino datasets are directly attached to executions as networked file systems, so you do not have to transfer their contents when executions start or complete.


== Limitations in an on-demand compute cluster
// https://university.dominodatalab.com/domino-201-advanced-features-v54/1508812

Project files are not accessible when you are using an link:8b4418[on-demand compute cluster].
To work around this limitation, you can copy the files you need into the project's default dataset, then see the instructions for accessing datasets in your cluster:

* link:a3b42e#_use_domino_datasets[On-demand Spark clusters]
* link:a5638c#_use_domino_datasets[On-demand Ray clusters]
* link:0919a6#_using_domino_datasets[On-demand Dask clusters]
* link:71871d#_domino_datasets[On-demand OpenMPI clusters]

== Next steps

* Learn about link:df7044[project file security and sharing].
* Learn about link:d95a3c#_special_files[special files in projects].
* Learn about link:5d0521[code, data, and artifacts in Git-based projects].
* Learn about link:a834bb[data and code in DFS-based projects].

----- user_guide/projects/work-with-project-assets.txt -----
:page-version: 6.1
:page-title: Work with Project assets
:page-permalink: cc299e
:page-order: 70

[[tr2]]
// System Admin views Assets Portfolio

[[tr3]]
// Collaborator views Assets Portfolio

[[tr4]]
// Project Manager views Assets Portfolio

The Assets Portfolio shows assets in projects that you own or in which you are a link:d7731d[collaborator].
These assets can be Domino endpoints, Apps, Launchers, or Scheduled Jobs.
Domino link:2611b7[System Administrators] see assets in all projects across the Domino instance, and Domino link:2611b7[Project Managers] see assets in all projects owned by users in their link:526a62[organizations].

== See the assets for your Project

[[tr1]]
// User views Assets Portfolio

. In the top navigation pane, click *Govern > Assets Portfolio*.
. When first opened, the Assets Portfolio shows Domino endpoints by default. Click an asset type to view.
+
image::/images/6.0/assets-portfolio.png[alt="Assets portfolio", width=1000, role=noshadow]
+
For each asset type, the table shows metrics and information specific to that asset.
+
. You can select which columns to view with the column-picker.
+
image::/images/6.0/column-picker.png[alt="Assets column picker", width=800, role=noshadow]

== Next steps

* Learn about link:df7044[project file security and sharing].
* Learn about link:d95a3c#_special_files[special files in projects].
* Learn about link:5d0521[code, data, and artifacts in Git-based projects].
* Learn about link:a834bb[data and code in DFS-based projects].

----- user_guide/projects/work-with-project-files.txt -----
:page-version: 6.1
:page-title: Work with Project files
:page-permalink: d95a3c
:page-order: 80


Domino manages a collection of files for every project.
To add files to a project:

* Upload them from the Domino web application.
* Upload them from the link:9355a5[Domino Command Line Interface].
* Upload them through the Domino API.
* Create and edit them in the Domino web application.
* Execute code in a workspace or job to generate them.

When you modify a project's files, it creates a new revision of the project.
When you start an execution from a Domino project, the files in that project are loaded onto the machine hosting the execution.
This machine is known as the executor.

The project files are mounted in the `/mnt` directory, which is in the file system root of the executor.
Domino tracks changes to this directory.
When a Run completes, Domino records changes to `/mnt` as a new revision of the project.
See link:de4abb[Domino File System] to learn how Domino loads files into runs and how runs change files.

You can also add external Git repositories to projects.
This makes the contents of those repositories available in runs and workspaces in the `/repos` directory of the executor.
See link:314004[Import Git Repositories].

== Sharing Project files

Project artifacts are the files in your project, which can include relatively small data files. The visibility and accessibility of project artifacts is controlled by the project owner. As a project owner, you can configure the project’s visibility and add collaborators with specific permissions.

Project visibility determines whether other Domino users can see your project and its files. Although the most permissive visibility setting (Public) lets everyone see and import your project artifacts, it does not allow everyone to modify them.

You can change a project’s visibility at Settings > Access & Sharing > Visibility in your project. See Set project visibility for details.

Project collaborators have specific permissions depending on the project roles you assign them. Some roles provide read-only access to the project artifacts, while others provide read/write access. Project roles are independent of the user’s role in Domino.

See Collaborator permissions and Invite collaborators for complete details.

The most secure configuration for a project is to set the project visibility to Private and add collaborators with the Results Consumer role. This configuration gives read-only access to specific users only; the project is effectively invisible to all other users. Only the project owner has full access to such a project. This is an appropriate configuration for projects that contain the most sensitive data and results.

By contrast, the most permissive configuration is to set the project visibility to Public and add the whole organization as a collaborator with the Project Owner role. This configuration gives full access to all users in the organization. For example, a sample project intended to be used as a sandbox might be configured this way.

== Special files

There are several special files reserved by Domino.
These files control the revisioning behavior, results display, and run comparison features of a project.

=== README files

All projects are initialized with a `README` file, which is available in the *Overview* section of your project. `README` files provide an overview of a project.
The contents of the `README` file in your project are written in https://daringfireball.net/projects/markdown/syntax[Markdown^], a lightweight and easy-to-read/write markup language.

[TIP]
====
If you plan to reference other project files from the `README` file, prepend `raw/latest` to the beginning of a file's relative path.

For example, if you have a file stored in a folder and the file's path is `images/overview.jpg`, you must reference this file in the `README` file as  `raw/latest/images/overview.jpg`.
====

=== .dominoignore

By default, all projects include a `.dominoignore` file in the project root folder.
This file functions like a https://git-scm.com/docs/gitignore[.gitignore file^], and can be used to exclude certain file patterns from being written to future revisions of the project files.
Domino ignores files that match the specified patterns when a new revision is created.
This includes revisions created by syncing from your local machine using the link:9355a5[CLI], as well as new revisions created by a run or workspace session.

To ignore a file pattern, add it to `.dominoignore`.
Patterns can be filenames, folder names, or UNIX shell regular expressions.
Adding a folder will ignore it along with all of its contents.
The `*` symbol in UNIX shell regular expressions is a wildcard and will match.
All paths must be relative to the project root.
Review the contents of the default `.dominoignore` in one of your projects to see commented examples of excluded patterns.

NOTE: A `.git/` directory is always ignored by Domino sync operations, even if that pattern is not listed in `.dominoignore`.

[[dominoresults]]
=== .dominoresults

Projects include a special file named `.dominoresults`.
This file controls which files the link:31abd5[results dashboard] lists for this project's runs.
It is constructed similar to `.dominoignore`, but lists file patterns to _include_ instead of *exclude*.
If no patterns are listed in this file, all files changed by a run will be included in the results dashboard.
If any patterns are listed in this file, only files which match those patterns will be included in the results dashboard for this project's runs.

For example, a `.dominoresults` file that contains the following lines will only list the following specified files in the results dashboard.

[source,shell]
----
histogram.pdf
output.txt
----

A `.dominoresults` file that contains the following lines lists all PDF files in the project, and any PNG files that are in the `results/` folder.

[source,shell]
----
*.pdf
results/*.png
----

=== dominostats.json

Domino's link:08118a[compare jobs] feature checks for a file named  `dominostats.json` to compare key measurables from individual runs.
This file is automatically deleted at the beginning of a run, and will only exist in the project revision produced by a run if a new version is written during execution.
See link:5b84c5[View Execution Performance] to learn more about this feature.

----- user_guide/reference/cli/cli-reference.txt -----
:page-version: 6.1
:page-title: Domino CLI reference
:page-permalink: 9355a5
:page-order: 20

== Install and get started with the CLI

link:e21e55[How to install and get started with the Domino CLI]

== Run your code

`run [--direct][--wait][--no-sync][--tier][--local] <file> [args...]`

[[tr1]]
"run" will copy all files in your project folder to the cloud, where it will execute the specified file with any arguments you have specified, unless you have specified the `--local` flag.

[[tr2]]
Use `domino sync` before `domino run <script>` if receiving an error like “blob file not found”.

[[tr3]]
The optional `--wait` flag runs your script synchronously, that is, the command does not return until your job finishes.

[[tr4]]
The optional `--direct` flag lets you run a shell command directly on the Domino machines.

[[tr5]]
The optional `--no-sync` flag runs the latest version of code on the server, without uploading any local changes on your computer.

[[tr6]]
The optional `--tier` flag lets you select a specific hardware tier to execute the specified file on.

[[tr7]]
The optional `--local` flag lets you run your script on your local machine.

[[tr8]]
The optional `--title` flag lets you supply a title for the run.

Local runs are useful if your desired compute environment is not immediately available, but you would like to record your results in Domino.
When a local run is invoked, Domino commits a snapshot of project files to the server and then runs the given command (for example, `python main.py`) on the local machine.
When the command has finished running, Domino detects results that have been produced and commits those back to the server, tracking them as “results” of the run as though the command ran on the server.

Viewing the “Results” for Run #16 will bring up a page showing the output of the `main.py` where colleagues can view and discuss the output.


.Examples
[source,shell]
----
domino run calculateStats.R
domino run runModel.m model1 .05
domino run --wait runModel.m model1 .05
domino run --direct "pip freeze | grep pandas"
domino run --tier "Large" calculateStats.R
domino run --local "python main.py"
----


== Projects

=== Create a new Project

`create [projectName]`

`create` will create a new Project by making a new folder on your computer and telling the Domino server about your new Project.
Optionally, you can specify the name of the Project.
If you don't specify a name, Domino will ask you for one.
To create a Project on behalf of an organization you belong to, use the `--owner` flag.

.Examples
[source,shell]
----
domino create
domino create myProject
domino create --owner myOrg myProject
----


=== Create a Project in a directory that already has files

[[tr9]]
`init [projectName]`

`init` will create a new Domino Project _inside your current folder_.
This is useful if you already have a working folder and you'd like to convert it to a Domino Project.
While the "create" command creates a new folder inside your current directory, `init` will initialize a Project from your current folder.
The `--owner` flag can be used to set an organization you belong to as the Project owner.

NOTE: `init` will *not* upload any of your files.
You'll have to use
`upload` or `run` after you run `init`.

.Examples
[source,shell]
----
domino init
domino init myProject
domino init --owner myOrg myProject
----


[[to-get-a-project-from-the-server-onto-your-computer-for-the-first-time]]
=== Get a Project from the server onto your computer for the first time

[[tr10]]
`get [username]/projectName`

`get` will find an existing Project on the server and copy it to your computer.
This is useful to get a project you created on a different computer or to get a Project that someone else created, and you are collaborating on.

If you are getting someone else's Project, specify their username and the Project name.
If you are getting one of your own Projects, you can omit the username.

.Examples
[source,shell]
----
domino get otherUser/someProject
domino get myProject
----


=== Connect a folder to an existing Project

[[tr11]]
`restore`

`restore` will `connect` your current directory to an existing Domino Project on the server.
Restore will look for evidence of a Project name inside `.domino/config.json`.
If it doesn't find such a file, it will prompt you for the name of an existing Project to use.

This is particularly useful if you are using Domino and Git to track the same folder.
If you clone a Project with Git, Git will likely have ignored the hidden files that identify it as a Domino Project.
So you can run `git clone` and then `domino restore` to re-connect the folder to its Domino Project.

.Example
[source,shell]
----
domino restore
----


== Files

[[to-synchronize-the-files-on-your-computer-with-the-server]]
=== Synchronize the files on your computer with the server

[[tr12]]
`sync`

`sync` will synchronize the Project folder on your computer with the Project stored on the server.
This is equivalent to running a download followed by an upload.

.Example
[source,shell]
----
domino sync
----

[[to-download-the-latest-version-of-your-files]]
=== Download the latest version of your files

[[tr13]]
`download`

// tag::download-dataset[]
`download` will download the latest copy of your files from the cloud into your current project folder.
If you have made changes that conflict with changes in the cloud, you will see both versions of the conflicting file side-by-side.

There are two reasons files in the cloud might change: first, your collaborators on a Project might make changes; second, you might have executed a run that produced new output files.

.Example
[source,shell]
----
domino download
----
// end::download-dataset[]

=== Upload your files without starting a run

[[tr14]]
`upload [-m "message"]`

`upload` will upload your current project folder contents to the cloud, but will not begin a new run.
The optional message flag lets you record a message, which will be displayed when browsing past commits on the
*Code*
tab of your project.

NOTE: When using the CLI to upload more than 1000 files at once, a timeout error might occur during the upload process. To avoid this, upload smaller batches of files.

.Example
[source,shell]
----
domino upload -m "upload data files"
----

== Datasets
NOTE: The following Dataset CLI actions may cause you to exceed your quota threshold if your admin has created one. See link:9c819c#dataset-quotas[Dataset quotas] for more information.

=== Upload folder to a Dataset

// tag::upload-dataset[]
[[tr15]]
Upload all the files in a folder from your local machine to an existing dataset.

CAUTION: This command overwrites existing files unless you use the `&#8209;&#8209;fileUploadSetting` option.

.Syntax:
[source,shell]
----
domino [--fileUploadSetting Ignore|Overwrite|Rename] [--targetRelativePath <destination path>] <project-owner>/<project-name>/<dataset-name> <folder path>
----

The optional `fileUploadSetting` flag handles path collisions:

* `Ignore`: If a file already exists in the Dataset, ignore the new file.
* `Overwrite`: If a file already exists in the Dataset, overwrite the existing file with the new file.
* `Rename`: If a file already exists in the Dataset, append `_1` to the uploaded filename.
For example, if `/Users/myUser/data/file.txt` already exists then the newly-uploaded file becomes `/Users/myUser/data/file_1.txt`.

The optional `targetRelativePath` flag enables uploading to a subdirectory in the Dataset:

* `<destination path>`: This is the path, relative to the root directory of the Dataset,  where files will be uploaded.
Note that other users with access to the Dataset can alter the file structure, which may require you to modify the destination path accordingly.
+
If `targetRelativePath` is not specified, files will be uploaded to the root directory of the Dataset.

NOTE: `fileUploadSetting`, `targetRelativePath`, and their values are case-sensitive.

.Example:
[source,shell]
----
domino upload-dataset --fileUploadSetting Overwrite --targetRelativePath Experiments/Run1/Analysis jsmith/global-predictions/global-data /Users/myUser/data
----

// end::upload-dataset[]

You can take advantage of the Datasets parallel upload capability, which allows for configurable chunk sizes and multi-threading. The two tuning parameters are:

. `DOMINO_UPLOAD_CHUNK_BYTES` (Default: 3145728 (3 MB)): This configures the chunk size. Larger chunks mean less chunk setup overhead. For a fast network, the setup overhead is a larger percentage of the overall time needed to upload a chunk.


. `DOMINO_UPLOAD_THREADS` (Default: 8): This configures the number of parallel threads for multi-threaded uploads and parallel chunk uploads. The number of threads is proportional to the number of cores in your operating system.

.Set the Environment variable for Linux:
[source,shell]
----
export DOMINO_UPLOAD_CHUNK_BYTES=<new-value>
export DOMINO_UPLOAD_THREADS=<new-value>
----

.Set the Environment variable for Windows (Powershell):
[source,shell]
----
set [DOMINO_UPLOAD_CHUNK_BYTES=[<string>]]
set [DOMINO_UPLOAD_THREADS=[<string>]]
----

=== Create a snapshot

Use the following command to create a snapshot that is visible and populated after the backend finishes copying the files in the Dataset.

NOTE: Do not modify the files in the Dataset until the snapshot is done.

// tag::create-snapshot[]
.Syntax:
[source,shell]
----
domino create-snapshot <project-owner>/<project-name>/<dataset-name>
----
// end::create-snapshot[]

=== Create a Dataset from a snapshot

Use the following command to create a Dataset from a snapshot where `<snapshot-integer>` maps to a valid snapshot number in the Dataset.

NOTE: The new Dataset can take time to populate.

//tag::create-dataset-from-snapshot[]
.Syntax:
[source,shell]
----
domino create-dataset-from-snapshot <project-owner>/<project-name>/<dataset-name> <snapshot-number> <new-dataset-name>
----
//end::create-dataset-from-snapshot[]

== Get help

`help`

`help` will print out a list of commands you can run, with information about each one, similar to this page.

.Examples
`domino help`
`domino help run`

== Get the version of your CLI

[[tr16]]
`version`

`version` will output your CLI version.

.Examples
`domino --version`

----- user_guide/reference/cli/download-files-with-the-cli.txt -----
:page-version: 6.1
:page-title: Download files with the CLI
:page-permalink: 8ec1ce
:page-order: 30

If you must download individual files from a project, you typically use the Files page.
However, if you have too many files to manage from the application, you can use a CLI command:

[[tr1]]
`domino download-results`.

This command accepts a filter that allows you to download a subset of the project.
Run the following to see documentation of this command:
[[tr2]]

[source,shell]
----
domino help download-results
----

As an example, you can download the contents of a folder with a command like this:
[[tr3]]

[source,shell]
----
domino download-results --filter 'myfolder/*' 548fda01e4b0a8f06b7e8d99
----

* The `--filter` option is what limits the downloaded files to the pattern that follows it
* `548fda01e4b0a8f06b7e8d99` is the run ID

You can also use a run number instead of an ID:
[[tr4]]

[source,shell]
----
domino download-results --filter 'myfolder/*' 1
----

This downloads the output files from a run, so you must have submitted at least one run in the project.
Files are listed in the local directory under `results/run-<runId or runNumber>`.

----- user_guide/reference/cli/force-restore-a-local-project.txt -----
:page-version: 6.1
:page-title: Force-restore a local Project
:page-permalink: 3ab378
:page-order: 40

[[tr1]]
If you encounter errors with the link:9355a5[Domino CLI], it might be due to a problem with your local project metadata.
This can happen if a file is modified by another program during the sync and link:f7292e[file
locking] is turned off.
You might be able to resolve this issue by performing a force-restore.
To do so, follow these steps:

. Go to your local copy and remove the Domino project information.
+
[source,shell]
----
cd <project folder>
rm -rf .domino
----

. To restore the Domino project information, run the following command and then enter the project name.
+
[source,shell]
----
domino restore
----

This downloads files on the server that are not in your local project.
Delete them if you do not need them.
If any of your local files are different from the server version, the local version will be pushed the next time you run `domino sync`.

----- user_guide/reference/cli/index.txt -----
:page-version: 6.1
:page-title: Domino CLI
:page-permalink: 30b067
:page-order: 20


link:e21e55[Install the Domino Command Line Interface (CLI)]::
Install the Domino Command Line to your machine.

link:9355a5[Domino CLI Reference]::
Commands to use with the CLI.

link:8ec1ce[Download Files with the CLI]::
Download all files, or a subset of files, from a project.

link:3ab378[Force Restore a Local Project]::
Fix errors with the CLI.

link:dbae6b[Move a Project Between Deployments]::
Move a project from one deployment to another, such as from a trial deployment to production.

link:0da1e9[Use the Domino CLI Behind a Proxy]::
Configure the Domino CLI to use your proxy.

----- user_guide/reference/cli/install-the-cli.txt -----
:page-version: 6.1
:page-title: Install the Domino Command Line Interface (CLI)
:page-sidebar: Install the Domino CLI
:page-permalink: e21e55
:page-order: 10

The Domino Command Line Interface (CLI) is a powerful tool that allows you to manage your Domino Environment from the command line. It can be used to perform various tasks, 
such as creating and deleting Projects, backup and restoring Projects, and downloading files.

This guide provides instructions to install the Domino CLI on your system. Instructions are provided for both graphical and headless installations.

== Headless vs. graphical installation

Depending on your use case, you can either go for a Headless or Graphical installation:

- Headless installation is an installation of the Domino CLI on a server or machine that does not have a graphical user interface (GUI). 
  This is commonly done for machines that are used to automate tasks or run Domino CLI tasks in a server environment or on a Domino Workspace.

- Graphical installation is an installation of the Domino CLI on a machine that has a GUI. This is the best option for users who want to use the Domino CLI with a graphical interface.

=== Install the Domino CLI using graphical mode

[[tr1]]
. In the top navigation, click *Account > Download CLI*. 

. Run the installer application depending on your operating system.
See link:#os-support[Operating System Support] for more information.
+
On Mac OS X, the installer prompts you to enter administrator credentials.
If you do not have administrator privileges, you can click *Cancel*; the installer prompts you to manually update your `PATH` instead.
Click *Ignore* to continue the installation process, then add `export PATH="$PATH:/Applications/domino"` (assuming the default installation folder) to your `bash_profile`.



. In a command prompt, type `domino login https://<your-domino-url>`.

+
The system prompts you to authorize using the CLI with Domino.

. Paste the following URL in your browser to use `authorizeCLI` to get the authentication code.
+
----
https://<your-domino-url>/authorizeCli
----
NOTE: You might be required to log in.
. Enter the authorization code provided and press Enter.
A message indicates that the login succeeded.



Congrats, you're all set!
Check out the link:9355a5[Domino CLI Reference] for documentation about using the CLI.

=== Install the Domino CLI on a headless Linux machine

A headless installation is an installation of the Domino CLI on a server or machine that does not have a GUI. 
This is commonly done for machines that are used to automate tasks or run Domino CLI tasks in a server environment or on a Domino Workspace.

==== Install CLI from the command line

===== Step 1: Install headless Java:

[source,bash]
----
apt install openjdk-8-jre-headless
----

===== Step 2: Download the CLI installer 
Replace `<domino-url>` in the link below with the Domino URL that is currently linked to the client.

[source,bash]
----
wget https://<domino-url>/download/client/unix -O /tmp/domino-install-unix.sh
----

===== Step 3: Run the CLI installer

[source,bash]
----
sh /tmp/domino-install-unix.sh
----

NOTE: If you encounter an `AtkWrapper` error, resolve it by commenting out `/etc/java-8-openjdk/accessibility.properties assistive_technologies` and re-running the installer.

[source,bash]
----
sed -i 's/^assistive_technologies=/#assistive_technologies=/g' /etc/java-8-openjdk/accessibility.properties
----

[[os-support]]
== Operating system support

Domino supports recent versions of the following operating systems:

* Ubuntu Linux
* Windows 10
* macOS

=== Linux

The Domino CLI can be installed on Linux machines using either the GUI or headless method.
[[tr2]]

=== Windows

Domino recommends that you use the version of the Domino CLI that comes bundled with a 64-bit JRE.
The 32-bit version is deprecated.
[[tr3]]

=== macOS

You must have root privileges to run the Domino CLI for macOS.
If you didn't enter administrator credentials during the installer process, you can add `export PATH="$PATH:/Applications/domino"` (assuming the default installation folder) to `bash_profile`.

Having trouble?
mailto:support@dominodatalab.com[Email us].

----- user_guide/reference/cli/move-project-between-deployments.txt -----
:page-version: 6.1
:page-title: Move a Project between deployments
:page-sidebar: Move Project between deployments
:page-permalink: dbae6b
:page-order: 50

If you have a project on a deployment, such as a Trial deployment, and want to move it to a deployment, such as a Production deployment, you can use the CLI.

IMPORTANT: This process only moves the latest files.
It doesn't move the history and revisions.

For more information about the CLI, see link:e21e55[Install the Domino Client (CLI)].

== Prerequisites

You must have the latest local copy of your project on your workstation.
If you don't have a local copy of the project, review the link:9355a5#to-get-a-project-from-the-server-onto-your-computer-for-the-first-time[Domino CLI Reference] to get it.

[[tr1]]
== Move the Project between deployments

. Go to the project on your local system that you'd like to migrate:
+
----
cd <your-project>
----
. `.domino` is what identifies it as a domino project.
+
To remove this (UNIX):
+
----
rm -rf .domino
----
To remove this (Windows):
+
----
rmdir /s/q .domino and del .domino*
----

. Log into the target deployment, initialize, and upload your project:
+
----
domino login <target URL>
----
+
----
domino init
----
+
----
domino upload
----

. Log into the new Domino system where your project is hosted and verify the access and collaboration settings are correct.
You might have to update the access and collaboration settings because they do not transfer with the project.

----- user_guide/reference/cli/use-the-cli-behind-a-proxy.txt -----
:page-version: 6.1
:page-title: Use the Domino CLI behind a proxy
:page-sidebar: Use Domino CLI behind proxy
:page-permalink: 0da1e9
:page-order: 60

To configure the Domino CLI to use your proxy, you must modify files in the client installation directory:
[[tr1]]
* On Mac, the directory is: `/Applications/domino`
[[tr2]]
* On Windows, the directory is:
`C:Users<your_username>AppDataLocalProgramsdomino`

In the root of the installation directory, there is a file named
`domino.vmoptions` with some configuration options available.
Add the line `-include-options ~/.domino/domino.vmoptions` at the _end_ of the file for any *nix based system (Windows users MUST specify the appropriate path where `domino.vmoptions` will be created):

[source,shell]
----
-Ddomino.defaultHost=
-Dhttp.proxyHost=
-Dhttp.proxyPort=
-Dhttp.nonProxyHosts=
-Dhttps.proxyHost=
-Dhttps.proxyPort=
-Dhttps.nonProxyHosts=
-include-options ~/.domino/domino.vmoptions
----

You must create the user level configuration file
`domino.vmoptions`
in the appropriate location/folder.
The folder `~/.domino/` may not exist, in which case you should run `mkdir ~/.domino` to create it.
You must have a user level configuration file to ensure your settings are never modified if the CLI is reinstalled or updated.
In the new `~/.domino/domino.vmoptions`, assign your proxy's port and host configuration:

[source,shell]
----
-Dhttp.proxyHost=YOURPROXYHOST
-Dhttp.proxyPort=1234
-Dhttps.proxyHost=YOURPROXYHOST
-Dhttps.proxyPort=1234
----

Replace `YOURPROXYHOST` and `1234` with your actual proxy connection information.

IMPORTANT: Do not include `http(s)://` in the proxyHost parameters.

== Proxy authentication with the Domino CLI

CLI supports Basic and NTLM proxy authentication.
Add the following parameters to your `~/.domino/domino.vmoptions` file for proxy authentication:

*Basic*
[[tr3]]

[source,shell]
----
# Make sure you have disabled the system proxy via this setting
-Djava.net.useSystemProxies=false
# This is required to enable basic authentication for Java versions >= Java 8u111
# More info here -> http://www.oracle.com/technetwork/java/javase/8u111-relnotes-3124969.html
-Djdk.http.auth.tunneling.disabledSchemes=
# (https) Do not include quotes
-Dhttps.proxyUser=YOURPROXYUSER
# (https) Do not include quotes, even if your password contains spaces
-Dhttps.proxyPassword=YOURPROXYPASSWORD
# (http) Do not include quotes
-Dhttp.proxyUser=YOURPROXYUSER
# (http) Do not include quotes, even if your password contains spaces
-Dhttp.proxyPassword=YOURPROXYPASSWORD
# Make sure to leave a blank line at the end
----

*NTLM*
[[tr4]]

[source,shell]
----
# This is an extension of Basic Proxy settings
# (http) Make sure to specify the NTLM domain for this Proxy
-Dhttp.auth.ntlm.domain=YOURNTLMDOMAIN
# (https) Make sure to specify the NTLM domain for this Proxy.
-Dhttps.auth.ntlm.domain=YOURNTLMDOMAIN
# Make sure to leave a blank line at the end
----

[NOTE]
====
The proxy username is case-sensitive when using NTLM proxy authentication.
====

----- user_guide/reference/domino-api-authentication.txt -----
:page-version: 6.1
:page-title: Domino API authentication
:page-permalink: 40b91f
:page-order: 10

Learn how to use the link:8c929e[Domino API] and the different methods you can use securely authenticate into Domino programmatically.

== Authenticate with a Service Account token

Domino provides a convenient way for external software and automation pipelines to access Domino APIs via Domino Service Accounts. These accounts are managed by the Domino Administrators and have long-lasting credentials, making them easy to use in other parts of the enterprise outside of Domino. 

See link:6921e5[Domino Service Accounts documentation] for more information.

Once you receive your token from an admin, you can use the token to authenticate and run Domino API calls.

The following example starts a Domino Job using the `SA_TOKEN`. As an additional security measure, you should store the SA token as a link:d8dde6[custom environment variable for secure credential storage].

[source,shell]
----
$ curl -H "Authorization: Bearer $SA_TOKEN" 
  --header 'Content-Type: application/json' 
  --data '{ "projectId": "12398gg098", "runCommand": "main.py"}' 
  $DOMINO_API_HOST/api/jobs/v1/jobs
----


== Authenticate with an API key

To interact with secured endpoints of the link:f31cde[Model Monitoring API] or link:8c929e[Domino Platform API], you must send an API authentication token or key along with your request.
This parameter identifies you as a specific Domino user so Domino can check for authorization.

== Find your Domino Rest API key

CAUTION: This feature will be deprecated in future versions of Domino.
Domino recommends using link:6921e5[Service Accounts] as they provide stable, token-based authentication for API calls, independent access management, and stronger security.

To authenticate your requests, include your API key with the header `X-Domino-Api-Key`.

=== Get your API key

. Go to *Account > Account Settings > API Key*.
. Click *Regenerate* to produce a API key. You will only be able to view this once.

CAUTION: Anyone with this key can authenticate to the Domino application as the user it represents and take any actions that user is authorized to perform.
Treat it like a sensitive password.

== Authenticate with a JWT Token


CAUTION: This JWT token functionality is disabled by default in Domino 5.5+ and will be deprecated in upcoming Domino versions. Instead, you should link:40b91f#_use_the_api_proxy_for_domino_api_authentication[use the API Proxy for Domino API authentication].  The JWT token functionality described on this page is only present for backwards-compatibility; to use it, you must enable the `EnableLegacyJwtTooling` link:6469bf#enablelegacyjwt[feature flag].

Domino uses the OpenID Connect (OIDC) protocol to generate user-specific JWT tokens, or Domino Tokens.
Domino securely signs this Domino token and makes it available in your Workspace, Job, App, Scheduled Job, or Launcher.
You can use it to authenticate to third-party resources or data sources.
Additionally, this token can be used to authenticate with the Domino API.

Because the token is short-lived and Domino automatically refreshes it, an example of when you can use it is if you want to limit a user's access to a data source exclusively from Domino. This has security advantages over provisioning long-lived keys that can be used anywhere or by anyone.

NOTE: Domino endpoints do not provide JWT tokens in their environment.





//Leave as per Dmitry. We might remove if there was a PM to say it was ok. Info is accurate still but not complete.
== Configure a service to trust Domino's JWT

Each system will have different steps for configuration but will likely require that you enter Domino's JWKS endpoint which is
`<Domino URL>/auth/realms/DominoRealm/protocol/openid-connect/certs>`.
For offline services, retrieve and record the JWKS data to use offline with your library.
Alternatively, you can use the public key or certificate which can be found at `<Domino URL>/auth/realms/DominoRealm`.

See https://auth0.com/docs/tokens/concepts/jwks[JSON Web Key Sets^] to learn more about JWKS.

//Leave as per Dmitry. We might remove if there was a PM to say it was ok. Info is accurate still but not complete
== Token claims

By default, the Domino Token contains standard claims like name, username, and email, but you add mappers in the `domino-play` client in Keycloak to include additional custom claims.
This can be useful if you intend to set up bounded claims in the system to which you are integrating.
For example, you might want to only authorize users with a specific organization claim or any other SAML assertion that flows into Domino from your Identity Provider.

== Use the API Proxy for Domino API Authentication

You can use the API proxy to authenticate calls to Domino APIs using the address and port exposed by the proxy process.
If you make a call to Domino and use this proxy, the proxy automatically adds the authentication information (the access JWT token) to the proxied request.


.Authentication with the API proxy

. The run starts immediately and the token is acquired when the first call is made through the proxy:
+
[source,shell]
----
$ curl "$DOMINO_API_PROXY/v4/users/self"
{"firstName":"Jack","lastName":"London","fullName":"Jack London","userName":"jack-london","email":"jack.london@oaklandpub.com","avatarUrl":"","id":"123456789"}
----

. The API proxy runs and `$DOMINO_API_PROXY` contains the proxy address.

.Authentication with JWT propagation

Prior to Domino 5.4, authentication was done using JWT propagation, where Domino injected a file into Jobs and Workspaces containing access credentials.  This has been deprecated, but if user workflows have not yet been migrated, the `EnableLegacyJwtTooling` link:6469bf#enablelegacyjwt[feature flag] is available.  It is set to `false` by default, but can be set to `true` to enable legacy JWT propagation behavior (while also retaining the proxy behavior).

It is recommended to migrate workflows to use the JWT proxy for API access instead, as the JWT proxy is more reliable and the legacy behavior will be deprecated and removed in future Domino versions.

See link:40b91f[Use a Token for Authentication] for more information about JWT propagation.

//Leave as per Dmitry. We might remove if there was a PM to say it was ok. Info is accurate still but not complete
== Additional resources

See https://jwt.io/[JWT.io^] for information about decoding and inspecting the content of a JWT token.

You can use the API proxy to authenticate calls to Domino APIs using the address and port exposed by the proxy process.
If you make a call to Domino and use this proxy, the proxy automatically adds the authentication information (the access JWT token) to the proxied request.


.Authentication with the API proxy

. The run starts immediately and the token is acquired when the first call is made through the proxy:
+
[source,shell]
----
$ curl "$DOMINO_API_PROXY/v4/users/self"
{"firstName":"Jack","lastName":"London","fullName":"Jack London","userName":"jack-london","email":"jack.london@oaklandpub.com","avatarUrl":"","id":"123456789"}
----

. The API proxy runs and `$DOMINO_API_PROXY` contains the proxy address.

.Authentication with JWT propagation

Prior to Domino 5.4, authentication was done using JWT propagation, where Domino injected a file into Jobs and Workspaces containing access credentials.  This has been deprecated, but if user workflows have not yet been migrated, the `EnableLegacyJwtTooling` link:6469bf#enablelegacyjwt[feature flag] is available.  It is set to `false` by default, but can be set to `true` to enable legacy JWT propagation behavior (while also retaining the proxy behavior).

It is recommended to migrate workflows to use the JWT proxy for API access instead, as the JWT proxy is more reliable and the legacy behavior will be deprecated and removed in future Domino versions.

See link:40b91f[Use a Token for Authentication] for more information about JWT propagation.

----- user_guide/reference/index.txt -----
:page-permalink: 1c333d
:page-version: 6.1
:page-title: Reference
:page-order: 350
:page-separator: true
:page-section: Support

link:40b91f[Domino API authentication]::
Securely authenticate into Domino programmatically.

link:30b067[Domino CLI]::
Use the Domino Command Line Interface (CLI) to work with Projects, files, and Datasets.

link:b9d997[Documentation maintenance and archive]::
Find out how Domino maintains the documentation and where to find the documentation for older, archived versions.

----- user_guide/reference/maintenance-and-archive.txt -----
:page-permalink: b9d997
:page-version: 6.1
:page-title: Documentation maintenance and archive
:page-order: 30

== Documentation maintenance policy

Documentation for the Cloud version and the most recent on-prem version of Domino are regularly updated to reflect the latest improvements to the product.
This includes fixes, enhancements to existing content, and quality-of-life improvements made by the Domino team.

Documentation sets for other maintained versions of Domino are updated to ensure accuracy and reflect existing functionality.
This includes critical content fixes and updates based on bug fixes and minor features.
These documentation sets, however, are not updated with enhanced content and quality-of-life improvements.

Domino will only maintain documentation for versions released in the trailing 18 months.

Documentation sets for unmaintained versions of Domino no longer receive updates and are not rendered on the Domino documentation site.
You can still read these docs from link:https://archive.docs.dominodatalab.com/[our archive site^].

== Domino documentation archive

Only maintained documentation sets appear under the Domino documentation menu.
Domino preserves all versions for historical reference on our link:https://archive.docs.dominodatalab.com/[archive documentation site^].

For more information about product version support, see our link:https://support.domino.ai/support/s/article/Domino-Product-Version-Lifecycle-and-Support-Policies-1718868045468[Release and lifecycle policy^].

----- user_guide/register-and-govern-models/index.txt -----
:page-version: 6.1
:page-permalink: 19df62
:page-title: Register models
:page-order: 330

Use Domino's model registry and integrated model review process to ensure that models meet organizational policies and regulations.

== Track and manage models with the model registry

link:3b6ae5[Register your models to the model registry] to create a system of record for your machine learning models.

NOTE: Models will only appear in the menu once the `ShortLived.ModelRegistry` feature flag is enabled.

----- user_guide/register-and-govern-models/model-registry.txt -----
:page-version: 6.1
:page-permalink: 3b6ae5
:page-title: Manage models with model registry
:page-order: 10

Track and manage your machine learning models with Domino's MLflow-based model registry. The model registry lets you do the following:

- Discover models in project-scoped and deployment-scoped registries.
- Record model metadata and lineage for auditability and reproducibility.
- Create custom model cards to capture notes on fairness, bias, and other important information.
- Manage model versions and deploy models to Domino-hosted or externally-hosted, endpoints.
- Use RBAC (role-based access control) to limit who can view, edit, and collaborate on registered models.

NOTE: Model registry doesn't support R-based models.

== Register your model

Domino uses MLflow to track and manage Domino experiments. Each Domino experiment can be associated with one or more models that you can publish to the model registry. You can register models using the MLflow API or through the Domino web UI.

=== Register models using the MLflow API

Register your model to the model registry programmatically with the MLflow API. This method is especially useful if you are already using link:da707d[Domino's experiment management workflow] to log training runs using MLflow. After you successfully train a model that you want to register, add the following code to name, tag, and register the model.

Use MLflow tags to add custom attributes or versions to a model in order to track additional metadata.

NOTE: Model registry tags are distinct and cannot be used with link:f9be5e[Project tags] or link:ae638c[Model Monitoring tags].

[source,python]
----
import random
import string
random_string = ''.join(random.choice(string.ascii_letters) for _ in range(3))
import mlflow.sklearn
from mlflow.store.artifact.runs_artifact_repo import RunsArtifactRepository
from mlflow import MlflowClient
from sklearn.ensemble import RandomForestRegressor

client = MlflowClient()

# Register model name in the model registry
name = "RandomForestRegression_" + random_string
registered_model = client.create_registered_model(name)

# create an experiment run in MLflow
params = {"n_estimators": 3, "random_state": 42}
rfr = RandomForestRegressor(**params).fit([[0, 1]], [1])
# Log MLflow entities
with mlflow.start_run() as run:
    mlflow.log_params(params)
    model_info = mlflow.sklearn.log_model(rfr, artifact_path="sklearn-model")
    runs_uri = model_info.model_uri

    # Create a new model version of the RandomForestRegression model from this run
    desc = "A testing version of the model"
    model_src = RunsArtifactRepository.get_underlying_uri(runs_uri)
    mv = client.create_model_version(name, model_src, run.info.run_id, description=desc)
    print("Name: {}".format(mv.name))
    print("Version: {}".format(mv.version))
    print("Description: {}".format(mv.description))
    print("Status: {}".format(mv.status))
    print("Stage: {}".format(mv.current_stage))
print("You created your first registered model!")
print(f"Go to the Models UI and click on the {name} model to see the Model Card.")
----

Starting with Domino 5.11.1, you can now upload large artifact files directly to blob storage without needing to go through the MLflow proxy server. This is an experimental feature that must be enabled inside user notebook code by setting the environment variable `MLFLOW_ENABLE_PROXY_MULTIPART_UPLOAD` to `true`.

[source, python]
----
import os

# Enable MLflow multipart uploads
os.environ['MLFLOW_ENABLE_PROXY_MULTIPART_UPLOAD'] = "true"

# (default: `10_485_760` (10 MB)) - set to 200MB instead here.  Select size based on maximum of 1000 chunks.
os.environ['MLFLOW_MULTIPART_UPLOAD_CHUNK_SIZE'] = '200_000_000'
----

This feature is useful for both `log_artifact` calls and registering large language models. It is currently supported only in AWS and GCP environments. There are 2 additional settings available for configuring this feature:

* `MLFLOW_MULTIPART_UPLOAD_MINIMUM_FILE_SIZE` - the minimum file size required to initiate multipart uploads.
* `MLFLOW_MULTIPART_UPLOAD_CHUNK_SIZE` - the size of each chunk of the multipart upload. Note that a file may be divided into a maximum of 1000 chunks.

See the https://mlflow.org/docs/latest/tracking/artifacts-stores.html#multipart-upload-for-proxied-artifact-access[MLflow multipart upload for proxied artifact access^] documentation for further details.

[[register-models-using-the-web-ui]]
=== Register models using the web UI

You can also register a model using the Domino web UI after an experiment run has been completed.

. Go to *Experiment* and select your experiment.
. Select your run.
. Select *Register model from run*.

== Model cards

Domino helps you manage your models through model cards. Model cards are automatically created whenever you publish a model to the registry. Model cards help you do the following:

- Ensure reproducibility and auditability by automatically tracking metadata, lineage, and downstream usage for each version of your models.
- Document the model with information like data science techniques, limitations, performance, fairness considerations, and explainability.

=== Lineage and metadata tracking

Domino automatically records the code, environment, workspace settings, and any datasets or data sources used for each experiment and associates them with the registered model. This system of record lets you track and reproduce the training context for your models for auditing and validation. Domino automatically adds all of this information to every model card for easy reference.

=== Custom model cards

When publishing your model, you can either import model cards from other sources or add your own markdown content to the model's description. Custom model cards can be used to bring together other resources to provide a single point of truth for your model. For example, you could link to relevant research, dashboards, or external applications that help you evaluate models for fairness and bias.

You can update model card markdown programmatically, as in this example:

[source,python]
----
with open('Model Card v2.md', 'r') as file:
    markdown_description = file.read()
    client.update_registered_model(name, markdown_description)
----

You can also directly edit the model card in the web UI:

. Go to *Models* > *Overview*.
. Select the *Edit* icon and edit the model card.

== Collaborate securely

Domino uses project-based access controls to determine who can view, manage, and deploy models. Users must authenticate into Domino to use MLflow and Domino APIs for model tasks. Project user roles govern the model registry actions that a user can perform. For more information on model registry user roles, see link:2611b7#model-registry[model registry roles].

== Model discoverability

You can toggle Models in the Model Registry to be "publicly" discoverable to other Domino users within the organization. By allowing models to be publicly discoverable, organizations can improve visibility, reduce redundancy, and increase collaboration between otherwise siloed projects.

For example, a data scientist may want to explore models from other teams to avoid duplicating work. Meanwhile, a data science leader may want a consolidated view of all models across the organization to measure the success of the data science team.

In the Model Registry, users see two lists:

*Global Models*: A list of all the models that are marked publicly discoverable, regardless of whether the user has adequate project permissions to use the models. This serves as a dashboard for data scientists who want to explore and use models from other teams.

*Collaborating Models*: A list of all the models that a user is allowed to see. This includes models marked as publicly discoverable (annotated by the globe icon). Data science leaders can use this as a consolidated dashboard to track model development, usage, and metrics across projects.

=== Create a publicly discoverable model

To make a new model publicly discoverable:

. link:#register-models-using-the-web-ui[Register a model from a run].
. Check the checkbox for *Model Discoverability*.

NOTE: The checkbox's default setting is determined by the `_com.cerebro.domino.registeredmodels.newModelDiscoverability.default_` configuration records setting, which is initially set to `true`.

=== Toggle an existing model as discoverable

Once the model is created, a System Admin or Project Owner can toggle the model in and out of public discoverability using one of two ways.

. Through the model card UI:
+
. Through the public API PATCH endpoint (API not published on link:f35c19[API guide]).

NOTE: If you turn off a Model's public discoverable setting, Domino users without project permissions will no longer be able to see the model in their global model list or view the model card.

== Read-only users

Domino shows a limited, read-only version of the model card for publicly discoverable models to users without project permissions. The read-only model card hides the discoverability toggle, doesn't allow edits to the model description, and does not render Domino endpoint components. Additionally, links on various components of the model card may lead to 403 error pages if the user doesn't have the proper permissions to view assets such as Data Sources, experiment runs, code, etc.

You can request access to a read-only model from the project owners:

. Go to the Model Card.
. In the Model Card header, click *How can I use this Model?*

== Example MLflow-supported models

See examples of how to use various MLflow-supported models such as XGBoost, Sklearn, PyTorch, TensorFlow, and custom Pyfunc models for tasks like registering various flavors of MLflow models, enabling integrated monitoring, and link:8dbc91[creating Domino endpoints].

. https://github.com/dominodatalab/reference-project-domino-mlflow-supported-models/blob/main/domino-mlflow-model-xgboost-imm.ipynb[Registering XGBoost model with integrated model monitoring].
. https://github.com/dominodatalab/reference-project-domino-mlflow-supported-models/blob/main/domino-mlflow-model-sklearn-imm.ipynb[Registering Sklearn model with integrated model monitoring].
. https://github.com/dominodatalab/reference-project-domino-mlflow-supported-models/blob/main/domino-mlflow-models.ipynb[Registering MLflow models (with column and tensor-based inputs) as Python functions in Domino].

----- user_guide/reproducibility/compare.txt -----
:page-version: 6.1
:page-permalink: ee0a08
:page-sidebar: Compare results and Project states
:page-title: Compare results
:page-order: 60

Domino provides comparison tools to help you analyze changes in Files, Jobs, and Project states so you can manage Project artifact changes and audit those changes more effectively.

== Compare Jobs

On the Jobs Dashboard, select exactly two Jobs by checking the checkbox in the left-hand column. Then click the icon indicating comparison, above the table.

The Job comparison view shows the following:

- Differences in summary metadata and diagnostic statistics
- Differences in the files as they were when the Job ran
- Differences in the Results of the Jobs

Certain file types will be rendered side by side for visual comparison. Other types (e.g., binaries) will simply be listed as different.

You have the option to comment not only on the comparison itself but also on the comparison between two files.


== Compare file versions

To compare different file versions, view a file and click the Compare Revisions button. This enables side-by-side viewing of two selected versions.


== Compare Project states from the Activity Feed

On the Activity Feed screen, select two Workspaces or Jobs and click the compare button at the top of the page.


== Relevant admin config settings

The following configuration records settings affect aspects of how collaboration functionality works:

[cols="2,3,^1",options="header",width="100%"]
|===
|Config key
|Description
|Example value

|`frontend.maxNumberOfRenderedFileDiffs`
|Maximum number of files shown when comparing two Jobs.
|`250`

|`frontend.maxTotalDiffBytesToRender`
|Maximum aggregate size of the files shown when comparing two Jobs.
|`1024000`
|===

== Next steps
View link:2e25bb[Jobs], link:d95a3c[Project files], and link:a7420a[Project activity].

----- user_guide/reproducibility/file-syncing.txt -----
:page-version: 6.1
:page-permalink: b4f02f
:page-title: File syncing and persistence
:page-order: 30

Domino automatically tracks files in your Project. In a link:ca786d[DFS-backed Project^], all files in the root of your Project folder will be tracked unless they are excluded by link:bfeeae[.dominoignore^] logic. When you run code, these files will show up at `/mnt` in the local file system. In a link:910370[Git-backed Project], Domino tracks files in your *artifacts* directory, which will show up at `/mnt/artifacts` inside a running Workspace or executing Job.

Keeping file-based data in these folders gives you the benefit of automatic reproducibility. However, due to how Domino synchronizes these files. Domino isn’t designed for high performance if your files are more than ~10GB in total size or more than ~100,000 individual files.

Domino stores the contents of Project files in the Domino Blob Store. The backing storage mechanism for the Blob Store varies based on the deployment infrastructure in which Domino is running:

[cols="^1,^1",options="header"]
|===
|Deployment infrastructure |Dataset storage implementation

|AWS |S3
|Azure |
|GCP |
|On-Prem or other cloud | NFS-compatible NAS
|===

When you first start a task in Domino that spins up a new compute resource to run your code, Domino hydrates the local file system on that compute resource, with your Project files. This happens, when:

* You start a Workspace (but not when you resume a paused Workspace)
* When you run a Job
* When a Domino endpoint or App starts (or restarts)

When a Job completes, or when you explicitly sync work within a Workspace session, Domino persists a new revision of your files to the Blob store. Note that Domino endpoints and Apps cannot persist local file system changes back to the Blob Store.

image::/images/5.8/reproducibility/blob-store.png[alt="Blob store", width=700]

When persisting changes, Domino will never destroy information. In that sense, the Blob Store is an _immutable revisioned file store_. For example, if you edit a file, Domino adds the new version but doesn't delete the old one. Or if you delete a file, Domino notes that the latest version of your Project has it deleted, but the previous version is still accessible by reverting to a past state.

== Next steps

These topics in this section explain how you can make your workflows reproducible in Domino.

link:5b3b05[Reproducibility use cases]::

Learn how to reproduce the results of a Job, Workspace, Model, App, or Launcher.

link:e53646[Selectively revert past materials]::

Selectively restore a part of a Project, such as the package library version, while keeping your latest code and data.

link:aa15a8[Remove a file from the DRE: Permanent deletion]::

Purge a file completely and permanently from the blob store.

link:e568d7[Track external data]::

Materialize external data as a file in Domino to benefit from the automatic tracking that Domino provides.

link:fda6d8[Tips for reproducibility in Domino]::

Tips for maximizing the power of the Domino Reproducibility Engine.

----- user_guide/reproducibility/index.txt -----
:page-version: 6.1
:page-permalink: 0264e0
:page-title: Domino Reproducibility Engine (DRE)
:page-sidebar: Reproducibility
:page-order: 90

The Domino Reproducibility Engine automatically tracks data science work, making it easy to reconstruct or reproduce it later.

== Reproducibility

Reproducibility in data science is critical for three reasons:

- Reproducibility is the foundation for collaboration and work reuse. Data science often involves resuming previous work (done by you or a colleague). You can only resume work if you can run the same inputs and produce the same results. That means having access to the exact code, data, and software packages as they were.
- Reproducibility is critical for auditability, review, and compliance. Especially in regulated industries, internal or external stakeholders may need to reconstruct data science work — either to validate it for production or to answer questions down the road. These tasks require reconstructing the materials used during the development process.
- Reproducibility is good science and hygiene in any research process. Much literature has been written about this, but one paper we recommend is link:https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003285[Ten Simple Rules for Reproducible Computational Research^] in PLOS Computational Biology.

== Domino Reproducibility Engine

Because reproducibility is a complex topic, the DRE doesn't live on a single screen or location in Domino. Rather, it is integrated deeply into the architecture of the platform and woven throughout many features.

At its core, the DRE works by automatically tracking _tuples_, which are specific versions of _input materials_ and _results_. Conceptually, a tuple contains the following components:

image::/images/5.8/reproducibility/tuple-components.png[alt="Tuple components", width=700]

* *Code*: the source code used, this could also include notebooks.
* *Data*: data sources and data files used and manipulated by the code.
* *Software*: the packages, drivers, and configuration in which your code runs.
* *Command*: in the context of a batch job, the executed command.
* *Results*: any outputs produced during the execution of your code. Typically these will be charts, additional data, serialized model files, and even rendered notebooks. These can also include statistics (e.g., RMSE, AUC) you want to track.

In Domino, there are several ways to materialize these components. Here is how the above conceptual materials are manifested in Domino.

[cols="1,3,3",options="header",width="100%"]
|===
|Concept
2+|Domino implementation

|
|*DFS-backed Project*
|*Git-backed Project*

|Code
|Your code will be in your *Project files* directly. In this case, the DRE keeps a space-efficient snapshot of your Project files.
|Your code will be in one or more *externally-hosted Git repos*. In this case, the DRE tracks the commit of each Git repo that was referenced when your code ran.

|Data
2+|Your code can access data that is stored inside the Domino platform, or data that lives externally. The DRE only has direct awareness of data that is stored inside the Domino Platform.

For data stored in your Domino project directly as files or link:56938d[Artifacts^], the DRE will automatically track these in a space-efficient manner.

For data stored in link:ba5bad[Datasets^], the DRE will track which Snapshot was used by your code.

For data in Data Sources, or data you access by connecting directly to databases or external sources, the DRE won't be able to automatically track it. To track this type of external data, see link:e568d7[Track external data^].

|Software
2+|Domino tracks the version of the link:0d73c6#_domino_compute_environments[compute Environment^] used when your code executes. A revision of compute Environment is a revision of a Docker image along with some supplemental configuration scripts.

|Results
|Files in your Project that are created or modified when your code executes.
|Files in the link:56938d[Artifacts^] section of your Project that are created or modified when your code executes.

|Statistics or results metadata
2+|For stats that show up in Domino's link:2e25bb[Jobs dashboard^], Domino will extract them from a file named `domino.json` in your Project's Files (or Artifacts). See link:5b84c5[Diagnostic statistics^] for more.

For our newer link:3b6ae5[MLflow-based experiment tracking^], use the MLfLow API to record your stats from your code.

|===

Each of these materials can change independently in Domino. E.g., you may make multiple discrete edits to your code, re-running experiments after each one; and then later make a big change to update a package in your compute Environment.

At select operations — e.g., running a Job, syncing your work in Workspace, publishing a Domino endpoint — Domino records the state of your materials at that point.

Domino automatically tracks the active revision of each component at the time you execute your code. This diagram of how the DRE keeps track of each _tuple_ as components are evolving separately.

image::/images/5.8/reproducibility/track-tuples-with-dre.png[alt="Track tuples with DRE", width=600]

Below is an example of the details of a Job in Domino, highlighting some of the reproducible materials. Note that the *Command* is a link that takes you to a full snapshot of the code that was executed.

image::/images/6.0/job-repro.png[alt="Reproducible materials", width=600]

== Next steps

These topics in this section explain how you can make your workflows reproducible in Domino.

link:5b3b05[Reproducibility use cases]::

Learn how to reproduce the results of a Job, Workspace, Model, App, or Launcher.

link:e53646[Selectively revert past materials]::

Selectively restore a part of a Project, such as the package library version, while keeping your latest code and data.

link:b4f02f[File syncing and persistence]::

Domino automatically tracks files in your Project and keeps previous versions in the blob store.

link:aa15a8[Remove a file from the DRE: Permanent deletion]::

Purge a file completely and permanently from the blob store.

link:e568d7[Track external data]::

Materialize external data as a file in Domino to benefit from the automatic tracking that Domino provides.

link:ee0a08[Compare results and Project states]::

Domino provides comparison tools to help you analyze changes in Files, Jobs, and Project states so you can manage Project artifact changes and audit those changes more effectively.

link:fda6d8[Tips for reproducibility in Domino]::

Tips for maximizing the power of the Domino Reproducibility Engine.

----- user_guide/reproducibility/permanent-deletion.txt -----
:page-version: 6.1
:page-permalink: aa15a8
:page-title: Remove a file from the DRE: Permanent deletion
:page-sidebar: Permanently delete files
:page-order: 40

In rare cases, you may need to purge a file completely from the Blob Store. For example, if you discover that a file contains data that must be deleted for compliance reasons.

Contact your Domino administrator to link:26c944[delete all the file instances] to permanently remove such a file.

IMPORTANT: A full delete will irrevocably remove all files that match the exact _contents_ of the file you are deleting. For example, if multiple projects all contain a `data.txt` file with the exact same contents, full-deleting it in one Project will full-delete it everywhere.

== Next steps

These topics in this section explain how you can make your workflows reproducible in Domino.

link:5b3b05[Reproducibility use cases]::

Learn how to reproduce the results of a Job, Workspace, Model, App, or Launcher.

link:e53646[Selectively revert past materials]::

Selectively restore a part of a Project, such as the package library version, while keeping your latest code and data.

link:b4f02f[File syncing and persistence]::

Domino automatically tracks files in your Project and keeps previous versions in the blob store.

link:e568d7[Track external data]::

Materialize external data as a file in Domino to benefit from the automatic tracking that Domino provides.

link:fda6d8[Tips for reproducibility in Domino]::

Tips for maximizing the power of the Domino Reproducibility Engine.

----- user_guide/reproducibility/reproducibility-tips.txt -----
:page-version: 6.1
:page-permalink: fda6d8
:page-title: Domino Reproducibility tips
:page-sidebar: Reproducibility tips
:page-order: 70

Learn how to make the most of the Domino Reproducibility Engine (DRE) with these tips. Since the DRE operates at a low level of abstraction like tracking Docker images and files, it offers a lot of flexibility and power.

== Jobs offer the strongest reproducibility environment

For results that have the strongest reproducibility requirements (e.g., for regulatory or compliance reasons), produce these results by executing Jobs, not through Workspaces. Since a Workspace is interactive, it's impossible to make every aspect of its state reproducible (for example, a data scientist can execute cells in a different order). Jobs are headless, meaning they run start-to-stop making them better suited for strong reproducibility. 


== Explicit package versions

Use explicit version numbers when installing packages in Python or R, and consolidate package installation in one place. 

It's best to put these in your compute link:5dd2c1[Environment definition]. Remember, however, if Domino detects a `requirements.txt` file in your Project files, it will `pip install` (or `conda install`) the requirements file before it executes your code. An analogous pattern that some R practitioners use is to have a single `install.R` file in their Project, which they run before anything else. 

If your package definitions are in a file like this, note that your software state is spread across both your compute Environment and your Project files. However, you may find this advantageous in some circumstances because you can use Domino's link:dae129[comparison feature^] to compare two versions of `requirements.txt` or `install.R` to see exactly what changed.

Regardless, use explicit package versions, e.g., `pip install torch==1.12.1` rather than `pip install torch`

That way, you'll get the same results running the code in the future, even if the latest version is newer.

For instructions on installing specific versions of R packages, see this link:https://stackoverflow.com/questions/17082341/installing-older-version-of-r-package[Stackoverflow post^].


== Don't use file names to track revision 

Because Domino keeps your files revisioned automatically, it's best if you _don't encode file names with your own versioning scheme_. For example, you don't need to append “v1”, “v2”, “final”, etc. to your version names. In fact, it's better if you don't do this, because Domino treats files with the same name as changes when it presents comparisons between Jobs.

== Other tips 

Because the DRE tracks all files that a Job produces, there are many clever tricks you can use to track things in files. For example:

* If you are using random numbers in your code, you can persist the seed value to a file, so that you can recreate it later if you need to reproduce the precise random behavior. In Python, you can link:https://docs.python.org/3/library/random.html#random.getstate[get the state^] of the random generator and pickle it to a file. In R you can use `set.seed` to set your own value (which you need to select on your own).

* Generalizing the above: R and Python both have powerful capabilities for persisting memory state to files. In R you can link:https://stat.ethz.ch/R-manual/R-devel/library/base/html/save.html[save objects^], even for your entire set of variables. Python lets you link:https://docs.python.org/3/library/pickle.html[pickle objects^]. You can use these techniques to easily create revisioned artifacts of intermediate values you've calculated. This may be useful to save time later, by resuming a computationally intensive process from an intermediate state instead of starting from scratch.

* You can create your own domain-specific formats with customized results you may want to track and compare over time. For example, you can create a “model summary.pdf” that has the same format between different Jobs.

== Next steps 

These topics in this section explain how you can make your workflows reproducible in Domino.

link:5b3b05[Reproducibility use cases]::

Learn how to reproduce the results of a Job, Workspace, Model, App, or Launcher.

link:e53646[Selectively revert past materials]::

Selectively restore a part of a Project, such as the package library version, while keeping your latest code and data.

link:b4f02f[File syncing and persistence]::

Domino automatically tracks files in your Project and keeps previous versions in the blob store.

link:aa15a8[Remove a file from the DRE: Permanent deletion]::

Purge a file completely and permanently from the blob store.

link:e568d7[Track external data]::

Materialize external data as a file in Domino to benefit from the automatic tracking that Domino provides.

----- user_guide/reproducibility/reproducibility-use-cases.txt -----
:page-version: 6.1
:page-permalink: 5b3b05
:page-title: Reproducibility use cases
:page-order: 10

Learn how to use the Domino Reproducibility Engine (DRE) to reproduce work across your data science workflows. In this article, you learn how to accomplish the following reproducibility scenarios:

- Reproduce Job results
- Find the Job that generated an Artifact
- Reproduce work from a Workspace
- Reproduce a deployed Model
- Reproduce an App
- Reproduce Launcher results

// tag::reproduce-a-result-of-a-job[]
== Reproduce Job results

The most robust way of creating reproducible work in Domino is with Jobs.

Domino detects file system changes made by a Job upon its completion, regardless of success or failure. Domino defines "results" as the modified or created files. In a DFS-backed project, these are visible in the Project's *Files* section; in a git-backed Project, they are in the Project's *Artifacts*.

This behavior applies to Jobs regardless of how they are initiated: manually through the web GUI; as a link:5dce1f[Scheduled Job^]; through the link:af97b7#_from_the_domino_cli[CLI^]; or through the link:af97b7#_from_the_domino_api[Domino API^]. This is also true for executions of Launchers because link:f4e1e3[Launchers^] run Jobs under the hood.

The link:2e25bb[Jobs dashboard^] lets you browse past Jobs. Click on the row for a Job to view the Results and additional reproducibility details of that Job.

The *Results* tab on the right pane will show all files that were created or modified by the Job when it ran.

The *Details* tab will show additional materials necessary to reproduce the results — e.g., the snapshot versions of any Datasets that were used, and the revision of the link:0d73c6#_domino_compute_environments[compute Environment^].

The Jobs Dashboard offers an even more specialized view to link:2e25bb#_compare_jobs[compare two Jobs^], so you can see what changed across the results and the inputs.

== Find the Job that generated an Artifact

Often you may have a specific Artifact — e.g., a chart or PDF — and need to find the Job that generated it. For example, to service an internal or external compliance request.

When viewing files, the revision menu at the top of the screen lets you browse past revisions. For any past revision that was produced by a Job, the revision menu will provide a link to the Job. From there, you can access all the details described in the section link:5b3b05#reproduce-a-result-of-a-job[Reproduce a result of a Job].


[TIP]
====
When including results such as charts or figures in external materials (i.e., presentations or documents outside of Domino such as Powerpoints, Word documents, or Google documents), provide a citation that includes the Domino revision info. Domino lets you link to the specific revision of a file so you can navigate back to the version of the result with one click. You can also include a reference to the Job number that produced the result.
====

== Reproduce work from a Workspace

Unlike Jobs, link:867b72[Workspaces] don't have a completion state. Workspaces are used for interactive development — they start and stop as data scientist modify and save their work.

To integrate this mode of working with the DRE, Domino tracks the sessions and commits performed in a Workspace, even if that goes on for weeks or months.

When working in your Workspace, simply commit your files whenever you want, using the slide-out *File Changes* tab on the left.

In the Workspaces section of your Project, you can select a current or past Workspace, and click to see its *History*.

The *History* section shows the *Sessions* in your Workspace, that is each segment of time you ran the Workspace between periods where you stopped it.

It also shows you all *Commits*, that is each time you committed files while working in the Workspace.

Click the row for a specific session to see all the commits performed during that Session.

== Reproduce a deployed Model

Whenever you link:8dbc91[deploy a Domino endpoint^] in Domino, Domino records a new revision of that Model. On the *Versions* tab of your Domino endpoint page, you can see all published versions.

From here, hovering over the version number of any version gives you a *Show details* link that takes you to a page with details about the specific version.

The details include links to the specific revision of the compute Environment and the Project state as it was when this version was deployed.


== Reproduce an App

When you publish new versions of an link:71635d[App^], Domino keeps a record of each one. You can browse versions on the *App Versions* tab of your App page.

Click a version row to view *Details*, including revisions of the compute Environment and Project files (or Git repos, for a Git-backed project).

== Reproduce Launcher results

When you run a Launcher, Domino invokes a Job under the hood. As a result, all results of Launchers are automatically reproducible and auditable. Simply browse for the result as you would for a link:5b3b05#reproduce-a-result-of-a-job[Job].

== Next steps 

These topics in this section explain how you can make your workflows reproducible in Domino.

link:e53646[Selectively revert past materials]::

Selectively restore a part of a Project, such as the package library version, while keeping your latest code and data.

link:b4f02f[File syncing and persistence]::

Domino automatically tracks files in your Project and keeps previous versions in the blob store.

link:aa15a8[Remove a file from the DRE: Permanent deletion]::

Purge a file completely and permanently from the blob store.

link:e568d7[Track external data]::

Materialize external data as a file in Domino to benefit from the automatic tracking that Domino provides.

link:fda6d8[Tips for reproducibility in Domino]::

Tips for maximizing the power of the Domino Reproducibility Engine.

----- user_guide/reproducibility/revert-materials.txt -----
:page-version: 6.1
:page-permalink: e53646
:page-title: Selectively revert past materials
:page-order: 20

Domino offers fine-grained controls to manage Project materials. This lets you revert certain Project materials to previous states without affecting others.

For example, you can revert to an older package library version while keeping your latest code and data intact. Similarly, you can run your current code using an older version of your data when needed.

Learn how to isolate and revert the following materials in a Domino Project to a previous version:

- Files
- Compute environments
- Dataset snapshots
- Job runs

== Revert file revisions in a Domino Project

When viewing a file, you can use the **Commit** menu to browse to past versions that you can revert. If you are browsing the root directory of your Project, you'll revert all Project files.

If you are browsing a specific file, you'll only revert that file.

== Revert compute Environment revisions

When viewing Details of a compute Environment, the *Revisions* tab will show you all versions.

After clicking into one version of an Environment, you can make it the Active Revision, or download materials to recreate that environment outside of Domino.

== Revert Dataset snapshots

When viewing a Dataset, the **Snapshot** menu lets you browse past Snapshots.

To revert to an old version, manually create a new snapshot. Replace all files with the contents from the desired old snapshot.

== Re-run a past Job using the latest code or compute Environment

If you select a Job on the Jobs dashboard, you can re-run it. 

* *Re-Run with Current Version* executes the exact command from the original Job, with the current state of your Project Files.
* *Re-Run with Original Version* uses the original command and the original Files from the selected Job, with the latest version of your compute Environment.

== Next steps 

These topics in this section explain how you can make your workflows reproducible in Domino.

link:5b3b05[Reproducibility use cases]::

Learn how to reproduce the results of a Job, Workspace, Model, App, or Launcher.

link:b4f02f[File syncing and persistence]::

Domino automatically tracks files in your Project and keeps previous versions in the blob store.

link:aa15a8[Remove a file from the DRE: Permanent deletion]::

Purge a file completely and permanently from the blob store.

link:e568d7[Track external data]::

Materialize external data as a file in Domino to benefit from the automatic tracking that Domino provides.

link:fda6d8[Tips for reproducibility in Domino]::

Tips for maximizing the power of the Domino Reproducibility Engine.

----- user_guide/reproducibility/track-external-data.txt -----
:page-version: 6.1
:page-permalink: e568d7
:page-title: Track external data
:page-order: 50

You can use the link:0264e0[Domino Reproducibility Engine] to track external data in Domino for reproducibility and auditability by persisting the data in Domino when your code runs. If your code connects to an external data source (e.g., a database, an API, a network drive, or a link:fa5f3a[Domino Data Source]), Domino does not know about the data coming from those sources.


== Materialize in-memory data as files for the Reproducibility Engine

To track external data for reproducibility reasons, you can materialize the in-memory data as a file so that the Domino Reproducibility Engine can track it. You can materialize it using any of the following options:

- link:ba5bad[Domino Dataset]
- A file like a CSV or Parquet in the Domino File System (DFS)
- link:440de9[Domino TrainingSet]

TIP: For best performance, keep your results set under 10GB if you are persisting files to the Domino file system.

=== Use Datasets to track external data

Persist a data frame as a link:ba5bad[Domino Dataset] using the following code:

.Python
[#Python-1]
--
To materialize a dataframe as a Dataset in Python, run the following:

[source,python]
----
# Save data to the "quick-start" dataset
csv_file_path = "/domino/datasets/local/quick-start/data.csv"
parquet_file_path = "/domino/datasets/local/quick-start/data.parquet"

# Write the DataFrame to a CSV file
df.to_csv(csv_file_path, encoding='utf-8', index=False)

# Write the DataFrame to a binary parquet format
df.to_parquet(parquet_file_path)
----
--

.R
[#R-1]
--
To materialize a table as a Dataset in R, use the following code:

[source,r]
----
library(arrow, warn.conflicts=FALSE)

# Save data to the "quick-start" dataset
csv_file_path <- "/domino/datasets/local/quick-start/data.csv"
parquet_file_path <- "/domino/datasets/local/quick-start/data.parquet"

# Write the table to a CSV file
write.csv(table, file=csv_file_path, fileEncoding="UTF-8")

# Write the table to a binary parquet format
write_parquet(table, parquet_file_path)
----
--

=== Store files in the DFS to track external data

Use the following code to persist a data frame as a CSV or binary file in the link:ca786d[Domino File System].

.Python
[#Python-2]
--
To materialize a data frame as a file in your working directory, run the following Python code in a notebook code cell, then link:262fef[sync the file changes] to persist the file in the backing repository.

[source,python]
----
## Save data to the project's default working directory `mnt` as an artifact add an additional sync step
csv_file_path = "/mnt/data.csv"
parquet_file_path = "/mnt/data.parquet"

# Write the DataFrame to a CSV file
df.to_csv(csv_file_path, encoding='utf-8', index=False)

# Write the DataFrame to a binary parquet format
df.to_parquet(parquet_file_path)
----
--

.R
[#R-2]
--
To materialize a table as a file in the root of the working `/mnt/` directory, run the following R code in RStudio console, then link:262fef[sync the file changes] to persist the file in the backing repository:

[source,r]
----
library(arrow, warn.conflicts=FALSE)

# Save data to the project's default working directory `mnt`
csv_file_path <- "/mnt/data.csv"
parquet_file_path <- "/mnt/data.parquet"

# Write the table to a CSV file
write.csv(table, file=csv_file_path, fileEncoding="UTF-8")

# Write the table to a binary parquet format
write_parquet(table, parquet_file_path)
----
--

=== Use TrainingSets to track external data

To save a data frame as a TrainingSet, run the following Python code.

[source, python]
----
from domino.training_sets import TrainingSetClient

training_set_version = TrainingSetClient.create_training_set_version(
    training_set_name="my-training-set",
    df=df
)

# To retrieve the data back as Data Frame
# tsv_by_num = TrainingSetClient.get_training_set_version(
#     training_set_name="my-training-set",
#     number=1,
# )
# training_df = tsv_by_num.load_training_pandas()
----

== Data Source audit logs

For external data accessed using a link:fbb41f[Domino Data Source connector], you can also use the link:f5934f[Data Source audit log] to track Data Source activity. 

Data Source logs provide a way to track user activity, which can be helpful for reproducibility and lineage. However, Domino cannot reproduce the state of your Data Source at the time of execution.


== Next steps 


This section explains how to ensure workflow reproducibility in Domino:

link:5b3b05[Reproducibility use cases]::

Learn how to reproduce the results of a Job, Workspace, Model, App, or Launcher.

link:e53646[Selectively revert past materials]::

Selectively restore a part of a Project, such as the package library version, while keeping your latest code and data.

link:b4f02f[File syncing and persistence]::

Domino tracks project files automatically, storing previous versions in the blob store.

link:aa15a8[Remove a file from the DRE: Permanent deletion]::

Purge a file completely and permanently from the blob store.

link:fda6d8[Tips for reproducibility in Domino]::

Tips for maximizing the power of the Domino Reproducibility Engine.

----- user_guide/share-data/index.txt -----
:page-version: 6.1
:page-permalink: df7044
:page-title: Share data securely
:page-order: 140

Data in Domino is reusable across multiple projects.
These topics explain how to keep your data secure and make it available to other Domino users.

link:8f5b7e[Dataset sharing and security]::

This topic explains how to share a Domino dataset and assign roles to other users that give them specific permissions to a dataset.

link:33ea62[Data source sharing and security]::

The access that a Domino user has to an external data source depends on the data source authentication configuration and whether it's shared with everyone in the Domino deployment or only with specific users and organizations.

link:7ec608[EDV sharing and security]::

When defining an external data volume (EDV), admins specify which Domino users or groups are allowed to access it.

----- user_guide/share-data/share-data-sources.txt -----
:page-version: 6.1
:page-permalink: 33ea62
:page-title: Share Data Sources securely
:page-order: 20

Domino data sources use Domino connectors to connect to external data stores.
Data source permissions determine who is authorized to access a data source.

Depending on their permissions and credentials, users can modify data in the external data store from Domino.
By understanding how data source credentials and sharing work, you can ensure that your collaborators have the appropriate kind of access to your data sources.

== Configure Data Source credentials

There are two methods of initializing credentials when creating a Domino Data Source: service account, and individual credentials. These two methods are defined below:

* Service account
+
With this approach, anyone with permissions to access the Domino Data Source uses the same credentials (those of the service account).
Only a Domino admin can create a data source that uses a service account, at *Admin* > *Data* > *Data Sources* > *Create a Data Source*.
+
CAUTION: If the service account has write access to the data store, then anyone with access to the data source can modify data in the external data store from Domino.
Make sure the service account has permissions that are appropriate for all of its Domino users.

* Individual credentials
+
Each user who accesses the data source must set up their own credentials.
Once their credentials are set up in Domino, they do not need to re-enter them when accessing the data source.
+
CAUTION: If a user's individual credentials give them write access to the data store, then they can modify data in the external data store from Domino.

[[share-a-datasource]]
== Share a Data Source

The Data Source owner or a Domino admin can grant access to the Data Source for specific users or organizations. Admin can also make it available to everyone in the Domino deployment.

When multiple users collaborate on a project, a data source used by one user might not be properly configured for another.
Domino will proactively surface such problems both from the project's Data page as well as from the Data tab in Domino Workspaces.

=== Change a Data Source's permissions

To edit the share permissions for a data source, go to *Data* > *Data Sources*, select your data source, and click *Update Permissions*.

* To limit access to this data source, select *Specific users or organizations* and add or remove users or organizations as appropriate.
* Admin can select *Everyone* to share this data source with all users in the deployment.

Note that you cannot change the credential method (service account or individual credentials) for an existing data source.
To change the credential method, you must create a new data source.
If you do this, consider deleting the old data source and notifying users that they should use the new one.

== Data API authentication and authorization

The link:140b48[Domino Data API] is used to access Domino data sources.
When you use the Data API from a Domino execution, your user identity is verified automatically to enforce Domino permissions.
The library attempts to use a link:40b91f[Domino JWT token], or, if not available, a link:40b91f[user API key].

The following is a summary of the user identity that will be used for access to a data source based on the Domino execution type:

[[tr15]]
// Data Sources in Workspaces
[[tr16]]
// Data Sources in Jobs
* *Workspaces and Jobs*: The user who started the execution.
[[tr17]]
// Data Sources in Launchers
* *Launchers*: The user who started the launcher regardless of who created the launcher.
[[tr18]]
// Data Sources in Apps
* *Domino Apps*: The user who published the app regardless of who is accessing the app.
[[tr19]]
// Data Sources in Domino endpoint
* *Domino endpoint* - No user identity.

For Domino endpoints and other advanced use cases that require establishing a different user identity, you inject an API key into an execution with an environment variable. Then, you can use it explicitly when retrieving a data source.

For detailed information, see link:490a1c[Authentication] in the API documentation.

== Security for direct connections to external data stores

When directly accessing an external data store, it is up to you to configure the appropriate authentication and access control.

In cases of directly accessing common AWS data stores (such as S3, Redshift, or Postgres) within an AWS deployment, link:eb6a88[Domino can propagate an IAM token] down through your query.

// Can we add any useful code examples here?

== Next steps

* link:fbb41f[Create data sources]
* link:fa5f3a[Work with data sources]

----- user_guide/share-data/share-datasets.txt -----
:page-version: 6.1
:page-permalink: 8f5b7e
:page-title: Share Datasets securely
:page-order: 10

Dataset owners can grant access to Domino users and groups.
Only authorized users can import and access a dataset.
Additionally, the actions that a user can perform on a dataset are determined by their Domino role.

[[tr1]]
When multiple users collaborate on a project, a data source used by one user might not be properly configured for another.
Domino proactively alerts you about such problems on the project's *Data* page and the *Data* tab in Domino workspaces.

[[tr10]]
// When user doesn't have access to the Data Source added to the Project he should see Data Source status Request access
[[tr11]]
// When User has access to the Data Source but did not authenticate yet he should see Data Source status Add Credentials
Domino notifies you when you do not have permissions or when you have not configured your individual credentials.
When you see this notification, request access from the data source owner.

== Set permissions to share a Dataset

You can change the permissions on a dataset to do the following:

* Add new users or organizations.
* Update roles for existing users or organizations.
* Delete users or organizations.

=== Change who can access a Dataset

. In the navigation pane, click *Data*.
. Click *Domino Datasets*.
. In the *Your Domino Datasets* area, go to the end of the row for the dataset whose permissions you want to change.
. Click the three vertical dots and click *Edit Permissions*.
. Enter the users or organizations to give them permission to the dataset.
+
NOTE: If you want to give all project members access to the dataset, click the *Add all project members* link.
. Specify the user or organization's role.
Dataset roles are independent of Domino's global user roles; see below for details about dataset roles.
. Click *Add*.
+
NOTE: In the *Current Permissions* area, you can modify the role as needed, or click the trash icon to delete permissions.

. Click *Save Permissions*.
+
NOTE: You must restart executions to pick up permission changes.

[[dataset-roles]]
== Dataset roles

Datasets have three roles: Owner, Editor, and Reader.
These are independent of Domino's global link:2611b7[user roles].
Roles determine the permissions that users have when they work with a dataset.

The table below describes the roles and access.

NOTE: The *Relates to Project Role* column is only useful if you are migrating from versions of Domino earlier than 5.4 where datasets were integrated with projects.

// What's below should probably be in static/content-reuse, and reused here and in the admin guide.

[cols="1a,2a,6a",options="header"]
|===
|Role | Relates to Project Role|  Description

|Owner
|Dataset Author
|
* Upload and view files, update metadata.
* Take, update, and view snapshots.
* Mark datasets and snapshots for deletion.
* Restore datasets and snapshots for deletion.
* Copy a snapshot to a new dataset.
* Download files and folders.
* Mount a dataset as read-write to link:942549[executions].
* Edit the dataset's permissions.

|Editor
|Project Owner, Project Contributor
|* Upload and view files, update metadata.
* Take, update, and view snapshots.
* Mark datasets and snapshots for deletion.
* Restore datasets and snapshots for deletion.
* Copy a snapshot to a new dataset.
* Download files and folders.
* Mount a dataset as read-write to link:942549[executions].

|Reader
|ResultsConsumer, ProjectImporter, LauncherUser
|* View files.
* View snapshots.
* Copy a snapshot to a new dataset.
* Download files and folders.
* Mount a dataset as read-only to link:942549[executions].

|===

== Shareable data in your Project

If you link:e6ed48[export and import project content] to share data with other members of your team, the consumers of your project will receive the entire contents of your project files in their Runs and Workspaces.
That works well if your project is small, simple, and narrowly scoped.

However, for large projects that produce many data artifacts, you might want to expose them to your consumers in smaller, curated subsets.
You can do this with Domino datasets.

Consider a project with a small folder full of code and nine folders with various kinds of output data, labeled `experiment1` through `experiment9`.
Each data folder is larger than 10GB, and the whole project is 100GB.
It would be impractical to ask your data consumers to import this project, but you also don't want to separate the data from the code that produced it by moving the data to a different project.

You can organize the data into datasets, with one dataset for each type of data in which your consumers are interested.
In this example, suppose you have two colleagues who want to consume your data.
One of them is only interested in the data from the `experiment1` folder, and the other is only interested in the data from `experiment9`.

You can link:0a8d11[create] and write two datasets with scripts like the following, where it's assumed you have named the datasets `experiment1-data` and `experiment9-data`.

.`experiment1-populate-dataset.sh`
[source,shell]
----
cp -R $DOMINO_WORKING_DIR/experiment1/. /domino/datasets/experiment1-data/
----

.`experiment9-populate-dataset.sh`
[source,shell]
----
cp -R $DOMINO_WORKING_DIR/experiment9/. /domino/datasets/experiment9-data/
----

Your consumers can then link:6942ab[mount] only the datasets in which they are interested.

If you are working with data at this scale, write it to datasets whenever you produce it, instead of storing it in your project files.


== Next steps

* link:0a8d11[Create datasets]
* link:ba5bad[Work with datasets]

----- user_guide/share-data/share-edv.txt -----
:page-version: 6.1
:page-permalink: 7ec608
:page-title: Share EDVs securely
:page-order: 30

This topic explains how security and sharing with external data volumes (EDVs).
EDVs can only be defined by Domino admins.
Users can mount EDVs in their projects.
Domino does not propagate the user's identity down into the underlying network-attached file system.

== EDV security settings

When an admin defines an EDV in Domino, they can specify two important security settings:

* *Mount as read-only*
+
Selecting this option means that users cannot modify the data in the EDV through Domino.
This is the recommended setting in most cases.

* *Volume Access*
+
This setting specifies which Domino users or groups are allowed to access the EDV.
This can be everyone in the Domino deployment, or only specific users and organizations.

== Share or get access to an EDV

Only an admin can modify the *Volume Access* setting for an EDV.
If an EDV is not available to all of the users who need to access it, contact your Domino admin.

To see whether an EDV is available to you, go to *Data* > *External Data Volumes* > *Add External Volume* in your project and check whether the EDV you need is listed.

== Next steps

* Learn more about link:0a8d11[EDV support].
* link:f12554[Work with EDVs] in your projects.

----- user_guide/troubleshoot/index.txt -----
:page-version: 6.1
:page-title: Troubleshooting guidelines for users
:page-sidebar: Troubleshooting
:page-permalink: 85722a
:page-order: 360

link:06c881[Troubleshoot Domino Models]::
Shows errors and how to fix them when working with Domino models.

link:f7292e[Work with Many Files]::
When working with a lot of files, Domino might run into the operating system's limit for the number of files that can be open. You can use a `.noLock` file to indicate that Domino must skip its normal step of locking your files during synchronization.

link:5c1ab6[Troubleshoot Imports]::
Troubleshoot common errors when working with imported file paths.

----- user_guide/troubleshoot/troubleshoot-domino-models.txt -----
:page-version: 6.1
:page-title: Troubleshoot Domino Models
:page-permalink: 06c881
:page-order: 10

If you need help troubleshooting link:8dbc91[Domino Models]:

[[tr5]]
// Errors are triggered as described

.dataTypeError: don't know how to serialize class
You might see this error with Python endpoints if you return values that are NumPy objects, rather than native Python primitives.
To fix this, convert your NumPy values to Python primitives.
Domino recommends that you call `numpy.asscalar` to do this.

.TypeError: <result> is not JSON serializable
The result of your endpoint function gets serialized to JSON before being sent in the response.
However some object types, such as Decimal or certain NumPy data types, are not JSON serializable by default.
For Decimal data types, cast the Decimal type to a Float type.
For NumPy data types, convert the values to Python primitives.
An easy way to do this is to call `numpy.asscalar` as described above.

.Exception in thread "Thread-9" java.io.IOException: Server returned HTTP response code: 500 for URL <model_url>
You might encounter this and other storage related errors in the build log when your model is too large.
Models are limited to 500MB.
If your project includes a large trained model dataset then Domino recommends that you exclude this upon publishing a new version of your model.

----- user_guide/troubleshoot/troubleshoot-imports.txt -----
:page-version: 6.1
:page-title: Troubleshoot imports
:page-permalink: 5c1ab6
:page-order: 30

When you execute imported files, each project directory in the importing project is located at `/mnt<project owner's username>/<project name>`.
The path for your project updates from `/mnt` to `/mnt<project owner's username>/<project name>`.
You might have to update file paths in your project or scripts to reflect these changes.

.Replace hardcoded file paths

If you have hardcoded any paths in your project to `/mnt`, replace the hardcoded paths with the environment variable, `$DOMINO_WORKING_DIR`.
This ensures that you have the correct path.
See link:6ac5a1[Domino Environment Variables] for more information.

.Use absolute paths in Python scripts

When you run Python scripts from an imported project, you might encounter the following error:

[source,text]
----
FileNotFoundError: [Error 2] No such file or directory:
----

Python scripts execute imported files from the current working directory.
If you have a relative path in the imported file, the script will try to find the file in the current directory and fail.

Update the Python script to use an absolute path based on the current location of the imported file.

For example, if you were to import `os`, update the path as follows:

[source,python]
----
import os
file_name = os.path.join(os.path.dirname(__file__), 'your_referenced_file.dat')
----

----- user_guide/troubleshoot/work-with-many-files.txt -----
:page-version: 6.1
:page-title: Work with many files
:page-permalink: f7292e
:page-order: 20

This topic describes how to work around problems that arise when your project has thousands (or more) of files.

.Domino gives you an error like, "too many open files"

To fix this, create a file in your project folder called `.noLock` ( the leading `.` is critical, and that capitalization matters).

*Additional information about the issue:*
When Domino synchronizes your files with the server, it takes out a lock on all your project files to ensure they don't change during the synchronization process.
If you have a large number of files, Domino runs into the operating system's limit for the number of files that can be open at once.
The `.noLock` file tells Domino to skip its normal step of locking your files during synchronization. You must be careful not to change any of your project files while Domino is uploading or downloading them.

----- user_guide/use-coding-assistants/configure-github-copilot-assistant.txt -----
:page-permalink: 00f51f
:page-version: 6.1
:page-iversion: 610
:page-title: Configure GitHub Copilot in VS Code
:page-sidebar: Configure GitHub Copilot in VS Code
:page-order: 240

Domino supports coding assistants such as link:https://marketplace.visualstudio.com/items?itemName=GitHub.copilot[GitHub Copilot^] to improve the development experience in VS Code Workspaces. Copilot is an AI-powered code completion tool that provides real-time suggestions as you type, ranging from single lines to full code blocks.

This guide explains how to install and use GitHub Copilot in two scenarios:

* VS Code running directly in a Domino Workspace
* VS Code connected to a Workspace via Remote SSH

== Prerequisites

Make sure your Workspace and GitHub account are ready for Copilot. You’ll need access to install extensions and upload files in a VS Code-compatible environment.

* A Domino Workspace that supports VS Code
* A link:https://github.com/features/copilot[GitHub account with Copilot access^]
* One of the following access methods:
** Direct access to VS Code from the Domino UI
** Remote SSH access to the Workspace (see SSH access setup)
* Ability to download and upload files into the Workspace
* VS Code must be installed in your Domino Environment
** If not, follow the Domino documentation link:fbb1db[to install VS Code]

=== Launch your workspace

Start by launching a Workspace that uses a VS Code-enabled Environment with GitHub Copilot installed.

. Open any Domino project and create a new VS Code Workspace.
. Select a Compute Environment that includes GitHub Copilot.
. Click *Launch* to start the Workspace.

== Set up GitHub Copilot

You can use Copilot in VS Code directly within Domino or through Remote SSH. Follow the steps below based on your access method.

=== For Remote SSH

If you're using the *VS Code Remote - SSH* extension to connect from your local VS Code to a running Domino Workspace:

. Install GitHub Copilot and Copilot Chat from the Microsoft Extension Marketplace in your local VS Code.
. Authenticate with GitHub when prompted (see below).
. Begin coding.

=== Set up for in-browser VS Code (code-server)

Follow the steps below to manually install Copilot in your Domino Workspace.

==== Step 1: Download the GitHub Copilot Extension

To install Copilot manually, you’ll need to download the .vsix files for both Copilot and Copilot Chat.

. Identify the required versions of link:https://marketplace.visualstudio.com/items?itemName=GitHub.copilot[Copilot^] and link:https://marketplace.visualstudio.com/items?itemName=GitHub.copilot-chat[Copilot-Chat^].
.. If unsure, locally install the same VS Code version Domino uses, install the extensions, and check the version numbers.

. Use the following URLs to download the `.vsix` files. Replace `COPILOT_VERSION` and `CHAT_VERSION` with your target versions:
+
[source,shell]
----
https://github.gallery.vsassets.io/_apis/public/gallery/publisher/github/extension/copilot/COPILOT_VERSION/assetbyname/Microsoft.VisualStudio.Services.VSIXPackage
----
+
[source,shell]
----
https://github.gallery.vsassets.io/_apis/public/gallery/publisher/github/extension/copilot-chat/CHAT_VERSION/assetbyname/Microsoft.VisualStudio.Services.VSIXPackage
----

. Upload the `.vsix` files into your Workspace through the Domino UI or mount a shared volume.

==== Step 2: Install the Copilot extension

Use the VS Code Extensions view to install the downloaded `.vsix` file directly in your Workspace.

. Open the *Extensions* panel:
.. Click the square icon in the left sidebar, or
.. Press *Ctrl+Shift+X*.
. Click the *⋯ (More Actions)* menu in the top-right corner of the *Extensions* panel.
. Select *Install from VSIX…*
. Locate and select the uploaded `.vsix` file.

==== Step 3: Authenticate with GitHub

After installing the extension, sign in with your GitHub account to activate Copilot and link it to your subscription.

. Once installed, a prompt appears in the bottom-right corner asking you to sign in to GitHub.
. Click *Sign in to GitHub* and follow the prompts.
. A GitHub login window opens with a device code.
. Click *Copy & Continue to GitHub*, then paste the code on the GitHub login page.
. Log into a GitHub account with an active Copilot subscription.
. Click *Authorize Visual-Studio-Code* to complete the setup.

Once authenticated, a small GitHub logo appears in the bottom-right corner of the VS Code interface. Click it to view Copilot status and logs. The status should show: *Status: Ready*.

==== Step 4: Enable Copilot (if not already active)

Sign in with your GitHub account to activate Copilot and link it to your subscription.

. Follow the prompts to sign in with your GitHub account.
. If required, generate a GitHub personal access token for authorization.
. Open the *Command Palette* (`Ctrl+Shift+P`), search for Copilot, and select *GitHub Copilot: Enable*.
. Configure any desired settings.

==== Step 5: Start coding

Once Copilot is enabled, begin typing in any editor window. Copilot will automatically offer suggestions:

* Press *Tab* to accept a suggestion.
* Press *Esc* to dismiss it.

Copilot works across many languages and is especially helpful when working with unfamiliar APIs or boilerplate code.

== Optional: Install Copilot automatically in Dockerfile

To add Copilot to your compute environment permanently, use these commands in your Dockerfile (replace `COPILOT_VERSION` and `CHAT_VERSION` with your target versions):

[source,dockerfile]
----
RUN curl -o /tmp/github.copilot.vsix -LSsf https://github.gallery.vsassets.io/_apis/public/gallery/publisher/github/extension/copilot/COPILOT_VERSION/assetbyname/Microsoft.VisualStudio.Services.VSIXPackage && \
    code-server --install-extension /tmp/github.copilot.vsix --force --extensions-dir /home/ubuntu/.local/share/code-server/extensions && \
    rm /tmp/github.copilot.vsix

RUN curl -o /tmp/github-chat.vsix -LSsf https://github.gallery.vsassets.io/_apis/public/gallery/publisher/github/extension/copilot-chat/CHAT_VERSION/assetbyname/Microsoft.VisualStudio.Services.VSIXPackage && \
    code-server --install-extension /tmp/github-chat.vsix --force --extensions-dir /home/ubuntu/.local/share/code-server/extensions && \
    rm /tmp/github-chat.vsix
----

== Next steps

* link:1f4149[Jupyter AI] provides an AI-powered assistant that you can configure to use with Domino.

----- user_guide/use-coding-assistants/configure-jupyterai-assistant.txt -----
:page-permalink: 1f4149
:page-version: 6.1
:page-iversion: 610
:page-title: Set up Jupyter AI in Jupyter environment
:page-sidebar: Set up Jupyter AI in Jupyter environment
:page-order: 230

Jupyter AI provides an AI-powered assistant that helps data scientists generate code, debug errors, summarize content, and interact with notebooks using natural language prompts. It works in both JupyterLab and notebook-based IDEs such as VS Code and Jupyter Notebook.

Jupyter AI supports multiple backends and integrates with models from providers such as OpenAI, Azure, Hugging Face, and SageMaker. Domino Standard Environments (DSEs) come with Jupyter AI pre-installed.

== Prerequisites

* A Domino Workspace running JupyterLab, Jupyter Notebook, or VS Code
* Access to a Domino Standard Environment (or a custom one with Jupyter AI installed)
* An API key from an LLM provider, such as OpenAI or Hugging Face, if using magic commands

== Ways to use Jupyter AI

You can use Jupyter AI in two modes:

* Chat-based assistant in JupyterLab (Jupyternaut)
* Magic commands in notebook-based IDEs

=== Option 1: Jupyternaut Chatbot (JupyterLab)

To launch the chatbot:

. Create a new Workspace using JupyterLab and a Domino Standard Environment.
. Click the chat icon in the left sidebar to open the assistant.
. Use the interface to ask questions, get explanations, or generate notebook content.

The link:https://github.com/jupyterlab/jupyter-ai[Jupyter AI documentation^] has more details about chatbot configuration.

=== Option 2 Magic commands in Notebook IDEs

In notebook-based IDEs like Jupyter or VS Code, you can access Jupyter AI using magic commands.

To get started:

. Launch a Workspace using Jupyter Notebooks or VS Code and a Domino Standard Environment.
. Obtain an API key from your LLM provider, such as OpenAI or Hugging Face. *Tip*: Use link:6ac5a1[Domino environment variables] to securely store API keys.
. Set the appropriate environment variable in a notebook cell.
. For OpenAI: `%env OPENAI_API_KEY=<your-api-key>`
. Load the extension: `%load_ext jupyter_ai_magics`
. Use `%ai` or `%%ai` to invoke the assistant.

The link:https://github.com/jupyterlab/jupyter-ai#magic-commands[Jupyter AI magic command documentation^] has a complete list of syntax and usage examples.

== Custom environment setup

If you’re not using a Domino Standard Environment (DSE), or if you want to preconfigure Jupyter AI for consistent use across projects and teams, you can install and configure it in a custom Compute Environment. This approach makes sure that:

* Jupyter AI is available by default in all Workspaces using the environment.
* API keys and assistant tools are pre-installed and ready to use.
* Users don't need to reconfigure magic commands or environment variables manually.

=== Step 1: Install Jupyter AI and rebuild JupyterLab

Add the following to your Dockerfile to install Jupyter AI and rebuild the JupyterLab interface to register the extension:

[source,dockerfile]
----
RUN pip install 'jupyter-ai[jupyterlab]' && \
    jupyter lab build
----

=== Step 2: Set your LLM API key as an environment variable

Add your preferred LLM provider’s API key to the Dockerfile:

[source,dockerfile]
----
ENV OPENAI_API_KEY=<your-api-key>
----

This makes the key available to Jupyter AI for authentication when making requests.

*Tip*: For sensitive credentials, consider using link:6ac5a1[Domino environment variables] instead of hardcoding them.

=== Step 3: Preload the magic command extension

To make sure that the `%ai` and `%%ai` magic commands are available without manual activation, configure IPython to load the extension at startup:

[source,dockerfile]
----
RUN ipython profile create && \
    echo "c.InteractiveShellApp.extensions=['jupyter_ai_magics']" >> /home/ubuntu/.ipython/profile_default/ipython_config.py
----

Once these steps are complete, any Workspace launched with this Compute Environment will have Jupyter AI ready to use, with no additional configuration required.

== Next steps

* Use link:00f51f[GitHub Copilot in VS Code] directly within Domino or through *Remote SSH*.

----- user_guide/use-coding-assistants/index.txt -----
:page-permalink: 5384dd
:page-version: 6.1
:page-iversion: 610
:page-title: Use coding assistants
:page-sidebar: Use coding assistants
:page-order: 220

Domino is designed to give data science teams the flexibility to use the tools they work with most effectively. By supporting your preferred IDEs instead of requiring a proprietary interface, Domino makes it easy to use coding assistants that are built to work within those environments.

== Prerequisites

This guide explains how to enable commonly used coding assistants in Domino. The general process includes:

=== Step 1: Choose an LLM backend

Determine which large language model (LLM) your organization has approved for use with source code. This could be a commercial offering like ChatGPT, or a self-hosted open-source model such as Llama.

Most modern coding assistants support configuration options that let you connect them to the LLM of your choice.

=== Step 2: Choose your IDE(s)

Select the IDEs where you want to enable coding assistants. Domino supports any compatible setup. The following combinations are recommended:

[cols="25%,50%,25%",options="header"]
|===
|IDE / Language
|Typical Usage
|Recommended Assistant

|link:1f4149[Jupyter / Python]
|Generate or revise code in notebooks via prompts.
|Jupyter AI

|link:00f51f[In-browser VS Code]
|Auto-complete and scaffold code inside Domino.
|GitHub Copilot

|link:00f51f[Local VS Code via SSH]
|Use Copilot with local VS Code connected to Domino compute. *SSH in Domino* can support other IDEs as well.
|GitHub Copilot
|===

=== Step 3: Modify your compute environment

Install the appropriate assistant tool in your Compute Environment and configure it to use your selected LLM.

Using a shared Compute Environment ensures all users benefit from a consistent, organization-approved setup.

== Next steps

* Use link:00f51f[GitHub Copilot in VS Code] directly within Domino or through *Remote SSH*.
* link:1f4149[Jupyter AI] provides an AI-powered assistant that you can configure to use with Domino.

----- user_guide/use-launchers.txt -----
:page-version: 6.1
:page-permalink: f4e1e3
:page-title: Use Launchers
:page-order: 310

// Original content: https://docs.google.com/document/d/1Zn_o-QZsoiGlyEd1O7wl-iha04a-enLYwOkVhUqHbaE/edit

[[Overview]]

// Briefly explain what this feature is, how it benefits customers, how it fits into the data science workflow, and how Domino does it. Each mention of another concept or feature should be linked inline to a relevant topic elsewhere in the docs.

Domino launchers can accelerate collaboration between data scientists and non-technical stakeholders to gather information, propose new data, perform tasks, or propose different parameters. Launchers are self-serve web forms that a data scientist quickly builds to expose pre-canned tasks to less technical users. 

Data scientists can automate analysis runs for others and empower business users to ask more questions and experiment independently. Launchers reduce the barrier of "asking a data scientist" and increase fruitful conversations between stakeholders. 

[[Launcher-fundamentals]]
== What is a Launcher?

A launcher is a web form. When data scientists build a launcher, they expose web UI controls (such as a textbox or a drop-down menu) to a consumer who runs the launcher.

When a consumer completes the web form and runs the launcher, Domino passes the inputs into a script created by the data scientist as command-line parameters that initiates a job in Domino.

The results from the launcher depend on the script. Data scientists have the flexibility to create any result they need: PDFs, images, rendered notebooks, HTML, and so on. See link:#Advanced-tips-and-tricks[tips and tricks] for ideas to take advantage of this flexibility. Because launchers execute jobs under the hood, each run of a launcher is tracked, fully reproducible, and auditable.

The launcher user reviews the results through an email notification and the Jobs dashboard.

[[Build-a-Launcher]]
== Build a Launcher as a data scientist

In your project, go to *Deployments > Launchers > New Launcher* and fill in the fields in the launcher wizard to create a new launcher.

The essential component of a launcher is the command.  The command runs as a job, with placeholder values for the command-line arguments.

Each argument you want to expose to the consumer must be written as `${arg}`. You can mix hard-coded parameters with consumer-input parameters:
[source,shell]
----
my_script.py -x=1 ${file} ${start_date}
----

Launchers support these input controls:

* *Text* - Input arbitrary text.
* *Select* - Select one item from a list.
* *Multi-select* - Select multiple items.
* *Date* - Select a date.
* *Checkbox* - Specify a binary yes/no value.
* *Upload file(s)* - Upload one or more files that are passed to your script.

NOTE: Domino captures results from the launcher run like it does from a job: any file your script creates or modifies. Commonly, launchers generate images or even HTML, which is included in email notifications sent to recipients and rendered in the results view for the associated job.

=== How Domino passes arguments to your script

When parameter values are executed in a job started by the launcher, the values are encased in single quotes.
This quotation preserves spaces and other special shell characters in the values.
However, such values cannot be used to expand to environment variables.

File parameters are passed into your file as the path to the file. Note that uploaded files are persisted as results in the project. Domino projects are not designed to handle files greater than several GB in size or more than thousands of files.

Multi-select parameters are passed in as the comma-separated list of all selected choices.

=== Parse arguments in your script

// pull from existing documentation: https://docs.dominodatalab.com/en/4.2/user_guide/064ada/create-a-launcher/#tr7

// Code snippets in launcher documentation works as described

.R
[#R-1]
--

For R, use the
https://stat.ethz.ch/R-manual/R-patched/library/base/html/commandArgs.html[commandArgs
function^].

[source,r]
----
args <- commandArgs(trailingOnly=TRUE)
p1 <- args[1]
p2 <- args[2]

# a file upload parameter
print(readLines(args[3], n = 1))

# a multi-select parameter
for (each in strsplit(args[4],",")) {
  cat(each, sep="
")
}
----

--

.Python
[#Python-1]
--

For Python, use
http://www.tutorialspoint.com/python/python_command_line_arguments.htm[sys.argv^].

[source,python]
----
import sys
p1 = sys.argv[1]
p2 = sys.argv[2]
# a file upload parameter
with open(sys.argv[3], 'r') as f:
    print f.readline()

# a multi-select parameter
for part in sys.argv[4].split(","):
  print part
----

--

NOTE: File parameters are passed into your file as the path to the file.
Multi-select parameters are passed in as the comma-separated list of all selected choices.

See the detailed examples below.


=== Examples: handle arguments in R and Python

.R
[#R-2]
--
Our simple R example lets users input two numbers, and behind the scenes, we run some R code that adds them and prints the sum.
Our script is in a file called `launcher.R`, and we tell Domino to run
`launcher.R 10 20`. So, our launcher's command is
`launcher.R ${A} ${B}`.

[source,r]
----
args <- commandArgs(trailingOnly = TRUE)
a <- as.integer(args[1])
b <- as.integer(args[2])
if (is.na(a)) {
  print("A is not a number")
} else if (is.na(b)){
  print("B is not a number")
} else {
  paste("The sum of", a, "and", b, "is:", a+b)
}
----

// The example launcher is set up to take A and B as parameters.

// image::/images/4.x/editsumlauncher.png[alt="Edit the launcher", width=600]

// When operational, the user sees this:

// image::/images/4.x/editsumlauncher.png[alt="Edit the launcher", width=600]
--

.Python
[#Python-2]
--
This Python example uses a script that creates an interactive scatter plot with https://github.com/bokeh/bokeh[Bokeh^] from a CSV file that anyone can upload with a web form.

The user provides this information:

* a file
* what to put on the X- and Y- axes
* some information about how to color the data points.

The Python script generates an interactive Bokeh scatterplot.

This code shows the complete Python script:

[source,python]
----
from bokeh.plotting import show, output_file
from bokeh.charts import Scatter
import pandas as pd
import sys

output_file("scatter.html")

data = pd.read_csv(sys.argv[1])
scatter = Scatter(data, x = sys.argv[2], y = sys.argv[3], color = sys.argv[4], legend = "top_left")
show(scatter)
----
--
// end of pull from existing documentation.


[[Run-a-Launcher]]
== Run a Launcher as a consumer

Navigate to the *Deployments > Launchers* section of a Project and click on a launcher.

Collaborators in the *Contributors* or *Launcher User* role can run a launcher. *Results Consumers* can view results from launchers but not run them.

An email with the results is sent to everyone on the notification list. A new job is also created in the Jobs dashboard, where the results and inputs are accessible.

[[Copy-Launcher-definition]]
== Copy Launcher definition

[[tr1]]
// View json definition of a Launcher via Advanced Edit

[[tr2]]
// Edit json definition of a Launcher via Advanced Edit

NOTE: This feature is only available in private deployments of Domino Enterprise.

Use the Advanced Launcher Editor to access the JSON representation of a Launcher.
This is useful if you want to copy Launcher definitions between projects.

. Go to the launcher.
. Click the gear icon and then click *Edit*.
. Click *Switch to JSON Edit Mode*.


[[Advanced-tips-and-tricks]]
== Advanced use cases, tips, and tricks

Combine with link:https://cran.r-project.org/web/packages/knitr/index.html[knitr^] for report generation::

Write an R script that knits a rich report in PDF, HTML, Markdown, etc., and flexibly expose parameters via a launcher to provide self-serve report generation to business users.

Combine with Domino's customized email notifications, e.g., link:5b84c5[email.html]::

If your launcher script generates an `email.html` file, that becomes the body of the notification email sent to the launcher consumer. Launchers work great to create custom visual results that are sent directly as email notifications.

Render Jupyter notebooks::

If you run a Domino job with the name of an `ipynb` file, Domino calculates the notebook and saves it. This automation gives you an easy way to let end users update a notebook without opening Jupyter. For more advanced output formats, create a shell script that uses nbconvert to render a notebook as HTML or a PDF and email that to the launcher consumer.

Process data::

Launchers are great when a user wants to upload a file and process the data, e.g., clean it and put it in a database.

Score or infer with explainability::

Launchers are great when a user wants to access input, run it through a model, and return the result with rich explainability data (provided via link:https://shap.readthedocs.io/en/latest/[shap^] or link:https://github.com/marcotcr/lime[lime^], for example).

[[launchers-on-remote-dataplanes]]
== Launchers on remote data planes

You may want to launch Jobs in a separate cluster or data plane from where the model was created, either for scalability or data locality reasons. Domino enables this workflow through its Launchers feature, allowing a Launcher to be deployed near where it adds the most value. When launching, select the hardware tier linked to your preferred local or remote data plane.

----- user_guide/work-with-netapp-volumes/add-remove-netapp-volumes.txt -----
:page-permalink: 306570
:page-version: 6.1
:page-title: Add or remove NetApp Volumes on projects
:page-sidebar: Add or remove NetApp Volumes
:page-order: 30

Attaching a NetApp Volume to a project ensures data safety for use in your apps, launchers, workspaces, or jobs. It also enables shared storage among multiple users and simplifies data backup. The workspace can still operate even when it stops or restarts. You'll need a role that allows you to see or work with a NetApp Volume to attach it to a project. If you have the *Reader* role, you can still attach the NetApp Volume, but you can only view the data.

You'll need a role that allows you to see or work with a NetApp Volume to attach it to a Project. If you have the Reader role, you can still attach the NetApp Volume, but you can only view the data.

== Add NetApp Volumes to Projects

Here is how you add NetApp Volumes to a Project:

. Close out any projects and go to the main Domino screen.
. From the toolbar, click *Data* > *NetApp Volumes*.
. Find the volume you want to add in the NetApp Volumes list.
. Click the three dots to the right and select *Add to project*.
. Use the menu to search for and select the project.
. Click *Confirm*.

If you want to add that volume to multiple projects, repeat the steps above.

== Remove NetApp Volumes from Projects

You may need to remove NetApp Volumes from projects if the volume has failed or is no longer needed. Here is how to remove NetApp Volumes:

. Open the *Project* with the NetApp Volume you want to remove.
. From the left panel, click *Data* > *NetApp Volumes*.
. Find the volume in the NetApp Volumes list.
. Click the three dots to the right and select *Remove from project*.

[[request-deletion]]
== Delete a NetApp Volume

You can delete your own NetApp Volumes if your Domino administrator has enabled that ability for your role. If not, you can mark a NetApp Volume for deletion by submitting a deletion request to your Domino administrator.

Once this action is completed, the data will be permanently lost. To mark a NetApp Volume for deletion:

. From the toolbar, click *Data* > *NetApp Volumes*.
. Find the volume in the NetApp Volumes list.
. Click the three dots to the right and: 
.. Select *Delete Volume*. 
.. Or select *Mark for deletion*.
. Read the confirmation message:
.. If you agree, click *Delete* or *Mark for deletion*.
.. To abort the action, click *Cancel*.

== Attach NetApp Volumes to workspaces or jobs

Attaching NetApp Volumes to your apps, launchers, workspaces, or jobs guarantees data safety. You can select these upon starting. Apps are available as *read-write* or *read-only* based on the permissions of the App owner.

* Users with the *Volume Owner* or *Editor* role may attach the volumes assigned to the project to workspaces or jobs within that project in *read-write* mode.
* Users with the *Reader* role should be able to attach NetApp Volumes currently assigned to the project to workspaces or jobs within that project in *read-only* mode.

All available and accessible volumes and snapshots should be mounted only when requested.

== Next steps

* link:e6887f[Create NetApp Volumes from Domino or a project] teaches you how to create and configure new NetApp Volumes from Domino or within a specific project.
* link:93fa12[View or edit details of a NetApp Volume] teaches you about viewing lists of NetApp Volumes, and editing the descriptions and permissions of them (only if you are the *Owner* or *Editor* of that NetApp Volume).

----- user_guide/work-with-netapp-volumes/create-netapp-volumes.txt -----
:page-permalink: e6887f
:page-version: 6.1
:page-title: Create NetApp Volumes from Domino or a project
:page-sidebar: Create NetApp Volumes
:page-order: 20

NetApp Volumes are created within an existing filesystem. You can create new NetApp Volumes within a specific project or from globally accessible data.

image::/images/6.0/user-guide/create-new-netapp-vol-empty.png[Create NetApp Volume, role=noshadow, width=500]

== Create a NetApp Volume from Domino

When you create a new volume from Domino's home page, you must manually link:306570[add the volume to a project].

. Open **Domino** to create a NetApp Volume.
. From the toolbar, click **Data** > **NetApp Volumes**.
. Click **Add NetApp Volume** > **Create Volume**.
. Enter the following details in the **Create NetApp Volume** window:
.. **Name*: Enter a name for the volume. Names must contain only letters, numbers, underscores, and hyphens.
.. **Description:** Enter a brief description of the volume.
.. **Data Plane:** Choose a data plane from the menu.
.. **NetApp Filesystem:** Choose a filesystem from the menu.
.. **Capacity:** Specify the maximum capacity of the volume.
. Click **Next**.
. link:#add-users-and-organizations[Add users or organizations] to your volume and assign their roles if needed.
. Click *Finish*.

== Create a NetApp Volume from a project

When you create a new volume within a project, it will automatically be added to that project.

. Open the **Project** where you want to create a NetApp Volume.
. From the left panel, click **Data** > **NetApp Volumes**.
. Click **Add NetApp Volume** > **Create Volume**.
. Enter the following details in the **Create NetApp Volume** window:
.. **Name:** Enter a name for the volume. Names must contain only letters, numbers, underscores, and hyphens.
.. **Description:** Enter a brief description of the volume.
.. **Data Plane:** Choose a data plane from the menu.
.. **NetApp Filesystem:** Choose a filesystem from the menu.
.. **Capacity:** Specify the maximum capacity of the volume.
. Click **Next**.
. link:#add-users-and-organizations[Add users or organizations] to your volume and assign their roles if needed.
. Click *Finish*.

[[add-users-and-organizations]]
== Add users or organizations to NetApp Volumes

There are different Roles preconfigured for use with your NetApp Volumes. If you are added to a volume and your assigned role is:

* **Reader:** You can view files and snapshots, and manage NetApp Volumes in projects as read-only.
* **Editor:** You can manage NetApp volumes by updating them, mounting them, marking them for deletion, or restoring them. You can also create and delete snapshots and manage users.
* **Owner:** You have all the capabilities of an Editor and the ability to update Volume grants.

image::/images/6.0/user-guide/volume-permissions.png[Set User Roles for NetApp Volume, role=noshadow, width=500]

To add users and set permissions for individuals or organizations, on the *Volume Permissions* screen:

. Type a few characters to select a user or organization from the menu.
. Select a *Role* for that user from the menu and click *Add*.
. Repeat steps 1-2 to add as many users or organizations as needed.
. Once you have added users, click *Finish*.

== Manage users on a NetApp Volume

You can add, remove, or update the roles of users later if needed:

. Open the *Project* with the NetApp Volume where you want to manage user roles.
. From the left panel, click *Data* > *NetApp Volumes*.
. Find the volume in the NetApp Volumes list.
. Click the three dots to the right and select *Edit permissions*.
. You can manage users by:
.. Removing users by clicking the delete icon.
.. Adding new users and assigning them roles.
.. Changing user roles as needed.
. Once finished, click *Save permissions*.

== Next steps

* link:306570[Add or remove NetApp Volumes from a project] shows you how to use the different roles that are preconfigured for use with your NetApp Volumes.

* link:93fa12[View or edit details of a NetApp Volume] teaches you about viewing lists of NetApp Volumes, and editing the descriptions and permissions of them (only if you are the *Owner* or *Editor* of that NetApp Volume).

* link:306570#request-deletion[Request deletion of a NetApp Volume] shows you why and how to request that an admin deletes a NetApp Volume.

----- user_guide/work-with-netapp-volumes/index.txt -----
:page-permalink: 06da1b
:page-version: 6.1
:page-title: Work with NetApp Volumes
:page-sidebar: NetApp Volumes
:page-order: 125

Domino Volumes for NetApp ONTAP (NetApp Volumes)  offer a native integration with NetApp’s enterprise-grade storage system. They allow you to mount NetApp Volumes from NetApp filesystems directly into Domino workloads, providing seamless access to data with the performance and efficiency benefits of ONTAP.

NetApp Volume snapshots enhance performance by minimizing data replication and reducing storage requirements during snapshot creation. 

image::/images/6.1/admin-guide/netapp-domino-diagram.png[alt="Domino Volumes for NetApp ONTAP", width=800]

Currently, link:https://docs.netapp.com/us-en/ontap/concepts/snapshot-copies-concept.html[NetApp Volumes^] support fast, cost-efficient snapshotting. Future releases will expose additional ONTAP capabilities through Domino, including cross-cloud synchronization, file-access auditing, and more.

By storing your data on an external NetApp volume, you can ensure that your file systems remain highly available and protected against accidental failures. NetApp Volumes offer reliable backup and recovery capabilities.

A NetApp ONTAP Filesystem connects a dataplane and Kubernetes storage class to a named storage container for volumes. The ONTAP Filesystem offers:

* Clear visibility into where volumes are stored.
* Support for multiple accounts and dataplanes, which is essential given the per-account limits on volume creation.
* Improved deletion flow by allowing volumes to be moved to cold storage, instead of being permanently deleted.

This helps enforce retention policies and reduces the risk of accidental data loss.

== What you can do

link:e6887f[Create NetApp Volumes]::
Create and configure new NetApp Volumes from Domino or the context of a specific project.

* Set up volume name, description, and access roles.
* Choose snapshot and synchronization options.

link:306570[Manage NetApp Volume access in Projects]::
Add or remove existing NetApp Volumes from any project:

* Attach volumes to enable data access within the project
* Detach volumes when no longer needed or to restrict access.

link:7fb123[Understand NetApp Volume roles]::
Learn about the built-in role types and their permissions:

* *Owner*: Full control, including permission editing, deletion requests, and creating instantaneous snapshots.
* *Editor*: Can modify descriptions, manage shared access, and read and write access.
* *Reader*: View-only access to the volume's contents.

link:93fa12[View and edit Volume details]::
Access your list of NetApp Volumes where you have permissions:

* View volume metadata and configuration.
* Edit description and permissions (Owner or Editor only).

link:306570#request-deletion[Request NetApp Volume deletion]::
If a volume is no longer needed:

* Delete the NetApp Volume or submit a deletion request to your Domino administrator.
* Ensure the volume is no longer attached to active projects or in use by users.

NetApp Volume snapshots improve performance by minimizing data replication and reducing storage requirements during creation.

----- user_guide/work-with-netapp-volumes/view-and-edit-netapp-volumes.txt -----
:page-permalink: 93fa12
:page-version: 6.1
:page-title: View and edit NetApp Volumes
:page-sidebar: View and edit NetApp Volumes
:page-order: 40

You can view a list of NetApp Volumes where you have *Owner*, *Editor*, or *Reader* roles.
You can edit the description and permissions if you are the *Owner* or *Editor* of a NetApp Volume.

== View a list of usable NetApp Volumes

The *Data* menu in Domino lets you view a list of NetApp Volumes that are usable for your workspace.

. Close any projects and go to the main Domino screen.
. From the toolbar, click *Data* > *NetApp Volumes*.
. You'll see a screen with a list of available NetApp Volumes.

The information shown in this list view for each NetApp Volume includes its *Name*, *Owner*, and *Size*, as well as:

* *Projects*: Displays a list of projects that the NetApp Volume is associated with.
* *Snapshots*: This shows the number of snapshots taken of the NetApp Volume. Clicking on a number shows a list of these snapshots.
* *Data Plane*: This refers to the data plane to which the NetApp Volume is connected.

You can also view more granular details for individual NetApp Volumes.

== View details of a NetApp Volume

Here is how you view the details of NetApp Volumes from a project:

. Open the *Project* with the NetApp Volume you want to view.
. From the left panel, click *Data* > *NetApp Volumes*.
. Find the volume in the NetApp Volumes list and click it.

Viewable details include:

* *Size*: Displays the size of the NetApp Volume.
* *Location*: This shows the location of the NetApp Volume, whether local or otherwise.
* *Data Plane*: This refers to the data plane to which the NetApp Volume is connected.
* *Mount path*: The location that the data is written to and read from.
* *Description*: A brief description of the NetApp Volume's contents or purpose. If you have the *Editor* or *Owner* role, you can edit the volume description.

Details include the volume name, description, data plane, storage class, permissions, public status, and mount location during executions.

== Edit details of a NetApp Volume

You can rename the volume, edit the description, and change permissions. Changing the volume name alters its mount path, which can cause errors in scripts that rely on a specific volume name. If you are the *Owner* or *Editor* of a NetApp Volume, edit details by:

. From the toolbar, click *Data* > *NetApp Volumes*.
. Find the volume in the NetApp Volumes list.
. Click the name of the volume you want to edit.
. From the *Details* pane, you can click the **Description** edit icon to update the description.
.. Click *Update* when done or *Cancel* to exit the Description pane.
. Rename the NetApp Volume by clicking the three dots and choosing *Rename Volume*.
.. Click *Confirm* to accept the change or *Cancel* to exit.

== Take a snapshot of your data

You can snapshot your data without incurring duplication. To do so, you'll need either *Owner* or *Editor* permissions to snapshot all files in a NetApp Volume.

. From the toolbar, click *Data* > *NetApp Volumes*.
. Find the volume in the NetApp Volumes list.
. Click the name of the volume you want to snapshot.
. Click *Take Snapshot* near the top of the *Details* view.
. Expand the *Latest data* menu to view a list of snapshots for that volume.

== Delete a snapshot of a NetApp Volume

*Owners* or *Editors* of a NetApp Volume can _permanently_ delete snapshots without marking them for deletion. If you are a *Reader* of a NetApp Volume, you can only mark a snapshot for deletion.

. From the toolbar, click *Data* > *NetApp Volumes*.
. Find the volume in the NetApp Volumes list.
. Click the name of the volume you want to view.
. Expand the *Latest data* menu to view a list of snapshots for that volume.
. Locate the snapshot you want to delete and select it.
. Click *Delete Snapshot* or *Mark Snapshot for Deletion*.
. At the prompt, choose *Delete* to delete the snapshot or *Cancel* to exit.

== Next steps

* link:e6887f[Create NetApp Volumes from Domino or a project] teaches you how to create and configure new NetApp Volumes from Domino or within a specific project.

* link:306570[Add or remove NetApp Volumes from a project] shows you how to use the different roles that are preconfigured for use with your NetApp Volumes.

* link:306570#request-deletion[Request deletion of a NetApp Volume] shows you why and how to request that an admin deletes a NetApp Volume.

----- user_guide/workspaces/create-workspace-from-checkpoint.txt -----
:page-version: 6.1
:page-title: Create a Workspace from a checkpoint
:page-permalink: 938fbf
:page-order: 70

//Why are we calling this checkpoints when we never use this terminology on the UI? Why don't we just refer to this as commits? It is more confusing to add another term here. Tara

Checkpoints are commits that you can return to any time to review the history of your work, branch your work, or link:74f8ed[remediate models] that are drifting or decaying.
Checkpoints are created every time you synchronize changes to artifacts or code within a workspace.

You can preview artifacts or code from any commit to identify a checkpoint and use that to recreate a workspace.
When you recreate a workspace from a previous commit, Domino creates a new branch where you can develop or train a new model.

Durable workspaces are persistent development environments that you can start, stop, and restart.
Each workspace has a persistent volume that stores your files.
Your changes are saved from one workspace session to the next so that you can decide when to commit changes to version control.

This feature is supported for:

* Both Domino or Git-based projects.
* Workspaces created in 5.0 and later.

[[tr1]]
// As a Domino user, I can access datasets in a reproduced workspace
[[tr2]]
// As a Domino user, I can access external data volumes in a reproduced workspace
Datasets and external data volumes are recreated in the new workspace and branch that you create from a checkpoint.

You can review a workspace's commit history and then recreate a workspace from a specific checkpoint.

[[tr3]]
// As a Domino user, for a DFS project, I can view details of a workspace's commit history
[[tr4]]
// As a Domino user, for a Git-based project, I can view details of a workspace's commit history
. In your project, go to *Workspaces*.
. Click *History* on the workspace you want to recreate.
To determine which commit you want to use, in the *Files* column (for DFS projects) or *Artifacts* column (for git-based projects), click the commit ID to browse the code and artifacts from a checkpoint.
. To recreate a workspace, click *Open* next to the checkpoint to use.
. Click *Open* to start the new workspace.

To publish a Domino endpoint from the new workspace, see link:74f8ed[Remediate a Domino endpoint].

----- user_guide/workspaces/index.txt -----
:page-version: 6.1
:page-title: Use Workspaces
:page-permalink: 867b72
:page-order: 180

A Domino workspace is an interactive session where you can conduct research, analyze data, train models, and more.
Use workspaces to work in the development environment of your choice, like https://jupyter.org/[Jupyter^] notebooks, https://rstudio.com/[RStudio^], https://code.visualstudio.com/[VS Code^], and many other customizable environments.

link:a86443[Adjust Workspace settings]::
After creating a workspace, you can change the hardware tier, environment, volume size, and so on.

link:e82a92[Enable package persistence in an environment]::
Package persistence helps you save time by keeping installed packages across workspace sessions.

link:9976a5[Streamline Local Development with SSH]::
Connect your local development tools to a remote Domino workspace using SSH.

link:e6e601[Launch a Workspace]::
Create a workspace so you can use your preferred tools in a reproducible and customizable environment.
Learn how to link:fbb1db[select or configure VS Code] to be the workspace in an environment.
You can also link:3be678[set custom preferences] such as themes for RStudio workspaces.

link:262fef[Sync changes in a Workspace]::
You can synchronize changes made to files in the `/mnt` directory to a Domino project.

link:0002fb[Manage Workspaces]::
View, stop, resume, and delete Workspaces that you or your collaborators created.

link:164503[View Workspace details]::
Use the logs to see information about user actions, view current and historical Workspace resource usage, and view sessions and commits that were made in a Workspace.

link:0d2247[Use Git in your Workspace]::
Use Git-based projects or import Git repositories to use in your Workspace.

link:938fbf[Create a Workspace from a checkpoint]::
Select any commit to create a Workspace from.

link:2c01ae[Run multiple applications in a Workspace]::
Run multiple applications in the same Workspace session if needed.

link:02ba4d[Save work in a Workspace]::
If you save work to the `mnt` directory, it is persisted when the workspace is stopped and resumed.

----- user_guide/workspaces/launch-a-workspace/index.txt -----
:page-version: 6.1
:page-title: Launch a Workspace
:page-permalink: e6e601
:page-order: 20

Domino Workspaces function as a server.
You can use your preferred tools in a reproducible and customizable Environment.

== Basic configuration

[[launch]]
[[tr1]]
// As a Domino user, I can start a restartable workspace session using Jupyter IDE
. In your Project's navigation pane, click *Workspaces*.
. Click *+ Create New Workspace*.
. Enter a name for your Workspace.

== Select a compute environment

. Select an Environment for your Workspace.
You can click one of Domino's pre-defined Environments, or create a custom Environment.
See link:e46d54[Customize Environments] to learn more about managing Environments.
+
[[workspaces_er]]
By default, Domino uses the latest revision of your Environment.
To use another revision of the Environment:
+
--
.. In the *Workspace Environments* section, click *Change* to see the revisions for the Environment.
+
image::/images/6.0/workspaces/workspace-revision.png[alt="Environment revisions", width=600]

.. Click the revision of the Environment you want for your Workspace.
The version you selected displays in the *Revision* field.
+
NOTE: If you see the *Not Recommended* warning, then Domino recommends using the active revision set by the Project owner.
--

NOTE: link:0093e8#restricted_projects[Restricted Projects] may limit your choice of Environments.

== Select a Workspace IDE

. Select the integrated development environment (IDE) that you'll use in your Workspace (such as Jupyter).
The selected Environment determines the available IDEs.
See link:03e062[Add Workspace IDEs] to learn how to manage IDEs.

== Select a Volume Size

. If your administrator has enabled volume provisioning recommendations, you can select a *Volume Size* recommended by Domino based on your Project size and previous usage, instead of the default volume size.
+
NOTE: In a link:910370[Git-based Project], the first Workspace you launch always uses the volume size link:dba65c#workspace-and-jobs-volume-size[configured in your Project settings].
Subsequent Workspace launches receive volume provisioning recommendations.

[[select-a-hardware-tier]]
== Select a hardware tier

. Click next and select a link:dba65c#hardware_tier[Hardware tier].
+
A hardware tier represents the compute hardware -- CPU, memory, and other resources like GPUs -- used for your Run.
It can be a virtual instance in a cloud services provider, or a physical machine running in your deployment's on-premise data center.
Workspace scheduling requires a server with adequate free resources; otherwise, the Workspace queues until resources free up or a new server is available. Selecting a smaller hardware tier can speed up provisioning. Opt for the smallest tier that fulfills your task's needs.
+
In a hybrid deployment, hardware tiers in the dropdown are grouped by link:95520d[data plane], such as `GCP europe-central2` below.
The `Local` data plane corresponds to running the Workspace in the Domino control plane cluster.
+
image::/images/6.0/workspaces/hardware-tier.png[alt="Launch a Workspace and select a hardware tier", width=600]

=== Mount data

Domino shows you what data is available to be mounted to your Workspace, based on the selected data plane. To ensure faster startup times, only select the data that you need.

TIP: If you want to use a data mount that is not available, note which data planes it is available in, then return to the first step in the modal to select a hardware tier in the corresponding data plane.

[[workspace-file-prep-latency]]
TIP: When a Workspace, Job, or other execution starts, all project files -- including from the main DFS repo, main git repo (if applicable), and any imported git repos and Domino Projects -- must be transferred over the network to the Persistent Volume for the execution.
Large file counts or very large individual files can significantly delay the startup process.
In contrast, files in a Domino Dataset or External Data Volume do not require network transfer.
If your executions spend considerable time preparing files during startup, assess if files in the main DFS repo, main git repo (if applicable), and any imported git repos and Domino Projects must be in that repo.
If not, you can relocate these files to a Domino Dataset or External Data Volume to expedite startup.

== Attach compute clusters

. Click *Next* to attach a compute cluster or view additional details of the Workspace (or click *Launch* to skip those options).

. If necessary, attach a compute cluster to your Workspace.
To learn more about clusters, see the following:
+
** link:68faaa[Spark on Domino]
** link:d13903[On-demand Ray]
** link:747a51[On-demand Dask]
** link:d60880[On-demand Open MPI]
+
image::/images/6.0/workspaces/compute-cluster.png[alt="Attach a compute cluster", width=600]

== Launch the Workspace

. Click *Launch*.
A new tab opens and the Workspace starts loading.

[IMPORTANT]
====
* If you cannot access a newly launched Workspace, check your browser's configuration settings.
Your browser must allow pop-up windows from Domino.

* By default, Domino limits each user to four Workspaces per Project.
To change this setting, contact your Domino administrator.
====

[[tr2]]
// As a Domino user, I cannot launch more workspace sessions in a single project than the default or configured limit (of 4)

== Next steps

* link:fbb1db[Open a VS Code Workspace.]
* link:3be678[Set custom preferences for RStudio Workspaces.]
* link:9976a5[Streamline Local Development with SSH.]

----- user_guide/workspaces/launch-a-workspace/launch-a-vs-code-workspace.txt -----
:page-version: 6.1
:page-title: Open a VS Code Workspace
:page-permalink: fbb1db
:page-order: 10

The link:0d73c6[Domino standard environments] support opening https://github.com/microsoft/vscode[Visual Studio Code^] (VS Code) in link:e6e601[workspaces].
VS Code is an open-source multi-language editor maintained by Microsoft.
Domino can serve the VS Code application to your browser with the power of https://github.com/cdr/code-server[code-server^] from https://coder.com/[Coder.com^].

== Install VS Code extensions

You can install VS Code extensions in the following ways:

. Use the pre-run script.
. Use VS Code's extensions manager.

=== Use the pre-run script

. Go to the environment.
. Click *Edit Environment*.
. In *Pre Run Script*, paste the following:
+
[source,shell]
----
code-server --install-extension <extension-name> --extensions-dir ${HOME}/.local/share/code-server/extensions
----
. Click *Build*.

=== Use VS Code's extensions manager

Use the VS Code's extensions manager to install extensions from the marketplace.
You must build the extensions in your environment to make them available in every new VS Code workspace.
[[tr3]]
// As a Domino user, I can build persistent VS Code extensions into my environment

. Find the extension you want to install in the https://marketplace.visualstudio.com/[Visual Studio Marketplace^].
For example, install the https://marketplace.visualstudio.com/items?itemName=scala-lang.scala[scala-lang^] extension.

. Microsoft obscures the download URL for the extension by default.
Open your browser's development tools, then click *Download extension*.
+
image::/images/4.x/download-vscode-extension.png[alt="Download extension", width=1000]
+
. Get the download URL for the extension from the request details in your browser's development tools.
It ends with `/vspackage`.
Copy this URL.
+
image::/images/4.x/vscode-extension-url.png[alt="Get vspackage download URL from browser development tools", width=1200]
+
. In Domino, link:f51038[create a new environment].
As the base image, use one of the VS Code-equipped Domino standard environments, listed in the link:#prereqs[prerequisites].
. Add the following instructions to your environment's Dockerfile.
Replace the folder names and example `/vspackage` URL with the extension URL you retrieved previously.
These commands download the extension, extract the required files, and add them to the appropriate folder.
+

[source,dockerfile]
----
USER root

RUN apt-get update
RUN apt-get install -y bsdtar
RUN mkdir -p /home/ubuntu/.local/share/code-server/extensions/ms-python.python-2019.3.6558
RUN cd /home/ubuntu/.local/share/code-server/extensions/ms-python.python-2019.3.6558
RUN curl -JL https://marketplace.visualstudio.com/_apis/public/gallery/publishers/ms-python/vsextensions/python/2019.3.6558/vspackage | bsdtar -xvf - extension

RUN cd /home/ubuntu/.local/share/code-server/extensions/ms-python.python-2019.3.6558/extension/ && mv * ../
RUN chown ubuntu:ubuntu /home/ubuntu/.local/share/code-server/

USER ubuntu
----

. Click *Build*.
After a successful build, you can use this new environment to launch VS Code workspace sessions with the extensions already installed.

== Install VS Code

[[tr4]]
// As a Domino user, I can add VS Code to an environment that does not already include it

You can add VS Code to an environment that does not include it.
The base environment must be 2018-05-23 or newer.

. Add the following to your compute environment Dockerfile instructions:
+
[source,dockerfile]
----
USER root

# Install VS Code (we install an older version so the Python extension works)
RUN curl -fOL https://github.com/cdr/code-server/releases/download/v3.10.2/code-server_3.10.2_amd64.deb && 
    dpkg -i code-server_3.10.2_amd64.deb

# Add a VS Code start script
RUN mkdir -p /opt/domino/workspaces/vscode && 
    chown -R ubuntu:ubuntu /opt/domino/workspaces/vscode && 
    echo "#!/bin/bash" >> /opt/domino/workspaces/vscode/start && 
    echo "SETTINGS_DIR=${DOMINO_WORKING_DIR}/.vscode" >> /opt/domino/workspaces/vscode/start && 
    echo "FILE=${SETTINGS_DIR}/settings.json" >> /opt/domino/workspaces/vscode/start && 
    echo "# Add a user setting file if it doesn't exist. Add in DOMINO_WORKING_DIR so it persists across sessions" >> /opt/domino/workspaces/vscode/start && 
    echo "if [ ! -f "$FILE" ]; then" >> /opt/domino/workspaces/vscode/start && 
    echo "sudo mkdir -p "${FILE%/*}"" >> /opt/domino/workspaces/vscode/start && 
    echo "sudo chown -R ubuntu:ubuntu ${SETTINGS_DIR}" >> /opt/domino/workspaces/vscode/start && 
    echo "printf "{" >> /opt/domino/workspaces/vscode/start && 
    echo "	\"extensions.autoUpdate\": false," >> /opt/domino/workspaces/vscode/start && 
    echo "	\"extensions.autoCheckUpdates\": false," >> /opt/domino/workspaces/vscode/start && 
    echo "	\"python.pythonPath\": \"$(which python)\"," >> /opt/domino/workspaces/vscode/start && 
    echo "	\"workbench.startupEditor\": \"none\"," >> /opt/domino/workspaces/vscode/start && 
    echo "	\"workbench.colorTheme\": \"Default Dark+\"" >> /opt/domino/workspaces/vscode/start && 
    echo "}" > ${FILE}" >> /opt/domino/workspaces/vscode/start && 
    echo "fi" >> /opt/domino/workspaces/vscode/start && 
    echo "code-server ${DOMINO_WORKING_DIR} --user-data-dir ${SETTINGS_DIR} --auth none --bind-addr 0.0.0.0:8888 --extensions-dir ${HOME}/.local/share/code-server/extensions --disable-telemetry" >> /opt/domino/workspaces/vscode/start && 
    chmod +x /opt/domino/workspaces/vscode/start

USER ubuntu
----

. Add the following to your compute environment's *Pluggable Workspace Tools*:
+
[source,yaml]
----
vscode:
   title: "vscode"
   iconUrl: "https://raw.github.com/dominodatalab/workspace-configs/develop/workspace-logos/vscode.svg?sanitize=true"
   start: [ "/opt/domino/workspaces/vscode/start" ]
   httpProxy:
      port: 8888
      requireSubdomain: false
----


----- user_guide/workspaces/launch-a-workspace/open-rstudio-with-custom-preferences.txt -----
:page-version: 6.1
:page-title: Set custom preferences for RStudio Workspaces
:page-permalink: 3be678
:page-order: 20

Use the instructions below based on your RStudio version, to set custom preferences (such as themes, key mappings, languages, and dictionaries) for RStudio workspace runs. For more information on RStudio session settings, see link:https://docs.posit.co/ide/server-pro/admin/reference/session_user_settings.html#customizing-session-settings[RStudio's official documentation site].

== RStudio version 1.3+

RStudio versions 1.3+ use `/home/ubuntu/.config/rstudio/rstudio-prefs.json` to store user preferences. You can use a link:5dd2c1[pre run script in a custom compute environment] to modify this file to launch RStudio with custom preferences. To learn more about the schema of `rstudio-prefs.json`, see link:https://docs.posit.co/ide/server-pro/admin/reference/session_user_settings.html#customizing-session-settings[RStudio's official documentation on Customizing Session Settings^].

=== Method 1: Write lines to settings file
[[tr1]]
// As a Domino user, I can add persistent RStudio workspace settings by writing to a settings file using my environment's pre-setup script

If you know how to manually add configuration lines to the `rstudio-prefs.json` settings file, you can write it in the pre run script.
For example:

[source,shell]
----
# Variables
DIRECTORY="/home/ubuntu/.config/rstudio/"
# Check if directory exist, else create the directory
if [ -d "${DIRECTORY}" ]; 
then
    # Show directory
    echo "directory ${DIRECTORY} exists"
else
    echo "The specified directory does not exist"

    echo 'Creating directory...'
    mkdir -p "${DIRECTORY}"
    
fi

# Check if the preference file exist, else create the preference file
if [ -f "${DIRECTORY}/rstudio-prefs.json" ]; then
    echo "RStudio-prefs json exists"

else
    # Create Rstudio prefs json
    echo "Creating RStudio Preference json file"
    touch "${DIRECTORY}/rstudio-prefs.json"
fi
    
# add Rstudio Preferences
echo 'Adding Rstudio Preferences'
cat <<EOF > /home/ubuntu/.config/rstudio/rstudio-prefs.json
{
    "editor_theme": "Cobalt",
    "posix_terminal_shell": "bash"
}
EOF
----


=== Method 2: Copy a saved settings file
[[tr2]]
// As a Domino user, I can add persistent RStudio workspace settings by copying an existing settings file from my project to my environment's pre-setup script

Use this method to copy the settings from an existing RStudio session. This method is useful if you aren't familiar with the syntax of `rstudio-prefs.json`, or if you want to use preferences from an existing preference file.

. Run a session and modify the RStudio preferences as needed.
. Before you stop the session, use the following R code to copy the `rstudio-prefs.json` file to the root of your project directory.
+
[source, r]
----
file.copy("/home/ubuntu/.config/rstudio/rstudio-prefs.json", ".")
----
. Add the following lines to the link:5dd2c1[pre run script of your environment definition] to load the preferences file (if it exists) on subsequent runs:
+
[source,shell]
----
# After setting preferences in an RStudio session, run the following command:
# file.copy("/home/ubuntu/.config/rstudio/rstudio-prefs.json", '.')


DIRECTORY="/home/ubuntu/.config/rstudio/"
SETTINGS_FILE="./rstudio-prefs.json"


if [ -f "${SETTINGS_FILE}" ]; then
    echo "File exists"
    echo "Checking if directory exist"
    if [ -d "${DIRECTORY}" ]; 
    then
        # Show directory
        echo "Directory '${DIRECTORY}' exists"

        # Copy Rstudio prefs json
        echo "Copying '${SETTINGS_FILE}' to '${DIRECTORY}'"
        cp ${SETTINGS_FILE} ${DIRECTORY}
    else
        echo "The specified directory does not exist"

        echo 'Creating directory...'
        mkdir -p "${DIRECTORY}"

        # Copy Rstudio prefs json
        echo "Copying '${SETTINGS_FILE}' to '${DIRECTORY}'"
        cp ${SETTINGS_FILE} ${DIRECTORY}
        
    fi
fi
----




== RStudio version 1.2 and lower

Older versions (1.2 and lower) of RStudio use `/home/ubuntu/.rstudio/monitored/user-settings/user-settings` to store user preferences. To launch RStudio with custom preferences, you can use the pre-setup script in a custom link:f51038[compute environment] to  modify this file.

=== Method 1: Write lines to settings file
[[tr3]]
// As a Domino user, I can add persistent RStudio workspace settings by writing to a settings file using my environment's pre-setup script

If you know what to add to the settings file, you can write it in the pre-setup script.
For example:

[source,shell]
----
# create the encompassing directory if it doesn't exist
mkdir -p /home/ubuntu/.rstudio/monitored/user-settings/
# write the theme to the preferences file inside the directory
echo 'uiPrefs={"theme" : "Mono Industrial"}' >> /home/ubuntu/ .rstudio/monitored/user-settings/user-settings
chown -R ubuntu:ubuntu /home/ubuntu/.rstudio <3>
# modifies a Domino script that would overwrite this settings file.
if [ -f .domino/launch-rstudio-server ]; then
    sed -i.bak 's# > ~/.rstudio/monitored/user-settings/user-settings# >> ~/.rstudio/monitored/user-settings/user-settings#' .domino/launch-rstudio-server <4>
    chown ubuntu:ubuntu .domino/launch-rstudio-server <3>
fi
----

=== Method 2: Copy a saved settings file
[[tr4]]
// As a Domino user, I can add persistent RStudio workspace settings by copying an existing settings file from my project to my environment's pre run script

Use this method to copy the settings from an existing RStudio session. This method is useful if you aren't familiar with the syntax of `rstudio-prefs.json`, or if you want to use preferences from an existing preference file.

. Run a session and modify the RStudio preferences as needed.
. Before you stop the session, use the following R code to copy the `user-settings` file to the root of your project directory.
+
[source, r]
----
file.copy("/home/ubuntu/.rstudio/monitored/user-settings/user-settings", ".")
----
. Add the following lines to the pre-setup script of your environment definition to load the preferences file (if it exists) on subsequent runs:
+
[source,shell]
----
if [ -f user-settings ]; then
    mkdir -p /home/ubuntu/.rstudio/monitored/user-settings/
    cp user-settings /home/ubuntu/.rstudio/monitored/user-settings
    sed -i.bak '/initialWorkingDirectory=/d' /home/ubuntu/.rstudio/monitored/user-settings/user-settings
    chown -R ubuntu:ubuntu /home/ubuntu/.rstudio
    if [ -f .domino/launch-rstudio-server ]; then
        sed -i.bak 's# > ~/.rstudio/monitored/user-settings/user-settings# >> ~/.rstudio/monitored/user-settings/user-settings#' .domino/launch-rstudio-server
        chown ubuntu:ubuntu .domino/launch-rstudio-server
    fi
fi
----

NOTE: The `sed` statement that deletes the `initialWorkingDirectory` variable ensures that your session starts with the correct working directory.

----- user_guide/workspaces/launch-a-workspace/streamline-local-development-ssh.txt -----
:page-permalink: 9976a5
:page-version: 6.1
:page-title: Streamline Local Development with SSH
:page-sidebar: Streamline Local Development with SSH
:page-order: 30

You can securely connect your local development tools to a remote Domino workspace using SSH. This allows you to interact with workspace files and computing resources directly from your local applications; improving productivity and enabling powerful remote workflows.

You can use SSH access with any local development tool, including:

* *VS Code* to browse and edit code files directly from your local IDE.
* *ParaView* to stream rendered visualizations from a remote workspace to your desktop client.
* Use the *Remote - SSH* extension if you import your VS Code extensions into *Cursor*.

This guide covers setup and connection steps for both tools.

== Prerequisites

To use SSH with Domino workspaces, make sure you have the following:

* Your Domino workspace is based on an SSH-enabled environment. For example, `openssh-server` is installed.
* `sshd` is running in the workspace.
* You have installed the new Domino CLI to authenticate and connect via SSH.
* You have started a workspace and have the *Workspace ID* (available in the workspace settings pane or under *Show Details*).
* Your local machine supports SSH and the necessary development tools:
** For *VS Code*: the *Remote - SSH* extension
** For *ParaView*: a local installation of *ParaView Desktop*
* For most IDEs compatibility, make sure to include a Domino SSH configuration file into your local SSH configuration.
** For Linux/Mac users, just add the line `Include ~/.domino/ssh/config` to your `~/.ssh/config` file.

== Connect with VS Code

Using VS Code’s *Remote - SSH* extension, you can access a Domino workspace directly from your local editor. This enables you to browse, edit, execute, and debug files.

Run the Domino connect command on your local machine (replace `<workspace_id>` and `<your-domino-url>` accordingly):

[source,shell]
----
dom connect <workspace_id> --domino-api-host=https://<your-domino-url> -l ubuntu
----

Follow the command's instructions. It will return:

* An SSH command
* A path to a configuration file

Next, configure VS Code:

. Copy the path to the config and include it in your `~/.ssh/config` file.
.. Or you can add the `~/.domino/ssh/config` into the *Remote.SSH: ConfigFile* settings on the Remote SSH extension.
. Open VS Code. From the sidebar, choose *Remote Explorer* > *SSH*.
. Select your workspace from the list (as configured by the `dom connect` output).

You are now connected to your remote workspace within VS Code and can work as if the files were local.

The link:https://code.visualstudio.com/docs/remote/ssh#_connect-to-a-remote-host[VS Code documentation^] provides full instructions for connecting to a remote host.

== Connect with ParaView

link:https://www.paraview.org/documentation/[ParaView^] allows offloading rendering to a remote server and streaming visualization results to your local machine. Domino workspaces can serve as that remote backend.

. Ensure your workspace environment has ParaView installed and SSH is enabled.
. Start the ParaView server inside the workspace:
.. In the environment's pre-run script: `pvserver`
.. Or manually with the terminal.

The default bind port is `11111`.

. Establish an SSH connection from your local machine:
+
[source,shell]
----
dom connect <workspace_id> --domino-api-host=https://<your-domino-url> -l ubuntu
----

. Add port forwarding to expose the remote ParaView server locally. If you’ve included this config in your default SSH file, the `-F` flag is optional:
+
[source,shell]
----
ssh -L 11111:localhost:11111 <workspace-id> -F /path/to/ssh/config
----

. Connect from your local ParaView Desktop:
. Go to *File* > *Connect* > *Add Server*.
. Fill in the following:
.. *Server Name*: any name
.. *Server Type*: Client / Server
.. *Host*: `localhost`
.. *Port*: `11111`
. Click *Configure* > *Startup Type: Manual* > *Save*.
. Select the server and click *Connect*.

You are now visualizing remote data with local interactivity.


== Connect with PyCharm

Run the Domino connect command on your local machine (replace `<workspace_id>` and `<your-domino-url>` accordingly), and add the `--port` argument to specify a custom local port:

[source,shell]
----
dom connect <workspace_id> --domino-api-host=https://<your-domino-url> -l ubuntu --port 2223
----

Follow the command's instructions. It will return:

* An SSH command
* A path to a config file. Make sure this file is included in your local SSH configuration file.

Next, configure PyCharm project:

. Open PyCharm and create a new Project under *Remote Development* > *SSH*.
. Set *Username* to the workspace linux user (same as the `-l` argument of `dom connect`, for example `ubuntu`).
. Set *Host* to the `<workspace_id>`.
. Set *Port* to the one you specified in the `--port` argument of `dom connect`, for example `2223`.
. Follow the wizard steps to connect and open your files.

Some considerations:

* Make sure to use the same `--port` next time you wish to connect to the same workspace so that you can use the same PyCharm configuration.
* If you need to access multiple workspaces at the same time, use different `--port` values for each so you can reference them in different PyCharm configurations.


== Custom TLS certificate authority

If your Domino instance is using custom CAs in the TLS certificate signatures, make sure you also install the certificates to your local environment.

On Mac you can use `sudo security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain your_certificate.crt`.
On windows you need to import the certificate into the Trusted Root Certification Authorities store using the Certificate Manager tool.


== Next steps

* link:f51038[Learn how to manage compute environments] in Domino, whether you need to create an environment, use one provided by Domino, or modify an existing environment.
* link:e82a92[Enable package persistence in an environment] to save time by keeping installed packages across workspace sessions.

----- user_guide/workspaces/manage-workspaces.txt -----
:page-version: 6.1
:page-title: Manage Workspaces
:page-permalink: 0002fb
:page-order: 40

== View Workspaces

[[view-workspaces]]
[[tr11]]
// As a Domino user, I can view a list of the workspaces I have started in a project

=== View the Workspaces that you created in a Project

. In your project, go to *Workspaces* > *My Workspaces*.

=== View all Workspaces in your Project

These workspaces might have been created by you, or other users.
[[tr12]]
// As a Domino user, I can view a list of all workspaces started by me and other users in a project

. In your project, go to *Workspaces* > *All Workspaces*.

=== View Workspaces in all your Projects
[[tr13]]
// As a Domino user, I can view information about running and stopped workspaces for each project in my Projects List page

. From the top navigation pane, click *Develop > Workspaces*.
. Here you'll see all workspaces, their status, and what project they are tied to.


[[view-deleted-workspaces]]
=== View deleted Workspaces
[[tr14]]
// As a Domino user, I can view all deleted workspaces in a project

. In your project, go to *Workspaces* > *Deleted*.

[[stop-workspace]]
== Stop a Workspace

[[tr8]]
// As a Domino user, I can stop a running workspace
You can stop a workspace and resume it later.
Frequently stopping and resuming a workspace session is a good way to manage compute costs.
Storage costs (for example, Amazon EBS), however, will continue to incur.

[IMPORTANT]
====
Stopping a workspace does not automatically sync your work to Domino.
See link:262fef[Sync Changes] to sync your work.
====

[[tr9]]
// As a Domino user, I can stop a workspace with active changes and those changes will not be automatically synced to the project Files

To stop a Workspace:

. Click *Stop*.
A window opens with additional information about stopping the workspace.

. Click *Stop My Workspace*.

When you stop your workspace, the following settings persist and will be available to you when you resume your workspace session:

* Files saved in the `/mnt` directory

The following settings will not persist and will reload when you resume your stopped workspace:

* Files outside the `/mnt` directory, including installed packages
* Objects in memory
* Datasets

If you're using a link:910370[Git-based project] and someone updates or deletes the credentials to the Git repository while your workspace session is stopped, the credentials will be updated when you resume your workspace.

== Resume a Workspace

You can restart a workspace that you stopped previously.
Saved changes are available and can be link:262fef[synced].

[[tr10]]
// As a Domino user, I can restart a stopped workspace and confirm that unsynced changes are still present

. In your project, click *Workspaces*.
. Go to the stopped workspace.
. Click *Start*.
The workspace opens in a new tab.


NOTE: When a workspace is restarted in a link:0093e8#restricted_projects[restricted project], Domino uses the current restricted revision of the environment. This revision may be different from the revision that was used to start the workspace previously.

== Delete a Workspace

[[tr15]]
// As a Domino user, I can delete a stopped workspace


[WARNING]
====
* Ensure that all your work is link:262fef[synced to Domino] before deleting a workspace.
Failure to do so will result in unrecoverable work.
* When you delete a workspace, all its snapshots are also deleted and cannot be recovered.
====


[[tr16]]
// As a Domino user, I cannot delete a running workspace
You can delete a workspace if it's no longer needed.
You must stop the workspace before it can be deleted.



[[tr17]]
// As a Domino user, I can stop a workspace from the workspaces list/dashboard page
. Go to the project that contains the workspace to delete.
. In the navigation pane, click *Workspaces*.
. Click *Stop* to stop the workspace.
. Click the red trash icon to delete the workspace. It will appear greyed out until all persistent volumes are detached.

. Click *Delete* to confirm that you'd like to delete the workspace.
Click the *Delete* tab to link:0002fb#view-deleted-workspaces[view deleted workspaces].


[[auto-delete]]
=== Auto-delete Workspaces

If your administrator has enabled the workspace auto-deletion feature, Domino automatically deletes workspaces that have been stopped for 10 days plus a 30-day grace period. Your administrator can modify these periods.

Domino notifies you (in the application and through email) before your workspace is deleted.

. The first notification indicates that your workspace has been marked for deletion and the grace period has started.
. Halfway through the grace period, Domino notifies you again and indicates whether the workspace has uncommitted changes.
. Two days before auto-deletion, Domino notifies you again, including whether the workspace has uncommitted changes.
. Finally, Domino notifies you when the workspace has been deleted.

To remove a workspace from the auto-deletion schedule, restart it.
The auto-delete period restarts the next time the workspace is stopped.


----- user_guide/workspaces/package-persistence-for-workspaces.txt -----
:page-permalink: e82a92
:page-version: 6.1
:page-title: Use Package Persistence in Domino
:page-sidebar: Use Package Persistence
:page-order: 25

Package persistence helps you save time by keeping installed packages across workspace sessions. Whether you're working in Python or R, you no longer need to reinstall your packages every time you start or restart a workspace, as long as the environment and workspace are configured to support it.

In addition to speeding up workspace launches, package persistence contributes to:

* *Better governance and compliance*, since environment updates can be cascaded through derived environments.
* *More robust automation*, with the ability to test updated environments before production workloads adopt them.
* *Easier troubleshooting and iteration*, as new environment revisions can be built from any existing base.

This guide explains how to enable and use package persistence, both from the environment settings and from within an active workspace.

== Enable package persistence in an environment

To use package persistence, it must first be enabled in an environment.

. Go to the **Revisions** tab of your compute environment.
. Create a new revision or edit an existing one.
. Check the box for **Enable Package Persistence**.
+
If unchecked, this feature will not be available in workspaces using the environment.

The *Package Persistence* checkbox in the workspace UI only appears if the environment supports it.

== Use package persistence in a workspace

Once package persistence is enabled in the environment, you can use it directly in your workspace.

=== Install packages

Any packages you install manually will persist across workspace sessions:

[cols="1a,2a",options="header"]
|===
|Language
|Install Command

|Python
|`pip install <package-name>`

|R
|`install.packages("<package-name>")`
|===

You can also use the link:https://packagemanager.posit.co/client/#/[Posit Package Manager^] to install R packages.

=== What happens to requirements.txt

* Domino automatically runs `requirements.txt` at startup.
* Before package persistence, this reinstalled packages every time.
* With persistence enabled, already-installed packages are skipped, speeding up workspace launch.

=== Control persistence on restart

When you restart a workspace that uses an environment with persistence enabled, you'll see a Package Persistence checkbox:

* Turn it on to keep your installed packages across sessions.
* Turn it off to start clean.

== Enabling package persistence for external environments

If you're using a custom (external) environment, you'll need to install the appropriate tools to support package persistence.

[cols="1a,1a,2a",options="header"]
|===
|Language
|Tool
|Installation Details

|Python
|uv
|Follow the link:https://docs.astral.sh/uv/#installation[uv installation guide^] to add it to your base or environment image.

|R
|renv
|Add this line to your Dockerfile: +
`RUN R -e "install.packages('renv')"`
|===

Once installed, Domino will be able to track and persist packages installed within workspaces using these tools.

== Home directory persistence

Domino also supports persistence of the `$HOME` directory, allowing you to retain user-specific files across workspace sessions. This is useful for preserving IDE settings, environment configs, or files related to tools like remote Ray clusters.

=== Enable $HOME directory persistence

* When launching a workspace, toggle **Home Directory Persistence** to retain the contents of the `$HOME` directory between sessions.
* This setting ensures that any config files stored in `$HOME`, such as IDE preferences, `.ssh` keys, or cluster configs, are preserved.

By enabling home directory persistence, you can keep essential configuration files and tools available across sessions. This reduces setup time and helps maintain a consistent workspace environment. This is especially useful for advanced workflows that depend on personalized settings or external integrations.

// == Custom directory persistence

// Domino now supports persisting directories beyond `/home` as part of the workspace persistence feature. Set the `DOMINO_WORKSPACE_PERSISTENCE_DIRS` environment variable to a colon-separated list of absolute paths you want to persist between workspace sessions.

// [source,bash]
// ----
// DOMINO_WORKSPACE_PERSISTENCE_DIRS=/opt/conda
// ----

// The first time a directory is persisted, workspace startup may take longer. You can monitor progress and diagnostics via logs containing `[directory-persistence]`.

== Next Steps

* link:164503[View Workspace details] about user actions, view current and historical Workspace resource usage, and view sessions and commits that were made in a Workspace.
* link:0d2247[Use Git-based projects] or import Git repositories to use in your Workspace.
* link:2c01ae[Run multiple applications] in the same Workspace session if needed.

----- user_guide/workspaces/run-multiple-apps-in-a-workspace.txt -----
:page-version: 6.1
:page-title: Run multiple applications in a Workspace
:page-permalink: 2c01ae
:page-order: 80

For security reasons, Domino workspace sessions are only accessible on one port.
For example, Jupyter typically uses port 8888.
When you launch a Jupyter workspace session, a Domino executor starts the Jupyter server in a link:942549[Run], and opens port 8888 to serve the Jupyter application to your browser.
If you used the Jupyter terminal to start another application on a different port, it would not be accessible.

NOTE: Domino workspaces reserve port 80 for nginx and port 9000 for the executor.

However, you might want to run multiple applications in the same workspace session.
For example, you might want to:

* Edit and debug Dash or Flask apps live.
* Use Tensorboard to view progress of a live training job.

Domino supports this with https://github.com/jupyterhub/jupyter-server-proxy[Jupyter Server Proxy^] and https://jupyterlab.readthedocs.io/en/stable/[JupyterLab^].

== Prerequisites

* Python 3+
* Jupyter Server Proxy

== Install Jupyter Server Proxy

[[tr1]]
// As a Domino user, I can install Jupyter Server Proxy in my environment


By default, link:0d73c6[Domino standard environments] have https://github.com/jupyterhub/jupyter-server-proxy[Jupyter Server Proxy^] installed.
If you are not on the recent version of the link:0d73c6[Domino Standard Environment], use the following steps to install it
in your link:f51038[Domino environment].

Add the following lines to your environment's Dockerfile instructions.

[source,bash]
----
USER root

# Install NodeJS
# You can omit this step if your environment already has NodeJS installed
RUN curl -sL https://deb.nodesource.com/setup_16.x | bash - && 
   apt-get install nodejs -y && 
   rm -rf /var/lib/apt/lists/*

# Install jupyterlab-server-proxy (omit if already installed)
RUN if [ -x /opt/conda/bin/conda ]; then 
      /opt/conda/bin/conda install -y -c conda-forge jupyter-server-proxy; 
   else 
      pip install --no-cache-dir jupyter-server-proxy; 
   fi

# Update JupyterLab config
RUN mkdir -p /home/ubuntu/.jupyterlab/ && 
   echo "c.ServerProxy.servers = {" >> /home/ubuntu/.jupyterlab/jupyter_lab_config.py && 
   echo "    'code': {" >> /home/ubuntu/.jupyterlab/jupyter_lab_config.py && 
   # VS Code example. Replace this command with what you want to proxy to
   echo "        'command': ['code-server', '--port', '{port}', '--auth', 'none']" >> /home/ubuntu/.jupyterlab/jupyter_lab_config.py && 
   echo "    }" >> /home/ubuntu/.jupyterlab/jupyter_lab_config.py && 
   echo "}" >> /home/ubuntu/.jupyterlab/jupyter_lab_config.py

USER ubuntu
----

== Access your App

It can be accessed via a URL in the form:
`https://your-domino-url/project-owner/project-name/notebookSession/run-ID/proxy/app-port/`

This URL can be generated using Domino environment variables as follows:

[source,bash]
----
echo -e "import os\nprint('https://your-domino-url/{}/{}/notebookSession/{}/proxy/8501/'.format(os.environ['DOMINO_PROJECT_OWNER'], os.environ['DOMINO_PROJECT_NAME'], os.environ['DOMINO_RUN_ID']))" | python3
----

Ensure you change the `your-domino-url` and port number to match your deployment and app.
----- user_guide/workspaces/save-work-in-a-workspace.txt -----
:page-version: 6.1
:page-title: Save work in a Workspace
:page-permalink: 02ba4d
:page-order: 90

[[tr4]]
// As a Domino user, I can make changes to the /mnt directory in a workspace that are persisted when the workspace is stopped and restarted

When you click *Save* in the IDE of your workspace, your work is saved in the workspace.


NOTE: Saving your work does not push it to the link:ca786d[Domino project's] Domino File System (DFS).
To push the changes, you must synchronize your work to Domino.

----- user_guide/workspaces/set-up-a-workspace.txt -----
:page-version: 6.1
:page-title: Adjust Workspace settings
:page-permalink: a86443
:page-order: 20

Settings are available for individual workspaces.

== Edit workspace settings

[[tr3]]
// As a Domino user, I can change the settings for an existing restartable workspace and restart the workspace to take up the new settings

You can edit your workspace settings to use a different hardware tier or a different environment.

. Go to the workspace with the settings you would like to change.
. Click *Settings*.
. Click *Edit Settings*.
. Make the changes and click *Save & Restart*.


== Change the workspace volume size

[[workspace-volume-size]]
[[tr22]]
// As a Domino user, I can change the default project volume size in project settings from 10Gb to a different value and see that change reflected in a new workspace / workspace settings

// tag::volume-size[]

Volumes represent the storage space dedicated to your workspace or job.
The default volume size for all workspaces and jobs in Domino is 10GiB, link:71d6ad#workspaces[configurable by an administrator].

If your administrator enabled the feature, when you open a workspace you can select a volume size that is recommended by Domino based on your project size and previous usage.

NOTE: In link:910370[Git-based projects], the first workspace you open always uses the volume size configured in your project settings.
Subsequent workspace launches receive volume provisioning recommendations.

You can change the size of your volume if you find that your workspace (or job) requires more storage space.

=== Change the size of your Workspace volume

. Go to your project and in the navigation pane click *Settings*.
. In *Workspace and Jobs Volume Size*, enter the new volume size.
The default minimum volume size is 4GiB and the default maximum volume size is 200GiB.
Your Domino administrator can change the default minimum and maximum volume size limits.
. Click *Update*.
+
[IMPORTANT]
====
Changes to the volume size are applied to new workspaces.
They do not impact existing workspaces.
====

// end::volume-size[]


== Configure long-running workspaces

Domino shuts down long-running workspaces after a period link:71d6ad#long-running[defined by your administrator].
You can adjust this period to make it shorter.
If you make a change, it applies to all workspaces owned by you.

. Click your account name.
. Click *Account Settings*:
. Click *Workspace Settings*.
+
In the *Workspace Settings* section, you can see the automatic shutdown period defined by your administrator.

. Select
*Shutdown my long-running workspaces <n> minutes/hours after startup*.
+
NOTE: If you leave the checkbox cleared, the time limit configured by your administrator applies.

. Select a different time limit for long-running workspaces.
+
NOTE: The shutdown time is the time since the workspace was started.
The workspace shuts down automatically after it has been running for this period.

----- user_guide/workspaces/sync-changes-in-a-workspace.txt -----
:page-version: 6.1
:page-title: Sync changes in a Workspace
:page-permalink: 262fef
:page-order: 30


[[tr5]]
// As a Domino user, I can view the changes made to files in the /mnt directory of a workspace and sync those changes back to the DFS for the project using the Sync to Domino button in the Files section of File Changes

You can sync recent changes to your files or all changes.
Domino recommends that you sync your work at least daily.

[IMPORTANT]
====
If you're using a Git-based project, the navigation pane in your workspace is different than if you were using a Domino project.
See link:0d2247[Use Git in your workspace] to learn more about saving and syncing your work using Git in a Git-based project.
====

[[sync-changes-to-files]]
== Sync changes to files

[[tr10]]
When you sync changes to remote Git repositories, Domino fetches the latest content from the remote branch (`git fetch`), then commits local changes on top of the updated branch (`git rebase`), and then pushes the commits to the remote.

. Click *File Changes* in the navigation pane of your workspace.
. Under *Files*, expand *File Changes* to view changes.
. Enter a commit message.
. Click *Sync to Domino*. Changes to files in the `/mnt` directory of your workspace will be synced to the Domino File System (DFS). Changes to files outside of the `/mnt` directory will not be synced.
+
[WARNING]
====
Files with names containing "?" can be saved in a workspace and synced to the DFS, but will not be viewable via the Code page.
====
[[tr6]]
// As a Domino user, I can view the changes synced from a workspace for a DFS project in the projects's Files browser.
. After syncing to Domino is complete, you can view your files in the
*Code*
section of your project.

== Sync all changes
[[tr7]]
// As a Domino user, I can add a commit message and then sync file changes back to a DFS project using the Sync All Changes button in File Changes, then confirm those changes and the associated commit message in the projects's Files browser

. Click *File Changes* in the navigation bar of your workspace.
. Under *All Changes*, enter a commit message.
. Click *Sync All Changes*. Domino will save all your work to the Domino File System (DFS).

.Sync in detached HEAD state
[[tr8]]

If you launch a workspace from a commit ID in a Git-based project and then try to sync or pull changes, you may see an error that it failed to commit and push repos because it's in a detached HEAD state.

In this scenario, your workspace is in a https://git-scm.com/docs/git-checkout#_detached_head[detached HEAD state].
In order to pull or sync, you must https://git-scm.com/docs/git-checkout[checkout a branch].

Different IDEs may provide different ways to checkout a branch. The following instructions will work for Jupyter Notebook.

. Click *New > Terminal*.
. Type `git checkout -b my-new-branch-name` into the terminal. Your new branch starts from the commit ID that you launched with.
. Re-try the sync or pull action.

[WARNING]
====
When saving your work, remember that changes to files outside the `/mnt` directory will not persist if you stop your workspace and resume the workspace at a later time.
====

== Force changes

When files are in conflict, you can choose to either resolve manually or force changes.
*Force my changes* overwrites remote files with changes in your workspace.
This means that the commit history on the remote will match the commit history in your workspace.

----- user_guide/workspaces/use-git-in-your-workspace.txt -----
:page-version: 6.1
:page-title: Use Git in your Workspace
:page-permalink: 0d2247
:page-order: 60

If you want to use Git workflows such as creating branches, committing code, and pushing or pulling changes in a workspace, you must use Git-based projects or imported Git repositories.

To use Git, you can:

== Use the IDE
[[tr1]]
// As a Domino user, I can see the state of the git repo and any file changes in the Domino UI controls, and I can perform git actions using those controls.
If you prefer a user interface, or you do not have much experience using a terminal, you can use your Git service provider's user interface (such as GitHub) with the actions in a Domino workspace's navigation pane. Click on *File Changes* to perform actions such as
*Select and Sync* (if link:6469bf#enableInWorkspaceBranchSelection[configured] in your environment),
*Sync to Domino*, and *Pull*.

== Use the terminal
[[tr2]]
// As a Domino user, I can perform Git actions in a workspace terminal and those changes will be correctly reflected in the Domino UI controls.
Start a terminal in your workspace and use Git just like you would in a local environment.
See the https://www.atlassian.com/git/tutorials/atlassian-git-cheatsheet[Git cheatsheet^] for common workflows.

== Use the plugin
Many IDEs integrate with Git or have plugins or extensions for Git.

Domino's Git-based projects fully support these features.
You can use them to interact with Git in Domino.
See the following resources for information about how to use Git with your IDE:
[[tr3]]
// As a Domino user, I can use Git integrations and/or plugins and/or extensions to perform Git actions in a Jupyter workspace; those changes will be reflected in the Domino UI controls.
** Jupyter – https://towardsdatascience.com/enhancing-data-science-workflows-mastering-version-control-for-jupyter-notebooks-b03c839e25ec/[How to version control Jupyter Notebooks^].

[[tr4]]
// As a Domino user, I can use Git integrations and/or plugins and/or extensions to perform Git actions in an RStudio workspace; those changes will be reflected in the Domino UI controls.
** RStudio – https://rstudio.com/resources/webinars/managing-part-2-github-and-rstudio/[Managing - Part 2 (GitHub and RStudio)^].

[[tr5]]
// As a Domino user, I can use Git integrations and/or plugins and/or extensions to perform Git actions in a VS Code workspace; those changes will be reflected in the Domino UI controls.
** VS Code – https://code.visualstudio.com/docs/editor/versioncontrol[Using version control in VS Code^].

== Commit and push changes to your Git repository

[NOTE]
====
In a Git-based project, the default working directory for your project's code is `/mnt/code`.
====

[[tr6]]
// As a Domino user, I can find the content of my project's base Git repo inside the /mnt/code directory in a workspace.

=== Commit changes to your code and push them to your repository
[[tr7]]
// As a Domino user, I can use the Domino UI controls in a workspace to sync (i.e. commit and push) my changes from the workspace to the project's git repo.

. Go to the workspace where you made changes to a file.
. In the navigation pane, click *File Changes*.
. In the *Code* section, expand *Uncommitted Changes* to view changes.
+
The up (↑) arrow in the *Code* section indicates the number of commits that your branch is ahead of the upstream branch, while the down (↓) arrow indicates the number of commits that your branch is behind the upstream branch.
. Enter a commit message.
. Click *Sync to Git*.
If your administrator link:6469bf#enableInWorkspaceBranchSelection[enabled the option for you to select the changed files to commit], click *Select and Sync*.
. If you clicked *Select and Sync*, select the files to sync.
. Enter a commit message and click *Sync to Git*.
+
Domino syncs and pushes your changes to your Git repository.

[IMPORTANT]
====
If you encounter merge conflicts when you pull or push code to your repository, see link:acd236[Resolve merge conflicts].
====

== Commit all changes to your Git repository

You can use Domino to commit and push all changes to the project's Git repository. This is often referred to as syncing your changes.

[[tr8]]
// As a Domino user, I can use the Domino UI controls in a workspace to sync (i.e. commit and push) all changes to the project's Git repo and associated artifacts using the Sync All Changes button.

. Go to your workspace.
. In the navigation pane, click *File Changes*.
. In *All Changes*, enter a commit message.
. Click *Sync All Changes*.
Domino commits changes to your code and syncs them to your Git repository.
It also commits changes to artifacts and syncs them to the Domino File System (DFS).
// Need an explanation somewhere of how Git-based projects and the DFS are related.

[NOTE]
====
In a Git-based project, the default working directory for your project's code is `/mnt/code`.
====

== Pull the latest changes from your Git repository

When you collaborate with others in Git-based projects, you must pull the latest changes to your code occasionally.

[[tr9]]
// As a Domino user, I can use the Domino UI controls in a workspace to pull any remote Git repo changes into the workspace.

. Go to your workspace.
. In the navigation pane, click *File Changes*.
. In the *Code* section, click *Pull*.
Domino pulls the latest changes into your workspace.

[NOTE]
====
In a Git-based project, the default working directory for your project's code is `/mnt/code`.
====


[IMPORTANT]
====
If you encounter merge conflicts when you pull or push code to your repository, see link:acd236[Resolve merge conflicts].
====

== Resolve merge conflicts

You might encounter merge conflicts when you collaborate with colleagues.
To resolve merge conflicts, you can use the Git service provider or a terminal.

=== Resolve conflicts with the Git service provider
[[tr10]]
// As a Domino user, I can use the Domino UI controls in a workspace to resolve a merge conflict encountered when trying to sync to the project's Git repo.
When merge conflicts arise, Domino creates a new branch with the conflicts in your repository and pushes them to your Git service provider (such as GitHub or Bitbucket).

. Go to your Git service provider.
. Use your Git service provider's application to create a pull request between the newly created branch and your original branch.
. Merge the pull request.
. Return to Domino.
+
NOTE: Do *not* pull any changes.
. Start a workspace terminal.
. Use the following to reset the branch:
+
----
git reset --hard origin/name-of-branch`
----
where `name-of-branch` is the name of your original branch.
. Click *Pull*.
Domino pulls the resolved merge conflicts.
Your branch is now free of merge conflicts.

=== Resolve conflicts with a terminal
[[tr11]]
// As a Domino user, I can address any merge conflicts using Git commands in a terminal in the workspace; that resolution will also be reflected in the content of the Domino UI controls.
If you prefer, you can resolve merge conflicts in a workspace terminal.
For information about the typical Git workflow to resolve merge conflicts, see https://www.atlassian.com/git/tutorials/using-branches/merge-conflicts[Git
merge conflicts^].

----- user_guide/workspaces/view-workspace-details.txt -----
:page-version: 6.1
:page-title: View Workspace details
:page-permalink: 164503
:page-order: 50

== View Workspace logs

Workspace logs contain critical information to help you track Data Source access events, troubleshoot errors, audit workspace activity, and more.


[[tr20]]
// As a Domino user, I can view the logs for a workspace from the card in the workspaces dashboard

. In your project, click *Workspaces*.
. To view logs for a specific Workspace, click *Logs*. 
.. Click *User* to see information about actions that you have taken in the Workspace.
.. Click *Setup* to see information about Domino and Kubernetes.
.. Click *Data Source Access* to see the logs for all the data sources accessed by the Workspace.

== View Workspace usage


[[tr19]]
// As a Domino user, I can view the CPU usage for a workspace from the card in the workspaces dashboard

In your project, go to *Workspaces* > *Usage*.

The CPU and memory usage for your workspace opens.

If the workspace or its attached compute clusters are configured with GPUs, GPU utilization and GPU memory graphs will also be shown.

// What version was the ability to view the last 30 minutes or session lifetime introduced in? We can add: . Click the *Usage* menu to change the view to see the *Last 30 mins* or *Session Lifetime*. Also note that the capitalization on these menu items in the product is inconsistent. Tara

== View Workspace history

You can see the sessions and commits that were made in a workspace.

[[tr21]]
// As a Domino user, I can view the history (sessions, commits) for a workspace from the card in the workspaces dashboard

. In the navigation pane, click *Workspaces*.
. For the workspace whose commits and sessions you want to see, click *History*.
The workspace history shows information such as when the workspace was launched and stopped.
